# M√≥dulo 07 - Deep Learning

> **üéØ Objetivo:** Implementar MLP con backprop + entender fundamentos de CNNs
> **Fase:** 2 - N√∫cleo de ML | **Semanas 17-20**
> **Curso del Pathway:** Introduction to Deep Learning

---

## üß† ¬øPor Qu√© Deep Learning?

```
DEEP LEARNING = Redes Neuronales Multicapa + Arquitecturas Especializadas

Ventajas sobre ML cl√°sico:
‚îú‚îÄ‚îÄ Aprende features autom√°ticamente (no feature engineering manual)
‚îú‚îÄ‚îÄ Puede modelar relaciones NO LINEALES complejas
‚îú‚îÄ‚îÄ Escala con m√°s datos y m√°s compute
‚îî‚îÄ‚îÄ Estado del arte en visi√≥n (CNNs), NLP (Transformers), etc.

Desventajas:
‚îú‚îÄ‚îÄ Requiere m√°s datos
‚îú‚îÄ‚îÄ "Caja negra" - menos interpretable
‚îî‚îÄ‚îÄ Costoso computacionalmente
```

---

## üìö Contenido del M√≥dulo

| Semana | Tema | Entregable |
|--------|------|------------|
| 17 | Perceptr√≥n y MLP | `activations.py` + forward pass |
| 18 | Backpropagation | `backward()` con Chain Rule |
| 19 | **CNNs: Teor√≠a** | Entender convoluci√≥n, pooling, stride |
| 20 | Optimizadores y Entrenamiento | `neural_network.py` completo |

---

## üíª Parte 1: Perceptr√≥n y Activaciones

### 1.1 La Neurona Artificial

```python
import numpy as np

"""
NEURONA ARTIFICIAL (Perceptr√≥n)

Inspiraci√≥n biol√≥gica:
- Recibe se√±ales de entrada (dendrites)
- Procesa y decide si "dispara" (soma)
- Env√≠a se√±al de salida (axon)

Modelo matem√°tico:
    z = Œ£ w·µ¢x·µ¢ + b = w¬∑x + b  (combinaci√≥n lineal)
    a = œÉ(z)                    (activaci√≥n)

Donde:
- x: vector de entradas
- w: vector de pesos (learnable)
- b: bias (learnable)
- œÉ: funci√≥n de activaci√≥n (introduce no-linealidad)
"""

def perceptron(x: np.ndarray, w: np.ndarray, b: float) -> float:
    """
    Un perceptr√≥n simple.

    Args:
        x: entrada (n_features,)
        w: pesos (n_features,)
        b: bias

    Returns:
        salida activada
    """
    z = np.dot(w, x) + b
    return 1 if z > 0 else 0  # Funci√≥n escal√≥n
```

### 1.2 Funciones de Activaci√≥n

```python
import numpy as np

class Activations:
    """Funciones de activaci√≥n y sus derivadas."""

    @staticmethod
    def sigmoid(z: np.ndarray) -> np.ndarray:
        """
        Sigmoid: œÉ(z) = 1 / (1 + e^(-z))

        Rango: (0, 1)
        Uso: Capa de salida para clasificaci√≥n binaria
        Problema: Vanishing gradient para |z| grande
        """
        z = np.clip(z, -500, 500)
        return 1 / (1 + np.exp(-z))

    @staticmethod
    def sigmoid_derivative(a: np.ndarray) -> np.ndarray:
        """œÉ'(z) = œÉ(z) ¬∑ (1 - œÉ(z)) = a ¬∑ (1 - a)"""
        return a * (1 - a)

    @staticmethod
    def relu(z: np.ndarray) -> np.ndarray:
        """
        ReLU: f(z) = max(0, z)

        Rango: [0, ‚àû)
        Uso: Capas ocultas (default moderno)
        Ventaja: No vanishing gradient para z > 0
        Problema: "Dying ReLU" si z < 0 siempre
        """
        return np.maximum(0, z)

    @staticmethod
    def relu_derivative(z: np.ndarray) -> np.ndarray:
        """ReLU'(z) = 1 si z > 0, 0 si z ‚â§ 0"""
        return (z > 0).astype(float)

    @staticmethod
    def tanh(z: np.ndarray) -> np.ndarray:
        """
        Tanh: f(z) = (e^z - e^(-z)) / (e^z + e^(-z))

        Rango: (-1, 1)
        Uso: Alternativa a sigmoid (centrado en 0)
        """
        return np.tanh(z)

    @staticmethod
    def tanh_derivative(a: np.ndarray) -> np.ndarray:
        """tanh'(z) = 1 - tanh¬≤(z) = 1 - a¬≤"""
        return 1 - a ** 2

    @staticmethod
    def softmax(z: np.ndarray) -> np.ndarray:
        """
        Softmax: softmax(z)·µ¢ = e^(z·µ¢) / Œ£‚±º e^(z‚±º)

        Rango: (0, 1), suma = 1
        Uso: Capa de salida para clasificaci√≥n multiclase
        Output: probabilidades de cada clase
        """
        # Restar m√°ximo para estabilidad num√©rica
        z_shifted = z - np.max(z, axis=-1, keepdims=True)
        exp_z = np.exp(z_shifted)
        return exp_z / np.sum(exp_z, axis=-1, keepdims=True)


# Demo
z = np.array([-2, -1, 0, 1, 2])
act = Activations()

print("z:", z)
print("sigmoid:", act.sigmoid(z))
print("relu:", act.relu(z))
print("tanh:", act.tanh(z))
print("softmax:", act.softmax(z))
```

### 1.3 El Problema XOR

```python
"""
XOR: La limitaci√≥n del Perceptr√≥n Simple

XOR truth table:
    x1  x2  |  y
    0   0   |  0
    0   1   |  1
    1   0   |  1
    1   1   |  0

Un perceptr√≥n simple NO puede resolver XOR porque:
- XOR no es linealmente separable
- No existe una l√≠nea que separe las clases

Soluci√≥n: Red multicapa (MLP)
- Una capa oculta puede aprender features intermedias
- Combinaci√≥n de features no lineales resuelve XOR
"""

# Datos XOR
X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_xor = np.array([0, 1, 1, 0])

# Un perceptr√≥n simple no puede aprender esto
# Necesitamos una red con al menos una capa oculta
```

---

## üíª Parte 2: Forward Propagation

### 2.1 Arquitectura MLP

```python
"""
MLP - Multilayer Perceptron

Arquitectura t√≠pica:
    Input Layer ‚Üí Hidden Layer(s) ‚Üí Output Layer

Ejemplo para clasificaci√≥n binaria:
    x (n_features) ‚Üí h (n_hidden) ‚Üí y (1)

Forward Pass:
    z‚ÇÅ = W‚ÇÅx + b‚ÇÅ        (capa 1: lineal)
    a‚ÇÅ = œÉ(z‚ÇÅ)           (capa 1: activaci√≥n)
    z‚ÇÇ = W‚ÇÇa‚ÇÅ + b‚ÇÇ       (capa 2: lineal)
    a‚ÇÇ = œÉ(z‚ÇÇ)           (capa 2: activaci√≥n = output)

Dimensiones:
    x: (n_features,)
    W‚ÇÅ: (n_hidden, n_features)
    b‚ÇÅ: (n_hidden,)
    z‚ÇÅ, a‚ÇÅ: (n_hidden,)
    W‚ÇÇ: (n_output, n_hidden)
    b‚ÇÇ: (n_output,)
    z‚ÇÇ, a‚ÇÇ: (n_output,)
"""
```

### 2.2 Implementaci√≥n Forward Pass

```python
import numpy as np
from typing import List, Dict

class Layer:
    """Una capa de la red neuronal."""

    def __init__(self, input_size: int, output_size: int, activation: str = 'relu'):
        """
        Args:
            input_size: n√∫mero de entradas
            output_size: n√∫mero de neuronas
            activation: 'relu', 'sigmoid', 'tanh', 'softmax', 'linear'
        """
        self.input_size = input_size
        self.output_size = output_size
        self.activation = activation

        # Inicializaci√≥n Xavier/He
        if activation == 'relu':
            # He initialization para ReLU
            std = np.sqrt(2.0 / input_size)
        else:
            # Xavier initialization
            std = np.sqrt(1.0 / input_size)

        self.W = np.random.randn(output_size, input_size) * std
        self.b = np.zeros(output_size)

        # Cache para backprop
        self.cache = {}

    def forward(self, x: np.ndarray) -> np.ndarray:
        """
        Forward pass de una capa.

        z = Wx + b
        a = activation(z)
        """
        self.cache['x'] = x

        # Transformaci√≥n lineal
        z = self.W @ x + self.b
        self.cache['z'] = z

        # Activaci√≥n
        if self.activation == 'relu':
            a = np.maximum(0, z)
        elif self.activation == 'sigmoid':
            a = 1 / (1 + np.exp(-np.clip(z, -500, 500)))
        elif self.activation == 'tanh':
            a = np.tanh(z)
        elif self.activation == 'softmax':
            z_shifted = z - np.max(z)
            exp_z = np.exp(z_shifted)
            a = exp_z / np.sum(exp_z)
        else:  # linear
            a = z

        self.cache['a'] = a
        return a


class NeuralNetwork:
    """Red Neuronal Multicapa."""

    def __init__(self, layer_sizes: List[int], activations: List[str]):
        """
        Args:
            layer_sizes: [input_size, hidden1, hidden2, ..., output_size]
            activations: ['relu', 'relu', ..., 'sigmoid'] para cada capa
        """
        assert len(activations) == len(layer_sizes) - 1

        self.layers = []
        for i in range(len(layer_sizes) - 1):
            layer = Layer(layer_sizes[i], layer_sizes[i+1], activations[i])
            self.layers.append(layer)

    def forward(self, x: np.ndarray) -> np.ndarray:
        """Forward pass a trav√©s de todas las capas."""
        a = x
        for layer in self.layers:
            a = layer.forward(a)
        return a

    def predict(self, X: np.ndarray) -> np.ndarray:
        """Predicci√≥n para m√∫ltiples muestras."""
        predictions = []
        for x in X:
            output = self.forward(x)
            if len(output) == 1:
                predictions.append(1 if output[0] > 0.5 else 0)
            else:
                predictions.append(np.argmax(output))
        return np.array(predictions)


# Demo
net = NeuralNetwork(
    layer_sizes=[2, 4, 1],  # 2 inputs ‚Üí 4 hidden ‚Üí 1 output
    activations=['relu', 'sigmoid']
)

# Forward pass
x = np.array([0.5, 0.3])
output = net.forward(x)
print(f"Input: {x}")
print(f"Output: {output}")
```

---

## üíª Parte 3: Backpropagation

### 3.1 Funciones de P√©rdida

```python
import numpy as np

def binary_cross_entropy(y_true: float, y_pred: float, eps: float = 1e-15) -> float:
    """
    Binary Cross-Entropy Loss.

    L = -[y¬∑log(≈∑) + (1-y)¬∑log(1-≈∑)]

    Args:
        y_true: etiqueta real (0 o 1)
        y_pred: predicci√≥n (probabilidad)
    """
    y_pred = np.clip(y_pred, eps, 1 - eps)
    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

def bce_derivative(y_true: float, y_pred: float, eps: float = 1e-15) -> float:
    """
    Derivada de BCE respecto a y_pred.

    ‚àÇL/‚àÇ≈∑ = -y/≈∑ + (1-y)/(1-≈∑)
    """
    y_pred = np.clip(y_pred, eps, 1 - eps)
    return -y_true / y_pred + (1 - y_true) / (1 - y_pred)

def categorical_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-15) -> float:
    """
    Categorical Cross-Entropy para multiclase.

    L = -Œ£·µ¢ y·µ¢¬∑log(≈∑·µ¢)

    Args:
        y_true: one-hot encoded (k,)
        y_pred: probabilidades softmax (k,)
    """
    y_pred = np.clip(y_pred, eps, 1 - eps)
    return -np.sum(y_true * np.log(y_pred))

def mse_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Mean Squared Error."""
    return np.mean((y_true - y_pred) ** 2)
```

### 3.2 Backpropagation: La Chain Rule en Acci√≥n

```python
"""
BACKPROPAGATION

Objetivo: Calcular ‚àÇL/‚àÇW y ‚àÇL/‚àÇb para cada capa.

Usando Chain Rule:
    ‚àÇL/‚àÇW‚ÇÇ = ‚àÇL/‚àÇa‚ÇÇ ¬∑ ‚àÇa‚ÇÇ/‚àÇz‚ÇÇ ¬∑ ‚àÇz‚ÇÇ/‚àÇW‚ÇÇ
    ‚àÇL/‚àÇW‚ÇÅ = ‚àÇL/‚àÇa‚ÇÇ ¬∑ ‚àÇa‚ÇÇ/‚àÇz‚ÇÇ ¬∑ ‚àÇz‚ÇÇ/‚àÇa‚ÇÅ ¬∑ ‚àÇa‚ÇÅ/‚àÇz‚ÇÅ ¬∑ ‚àÇz‚ÇÅ/‚àÇW‚ÇÅ

Patr√≥n:
1. Calcular ‚àÇL/‚àÇa_output (derivada de la loss)
2. Para cada capa, de atr√°s hacia adelante:
   a. Œ¥ = ‚àÇL/‚àÇz = ‚àÇL/‚àÇa ¬∑ ‚àÇa/‚àÇz (error de la capa)
   b. ‚àÇL/‚àÇW = Œ¥ ¬∑ x^T
   c. ‚àÇL/‚àÇb = Œ¥
   d. Propagar: ‚àÇL/‚àÇa_prev = W^T ¬∑ Œ¥
"""

def backward_layer(layer, dL_da: np.ndarray) -> tuple:
    """
    Backward pass de una capa.

    Args:
        layer: capa con cache del forward pass
        dL_da: gradiente de la loss respecto a la activaci√≥n

    Returns:
        dL_dx: gradiente respecto a la entrada
        dL_dW: gradiente respecto a los pesos
        dL_db: gradiente respecto al bias
    """
    z = layer.cache['z']
    x = layer.cache['x']
    a = layer.cache['a']

    # Derivada de la activaci√≥n: ‚àÇa/‚àÇz
    if layer.activation == 'sigmoid':
        da_dz = a * (1 - a)
    elif layer.activation == 'relu':
        da_dz = (z > 0).astype(float)
    elif layer.activation == 'tanh':
        da_dz = 1 - a ** 2
    elif layer.activation == 'softmax':
        # Para softmax + cross-entropy, usamos el gradiente simplificado
        da_dz = np.ones_like(z)  # se maneja especialmente
    else:  # linear
        da_dz = np.ones_like(z)

    # Œ¥ = ‚àÇL/‚àÇz = ‚àÇL/‚àÇa ¬∑ ‚àÇa/‚àÇz
    delta = dL_da * da_dz

    # Gradientes
    dL_dW = np.outer(delta, x)
    dL_db = delta
    dL_dx = layer.W.T @ delta

    return dL_dx, dL_dW, dL_db
```

### 3.3 Red Neuronal Completa con Backprop

```python
import numpy as np
from typing import List, Tuple

class NeuralNetworkFull:
    """Red Neuronal con Backpropagation completo."""

    def __init__(self, layer_sizes: List[int], activations: List[str]):
        self.layers = []
        for i in range(len(layer_sizes) - 1):
            layer = Layer(layer_sizes[i], layer_sizes[i+1], activations[i])
            self.layers.append(layer)

        self.loss_history = []

    def forward(self, x: np.ndarray) -> np.ndarray:
        a = x
        for layer in self.layers:
            a = layer.forward(a)
        return a

    def backward(self, y_true: np.ndarray) -> List[Tuple[np.ndarray, np.ndarray]]:
        """
        Backward pass: calcula gradientes para todas las capas.

        Returns:
            Lista de (dW, db) para cada capa
        """
        gradients = []

        # Obtener predicci√≥n (√∫ltima activaci√≥n)
        y_pred = self.layers[-1].cache['a']

        # Gradiente inicial: ‚àÇL/‚àÇa_output
        # Para sigmoid + BCE: simplificado a (y_pred - y_true)
        # Para softmax + CCE: tambi√©n (y_pred - y_true)
        if self.layers[-1].activation in ['sigmoid', 'softmax']:
            dL_da = y_pred - y_true
        else:
            # MSE: 2(y_pred - y_true)
            dL_da = 2 * (y_pred - y_true)

        # Propagar hacia atr√°s
        for layer in reversed(self.layers):
            dL_dx, dL_dW, dL_db = backward_layer(layer, dL_da)
            gradients.insert(0, (dL_dW, dL_db))
            dL_da = dL_dx

        return gradients

    def update_weights(self, gradients: List[Tuple], learning_rate: float):
        """Actualiza pesos usando gradient descent."""
        for layer, (dW, db) in zip(self.layers, gradients):
            layer.W -= learning_rate * dW
            layer.b -= learning_rate * db

    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        epochs: int = 1000,
        learning_rate: float = 0.1,
        verbose: bool = True
    ):
        """Entrena la red."""
        for epoch in range(epochs):
            total_loss = 0

            for xi, yi in zip(X, y):
                # Forward
                output = self.forward(xi)

                # Loss
                if isinstance(yi, (int, float)):
                    yi_arr = np.array([yi])
                else:
                    yi_arr = yi
                loss = binary_cross_entropy(yi_arr[0], output[0])
                total_loss += loss

                # Backward
                gradients = self.backward(yi_arr)

                # Update
                self.update_weights(gradients, learning_rate)

            avg_loss = total_loss / len(X)
            self.loss_history.append(avg_loss)

            if verbose and epoch % 100 == 0:
                print(f"Epoch {epoch}: Loss = {avg_loss:.4f}")

    def predict(self, X: np.ndarray) -> np.ndarray:
        predictions = []
        for x in X:
            output = self.forward(x)
            predictions.append(1 if output[0] > 0.5 else 0)
        return np.array(predictions)


# Demo: Resolver XOR
print("=== Entrenando para XOR ===")
X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_xor = np.array([0, 1, 1, 0])

net = NeuralNetworkFull(
    layer_sizes=[2, 4, 1],
    activations=['tanh', 'sigmoid']
)

net.fit(X_xor, y_xor, epochs=5000, learning_rate=0.5, verbose=True)

print("\n=== Predicciones XOR ===")
for x, y in zip(X_xor, y_xor):
    pred = net.forward(x)[0]
    print(f"{x} -> {pred:.4f} (target: {y})")
```

---

## üíª Parte 4: Optimizadores

### 4.1 SGD (Stochastic Gradient Descent)

```python
class SGD:
    """Vanilla Stochastic Gradient Descent."""

    def __init__(self, learning_rate: float = 0.01):
        self.lr = learning_rate

    def update(self, layer, dW: np.ndarray, db: np.ndarray):
        layer.W -= self.lr * dW
        layer.b -= self.lr * db
```

### 4.2 SGD con Momentum

```python
class SGDMomentum:
    """
    SGD con Momentum.

    v_t = Œ≤¬∑v_{t-1} + (1-Œ≤)¬∑‚àáL
    Œ∏ = Œ∏ - lr¬∑v_t

    Momentum ayuda a:
    - Acelerar convergencia
    - Escapar de m√≠nimos locales
    - Reducir oscilaciones
    """

    def __init__(self, learning_rate: float = 0.01, momentum: float = 0.9):
        self.lr = learning_rate
        self.momentum = momentum
        self.velocities = {}

    def update(self, layer, dW: np.ndarray, db: np.ndarray, layer_id: int):
        if layer_id not in self.velocities:
            self.velocities[layer_id] = {
                'W': np.zeros_like(dW),
                'b': np.zeros_like(db)
            }

        v = self.velocities[layer_id]

        # Actualizar velocidad
        v['W'] = self.momentum * v['W'] + (1 - self.momentum) * dW
        v['b'] = self.momentum * v['b'] + (1 - self.momentum) * db

        # Actualizar par√°metros
        layer.W -= self.lr * v['W']
        layer.b -= self.lr * v['b']
```

### 4.3 Adam Optimizer

```python
class Adam:
    """
    Adam: Adaptive Moment Estimation.

    Combina:
    - Momentum (primer momento)
    - RMSprop (segundo momento)

    m_t = Œ≤‚ÇÅ¬∑m_{t-1} + (1-Œ≤‚ÇÅ)¬∑g_t       (momentum)
    v_t = Œ≤‚ÇÇ¬∑v_{t-1} + (1-Œ≤‚ÇÇ)¬∑g_t¬≤      (velocidad adaptativa)
    mÃÇ_t = m_t / (1 - Œ≤‚ÇÅ^t)              (correcci√≥n de bias)
    vÃÇ_t = v_t / (1 - Œ≤‚ÇÇ^t)
    Œ∏ = Œ∏ - lr ¬∑ mÃÇ_t / (‚àövÃÇ_t + Œµ)
    """

    def __init__(
        self,
        learning_rate: float = 0.001,
        beta1: float = 0.9,
        beta2: float = 0.999,
        epsilon: float = 1e-8
    ):
        self.lr = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = {}
        self.v = {}
        self.t = 0

    def update(self, layer, dW: np.ndarray, db: np.ndarray, layer_id: int):
        if layer_id not in self.m:
            self.m[layer_id] = {'W': np.zeros_like(dW), 'b': np.zeros_like(db)}
            self.v[layer_id] = {'W': np.zeros_like(dW), 'b': np.zeros_like(db)}

        self.t += 1
        m, v = self.m[layer_id], self.v[layer_id]

        # Actualizar momentos
        m['W'] = self.beta1 * m['W'] + (1 - self.beta1) * dW
        m['b'] = self.beta1 * m['b'] + (1 - self.beta1) * db
        v['W'] = self.beta2 * v['W'] + (1 - self.beta2) * dW**2
        v['b'] = self.beta2 * v['b'] + (1 - self.beta2) * db**2

        # Correcci√≥n de bias
        m_hat_W = m['W'] / (1 - self.beta1**self.t)
        m_hat_b = m['b'] / (1 - self.beta1**self.t)
        v_hat_W = v['W'] / (1 - self.beta2**self.t)
        v_hat_b = v['b'] / (1 - self.beta2**self.t)

        # Actualizar par√°metros
        layer.W -= self.lr * m_hat_W / (np.sqrt(v_hat_W) + self.epsilon)
        layer.b -= self.lr * m_hat_b / (np.sqrt(v_hat_b) + self.epsilon)
```

---

## üì¶ Entregable del M√≥dulo

### `neural_network.py`

```python
"""
Neural Network Module

Implementaci√≥n desde cero de:
- MLP (Multilayer Perceptron)
- Backpropagation
- Optimizadores (SGD, Momentum, Adam)
- Funciones de activaci√≥n

Autor: [Tu nombre]
M√≥dulo: 06 - Deep Learning
"""

import numpy as np
from typing import List, Tuple, Optional


# ============================================================
# ACTIVACIONES
# ============================================================

def sigmoid(z):
    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

def sigmoid_deriv(a):
    return a * (1 - a)

def relu(z):
    return np.maximum(0, z)

def relu_deriv(z):
    return (z > 0).astype(float)

def tanh_deriv(a):
    return 1 - a**2

def softmax(z):
    exp_z = np.exp(z - np.max(z))
    return exp_z / np.sum(exp_z)


# ============================================================
# CAPA
# ============================================================

class Layer:
    def __init__(self, input_size: int, output_size: int, activation: str = 'relu'):
        self.activation = activation
        scale = np.sqrt(2.0 / input_size) if activation == 'relu' else np.sqrt(1.0 / input_size)
        self.W = np.random.randn(output_size, input_size) * scale
        self.b = np.zeros(output_size)
        self.cache = {}

    def forward(self, x: np.ndarray) -> np.ndarray:
        self.cache['x'] = x
        z = self.W @ x + self.b
        self.cache['z'] = z

        if self.activation == 'relu':
            a = relu(z)
        elif self.activation == 'sigmoid':
            a = sigmoid(z)
        elif self.activation == 'tanh':
            a = np.tanh(z)
        elif self.activation == 'softmax':
            a = softmax(z)
        else:
            a = z

        self.cache['a'] = a
        return a

    def backward(self, dL_da: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        z, x, a = self.cache['z'], self.cache['x'], self.cache['a']

        if self.activation == 'sigmoid':
            da_dz = sigmoid_deriv(a)
        elif self.activation == 'relu':
            da_dz = relu_deriv(z)
        elif self.activation == 'tanh':
            da_dz = tanh_deriv(a)
        else:
            da_dz = np.ones_like(z)

        delta = dL_da * da_dz
        dL_dW = np.outer(delta, x)
        dL_db = delta
        dL_dx = self.W.T @ delta

        return dL_dx, dL_dW, dL_db


# ============================================================
# OPTIMIZADORES
# ============================================================

class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr

    def step(self, layers, gradients):
        for layer, (dW, db) in zip(layers, gradients):
            layer.W -= self.lr * dW
            layer.b -= self.lr * db


class Adam:
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        self.lr, self.beta1, self.beta2, self.eps = lr, beta1, beta2, eps
        self.m, self.v, self.t = {}, {}, 0

    def step(self, layers, gradients):
        self.t += 1
        for i, (layer, (dW, db)) in enumerate(zip(layers, gradients)):
            if i not in self.m:
                self.m[i] = {'W': np.zeros_like(dW), 'b': np.zeros_like(db)}
                self.v[i] = {'W': np.zeros_like(dW), 'b': np.zeros_like(db)}

            self.m[i]['W'] = self.beta1 * self.m[i]['W'] + (1 - self.beta1) * dW
            self.m[i]['b'] = self.beta1 * self.m[i]['b'] + (1 - self.beta1) * db
            self.v[i]['W'] = self.beta2 * self.v[i]['W'] + (1 - self.beta2) * dW**2
            self.v[i]['b'] = self.beta2 * self.v[i]['b'] + (1 - self.beta2) * db**2

            m_hat_W = self.m[i]['W'] / (1 - self.beta1**self.t)
            m_hat_b = self.m[i]['b'] / (1 - self.beta1**self.t)
            v_hat_W = self.v[i]['W'] / (1 - self.beta2**self.t)
            v_hat_b = self.v[i]['b'] / (1 - self.beta2**self.t)

            layer.W -= self.lr * m_hat_W / (np.sqrt(v_hat_W) + self.eps)
            layer.b -= self.lr * m_hat_b / (np.sqrt(v_hat_b) + self.eps)


# ============================================================
# RED NEURONAL
# ============================================================

class NeuralNetwork:
    def __init__(self, layer_sizes: List[int], activations: List[str]):
        self.layers = [Layer(layer_sizes[i], layer_sizes[i+1], activations[i])
                       for i in range(len(layer_sizes)-1)]
        self.loss_history = []

    def forward(self, x: np.ndarray) -> np.ndarray:
        for layer in self.layers:
            x = layer.forward(x)
        return x

    def backward(self, y_true: np.ndarray) -> List[Tuple]:
        y_pred = self.layers[-1].cache['a']
        dL_da = y_pred - y_true

        gradients = []
        for layer in reversed(self.layers):
            dL_da, dW, db = layer.backward(dL_da)
            gradients.insert(0, (dW, db))
        return gradients

    def fit(self, X, y, epochs=1000, lr=0.1, optimizer='sgd', verbose=True):
        opt = Adam(lr) if optimizer == 'adam' else SGD(lr)

        for epoch in range(epochs):
            total_loss = 0
            for xi, yi in zip(X, y):
                yi_arr = np.atleast_1d(yi)
                output = self.forward(xi)

                # BCE loss
                output_clip = np.clip(output, 1e-15, 1-1e-15)
                loss = -np.sum(yi_arr * np.log(output_clip) + (1-yi_arr) * np.log(1-output_clip))
                total_loss += loss

                gradients = self.backward(yi_arr)
                opt.step(self.layers, gradients)

            self.loss_history.append(total_loss / len(X))
            if verbose and epoch % (epochs//10) == 0:
                print(f"Epoch {epoch}: Loss = {self.loss_history[-1]:.4f}")

    def predict(self, X: np.ndarray) -> np.ndarray:
        return np.array([1 if self.forward(x)[0] > 0.5 else 0 for x in X])

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        return np.mean(self.predict(X) == y)


# ============================================================
# TESTS
# ============================================================

if __name__ == "__main__":
    print("=== Test: XOR Problem ===")
    X = np.array([[0,0], [0,1], [1,0], [1,1]])
    y = np.array([0, 1, 1, 0])

    net = NeuralNetwork([2, 4, 1], ['tanh', 'sigmoid'])
    net.fit(X, y, epochs=5000, lr=0.5, verbose=True)

    print("\nPredicciones:")
    for xi, yi in zip(X, y):
        pred = net.forward(xi)[0]
        print(f"{xi} -> {pred:.4f} (target: {yi})")

    print(f"\nAccuracy: {net.score(X, y):.2%}")
    print("\n‚úì Test XOR completado!")
```

---

## üíª Parte 5: CNNs - Redes Convolucionales (Semana 19)

> ‚ö†Ô∏è **Nota:** En este m√≥dulo NO implementamos CNNs desde cero (es complejo). El objetivo es **entender la teor√≠a** para el curso de Deep Learning de CU Boulder.

### 5.1 ¬øPor Qu√© CNNs para Im√°genes?

```
PROBLEMA CON MLP PARA IM√ÅGENES:

Imagen MNIST: 28x28 = 784 p√≠xeles
MLP fully connected a capa de 256 neuronas:
  ‚Üí 784 √ó 256 = 200,704 par√°metros (¬°solo primera capa!)

Imagen HD: 1920x1080x3 = 6,220,800 p√≠xeles
  ‚Üí Imposible conectar todo con todo

SOLUCI√ìN: CONVOLUCI√ìN
- Procesar regiones locales (no toda la imagen)
- Compartir pesos (el mismo filtro en toda la imagen)
- Detectar patrones sin importar su posici√≥n
```

### 5.2 La Operaci√≥n de Convoluci√≥n

```python
import numpy as np

def convolve2d_simple(image: np.ndarray, kernel: np.ndarray) -> np.ndarray:
    """
    Convoluci√≥n 2D simplificada (para entender el concepto).

    La convoluci√≥n desliza un kernel (filtro) sobre la imagen
    y calcula el producto punto en cada posici√≥n.

    Args:
        image: Imagen de entrada (H, W)
        kernel: Filtro (kH, kW), t√≠picamente 3x3 o 5x5

    Returns:
        Feature map (H-kH+1, W-kW+1)
    """
    H, W = image.shape
    kH, kW = kernel.shape

    # Tama√±o del output (sin padding)
    out_H = H - kH + 1
    out_W = W - kW + 1

    output = np.zeros((out_H, out_W))

    for i in range(out_H):
        for j in range(out_W):
            # Extraer regi√≥n de la imagen
            region = image[i:i+kH, j:j+kW]
            # Producto punto con el kernel
            output[i, j] = np.sum(region * kernel)

    return output


# Ejemplo: Detecci√≥n de bordes verticales
image = np.array([
    [0, 0, 0, 1, 1, 1],
    [0, 0, 0, 1, 1, 1],
    [0, 0, 0, 1, 1, 1],
    [0, 0, 0, 1, 1, 1],
])

# Kernel Sobel para bordes verticales
sobel_vertical = np.array([
    [-1, 0, 1],
    [-2, 0, 2],
    [-1, 0, 1]
])

edges = convolve2d_simple(image, sobel_vertical)
print("Feature map (bordes verticales):")
print(edges)
```

### 5.3 Conceptos Clave de CNNs

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  VOCABULARIO CNN                                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  KERNEL (FILTRO)                                                ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Matriz peque√±a (3x3, 5x5) que detecta patrones             ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Los valores del kernel son APRENDIDOS (backprop)           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Diferentes kernels detectan diferentes features            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  STRIDE                                                         ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Cu√°ntos p√≠xeles se mueve el kernel en cada paso            ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ stride=1: mueve 1 p√≠xel (output grande)                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ stride=2: mueve 2 p√≠xeles (output m√°s peque√±o)             ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  PADDING                                                        ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ A√±adir ceros alrededor de la imagen                        ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ 'valid': sin padding (output m√°s peque√±o)                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ 'same': padding para mantener tama√±o                       ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  POOLING                                                        ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Reduce dimensiones (downsampling)                          ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Max Pooling: toma el m√°ximo de cada regi√≥n                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Average Pooling: toma el promedio                          ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  FEATURE MAP                                                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Output de aplicar un filtro (lo que "ve" el filtro)        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 5.4 C√°lculo de Dimensiones (Importante para Ex√°menes)

```python
def output_size(input_size: int, kernel_size: int,
                stride: int = 1, padding: int = 0) -> int:
    """
    F√≥rmula para calcular tama√±o del output de convoluci√≥n.

    output_size = floor((input + 2*padding - kernel) / stride) + 1
    """
    return (input_size + 2 * padding - kernel_size) // stride + 1


# Ejemplos t√≠picos de examen:
print("=== Ejercicios de dimensiones ===")

# Ejemplo 1: MNIST sin padding
# Input: 28x28, Kernel: 5x5, Stride: 1, Padding: 0
out = output_size(28, 5, stride=1, padding=0)
print(f"MNIST 28x28, kernel 5x5, stride 1: output = {out}x{out}")  # 24x24

# Ejemplo 2: Con padding 'same'
# Para mantener tama√±o con kernel 3x3, necesitas padding=1
out = output_size(28, 3, stride=1, padding=1)
print(f"MNIST 28x28, kernel 3x3, padding 1: output = {out}x{out}")  # 28x28

# Ejemplo 3: Max Pooling 2x2 stride 2
out = output_size(24, 2, stride=2, padding=0)
print(f"24x24, pooling 2x2 stride 2: output = {out}x{out}")  # 12x12
```

### 5.5 Arquitectura T√≠pica de CNN

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ARQUITECTURA LENET-5 (Cl√°sica para MNIST)                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  Input: 28x28x1 (imagen grayscale)                              ‚îÇ
‚îÇ         ‚îÇ                                                       ‚îÇ
‚îÇ         ‚ñº                                                       ‚îÇ
‚îÇ  [CONV 5x5, 6 filtros] ‚Üí 24x24x6                                ‚îÇ
‚îÇ         ‚îÇ                                                       ‚îÇ
‚îÇ         ‚ñº                                                       ‚îÇ
‚îÇ  [ReLU]                                                         ‚îÇ
‚îÇ         ‚îÇ                                                       ‚îÇ
‚îÇ         ‚ñº                                                       ‚îÇ
‚îÇ  [MaxPool 2x2] ‚Üí 12x12x6                                        ‚îÇ
‚îÇ         ‚îÇ                                                       ‚îÇ
‚îÇ         ‚ñº                                                       ‚îÇ
‚îÇ  [CONV 5x5, 16 filtros] ‚Üí 8x8x16                                ‚îÇ
‚îÇ         ‚îÇ                                                       ‚îÇ
‚îÇ         ‚ñº                                                       ‚îÇ
‚îÇ  [ReLU]                                                         ‚îÇ
‚îÇ         ‚îÇ                                                       ‚îÇ
‚îÇ         ‚ñº                                                       ‚îÇ
‚îÇ  [MaxPool 2x2] ‚Üí 4x4x16 = 256 neuronas                          ‚îÇ
‚îÇ         ‚îÇ                                                       ‚îÇ
‚îÇ         ‚ñº                                                       ‚îÇ
‚îÇ  [Flatten] ‚Üí 256                                                ‚îÇ
‚îÇ         ‚îÇ                                                       ‚îÇ
‚îÇ         ‚ñº                                                       ‚îÇ
‚îÇ  [FC 120] ‚Üí 120                                                 ‚îÇ
‚îÇ         ‚îÇ                                                       ‚îÇ
‚îÇ         ‚ñº                                                       ‚îÇ
‚îÇ  [FC 84] ‚Üí 84                                                   ‚îÇ
‚îÇ         ‚îÇ                                                       ‚îÇ
‚îÇ         ‚ñº                                                       ‚îÇ
‚îÇ  [FC 10 + Softmax] ‚Üí 10 clases (d√≠gitos 0-9)                    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 5.6 Max Pooling

```python
def max_pool2d(x: np.ndarray, pool_size: int = 2) -> np.ndarray:
    """
    Max Pooling 2D.

    Reduce dimensiones tomando el m√°ximo de cada regi√≥n.
    Hace la red m√°s robusta a peque√±as traslaciones.

    Args:
        x: Feature map (H, W)
        pool_size: Tama√±o de la ventana (t√≠picamente 2)

    Returns:
        Pooled output (H//pool_size, W//pool_size)
    """
    H, W = x.shape
    out_H, out_W = H // pool_size, W // pool_size

    output = np.zeros((out_H, out_W))

    for i in range(out_H):
        for j in range(out_W):
            region = x[i*pool_size:(i+1)*pool_size,
                      j*pool_size:(j+1)*pool_size]
            output[i, j] = np.max(region)

    return output


# Ejemplo
feature_map = np.array([
    [1, 3, 2, 4],
    [5, 6, 1, 2],
    [3, 2, 1, 0],
    [1, 2, 3, 4]
])

pooled = max_pool2d(feature_map, pool_size=2)
print("Original 4x4:")
print(feature_map)
print("\nMax Pooled 2x2:")
print(pooled)  # [[6, 4], [3, 4]]
```

### 5.7 Por Qu√© Funcionan las CNNs

```
INTUICI√ìN:

1. CAPAS INICIALES: Detectan features simples
   - Bordes horizontales, verticales, diagonales
   - Cambios de color, texturas

2. CAPAS MEDIAS: Combinan features simples
   - Esquinas, curvas, patrones

3. CAPAS PROFUNDAS: Features de alto nivel
   - Partes de objetos (ojos, ruedas, letras)

4. CAPAS FINALES: Objetos completos
   - "Esto es un 7", "Esto es un gato"

VENTAJAS CLAVE:
‚îú‚îÄ‚îÄ Parameter sharing: mismo filtro en toda la imagen
‚îú‚îÄ‚îÄ Sparse connectivity: cada output depende de regi√≥n local
‚îú‚îÄ‚îÄ Translation invariance: detecta patrones sin importar posici√≥n
‚îî‚îÄ‚îÄ Hierarchical features: de simple a complejo
```

### 5.8 Recursos para Profundizar en CNNs

| Recurso | Descripci√≥n |
|---------|-------------|
| [3B1B - But what is a convolution?](https://www.youtube.com/watch?v=KuXjwB4LzSA) | Intuici√≥n visual |
| [CS231n Stanford](http://cs231n.stanford.edu/) | Curso completo de CNNs |
| Deep Learning Book, Cap. 9 | Teor√≠a formal |

---

## üìù Derivaci√≥n Anal√≠tica: Backpropagation a Mano (v3.2)

> üéì **Simulaci√≥n de Examen:** *"Derive las ecuaciones de backpropagation para una red de 2 capas"*. Este es un cl√°sico de ex√°menes de posgrado.

### Red de 2 Capas: Derivaci√≥n Completa

**Arquitectura:**
- Input: $x$ (vector de features)
- Capa 1: $z_1 = W_1 x + b_1$, $a_1 = \sigma(z_1)$
- Capa 2: $z_2 = W_2 a_1 + b_2$, $\hat{y} = \sigma(z_2)$
- Loss: $L = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$

#### Paso 1: Gradiente de la Capa de Salida

$$\frac{\partial L}{\partial z_2} = \hat{y} - y = \delta_2$$

(Resultado elegante gracias a la combinaci√≥n sigmoid + cross-entropy)

$$\frac{\partial L}{\partial W_2} = \delta_2 \cdot a_1^T$$

$$\frac{\partial L}{\partial b_2} = \delta_2$$

#### Paso 2: Propagar el Error Hacia Atr√°s (Capa Oculta)

$$\frac{\partial L}{\partial a_1} = W_2^T \delta_2$$

$$\frac{\partial L}{\partial z_1} = \frac{\partial L}{\partial a_1} \odot \sigma'(z_1) = W_2^T \delta_2 \odot a_1 \odot (1 - a_1) = \delta_1$$

$$\frac{\partial L}{\partial W_1} = \delta_1 \cdot x^T$$

$$\frac{\partial L}{\partial b_1} = \delta_1$$

#### Resumen: Las 4 Ecuaciones de Backprop

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ECUACIONES DE BACKPROPAGATION                               ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 1. Œ¥_L = ‚àá_a L ‚äô œÉ'(z_L)     Error en capa final           ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 2. Œ¥_l = (W_{l+1}^T Œ¥_{l+1}) ‚äô œÉ'(z_l)   Propagar atr√°s    ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 3. ‚àÇL/‚àÇW_l = Œ¥_l ¬∑ a_{l-1}^T   Gradiente de pesos           ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 4. ‚àÇL/‚àÇb_l = Œ¥_l              Gradiente de bias             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Tu Entregable

Escribe en un documento (Markdown o LaTeX):
1. Derivaci√≥n completa de backprop para red de 2 capas
2. Por qu√© $\delta_L = \hat{y} - y$ cuando usamos sigmoid + cross-entropy
3. Diagrama de grafo computacional mostrando el flujo de gradientes

---

## üß™ Overfit on Small Batch: Debugging de Redes Neuronales (v3.3)

> ‚ö†Ô∏è **CR√çTICO:** Esta es la t√©cnica #1 de debugging en Deep Learning. Si tu red no puede hacer overfitting en 10 ejemplos, tiene un bug.

### El Principio

```
REGLA DE ORO DEL DEBUGGING EN DL:

Una red neuronal DEBE poder memorizar un dataset peque√±o.

Si entrenas con:
- 10 ejemplos
- Muchas √©pocas (1000+)
- Sin regularizaci√≥n

El loss DEBE llegar a ~0.00 (o muy cercano).

Si NO llega a 0 ‚Üí TU IMPLEMENTACI√ìN TIENE UN BUG
```

### Por Qu√© Funciona

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ OVERFIT TEST                                                ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ Dataset peque√±o (10 ejemplos):                              ‚îÇ
‚îÇ - Capacidad de la red >> complejidad del dataset            ‚îÇ
‚îÇ - La red puede "memorizar" cada ejemplo perfectamente       ‚îÇ
‚îÇ - Loss debe ‚Üí 0 si backprop funciona                        ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ Si loss NO baja:                                            ‚îÇ
‚îÇ - Gradiente mal calculado                                   ‚îÇ
‚îÇ - Learning rate incorrecto                                  ‚îÇ
‚îÇ - Arquitectura rota (dimensiones)                           ‚îÇ
‚îÇ - Bug en forward o backward pass                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Script: `overfit_test.py` (Entregable Obligatorio v3.3)

```python
"""
Overfit Test - Validaci√≥n de Redes Neuronales
Si tu red no puede hacer overfit en 10 ejemplos, est√° rota.

Autor: [Tu nombre]
M√≥dulo: 07 - Deep Learning
"""
import numpy as np
from typing import List, Tuple


def overfit_test(
    model,
    X_small: np.ndarray,
    y_small: np.ndarray,
    epochs: int = 2000,
    target_loss: float = 0.01,
    verbose: bool = True
) -> Tuple[bool, List[float]]:
    """
    Test de overfitting: la red debe memorizar un dataset peque√±o.

    Args:
        model: Tu red neuronal (debe tener .fit() y .forward())
        X_small: Dataset peque√±o (10-20 ejemplos)
        y_small: Labels del dataset
        epochs: √âpocas de entrenamiento
        target_loss: Loss objetivo (default: 0.01)
        verbose: Mostrar progreso

    Returns:
        (passed, loss_history)
    """
    if verbose:
        print("=" * 60)
        print("OVERFIT TEST: ¬øPuede tu red memorizar 10 ejemplos?")
        print("=" * 60)
        print(f"Dataset size: {len(y_small)}")
        print(f"Epochs: {epochs}")
        print(f"Target loss: {target_loss}")
        print("-" * 60)

    # Entrenar
    loss_history = []
    for epoch in range(epochs):
        # Forward pass para todos los ejemplos
        total_loss = 0.0
        for i in range(len(y_small)):
            output = model.forward(X_small[i])
            loss = np.mean((output - y_small[i]) ** 2)  # MSE
            total_loss += loss

            # Backward y update (asumiendo que model tiene estos m√©todos)
            model.backward(y_small[i])
            model.update(learning_rate=0.1)

        avg_loss = total_loss / len(y_small)
        loss_history.append(avg_loss)

        if verbose and epoch % 500 == 0:
            print(f"Epoch {epoch:4d}: Loss = {avg_loss:.6f}")

    final_loss = loss_history[-1]
    passed = final_loss < target_loss

    if verbose:
        print("-" * 60)
        print(f"Final Loss: {final_loss:.6f}")
        if passed:
            print("‚úì PASSED: Tu red puede hacer overfitting")
            print("  ‚Üí El forward y backward pass funcionan correctamente")
        else:
            print("‚úó FAILED: Tu red NO puede hacer overfitting")
            print("  ‚Üí Revisa tu implementaci√≥n de backprop")
            print("  Posibles causas:")
            print("  - Gradiente mal calculado")
            print("  - Learning rate muy bajo")
            print("  - Bug en forward pass")
            print("  - Dimensiones incorrectas")

    return passed, loss_history


# ============================================================
# EJEMPLO: Test con XOR (debe pasar)
# ============================================================

def test_xor_overfit():
    """Test: Una red peque√±a debe resolver XOR perfectamente."""
    print("\n" + "=" * 60)
    print("TEST: Overfit on XOR Problem")
    print("=" * 60)

    # XOR dataset (4 ejemplos)
    X = np.array([
        [0, 0],
        [0, 1],
        [1, 0],
        [1, 1]
    ], dtype=np.float64)

    y = np.array([
        [0],
        [1],
        [1],
        [0]
    ], dtype=np.float64)

    # Crear red simple (2 -> 8 -> 1)
    # NOTA: Reemplaza esto con tu clase NeuralNetwork
    class SimpleNet:
        def __init__(self):
            np.random.seed(42)
            self.W1 = np.random.randn(8, 2) * 0.5
            self.b1 = np.zeros((8, 1))
            self.W2 = np.random.randn(1, 8) * 0.5
            self.b2 = np.zeros((1, 1))

            # Cache para backprop
            self.cache = {}

        def sigmoid(self, z):
            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

        def forward(self, x):
            x = x.reshape(-1, 1)
            z1 = self.W1 @ x + self.b1
            a1 = self.sigmoid(z1)
            z2 = self.W2 @ a1 + self.b2
            a2 = self.sigmoid(z2)

            self.cache = {'x': x, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2}
            return a2.flatten()

        def backward(self, y_true):
            y_true = np.array(y_true).reshape(-1, 1)
            a2 = self.cache['a2']
            a1 = self.cache['a1']
            x = self.cache['x']

            # Gradientes
            dz2 = a2 - y_true
            self.dW2 = dz2 @ a1.T
            self.db2 = dz2

            da1 = self.W2.T @ dz2
            dz1 = da1 * a1 * (1 - a1)
            self.dW1 = dz1 @ x.T
            self.db1 = dz1

        def update(self, learning_rate):
            self.W1 -= learning_rate * self.dW1
            self.b1 -= learning_rate * self.db1
            self.W2 -= learning_rate * self.dW2
            self.b2 -= learning_rate * self.db2

    # Ejecutar test
    model = SimpleNet()
    passed, history = overfit_test(model, X, y, epochs=2000, target_loss=0.01)

    # Verificar predicciones finales
    print("\nPredicciones finales:")
    for i in range(len(X)):
        pred = model.forward(X[i])
        print(f"  Input: {X[i]} ‚Üí Pred: {pred[0]:.3f} (Target: {y[i][0]})")

    return passed


if __name__ == "__main__":
    test_xor_overfit()
```

### Checklist de Debugging con Overfit Test

| S√≠ntoma | Diagn√≥stico | Soluci√≥n |
|---------|-------------|----------|
| Loss no baja | Gradiente = 0 o NaN | Verificar derivadas con grad_check |
| Loss baja muy lento | Learning rate muy bajo | Aumentar LR (probar 0.1, 0.5, 1.0) |
| Loss oscila mucho | Learning rate muy alto | Reducir LR |
| Loss sube | Signos invertidos en gradiente | Revisar forward/backward |
| Loss = NaN | Overflow en exp/softmax | Usar versiones num√©ricamente estables |

---

## üéØ El Reto del Tablero Blanco (Metodolog√≠a Feynman)

Explica en **m√°ximo 5 l√≠neas** sin jerga t√©cnica:

1. **¬øQu√© es backpropagation?**
   > Pista: Piensa en "culpar" a cada peso por el error.

2. **¬øPor qu√© ReLU es mejor que sigmoid en capas ocultas?**
   > Pista: Piensa en qu√© pasa con el gradiente de sigmoid cuando z es muy grande o muy peque√±o.

3. **¬øQu√© hace una convoluci√≥n en una imagen?**
   > Pista: Piensa en "deslizar una lupa" buscando un patr√≥n espec√≠fico.

4. **¬øPor qu√© usamos pooling?**
   > Pista: Piensa en "resumir" una regi√≥n y hacerla m√°s peque√±a.

---

## ‚úÖ Checklist de Finalizaci√≥n (v3.3)

### Conocimiento
- [ ] Entiendo la analog√≠a neurona biol√≥gica ‚Üí neurona artificial
- [ ] Implement√© sigmoid, ReLU, tanh, softmax y sus derivadas
- [ ] Entiendo por qu√© XOR no es linealmente separable
- [ ] Implement√© forward pass para MLP
- [ ] Entiendo la Chain Rule aplicada a backpropagation
- [ ] Implement√© backward pass calculando gradientes
- [ ] Implement√© SGD, SGD+Momentum y Adam
- [ ] Mi red resuelve el problema XOR

### CNNs (Teor√≠a)
- [ ] Entiendo qu√© es convoluci√≥n, stride, padding y pooling
- [ ] Puedo calcular dimensiones de output de una CNN
- [ ] Conozco la arquitectura LeNet-5

### Entregables de C√≥digo
- [ ] `neural_network.py` con tests pasando
- [ ] `mypy src/` pasa sin errores
- [ ] `pytest tests/` pasa sin errores

### Overfit Test (v3.3 - Obligatorio)
- [ ] **`overfit_test.py` implementado**
- [ ] **Mi red hace overfit en XOR (loss < 0.01)**
- [ ] Si el test falla, debugge√© con grad_check

### Derivaci√≥n Anal√≠tica (Obligatorio)
- [ ] Deriv√© las ecuaciones de backprop a mano
- [ ] Documento con derivaci√≥n completa (Markdown o LaTeX)
- [ ] Diagrama de grafo computacional

### Metodolog√≠a Feynman
- [ ] Puedo explicar backpropagation en 5 l√≠neas sin jerga
- [ ] Puedo explicar ReLU vs sigmoid en 5 l√≠neas
- [ ] Puedo explicar convoluci√≥n en 5 l√≠neas
- [ ] Puedo explicar pooling en 5 l√≠neas

---

## üîó Navegaci√≥n

| Anterior | √çndice | Siguiente |
|----------|--------|-----------|
| [06_UNSUPERVISED_LEARNING](06_UNSUPERVISED_LEARNING.md) | [00_INDICE](00_INDICE.md) | [08_PROYECTO_MNIST](08_PROYECTO_MNIST.md) |
