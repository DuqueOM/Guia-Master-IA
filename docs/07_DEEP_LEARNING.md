# MÃ³dulo 07 - Deep Learning

> **ðŸŽ¯ Objetivo:** Implementar MLP con backprop + entender fundamentos de CNNs
> **Fase:** 2 - NÃºcleo de ML | **Semanas 17-20**
> **Curso del Pathway:** Introduction to Deep Learning

---

<a id="m07-0"></a>

## ðŸ§­ CÃ³mo usar este mÃ³dulo (modo 0â†’100)

**PropÃ³sito:** que puedas construir y depurar una red neuronal desde cero:

- forward pass
- backpropagation
- optimizaciÃ³n (SGD/Momentum/Adam)
- sanity checks (overfit test)

### Objetivos de aprendizaje (medibles)

Al terminar este mÃ³dulo podrÃ¡s:

- **Implementar** un MLP que resuelva XOR.
- **Explicar** backprop como chain rule aplicada a un grafo computacional.
- **Depurar** entrenamiento con overfit test (si no memoriza, hay bug).
- **Entender** teorÃ­a de CNNs (convoluciÃ³n, stride, padding, pooling).

Enlaces rÃ¡pidos:

- [03_CALCULO_MULTIVARIANTE.md](03_CALCULO_MULTIVARIANTE.md) (Chain Rule)
- [GLOSARIO.md](GLOSARIO.md)
- [RECURSOS.md](RECURSOS.md)
- [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)
- [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md)

### Recursos (cuÃ¡ndo usarlos)

| Prioridad | Recurso | CuÃ¡ndo usarlo en este mÃ³dulo | Para quÃ© |
|----------|---------|------------------------------|----------|
| **Obligatorio** | [03_CALCULO_MULTIVARIANTE.md](03_CALCULO_MULTIVARIANTE.md) | Antes de implementar `backward()` (Semana 18) | Asegurar Chain Rule y gradientes bÃ¡sicos |
| **Obligatorio** | `study_tools/DRYRUN_BACKPROPAGATION.md` | Justo antes de tu primera implementaciÃ³n completa de Backprop | Hacer â€œdry-runâ€ y detectar errores de gradiente antes del cÃ³digo |
| **Obligatorio** | `study_tools/EXAMEN_ADMISION_SIMULADO.md` | DespuÃ©s de que tu MLP resuelva XOR y antes de cerrar el mÃ³dulo | ValidaciÃ³n tipo examen (sin IDE/internet) |
| **Complementario** | [3Blue1Brown: Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) | Semana 17â€“18, cuando necesites intuiciÃ³n de backprop | Visualizar forward/backward y por quÃ© aprende |
| **Complementario** | [TensorFlow Playground](https://playground.tensorflow.org/) | Semana 17â€“18, cuando estudies por quÃ© una capa lineal no resuelve XOR y cÃ³mo las activaciones cambian la geometrÃ­a | Ver en tiempo real cÃ³mo la red â€œdoblaâ€ el espacio para separar clases |
| **Complementario** | [Deep Learning Book](https://www.deeplearningbook.org/) | Semana 19â€“20 (CNNs/entrenamiento), si quieres rigor | Referencia profunda (gratis) |
| **Opcional** | [RECURSOS.md](RECURSOS.md) | Al terminar el mÃ³dulo (para profundizar en DL/CNNs) | Seleccionar refuerzos sin romper el plan |

---

## ðŸ§  Â¿Por QuÃ© Deep Learning?

```
DEEP LEARNING = Redes Neuronales Multicapa + Arquitecturas Especializadas

Ventajas sobre ML clÃ¡sico:
â”œâ”€â”€ Aprende features automÃ¡ticamente (no feature engineering manual)
â”œâ”€â”€ Puede modelar relaciones NO LINEALES complejas
â”œâ”€â”€ Escala con mÃ¡s datos y mÃ¡s compute
â””â”€â”€ Estado del arte en visiÃ³n (CNNs), NLP (Transformers), etc.

Desventajas:
â”œâ”€â”€ Requiere mÃ¡s datos
â”œâ”€â”€ "Caja negra" - menos interpretable
â””â”€â”€ Costoso computacionalmente
```

### IntuiciÃ³n geomÃ©trica: Deep Learning como â€œdoblar el espacioâ€ (origami)

Una capa lineal `z = Wx + b` solo puede **rotar, estirar o inclinar** el espacio: siempre produce una frontera de decisiÃ³n lineal (un hiperplano). Por eso un modelo lineal no puede separar XOR.

La no linealidad (ReLU/sigmoid/tanh) es lo que permite â€œdoblarâ€ el espacio:

- despuÃ©s del primer doblez, puntos que antes estaban mezclados pueden quedar en regiones separables
- con varias capas, encadenas dobleces hasta que en la Ãºltima capa los datos son separables con un hiperplano

VisualizaciÃ³n sugerida:

- dibuja XOR en 2D
- intenta separarlo con una sola lÃ­nea (imposible)
- luego imagina un doblez que junta los puntos de la misma clase

---

## ðŸ“š Contenido del MÃ³dulo

| Semana | Tema | Entregable |
|--------|------|------------|
| 17 | PerceptrÃ³n y MLP | `activations.py` + forward pass |
| 18 | Backpropagation | `backward()` con Chain Rule |
| 19 | **CNNs: TeorÃ­a** | Entender convoluciÃ³n, pooling, stride |
| 20 | Optimizadores y Entrenamiento | `neural_network.py` completo |

---

## ðŸ’» Parte 1: PerceptrÃ³n y Activaciones

### 1.1 La Neurona Artificial

```python
import numpy as np

"""
NEURONA ARTIFICIAL (PerceptrÃ³n)

InspiraciÃ³n biolÃ³gica:
- Recibe seÃ±ales de entrada (dendrites)
- Procesa y decide si "dispara" (soma)
- EnvÃ­a seÃ±al de salida (axon)

Modelo matemÃ¡tico:
    z = Î£ wáµ¢xáµ¢ + b = wÂ·x + b  (combinaciÃ³n lineal)
    a = Ïƒ(z)                    (activaciÃ³n)

Donde:
- x: vector de entradas
- w: vector de pesos (learnable)
- b: bias (learnable)
- Ïƒ: funciÃ³n de activaciÃ³n (introduce no-linealidad)
"""

def perceptron(x: np.ndarray, w: np.ndarray, b: float) -> float:
    """
    Un perceptrÃ³n simple.

    Args:
        x: entrada (n_features,)
        w: pesos (n_features,)
        b: bias

    Returns:
        salida activada
    """
    z = np.dot(w, x) + b
    return 1 if z > 0 else 0  # FunciÃ³n escalÃ³n
```

### 1.2 Funciones de ActivaciÃ³n

```python
import numpy as np

class Activations:
    """Funciones de activaciÃ³n y sus derivadas."""

    @staticmethod
    def sigmoid(z: np.ndarray) -> np.ndarray:
        """
        Sigmoid: Ïƒ(z) = 1 / (1 + e^(-z))

        Rango: (0, 1)
        Uso: Capa de salida para clasificaciÃ³n binaria
        Problema: Vanishing gradient para |z| grande
        """
        z = np.clip(z, -500, 500)
        return 1 / (1 + np.exp(-z))

    @staticmethod
    def sigmoid_derivative(a: np.ndarray) -> np.ndarray:
        """Ïƒ'(z) = Ïƒ(z) Â· (1 - Ïƒ(z)) = a Â· (1 - a)"""
        return a * (1 - a)

    @staticmethod
    def relu(z: np.ndarray) -> np.ndarray:
        """
        ReLU: f(z) = max(0, z)

        Rango: [0, âˆž)
        Uso: Capas ocultas (default moderno)
        Ventaja: No vanishing gradient para z > 0
        Problema: "Dying ReLU" si z < 0 siempre
        """
        return np.maximum(0, z)

    @staticmethod
    def relu_derivative(z: np.ndarray) -> np.ndarray:
        """ReLU'(z) = 1 si z > 0, 0 si z â‰¤ 0"""
        return (z > 0).astype(float)

    @staticmethod
    def tanh(z: np.ndarray) -> np.ndarray:
        """
        Tanh: f(z) = (e^z - e^(-z)) / (e^z + e^(-z))

        Rango: (-1, 1)
        Uso: Alternativa a sigmoid (centrado en 0)
        """
        return np.tanh(z)

    @staticmethod
    def tanh_derivative(a: np.ndarray) -> np.ndarray:
        """tanh'(z) = 1 - tanhÂ²(z) = 1 - aÂ²"""
        return 1 - a ** 2

    @staticmethod
    def softmax(z: np.ndarray) -> np.ndarray:
        """
        Softmax: softmax(z)áµ¢ = e^(záµ¢) / Î£â±¼ e^(zâ±¼)

        Rango: (0, 1), suma = 1
        Uso: Capa de salida para clasificaciÃ³n multiclase
        Output: probabilidades de cada clase
        """
        # Restar mÃ¡ximo para estabilidad numÃ©rica
        z_shifted = z - np.max(z, axis=-1, keepdims=True)
        exp_z = np.exp(z_shifted)
        return exp_z / np.sum(exp_z, axis=-1, keepdims=True)


# Demo
z = np.array([-2, -1, 0, 1, 2])
act = Activations()

print("z:", z)
print("sigmoid:", act.sigmoid(z))
print("relu:", act.relu(z))
print("tanh:", act.tanh(z))
print("softmax:", act.softmax(z))
```

### 1.3 El Problema XOR

```python
"""
XOR: La limitaciÃ³n del PerceptrÃ³n Simple

XOR truth table:
    x1  x2  |  y
    0   0   |  0
    0   1   |  1
    1   0   |  1
    1   1   |  0

Un perceptrÃ³n simple NO puede resolver XOR porque:
- XOR no es linealmente separable
- No existe una lÃ­nea que separe las clases

SoluciÃ³n: Red multicapa (MLP)
- Una capa oculta puede aprender features intermedias
- CombinaciÃ³n de features no lineales resuelve XOR
"""

# Datos XOR
X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_xor = np.array([0, 1, 1, 0])

# Un perceptrÃ³n simple no puede aprender esto
# Necesitamos una red con al menos una capa oculta
```

---

## ðŸ’» Parte 2: Forward Propagation

### 2.1 Arquitectura MLP

```python
"""
MLP - Multilayer Perceptron

Arquitectura tÃ­pica:
    Input Layer â†’ Hidden Layer(s) â†’ Output Layer

Ejemplo para clasificaciÃ³n binaria:
    x (n_features) â†’ h (n_hidden) â†’ y (1)

Forward Pass:
    zâ‚ = Wâ‚x + bâ‚        (capa 1: lineal)
    aâ‚ = Ïƒ(zâ‚)           (capa 1: activaciÃ³n)
    zâ‚‚ = Wâ‚‚aâ‚ + bâ‚‚       (capa 2: lineal)
    aâ‚‚ = Ïƒ(zâ‚‚)           (capa 2: activaciÃ³n = output)

Dimensiones:
    x: (n_features,)
    Wâ‚: (n_hidden, n_features)
    bâ‚: (n_hidden,)
    zâ‚, aâ‚: (n_hidden,)
    Wâ‚‚: (n_output, n_hidden)
    bâ‚‚: (n_output,)
    zâ‚‚, aâ‚‚: (n_output,)
"""
```

### 2.2 ImplementaciÃ³n Forward Pass

```python
import numpy as np
from typing import List, Dict

class Layer:
    """Una capa de la red neuronal."""

    def __init__(self, input_size: int, output_size: int, activation: str = 'relu'):
        """
        Args:
            input_size: nÃºmero de entradas
            output_size: nÃºmero de neuronas
            activation: 'relu', 'sigmoid', 'tanh', 'softmax', 'linear'
        """
        self.input_size = input_size
        self.output_size = output_size
        self.activation = activation

        # InicializaciÃ³n Xavier/He
        if activation == 'relu':
            # He initialization para ReLU
            std = np.sqrt(2.0 / input_size)
        else:
            # Xavier initialization
            std = np.sqrt(1.0 / input_size)

        self.W = np.random.randn(output_size, input_size) * std
        self.b = np.zeros(output_size)

        # Cache para backprop
        self.cache = {}

    def forward(self, x: np.ndarray) -> np.ndarray:
        """
        Forward pass de una capa.

        z = Wx + b
        a = activation(z)
        """
        self.cache['x'] = x

        # TransformaciÃ³n lineal
        z = self.W @ x + self.b
        self.cache['z'] = z

        # ActivaciÃ³n
        if self.activation == 'relu':
            a = np.maximum(0, z)
        elif self.activation == 'sigmoid':
            a = 1 / (1 + np.exp(-np.clip(z, -500, 500)))
        elif self.activation == 'tanh':
            a = np.tanh(z)
        elif self.activation == 'softmax':
            z_shifted = z - np.max(z)
            exp_z = np.exp(z_shifted)
            a = exp_z / np.sum(exp_z)
        else:  # linear
            a = z

        self.cache['a'] = a
        return a


class NeuralNetwork:
    """Red Neuronal Multicapa."""

    def __init__(self, layer_sizes: List[int], activations: List[str]):
        """
        Args:
            layer_sizes: [input_size, hidden1, hidden2, ..., output_size]
            activations: ['relu', 'relu', ..., 'sigmoid'] para cada capa
        """
        assert len(activations) == len(layer_sizes) - 1

        self.layers = []
        for i in range(len(layer_sizes) - 1):
            layer = Layer(layer_sizes[i], layer_sizes[i+1], activations[i])
            self.layers.append(layer)

    def forward(self, x: np.ndarray) -> np.ndarray:
        """Forward pass a travÃ©s de todas las capas."""
        a = x
        for layer in self.layers:
            a = layer.forward(a)
        return a

    def predict(self, X: np.ndarray) -> np.ndarray:
        """PredicciÃ³n para mÃºltiples muestras."""
        predictions = []
        for x in X:
            output = self.forward(x)
            if len(output) == 1:
                predictions.append(1 if output[0] > 0.5 else 0)
            else:
                predictions.append(np.argmax(output))
        return np.array(predictions)


# Demo
net = NeuralNetwork(
    layer_sizes=[2, 4, 1],  # 2 inputs â†’ 4 hidden â†’ 1 output
    activations=['relu', 'sigmoid']
)

# Forward pass
x = np.array([0.5, 0.3])
output = net.forward(x)
print(f"Input: {x}")
print(f"Output: {output}")
```

---

## ðŸ’» Parte 3: Backpropagation

### 3.0 Backpropagation â€” Nivel: intermedio/avanzado

**PropÃ³sito:** este bloque te lleva de â€œsÃ© que backprop existeâ€ a **poder derivarlo, implementarlo y depurarlo** bajo condiciones tipo examen.

#### Objetivos de aprendizaje (medibles)

Al terminar este bloque podrÃ¡s:

- **Recordar** la notaciÃ³n estÃ¡ndar de una capa (`z = Wx + b`, `a = Ï†(z)`) y el rol de cada variable.
- **Explicar** por quÃ© backprop es simplemente *regla de la cadena aplicada a un grafo computacional*.
- **Aplicar** backprop para calcular `âˆ‚L/âˆ‚W` y `âˆ‚L/âˆ‚b` en una red MLP de 2 capas.
- **Analizar** fallas tÃ­picas (signos, shapes, overflow) usando pruebas de sanidad.
- **Evaluar** si tu implementaciÃ³n es correcta con un *overfit test* y (cuando aplique) *gradient checking*.
- **Crear** una implementaciÃ³n mÃ­nima (NumPy) de forward + backward y entrenarla en un toy dataset.

#### MotivaciÃ³n / por quÃ© importa

Backpropagation es el mecanismo que hace posible que redes con millones de parÃ¡metros se ajusten a datos. En prÃ¡ctica:

- **VisiÃ³n (CV):** CNNs y modelos de clasificaciÃ³n/segmentaciÃ³n se entrenan con backprop.
- **NLP:** aunque los Transformers no se implementan aquÃ­, el entrenamiento sigue siendo backprop sobre un grafo computacional.
- **Industria:** cuando un entrenamiento â€œno aprendeâ€, casi siempre el diagnÃ³stico comienza revisando gradientes, estabilidad numÃ©rica y shapes.

#### Prerrequisitos y nivel de entrada

- **CÃ¡lculo:** derivadas, derivadas parciales, regla de la cadena.
- **Ãlgebra lineal:** multiplicaciÃ³n matriz-vector, transpuesta.
- **Probabilidad / pÃ©rdidas:** cross-entropy como pÃ©rdida para clasificaciÃ³n.

Mini-recordatorio (enlaces directos):

- [GLOSARIO: Chain Rule](GLOSARIO.md#chain-rule)
- [GLOSARIO: Gradient](GLOSARIO.md#gradient)
- [GLOSARIO: Backpropagation](GLOSARIO.md#backpropagation)
- [GLOSARIO: Binary Cross-Entropy](GLOSARIO.md#binary-cross-entropy)

#### Resumen ejecutivo (big idea)

Backpropagation calcula gradientes **de manera eficiente** reutilizando resultados intermedios del forward pass. En vez de derivar a mano una expresiÃ³n enorme, modelas el cÃ¡lculo como un **grafo** de operaciones simples (sumas, productos, activaciones). Luego aplicas la regla de la cadena localmente y propagas â€œresponsabilidad del errorâ€ desde la salida hasta los parÃ¡metros.

La idea operacional es:

- Haces un **forward pass** guardando `x`, `z`, `a` de cada capa.
- Calculas la pÃ©rdida `L`.
- Empiezas en la salida con un gradiente inicial y haces un **backward pass** capa por capa:
  - `Î´ = âˆ‚L/âˆ‚z` (el â€œerrorâ€ local)
  - `âˆ‚L/âˆ‚W = Î´ âŠ— x` y `âˆ‚L/âˆ‚b = Î´`
  - propagas hacia atrÃ¡s: `âˆ‚L/âˆ‚x = Wáµ€ Î´`

#### VisualizaciÃ³n crÃ­tica: el grafo computacional de Backprop (hacer clic mental)

Para entender backprop, no mires fÃ³rmulas planas: mira el grafo.

Una neurona simple:

`L(a)  â†  a = Ïƒ(z)  â†  z = wÂ·x + b`

El gradiente fluye rÃ­o arriba (de derecha a izquierda):

1) **Llegada del error:** recibes `âˆ‚L/âˆ‚a`.
2) **Compuerta sigmoide:** multiplicas por la derivada local `Ïƒ'(z)`.
3) **SeÃ±al en z:**

`Î´ = âˆ‚L/âˆ‚z = (âˆ‚L/âˆ‚a) Â· Ïƒ'(z)`

4) **BifurcaciÃ³n lineal (`z = wÂ·x + b`):**

- Hacia `w`: `âˆ‚L/âˆ‚w = Î´ Â· x`
- Hacia `b`: `âˆ‚L/âˆ‚b = Î´`
- Hacia `x`: `âˆ‚L/âˆ‚x = Î´ Â· w`

Regla mnemotÃ©cnica:

- Gradiente del **peso** = **error local (`Î´`) Ã— entrada (`x`)**
- Gradiente hacia atrÃ¡s = **error local (`Î´`) Ã— peso (`w`)**

#### Mapa del contenido y tiempo estimado

- **IntuiciÃ³n + vocabulario:** 20â€“35 min
- **FormalizaciÃ³n (notaciÃ³n + shapes):** 30â€“45 min
- **DerivaciÃ³n guiada (2 capas):** 45â€“75 min
- **Worked example numÃ©rico (paso a paso):** 45â€“60 min
- **ImplementaciÃ³n prÃ¡ctica (NumPy) + pruebas de sanidad:** 2â€“4 h

#### NÃºcleo: explicaciÃ³n progresiva por capas

##### a) IntuiciÃ³n / metÃ¡fora

Piensa en una red como una fÃ¡brica con varias estaciones. La salida estÃ¡ mal (pÃ©rdida alta) y quieres saber **cuÃ¡nto contribuyÃ³ cada perilla** (peso) al error. Backprop es un procedimiento para *repartir la culpa* desde el error final hacia atrÃ¡s, estaciÃ³n por estaciÃ³n.

##### b) Conceptos clave (glosario mÃ­nimo)

- **Forward pass:** computar `z` y `a` desde la entrada hasta la salida.
- **Loss `L`:** nÃºmero que mide â€œquÃ© tan malâ€ predice el modelo.
- **Gradiente:** vector de derivadas que indica cÃ³mo cambia `L` si mueves parÃ¡metros.
- **Delta `Î´`:** gradiente local `âˆ‚L/âˆ‚z` en una capa (la seÃ±al que se propaga hacia atrÃ¡s).

##### c) FormalizaciÃ³n (fÃ³rmulas + shapes)

Para una capa totalmente conectada:

- `z = Wx + b`
- `a = Ï†(z)`

Shapes recomendados (para evitar errores silenciosos):

- `x`: `(n_in,)` o `(n_in, 1)`
- `W`: `(n_out, n_in)`
- `b`: `(n_out,)` o `(n_out, 1)`
- `z, a`: `(n_out,)` o `(n_out, 1)`

##### d) DemostraciÃ³n / derivaciÃ³n (idea central)

En cada capa usas regla de la cadena:

- `âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚W`
- `âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚b`
- `âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚x`

Y como `z = Wx + b`:

- `âˆ‚z/âˆ‚W` depende de `x`
- `âˆ‚z/âˆ‚b = 1`
- `âˆ‚z/âˆ‚x = W`

Esto produce el patrÃ³n computacional:

```
dL_da  â†’  (multiplicar por Ï†'(z))  â†’  Î´ = dL_dz
                       â”‚
                       â”œâ”€â”€ dL_dW = Î´ âŠ— x
                       â”œâ”€â”€ dL_db = Î´
                       â””â”€â”€ dL_dx = Wáµ€ Î´
```

##### e) Ejemplo resuelto (worked example) paso a paso

Objetivo del ejemplo: una red **2-2-1** (2 entradas, 2 ocultas, 1 salida) con sigmoid en salida para clasificaciÃ³n binaria. El entregable es poder escribir:

- forward: `z1, a1, z2, a2`
- backward: `Î´2, dW2, db2, Î´1, dW1, db1`

GuÃ­a de trabajo (sin nÃºmeros para que puedas rellenar tÃº):

1. **Forward**
   - `z1 = W1 x + b1`
   - `a1 = Ï†(z1)`
   - `z2 = W2 a1 + b2`
   - `a2 = Ïƒ(z2)`
2. **Loss**
   - `L = BCE(y, a2)`
3. **Backward**
   - Para sigmoid + BCE (caso tÃ­pico): `Î´2 = a2 - y`
   - `dW2 = Î´2 âŠ— a1`
   - `db2 = Î´2`
   - `Î´1 = (W2áµ€ Î´2) âŠ™ Ï†'(z1)`
   - `dW1 = Î´1 âŠ— x`
   - `db1 = Î´1`

Ejemplo numÃ©rico completo (forward y backward, con nÃºmeros):

Definimos:

- Entrada: `x = [1.0, -2.0]`
- Etiqueta: `y = 1`
- ActivaciÃ³n oculta: `Ï† = ReLU`
- ActivaciÃ³n salida: `Ïƒ` (sigmoid)

ParÃ¡metros:

- `W1 = [[0.1, -0.2], [0.4, 0.3]]`, `b1 = [0.0, 0.1]`
- `W2 = [[-0.3, 0.2]]`, `b2 = [0.05]`

1) Forward

- `z1 = W1x + b1`
  - `z1_1 = 0.1Â·1 + (-0.2)Â·(-2) + 0.0 = 0.5`
  - `z1_2 = 0.4Â·1 + 0.3Â·(-2) + 0.1 = -0.1`
  - `z1 = [0.5, -0.1]`
- `a1 = ReLU(z1) = [0.5, 0.0]`

- `z2 = W2a1 + b2 = (-0.3)Â·0.5 + 0.2Â·0.0 + 0.05 = -0.10`
- `a2 = Ïƒ(z2) â‰ˆ 0.4750`

2) Loss (Binary Cross-Entropy)

- `L = -log(a2) â‰ˆ -log(0.4750) â‰ˆ 0.744`

3) Backward

- Para sigmoid + BCE: `Î´2 = a2 - y â‰ˆ 0.4750 - 1 = -0.5250`

- Gradientes en salida:
  - `dW2 = Î´2 âŠ— a1 = [-0.5250Â·0.5, -0.5250Â·0.0] â‰ˆ [-0.2625, 0.0]`
  - `db2 = Î´2 â‰ˆ -0.5250`

- PropagaciÃ³n a la capa oculta:
  - `dL/da1 = W2áµ€ Î´2 = [-0.3, 0.2]áµ€ Â· (-0.5250) â‰ˆ [0.1575, -0.1050]`
  - `ReLU'(z1) = [1, 0]` (porque `z1_1>0` y `z1_2<0`)
  - `Î´1 = dL/da1 âŠ™ ReLU'(z1) â‰ˆ [0.1575, 0.0]`

- Gradientes en primera capa:
  - `dW1 = Î´1 âŠ— x`
    - para neurona 1: `[0.1575Â·1.0, 0.1575Â·(-2.0)] â‰ˆ [0.1575, -0.3150]`
    - para neurona 2: `[0, 0]`
  - `db1 = Î´1 â‰ˆ [0.1575, 0.0]`

Chequeo mental:

- Los gradientes â€œse apaganâ€ donde `ReLU'(z)=0`.
- `dW` siempre tiene la misma shape que `W`.

##### f) ImplementaciÃ³n prÃ¡ctica (laboratorio)

Checklist mÃ­nimo de implementaciÃ³n (sin â€œmagiaâ€):

- una clase/capa que guarde `x`, `z`, `a` en cache
- un `backward()` que devuelva `dL_dx`, `dL_dW`, `dL_db`
- un training loop que muestre una curva de pÃ©rdida descendente

Protocolos de ejecuciÃ³n (integraciÃ³n v4/v5):

- **v4.0 (Semana 18):** antes de programar, completar `study_tools/DRYRUN_BACKPROPAGATION.md`.
- **v5.0 (validaciÃ³n):** si el entrenamiento no converge, hacer:
  - *Overfit on small batch* (este mÃ³dulo ya lo incluye mÃ¡s abajo).
  - si el error persiste, revisar *gradient checking* (ver checklist general en `CHECKLIST.md`).

##### g) Variantes, limitaciones y casos frontera

- **Softmax + Cross-Entropy:** el gradiente de salida tambiÃ©n se simplifica a `y_pred - y_true` (cuando `y_true` es one-hot).
- **Sigmoid en capas ocultas:** riesgo de *vanishing gradients* si `|z|` crece.
- **ReLU:** riesgo de *dying ReLU* (neurona que queda en 0 siempre).
- **Estabilidad numÃ©rica:** usar `clip`, restar `max(z)` en softmax, y `eps` en logs.

#### Visuales (para estudiar y recordar)

Grafo computacional mÃ­nimo (una capa):

```
x â”€â”€â–º (Wx + b) â”€â”€â–º z â”€â”€â–º Ï†(z) â”€â”€â–º a â”€â”€â–º L
         â–²                    â–²
         â”‚                    â”‚
         W,b                  Ï†'
```

#### Diagrama de flujo: forward (verde) / backward (rojo)

```
FORWARD (verde)
x â†’ z1=W1x+b1 â†’ a1=Ï†(z1) â†’ z2=W2a1+b2 â†’ a2 â†’ L

BACKWARD (rojo)
L â†’ dL/da2 â†’ Î´2=dL/dz2 â†’ dW2,db2 â†’ Î´1 â†’ dW1,db1
```

Regla prÃ¡ctica para implementarlo:

- **Forward:** guarda en cache `x, z, a` por capa.
- **Backward:** empieza por el Ãºltimo `Î´` y propaga hacia atrÃ¡s con `Wáµ€`.

#### Fallas tÃ­picas (con visual): Vanishing Gradient vs Dying ReLU

**1) Vanishing gradient (sigmoid/tanh en capas ocultas)**

Si `|z|` es grande, `Ïƒ(z)` se satura y `Ïƒ'(z) â‰ˆ 0`.
En backprop, multiplicas muchas derivadas pequeÃ±as:

```
Î´1 = (W2áµ€ Î´2) âŠ™ Ï†'(z1)
Î´0 = (W1áµ€ Î´1) âŠ™ Ï†'(z0)
...

si Ï†'(z) â‰ˆ 0 en varias capas â†’ Î´ se vuelve ~0
```

SÃ­ntomas:

- loss baja muy lento
- pesos de capas tempranas casi no cambian

MitigaciÃ³n (en este nivel):

- usa ReLU en ocultas (o inicializaciones cuidadosas)
- normaliza features

**2) Dying ReLU**

ReLU: `Ï†(z)=max(0,z)` y `Ï†'(z)=0` si `z<0`.

Si una neurona queda siempre con `z<0`, su gradiente se vuelve 0 y â€œmuereâ€:

```
z < 0  â†’  a = 0
Ï†'(z)=0  â†’  Î´ = Î´_next âŠ™ 0 = 0
```

SÃ­ntomas:

- muchas activaciones exactamente 0
- algunas neuronas nunca â€œrevivenâ€

MitigaciÃ³n (en este nivel):

- baja learning rate
- inicializa pesos con escalas razonables
- considera LeakyReLU (conceptual)

#### Actividades activas (aprendizaje activo)

- **Retrieval practice (5â€“10 min):** sin mirar notas, escribe las 6 ecuaciones: `Î´2`, `dW2`, `db2`, `Î´1`, `dW1`, `db1`.
- **Interleaving:** alterna ejercicios de backprop con ejercicios de shapes (recomendado: `study_tools/DRILL_DIMENSIONES_NUMPY.md`).
- **GeneraciÃ³n:** crea tu propio mini-ejemplo con una red 3-3-1 y verifica a mano una iteraciÃ³n.

#### EvaluaciÃ³n (formativa y sumativa)

- **Quiz conceptual:**
  - Â¿QuÃ© representa `Î´` y por quÃ© es Ãºtil?
  - Â¿Por quÃ© `Î´2 = a2 - y` en sigmoid+BCE?
- **Prueba prÃ¡ctica:** tu red debe:
  - resolver XOR, y
  - pasar el *overfit test* sobre un minibatch.

#### Cheat sheet (repaso rÃ¡pido)

- `z = Wx + b`
- `a = Ï†(z)`
- `Î´ = âˆ‚L/âˆ‚z = (âˆ‚L/âˆ‚a) âŠ™ Ï†'(z)`
- `âˆ‚L/âˆ‚W = Î´ âŠ— x`
- `âˆ‚L/âˆ‚b = Î´`
- `âˆ‚L/âˆ‚x = Wáµ€ Î´`

#### Errores comunes y FAQs

- **(Shapes)** confundir `(n,)` con `(n,1)` y obtener gradientes transpuestos.
- **(Signos)** usar `y - y_pred` en lugar de `y_pred - y` y â€œsubirâ€ la loss.
- **(Softmax)** implementar softmax sin restar el mÃ¡ximo â†’ overflow.
- **(Debug)** si la red no puede memorizar 4 puntos de XOR, *no* es â€œfalta de datosâ€; es un bug.

#### Recursos complementarios (orientados a prÃ¡ctica)

- [RECURSOS.md](RECURSOS.md)
- `study_tools/DRYRUN_BACKPROPAGATION.md`
- `study_tools/EXAMEN_ADMISION_SIMULADO.md`
- [3Blue1Brown: Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)

### ConsolidaciÃ³n (Backpropagation)

#### Errores comunes

- **Shapes mal:** confundir `(n,)` con `(n,1)` y obtener transpuestas inesperadas.
- **Signo del gradiente:** si actualizas con `+ lr * grad`, subes la loss.
- **No cachear:** si no guardas `x, z, a`, terminas recomputando o usando valores incorrectos.
- **ExplosiÃ³n numÃ©rica:** logits grandes â†’ `exp` overflow â†’ `nan`.

#### Debugging / validaciÃ³n (v5)

- **Overfit on small batch:** si no puede memorizar 4 puntos (XOR), asume bug.
- Revisa `nan/inf`:
  - `np.exp` sin `clip`
  - `np.log` sin `eps`
- Registra hallazgos en `study_tools/DIARIO_ERRORES.md`.
- Protocolos completos:
  - [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)
  - [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md)

#### Reto Feynman (tablero blanco)

Explica en 5 lÃ­neas o menos:

1) Â¿QuÃ© es `Î´` y por quÃ© es la seÃ±al que â€œviaja hacia atrÃ¡sâ€?
2) Â¿Por quÃ© `dW = Î´ âŠ— x` tiene sentido dimensionalmente?
3) Â¿CÃ³mo distinguirÃ­as vanishing gradient vs dying ReLU en logs/activaciones?

---

## ðŸ’» Parte 4: Optimizadores

### 4.1 SGD (Stochastic Gradient Descent)

```python
class SGD:
    """Vanilla Stochastic Gradient Descent."""

    def __init__(self, learning_rate: float = 0.01):
        self.lr = learning_rate

    def update(self, layer, dW: np.ndarray, db: np.ndarray):
        layer.W -= self.lr * dW
        layer.b -= self.lr * db
```

### 4.2 SGD con Momentum

```python
class SGDMomentum:
    """
    SGD con Momentum.

    v_t = Î²Â·v_{t-1} + (1-Î²)Â·âˆ‡L
    Î¸ = Î¸ - lrÂ·v_t

    Momentum ayuda a:
    - Acelerar convergencia
    - Escapar de mÃ­nimos locales
    - Reducir oscilaciones
    """

    def __init__(self, learning_rate: float = 0.01, momentum: float = 0.9):
        self.lr = learning_rate
        self.momentum = momentum
        self.velocities = {}

    def update(self, layer, dW: np.ndarray, db: np.ndarray, layer_id: int):
        if layer_id not in self.velocities:
            self.velocities[layer_id] = {
                'W': np.zeros_like(dW),
                'b': np.zeros_like(db)
            }

        v = self.velocities[layer_id]

        # Actualizar velocidad
        v['W'] = self.momentum * v['W'] + (1 - self.momentum) * dW
        v['b'] = self.momentum * v['b'] + (1 - self.momentum) * db

        # Actualizar parÃ¡metros
        layer.W -= self.lr * v['W']
        layer.b -= self.lr * v['b']
```

### 4.3 Adam Optimizer

```python
class Adam:
    """
    Adam: Adaptive Moment Estimation.

    Combina:
    - Momentum (primer momento)
    - RMSprop (segundo momento)

    m_t = Î²â‚Â·m_{t-1} + (1-Î²â‚)Â·g_t       (momentum)
    v_t = Î²â‚‚Â·v_{t-1} + (1-Î²â‚‚)Â·g_tÂ²      (velocidad adaptativa)
    mÌ‚_t = m_t / (1 - Î²â‚^t)              (correcciÃ³n de bias)
    vÌ‚_t = v_t / (1 - Î²â‚‚^t)
    Î¸ = Î¸ - lr Â· mÌ‚_t / (âˆšvÌ‚_t + Îµ)
    """

    def __init__(
        self,
        learning_rate: float = 0.001,
        beta1: float = 0.9,
        beta2: float = 0.999,
        epsilon: float = 1e-8
    ):
        self.lr = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = {}
        self.v = {}
        self.t = 0

    def update(self, layer, dW: np.ndarray, db: np.ndarray, layer_id: int):
        if layer_id not in self.m:
            self.m[layer_id] = {'W': np.zeros_like(dW), 'b': np.zeros_like(db)}
            self.v[layer_id] = {'W': np.zeros_like(dW), 'b': np.zeros_like(db)}

        self.t += 1
        m, v = self.m[layer_id], self.v[layer_id]

        # Actualizar momentos
        m['W'] = self.beta1 * m['W'] + (1 - self.beta1) * dW
        m['b'] = self.beta1 * m['b'] + (1 - self.beta1) * db
        v['W'] = self.beta2 * v['W'] + (1 - self.beta2) * dW**2
        v['b'] = self.beta2 * v['b'] + (1 - self.beta2) * db**2

        # CorrecciÃ³n de bias
        m_hat_W = m['W'] / (1 - self.beta1**self.t)
        m_hat_b = m['b'] / (1 - self.beta1**self.t)
        v_hat_W = v['W'] / (1 - self.beta2**self.t)
        v_hat_b = v['b'] / (1 - self.beta2**self.t)

        # Actualizar parÃ¡metros
        layer.W -= self.lr * m_hat_W / (np.sqrt(v_hat_W) + self.epsilon)
        layer.b -= self.lr * m_hat_b / (np.sqrt(v_hat_b) + self.epsilon)
```

---

## ðŸ“¦ Entregable del MÃ³dulo

### `neural_network.py`

```python
"""
Neural Network Module

ImplementaciÃ³n desde cero de:
- MLP (Multilayer Perceptron)
- Backpropagation
- Optimizadores (SGD, Momentum, Adam)
- Funciones de activaciÃ³n

Autor: [Tu nombre]
MÃ³dulo: 06 - Deep Learning
"""

import numpy as np
from typing import List, Tuple, Optional


# ============================================================
# ACTIVACIONES
# ============================================================

def sigmoid(z):
    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

def sigmoid_deriv(a):
    return a * (1 - a)

def relu(z):
    return np.maximum(0, z)

def relu_deriv(z):
    return (z > 0).astype(float)

def tanh_deriv(a):
    return 1 - a**2

def softmax(z):
    exp_z = np.exp(z - np.max(z))
    return exp_z / np.sum(exp_z)


# ============================================================
# CAPA
# ============================================================

class Layer:
    def __init__(self, input_size: int, output_size: int, activation: str = 'relu'):
        self.activation = activation
        scale = np.sqrt(2.0 / input_size) if activation == 'relu' else np.sqrt(1.0 / input_size)
        self.W = np.random.randn(output_size, input_size) * scale
        self.b = np.zeros(output_size)
        self.cache = {}

    def forward(self, x: np.ndarray) -> np.ndarray:
        self.cache['x'] = x
        z = self.W @ x + self.b
        self.cache['z'] = z

        if self.activation == 'relu':
            a = relu(z)
        elif self.activation == 'sigmoid':
            a = sigmoid(z)
        elif self.activation == 'tanh':
            a = np.tanh(z)
        elif self.activation == 'softmax':
            a = softmax(z)
        else:
            a = z

        self.cache['a'] = a
        return a

    def backward(self, dL_da: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        z, x, a = self.cache['z'], self.cache['x'], self.cache['a']

        if self.activation == 'sigmoid':
            da_dz = sigmoid_deriv(a)
        elif self.activation == 'relu':
            da_dz = relu_deriv(z)
        elif self.activation == 'tanh':
            da_dz = tanh_deriv(a)
        else:
            da_dz = np.ones_like(z)

        delta = dL_da * da_dz
        dL_dW = np.outer(delta, x)
        dL_db = delta
        dL_dx = self.W.T @ delta

        return dL_dx, dL_dW, dL_db


# ============================================================
# OPTIMIZADORES
# ============================================================

class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr

    def step(self, layers, gradients):
        for layer, (dW, db) in zip(layers, gradients):
            layer.W -= self.lr * dW
            layer.b -= self.lr * db


class Adam:
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        self.lr, self.beta1, self.beta2, self.eps = lr, beta1, beta2, eps
        self.m, self.v, self.t = {}, {}, 0

    def step(self, layers, gradients):
        self.t += 1
        for i, (layer, (dW, db)) in enumerate(zip(layers, gradients)):
            if i not in self.m:
                self.m[i] = {'W': np.zeros_like(dW), 'b': np.zeros_like(db)}
                self.v[i] = {'W': np.zeros_like(dW), 'b': np.zeros_like(db)}

            self.m[i]['W'] = self.beta1 * self.m[i]['W'] + (1 - self.beta1) * dW
            self.m[i]['b'] = self.beta1 * self.m[i]['b'] + (1 - self.beta1) * db
            self.v[i]['W'] = self.beta2 * self.v[i]['W'] + (1 - self.beta2) * dW**2
            self.v[i]['b'] = self.beta2 * self.v[i]['b'] + (1 - self.beta2) * db**2

            m_hat_W = self.m[i]['W'] / (1 - self.beta1**self.t)
            m_hat_b = self.m[i]['b'] / (1 - self.beta1**self.t)
            v_hat_W = self.v[i]['W'] / (1 - self.beta2**self.t)
            v_hat_b = self.v[i]['b'] / (1 - self.beta2**self.t)

            layer.W -= self.lr * m_hat_W / (np.sqrt(v_hat_W) + self.eps)
            layer.b -= self.lr * m_hat_b / (np.sqrt(v_hat_b) + self.eps)


# ============================================================
# RED NEURONAL
# ============================================================

class NeuralNetwork:
    def __init__(self, layer_sizes: List[int], activations: List[str]):
        self.layers = [Layer(layer_sizes[i], layer_sizes[i+1], activations[i])
                       for i in range(len(layer_sizes)-1)]
        self.loss_history = []

    def forward(self, x: np.ndarray) -> np.ndarray:
        for layer in self.layers:
            x = layer.forward(x)
        return x

    def backward(self, y_true: np.ndarray) -> List[Tuple]:
        y_pred = self.layers[-1].cache['a']
        dL_da = y_pred - y_true

        gradients = []
        for layer in reversed(self.layers):
            dL_da, dW, db = layer.backward(dL_da)
            gradients.insert(0, (dW, db))
        return gradients

    def fit(self, X, y, epochs=1000, lr=0.1, optimizer='sgd', verbose=True):
        opt = Adam(lr) if optimizer == 'adam' else SGD(lr)

        for epoch in range(epochs):
            total_loss = 0
            for xi, yi in zip(X, y):
                yi_arr = np.atleast_1d(yi)
                output = self.forward(xi)

                # BCE loss
                output_clip = np.clip(output, 1e-15, 1-1e-15)
                loss = -np.sum(yi_arr * np.log(output_clip) + (1-yi_arr) * np.log(1-output_clip))
                total_loss += loss

                gradients = self.backward(yi_arr)
                opt.step(self.layers, gradients)

            self.loss_history.append(total_loss / len(X))
            if verbose and epoch % (epochs//10) == 0:
                print(f"Epoch {epoch}: Loss = {self.loss_history[-1]:.4f}")

    def predict(self, X: np.ndarray) -> np.ndarray:
        return np.array([1 if self.forward(x)[0] > 0.5 else 0 for x in X])

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        return np.mean(self.predict(X) == y)


# ============================================================
# TESTS
# ============================================================

if __name__ == "__main__":
    print("=== Test: XOR Problem ===")
    X = np.array([[0,0], [0,1], [1,0], [1,1]])
    y = np.array([0, 1, 1, 0])

    net = NeuralNetwork([2, 4, 1], ['tanh', 'sigmoid'])
    net.fit(X, y, epochs=5000, lr=0.5, verbose=True)

    print("\nPredicciones:")
    for xi, yi in zip(X, y):
        pred = net.forward(xi)[0]
        print(f"{xi} -> {pred:.4f} (target: {yi})")

    print(f"\nAccuracy: {net.score(X, y):.2%}")
    print("\nâœ“ Test XOR completado!")
```

---

## ðŸ’» Parte 5: CNNs - Redes Convolucionales (Semana 19)

> âš ï¸ **Nota:** En este mÃ³dulo NO implementamos CNNs desde cero (es complejo). El objetivo es **entender la teorÃ­a** para el curso de Deep Learning de CU Boulder.

### Protocolo D (visualizaciÃ³n generativa): convoluciÃ³n sobre una imagen real

Para que â€œconvoluciÃ³nâ€ no sea solo una fÃ³rmula, ejecuta el script:

- [`visualizations/viz_convolution.py`](../visualizations/viz_convolution.py)

Uso recomendado (con una imagen propia):

```bash
python3 visualizations/viz_convolution.py /ruta/a/tu_imagen.png
```

QuÃ© debes observar:

- el **Sobel X** responde fuerte a bordes verticales
- la **magnitud** combina bordes en varias direcciones

Entregable sugerido: captura de *input vs feature map* + explicaciÃ³n en 5 lÃ­neas de quÃ© patrÃ³n detecta el filtro.

### 5.1 Â¿Por QuÃ© CNNs para ImÃ¡genes?

```
PROBLEMA CON MLP PARA IMÃGENES:

Imagen MNIST: 28x28 = 784 pÃ­xeles
MLP fully connected a capa de 256 neuronas:
  â†’ 784 Ã— 256 = 200,704 parÃ¡metros (Â¡solo primera capa!)

Imagen HD: 1920x1080x3 = 6,220,800 pÃ­xeles
  â†’ Imposible conectar todo con todo

SOLUCIÃ“N: CONVOLUCIÃ“N
- Procesar regiones locales (no toda la imagen)
- Compartir pesos (el mismo filtro en toda la imagen)
- Detectar patrones sin importar su posiciÃ³n
```

### 5.2 La OperaciÃ³n de ConvoluciÃ³n

```python
import numpy as np

def convolve2d_simple(image: np.ndarray, kernel: np.ndarray) -> np.ndarray:
    """
    ConvoluciÃ³n 2D simplificada (para entender el concepto).

    La convoluciÃ³n desliza un kernel (filtro) sobre la imagen
    y calcula el producto punto en cada posiciÃ³n.

    Args:
        image: Imagen de entrada (H, W)
        kernel: Filtro (kH, kW), tÃ­picamente 3x3 o 5x5

    Returns:
        Feature map (H-kH+1, W-kW+1)
    """
    H, W = image.shape
    kH, kW = kernel.shape

    # TamaÃ±o del output (sin padding)
    out_H = H - kH + 1
    out_W = W - kW + 1

    output = np.zeros((out_H, out_W))

    for i in range(out_H):
        for j in range(out_W):
            # Extraer regiÃ³n de la imagen
            region = image[i:i+kH, j:j+kW]
            # Producto punto con el kernel
            output[i, j] = np.sum(region * kernel)

    return output


# Ejemplo: DetecciÃ³n de bordes verticales
image = np.array([
    [0, 0, 0, 1, 1, 1],
    [0, 0, 0, 1, 1, 1],
    [0, 0, 0, 1, 1, 1],
    [0, 0, 0, 1, 1, 1],
])

# Kernel Sobel para bordes verticales
sobel_vertical = np.array([
    [-1, 0, 1],
    [-2, 0, 2],
    [-1, 0, 1]
])

edges = convolve2d_simple(image, sobel_vertical)
print("Feature map (bordes verticales):")
print(edges)
```

### 5.3 Conceptos Clave de CNNs

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VOCABULARIO CNN                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  KERNEL (FILTRO)                                                â”‚
â”‚  â”œâ”€â”€ Matriz pequeÃ±a (3x3, 5x5) que detecta patrones             â”‚
â”‚  â”œâ”€â”€ Los valores del kernel son APRENDIDOS (backprop)           â”‚
â”‚  â””â”€â”€ Diferentes kernels detectan diferentes features            â”‚
â”‚                                                                 â”‚
â”‚  STRIDE                                                         â”‚
â”‚  â”œâ”€â”€ CuÃ¡ntos pÃ­xeles se mueve el kernel en cada paso            â”‚
â”‚  â”œâ”€â”€ stride=1: mueve 1 pÃ­xel (output grande)                    â”‚
â”‚  â””â”€â”€ stride=2: mueve 2 pÃ­xeles (output mÃ¡s pequeÃ±o)             â”‚
â”‚                                                                 â”‚
â”‚  PADDING                                                        â”‚
â”‚  â”œâ”€â”€ AÃ±adir ceros alrededor de la imagen                        â”‚
â”‚  â”œâ”€â”€ 'valid': sin padding (output mÃ¡s pequeÃ±o)                  â”‚
â”‚  â””â”€â”€ 'same': padding para mantener tamaÃ±o                       â”‚
â”‚                                                                 â”‚
â”‚  POOLING                                                        â”‚
â”‚  â”œâ”€â”€ Reduce dimensiones (downsampling)                          â”‚
â”‚  â”œâ”€â”€ Max Pooling: toma el mÃ¡ximo de cada regiÃ³n                 â”‚
â”‚  â””â”€â”€ Average Pooling: toma el promedio                          â”‚
â”‚                                                                 â”‚
â”‚  FEATURE MAP                                                    â”‚
â”‚  â””â”€â”€ Output de aplicar un filtro (lo que "ve" el filtro)        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.4 CÃ¡lculo de Dimensiones (Importante para ExÃ¡menes)

```python
def output_size(input_size: int, kernel_size: int,
                stride: int = 1, padding: int = 0) -> int:
    """
    FÃ³rmula para calcular tamaÃ±o del output de convoluciÃ³n.

    output_size = floor((input + 2*padding - kernel) / stride) + 1
    """
    return (input_size + 2 * padding - kernel_size) // stride + 1


# Ejemplos tÃ­picos de examen:
print("=== Ejercicios de dimensiones ===")

# Ejemplo 1: MNIST sin padding
# Input: 28x28, Kernel: 5x5, Stride: 1, Padding: 0
out = output_size(28, 5, stride=1, padding=0)
print(f"MNIST 28x28, kernel 5x5, stride 1: output = {out}x{out}")  # 24x24

# Ejemplo 2: Con padding 'same'
# Para mantener tamaÃ±o con kernel 3x3, necesitas padding=1
out = output_size(28, 3, stride=1, padding=1)
print(f"MNIST 28x28, kernel 3x3, padding 1: output = {out}x{out}")  # 28x28

# Ejemplo 3: Max Pooling 2x2 stride 2
out = output_size(24, 2, stride=2, padding=0)
print(f"24x24, pooling 2x2 stride 2: output = {out}x{out}")  # 12x12
```

### 5.5 Arquitectura TÃ­pica de CNN

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ARQUITECTURA LENET-5 (ClÃ¡sica para MNIST)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Input: 28x28x1 (imagen grayscale)                              â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [CONV 5x5, 6 filtros] â†’ 24x24x6                                â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [ReLU]                                                         â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [MaxPool 2x2] â†’ 12x12x6                                        â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [CONV 5x5, 16 filtros] â†’ 8x8x16                                â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [ReLU]                                                         â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [MaxPool 2x2] â†’ 4x4x16 = 256 neuronas                          â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [Flatten] â†’ 256                                                â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [FC 120] â†’ 120                                                 â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [FC 84] â†’ 84                                                   â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [FC 10 + Softmax] â†’ 10 clases (dÃ­gitos 0-9)                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.6 Max Pooling

```python
def max_pool2d(x: np.ndarray, pool_size: int = 2) -> np.ndarray:
    """
    Max Pooling 2D.

    Reduce dimensiones tomando el mÃ¡ximo de cada regiÃ³n.
    Hace la red mÃ¡s robusta a pequeÃ±as traslaciones.

    Args:
        x: Feature map (H, W)
        pool_size: TamaÃ±o de la ventana (tÃ­picamente 2)

    Returns:
        Pooled output (H//pool_size, W//pool_size)
    """
    H, W = x.shape
    out_H, out_W = H // pool_size, W // pool_size

    output = np.zeros((out_H, out_W))

    for i in range(out_H):
        for j in range(out_W):
            region = x[i*pool_size:(i+1)*pool_size,
                      j*pool_size:(j+1)*pool_size]
            output[i, j] = np.max(region)

    return output


# Ejemplo
feature_map = np.array([
    [1, 3, 2, 4],
    [5, 6, 1, 2],
    [3, 2, 1, 0],
    [1, 2, 3, 4]
])

pooled = max_pool2d(feature_map, pool_size=2)
print("Original 4x4:")
print(feature_map)
print("\nMax Pooled 2x2:")
print(pooled)  # [[6, 4], [3, 4]]
```

### 5.7 Por QuÃ© Funcionan las CNNs

```
INTUICIÃ“N:

1. CAPAS INICIALES: Detectan features simples
   - Bordes horizontales, verticales, diagonales
   - Cambios de color, texturas

2. CAPAS MEDIAS: Combinan features simples
   - Esquinas, curvas, patrones

3. CAPAS PROFUNDAS: Features de alto nivel
   - Partes de objetos (ojos, ruedas, letras)

4. CAPAS FINALES: Objetos completos
   - "Esto es un 7", "Esto es un gato"

VENTAJAS CLAVE:
â”œâ”€â”€ Parameter sharing: mismo filtro en toda la imagen
â”œâ”€â”€ Sparse connectivity: cada output depende de regiÃ³n local
â”œâ”€â”€ Translation invariance: detecta patrones sin importar posiciÃ³n
â””â”€â”€ Hierarchical features: de simple a complejo
```

### 5.8 Recursos para Profundizar en CNNs

| Recurso | DescripciÃ³n |
|---------|-------------|
| [3B1B - But what is a convolution?](https://www.youtube.com/watch?v=KuXjwB4LzSA) | IntuiciÃ³n visual |
| [CS231n Stanford](http://cs231n.stanford.edu/) | Curso completo de CNNs |
| Deep Learning Book, Cap. 9 | TeorÃ­a formal |

---

## ðŸ“ DerivaciÃ³n AnalÃ­tica: Backpropagation a Mano (v3.2)

> ðŸŽ“ **SimulaciÃ³n de Examen:** *"Derive las ecuaciones de backpropagation para una red de 2 capas"*. Este es un clÃ¡sico de exÃ¡menes de posgrado.

### Red de 2 Capas: DerivaciÃ³n Completa

**Arquitectura:**
- Input: $x$ (vector de features)
- Capa 1: $z_1 = W_1 x + b_1$, $a_1 = \sigma(z_1)$
- Capa 2: $z_2 = W_2 a_1 + b_2$, $\hat{y} = \sigma(z_2)$
- Loss: $L = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$

#### Paso 1: Gradiente de la Capa de Salida

$$\frac{\partial L}{\partial z_2} = \hat{y} - y = \delta_2$$

(Resultado elegante gracias a la combinaciÃ³n sigmoid + cross-entropy)

$$\frac{\partial L}{\partial W_2} = \delta_2 \cdot a_1^T$$

$$\frac{\partial L}{\partial b_2} = \delta_2$$

#### Paso 2: Propagar el Error Hacia AtrÃ¡s (Capa Oculta)

$$\frac{\partial L}{\partial a_1} = W_2^T \delta_2$$

$$\frac{\partial L}{\partial z_1} = \frac{\partial L}{\partial a_1} \odot \sigma'(z_1) = W_2^T \delta_2 \odot a_1 \odot (1 - a_1) = \delta_1$$

$$\frac{\partial L}{\partial W_1} = \delta_1 \cdot x^T$$

$$\frac{\partial L}{\partial b_1} = \delta_1$$

#### Resumen: Las 4 Ecuaciones de Backprop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ECUACIONES DE BACKPROPAGATION                               â”‚
â”‚                                                             â”‚
â”‚ 1. Î´_L = âˆ‡_a L âŠ™ Ïƒ'(z_L)     Error en capa final           â”‚
â”‚                                                             â”‚
â”‚ 2. Î´_l = (W_{l+1}^T Î´_{l+1}) âŠ™ Ïƒ'(z_l)   Propagar atrÃ¡s    â”‚
â”‚                                                             â”‚
â”‚ 3. âˆ‚L/âˆ‚W_l = Î´_l Â· a_{l-1}^T   Gradiente de pesos           â”‚
â”‚                                                             â”‚
â”‚ 4. âˆ‚L/âˆ‚b_l = Î´_l              Gradiente de bias             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tu Entregable

Escribe en un documento (Markdown o LaTeX):
1. DerivaciÃ³n completa de backprop para red de 2 capas
2. Por quÃ© $\delta_L = \hat{y} - y$ cuando usamos sigmoid + cross-entropy
3. Diagrama de grafo computacional mostrando el flujo de gradientes

---

## ðŸ§ª Overfit on Small Batch: Debugging de Redes Neuronales (v3.3)

> âš ï¸ **CRÃTICO:** Esta es la tÃ©cnica #1 de debugging en Deep Learning. Si tu red no puede hacer overfitting en 10 ejemplos, tiene un bug.

### El Principio

```
REGLA DE ORO DEL DEBUGGING EN DL:

Una red neuronal DEBE poder memorizar un dataset pequeÃ±o.

Si entrenas con:
- 10 ejemplos
- Muchas Ã©pocas (1000+)
- Sin regularizaciÃ³n

El loss DEBE llegar a ~0.00 (o muy cercano).

Si NO llega a 0 â†’ TU IMPLEMENTACIÃ“N TIENE UN BUG
```

### Por QuÃ© Funciona

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OVERFIT TEST                                                â”‚
â”‚                                                             â”‚
â”‚ Dataset pequeÃ±o (10 ejemplos):                              â”‚
â”‚ - Capacidad de la red >> complejidad del dataset            â”‚
â”‚ - La red puede "memorizar" cada ejemplo perfectamente       â”‚
â”‚ - Loss debe â†’ 0 si backprop funciona                        â”‚
â”‚                                                             â”‚
â”‚ Si loss NO baja:                                            â”‚
â”‚ - Gradiente mal calculado                                   â”‚
â”‚ - Learning rate incorrecto                                  â”‚
â”‚ - Arquitectura rota (dimensiones)                           â”‚
â”‚ - Bug en forward o backward pass                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Script: `overfit_test.py` (Entregable Obligatorio v3.3)

```python
"""
Overfit Test - ValidaciÃ³n de Redes Neuronales
Si tu red no puede hacer overfit en 10 ejemplos, estÃ¡ rota.

Autor: [Tu nombre]
MÃ³dulo: 07 - Deep Learning
"""
import numpy as np
from typing import List, Tuple


def overfit_test(
    model,
    X_small: np.ndarray,
    y_small: np.ndarray,
    epochs: int = 2000,
    target_loss: float = 0.01,
    verbose: bool = True
) -> Tuple[bool, List[float]]:
    """
    Test de overfitting: la red debe memorizar un dataset pequeÃ±o.

    Args:
        model: Tu red neuronal (debe tener .fit() y .forward())
        X_small: Dataset pequeÃ±o (10-20 ejemplos)
        y_small: Labels del dataset
        epochs: Ã‰pocas de entrenamiento
        target_loss: Loss objetivo (default: 0.01)
        verbose: Mostrar progreso

    Returns:
        (passed, loss_history)
    """
    if verbose:
        print("=" * 60)
        print("OVERFIT TEST: Â¿Puede tu red memorizar 10 ejemplos?")
        print("=" * 60)
        print(f"Dataset size: {len(y_small)}")
        print(f"Epochs: {epochs}")
        print(f"Target loss: {target_loss}")
        print("-" * 60)

    # Entrenar
    loss_history = []
    for epoch in range(epochs):
        # Forward pass para todos los ejemplos
        total_loss = 0.0
        for i in range(len(y_small)):
            output = model.forward(X_small[i])
            loss = np.mean((output - y_small[i]) ** 2)  # MSE
            total_loss += loss

            # Backward y update (asumiendo que model tiene estos mÃ©todos)
            model.backward(y_small[i])
            model.update(learning_rate=0.1)

        avg_loss = total_loss / len(y_small)
        loss_history.append(avg_loss)

        if verbose and epoch % 500 == 0:
            print(f"Epoch {epoch:4d}: Loss = {avg_loss:.6f}")

    final_loss = loss_history[-1]
    passed = final_loss < target_loss

    if verbose:
        print("-" * 60)
        print(f"Final Loss: {final_loss:.6f}")
        if passed:
            print("âœ“ PASSED: Tu red puede hacer overfitting")
            print("  â†’ El forward y backward pass funcionan correctamente")
        else:
            print("âœ— FAILED: Tu red NO puede hacer overfitting")
            print("  â†’ Revisa tu implementaciÃ³n de backprop")
            print("  Posibles causas:")
            print("  - Gradiente mal calculado")
            print("  - Learning rate muy bajo")
            print("  - Bug en forward pass")
            print("  - Dimensiones incorrectas")

    return passed, loss_history


# ============================================================
# EJEMPLO: Test con XOR (debe pasar)
# ============================================================

def test_xor_overfit():
    """Test: Una red pequeÃ±a debe resolver XOR perfectamente."""
    print("\n" + "=" * 60)
    print("TEST: Overfit on XOR Problem")
    print("=" * 60)

    # XOR dataset (4 ejemplos)
    X = np.array([
        [0, 0],
        [0, 1],
        [1, 0],
        [1, 1]
    ], dtype=np.float64)

    y = np.array([
        [0],
        [1],
        [1],
        [0]
    ], dtype=np.float64)

    # Crear red simple (2 -> 8 -> 1)
    # NOTA: Reemplaza esto con tu clase NeuralNetwork
    class SimpleNet:
        def __init__(self):
            np.random.seed(42)
            self.W1 = np.random.randn(8, 2) * 0.5
            self.b1 = np.zeros((8, 1))
            self.W2 = np.random.randn(1, 8) * 0.5
            self.b2 = np.zeros((1, 1))

            # Cache para backprop
            self.cache = {}

        def sigmoid(self, z):
            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

        def forward(self, x):
            x = x.reshape(-1, 1)
            z1 = self.W1 @ x + self.b1
            a1 = self.sigmoid(z1)
            z2 = self.W2 @ a1 + self.b2
            a2 = self.sigmoid(z2)

            self.cache = {'x': x, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2}
            return a2.flatten()

        def backward(self, y_true):
            y_true = np.array(y_true).reshape(-1, 1)
            a2 = self.cache['a2']
            a1 = self.cache['a1']
            x = self.cache['x']

            # Gradientes
            dz2 = a2 - y_true
            self.dW2 = dz2 @ a1.T
            self.db2 = dz2

            da1 = self.W2.T @ dz2
            dz1 = da1 * a1 * (1 - a1)
            self.dW1 = dz1 @ x.T
            self.db1 = dz1

        def update(self, learning_rate):
            self.W1 -= learning_rate * self.dW1
            self.b1 -= learning_rate * self.db1
            self.W2 -= learning_rate * self.dW2
            self.b2 -= learning_rate * self.db2

    # Ejecutar test
    model = SimpleNet()
    passed, history = overfit_test(model, X, y, epochs=2000, target_loss=0.01)

    # Verificar predicciones finales
    print("\nPredicciones finales:")
    for i in range(len(X)):
        pred = model.forward(X[i])
        print(f"  Input: {X[i]} â†’ Pred: {pred[0]:.3f} (Target: {y[i][0]})")

    return passed


if __name__ == "__main__":
    test_xor_overfit()
```

### Checklist de Debugging con Overfit Test

| SÃ­ntoma | DiagnÃ³stico | SoluciÃ³n |
|---------|-------------|----------|
| Loss no baja | Gradiente = 0 o NaN | Verificar derivadas con grad_check |
| Loss baja muy lento | Learning rate muy bajo | Aumentar LR (probar 0.1, 0.5, 1.0) |
| Loss oscila mucho | Learning rate muy alto | Reducir LR |
| Loss sube | Signos invertidos en gradiente | Revisar forward/backward |
| Loss = NaN | Overflow en exp/softmax | Usar versiones numÃ©ricamente estables |

---

## ðŸŽ¯ El Reto del Tablero Blanco (MetodologÃ­a Feynman)

Explica en **mÃ¡ximo 5 lÃ­neas** sin jerga tÃ©cnica:

1. **Â¿QuÃ© es backpropagation?**
   > Pista: Piensa en "culpar" a cada peso por el error.

2. **Â¿Por quÃ© ReLU es mejor que sigmoid en capas ocultas?**
   > Pista: Piensa en quÃ© pasa con el gradiente de sigmoid cuando z es muy grande o muy pequeÃ±o.

3. **Â¿QuÃ© hace una convoluciÃ³n en una imagen?**
   > Pista: Piensa en "deslizar una lupa" buscando un patrÃ³n especÃ­fico.

4. **Â¿Por quÃ© usamos pooling?**
   > Pista: Piensa en "resumir" una regiÃ³n y hacerla mÃ¡s pequeÃ±a.

---

## âœ… Checklist de FinalizaciÃ³n (v3.3)

### Conocimiento
- [ ] Entiendo la analogÃ­a neurona biolÃ³gica â†’ neurona artificial
- [ ] ImplementÃ© sigmoid, ReLU, tanh, softmax y sus derivadas
- [ ] Entiendo por quÃ© XOR no es linealmente separable
- [ ] ImplementÃ© forward pass para MLP
- [ ] Entiendo la Chain Rule aplicada a backpropagation
- [ ] ImplementÃ© backward pass calculando gradientes
- [ ] ImplementÃ© SGD, SGD+Momentum y Adam
- [ ] Mi red resuelve el problema XOR

### CNNs (TeorÃ­a)
- [ ] Entiendo quÃ© es convoluciÃ³n, stride, padding y pooling
- [ ] Puedo calcular dimensiones de output de una CNN
- [ ] Conozco la arquitectura LeNet-5

### Entregables de CÃ³digo
- [ ] `neural_network.py` con tests pasando
- [ ] `mypy src/` pasa sin errores
- [ ] `pytest tests/` pasa sin errores

### Overfit Test (v3.3 - Obligatorio)
- [ ] **`overfit_test.py` implementado**
- [ ] **Mi red hace overfit en XOR (loss < 0.01)**
- [ ] Si el test falla, debuggeÃ© con grad_check

### DerivaciÃ³n AnalÃ­tica (Obligatorio)
- [ ] DerivÃ© las ecuaciones de backprop a mano
- [ ] Documento con derivaciÃ³n completa (Markdown o LaTeX)
- [ ] Diagrama de grafo computacional

### MetodologÃ­a Feynman
- [ ] Puedo explicar backpropagation en 5 lÃ­neas sin jerga
- [ ] Puedo explicar ReLU vs sigmoid en 5 lÃ­neas
- [ ] Puedo explicar convoluciÃ³n en 5 lÃ­neas
- [ ] Puedo explicar pooling en 5 lÃ­neas

---

## ðŸ”— NavegaciÃ³n

| Anterior | Ãndice | Siguiente |
|----------|--------|-----------|
| [06_UNSUPERVISED_LEARNING](06_UNSUPERVISED_LEARNING.md) | [00_INDICE](00_INDICE.md) | [08_PROYECTO_MNIST](08_PROYECTO_MNIST.md) |
