# MÃ³dulo 07 - Deep Learning

> **ğŸ¯ Objetivo:** Implementar MLP con backprop + CNN forward (NumPy) + entrenamiento CNN con PyTorch
> **Fase:** 2 - NÃºcleo de ML | **Semanas 17-20**
> **Curso del Pathway:** Introduction to Deep Learning

---

<a id="m07-0"></a>

## ğŸ§­ CÃ³mo usar este mÃ³dulo (modo 0â†’100)

**PropÃ³sito:** que puedas construir y depurar una red neuronal desde cero:

- forward pass
- backpropagation
- optimizaciÃ³n (SGD/Momentum/Adam)
- sanity checks (overfit test)

### Objetivos de aprendizaje (medibles)

Al terminar este mÃ³dulo podrÃ¡s:

- **Implementar** un MLP que resuelva XOR.
- **Explicar** backprop como chain rule aplicada a un grafo computacional.
- **Depurar** entrenamiento con overfit test (si no memoriza, hay bug).
- **Implementar** forward pass de una CNN simple (convoluciÃ³n + pooling) en NumPy para dominar dimensiones.
- **Entrenar** una CNN equivalente usando PyTorch (`torch.nn`) sin implementar backward manual.

Enlaces rÃ¡pidos:

- [03_CALCULO_MULTIVARIANTE.md](03_CALCULO_MULTIVARIANTE.md) (Chain Rule)
- [GLOSARIO.md](GLOSARIO.md)
- [RECURSOS.md](RECURSOS.md)
- [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)
- [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md)
- EvaluaciÃ³n (rÃºbrica): [study_tools/RUBRICA_v1.md](../study_tools/RUBRICA_v1.md) (scope `M07` en `rubrica.csv`; cierre Semana 20)

### Recursos (cuÃ¡ndo usarlos)

| Prioridad | Recurso | CuÃ¡ndo usarlo en este mÃ³dulo | Para quÃ© |
|----------|---------|------------------------------|----------|
| **Obligatorio** | [03_CALCULO_MULTIVARIANTE.md](03_CALCULO_MULTIVARIANTE.md) | Antes de implementar `backward()` (Semana 18) | Asegurar Chain Rule y gradientes bÃ¡sicos |
| **Obligatorio** | `study_tools/DRYRUN_BACKPROPAGATION.md` | Justo antes de tu primera implementaciÃ³n completa de Backprop | Hacer â€œdry-runâ€ y detectar errores de gradiente antes del cÃ³digo |
| **Obligatorio** | `study_tools/EXAMEN_ADMISION_SIMULADO.md` | DespuÃ©s de que tu MLP resuelva XOR y antes de cerrar el mÃ³dulo | ValidaciÃ³n tipo examen (sin IDE/internet) |
| **Complementario** | [3Blue1Brown: Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) | Semana 17â€“18, cuando necesites intuiciÃ³n de backprop | Visualizar forward/backward y por quÃ© aprende |
| **Complementario** | [TensorFlow Playground](https://playground.tensorflow.org/) | Semana 17â€“18, cuando estudies por quÃ© una capa lineal no resuelve XOR y cÃ³mo las activaciones cambian la geometrÃ­a | Ver en tiempo real cÃ³mo la red â€œdoblaâ€ el espacio para separar clases |
| **Complementario** | [Deep Learning Book](https://www.deeplearningbook.org/) | Semana 19â€“20 (CNNs/entrenamiento), si quieres rigor | Referencia profunda (gratis) |
| **Opcional** | [RECURSOS.md](RECURSOS.md) | Al terminar el mÃ³dulo (para profundizar en DL/CNNs) | Seleccionar refuerzos sin romper el plan |

---

## ğŸ§  Â¿Por QuÃ© Deep Learning?

```
DEEP LEARNING = Redes Neuronales Multicapa + Arquitecturas Especializadas

Ventajas sobre ML clÃ¡sico:
â”œâ”€â”€ Aprende features automÃ¡ticamente (no feature engineering manual)
â”œâ”€â”€ Puede modelar relaciones NO LINEALES complejas
â”œâ”€â”€ Escala con mÃ¡s datos y mÃ¡s compute
â””â”€â”€ Estado del arte en visiÃ³n (CNNs), NLP (Transformers), etc.

Desventajas:
â”œâ”€â”€ Requiere mÃ¡s datos
â”œâ”€â”€ "Caja negra" - menos interpretable
â””â”€â”€ Costoso computacionalmente
```

### IntuiciÃ³n geomÃ©trica: Deep Learning como â€œdoblar el espacioâ€ (origami)

Una capa lineal `z = Wx + b` solo puede **rotar, estirar o inclinar** el espacio: siempre produce una frontera de decisiÃ³n lineal (un hiperplano). Por eso un modelo lineal no puede separar XOR.

La no linealidad (ReLU/sigmoid/tanh) es lo que permite â€œdoblarâ€ el espacio:

- despuÃ©s del primer doblez, puntos que antes estaban mezclados pueden quedar en regiones separables
- con varias capas, encadenas dobleces hasta que en la Ãºltima capa los datos son separables con un hiperplano

VisualizaciÃ³n sugerida:

- dibuja XOR en 2D
- intenta separarlo con una sola lÃ­nea (imposible)
- luego imagina un doblez que junta los puntos de la misma clase

---

## ğŸ“š Contenido del MÃ³dulo

| Semana | Tema | Entregable |
|--------|------|------------|
| 17 | PerceptrÃ³n y MLP | `activations.py` + forward pass |
| 18 | Backpropagation | `backward()` con Chain Rule |
| 19 | **CNNs: TeorÃ­a + Forward (NumPy)** | ConvoluciÃ³n/pooling (forward) + quiz de dimensiones |
| 20 | **PyTorch para CNNs + Sequence Modeling (Light)** | `scripts/train_cnn_pytorch.py` + `scripts/simple_rnn_forward.py` |

---

## ğŸ§µ Semana 20 (extra): Sequence Modeling (Light) â€” RNN forward pass

**Objetivo:** entender dimensiones en datos secuenciales sin entrenar.

- **Ejecutable:**
  - `python3 scripts/simple_rnn_forward.py`
- **QuÃ© debes entender:**
  - `x.shape = (batch, time, features)`
  - `h.shape = (batch, time, hidden)`
  - `y.shape = (batch, time, out)`

---

## ğŸ’» Parte 1: PerceptrÃ³n y Activaciones

### 1.1 La Neurona Artificial

```python
import numpy as np  # Importa NumPy para operaciones eficientes con arrays

"""
NEURONA ARTIFICIAL (PerceptrÃ³n)

InspiraciÃ³n biolÃ³gica:
- Recibe seÃ±ales de entrada (dendrites)
- Procesa y decide si "dispara" (soma)
- EnvÃ­a seÃ±al de salida (axon)

Modelo matemÃ¡tico:
    z = Î£ wáµ¢xáµ¢ + b = wÂ·x + b  (combinaciÃ³n lineal)
    a = Ïƒ(z)                    (activaciÃ³n)

Donde:
- x: vector de entradas
- w: vector de pesos (learnable)
- b: bias (learnable)
- Ïƒ: funciÃ³n de activaciÃ³n (introduce no-linealidad)
"""

def perceptron(x: np.ndarray, w: np.ndarray, b: float) -> float:  # PerceptrÃ³n: calcula z=wÂ·x+b y aplica funciÃ³n escalÃ³n (clasificaciÃ³n lineal)
    """
    Un perceptrÃ³n simple.

    Args:
        x: entrada (n_features,)
        w: pesos (n_features,)
        b: bias

    Returns:
        salida activada
    """
    z = np.dot(w, x) + b  # Calcula combinaciÃ³n lineal de entradas y pesos
    return 1 if z > 0 else 0  # FunciÃ³n escalÃ³n: 1 si z>0, 0 si zâ‰¤0
```

### 1.2 Funciones de ActivaciÃ³n

```python
import numpy as np  # Importa NumPy para operaciones matemÃ¡ticas

class Activations:  # Agrupa activaciones tÃ­picas de redes neuronales y sus derivadas (API educativa/organizada)
    """Funciones de activaciÃ³n y sus derivadas."""  # Docstring de clase: documenta propÃ³sito; no cambia el cÃ¡lculo en runtime

    @staticmethod  # Define mÃ©todo estÃ¡tico: no necesita `self`/estado; se usa como Activations.sigmoid(z)
    def sigmoid(z: np.ndarray) -> np.ndarray:  # Sigmoide: mapea logits reales a (0,1), tÃ­pica en salida binaria
        """
        Sigmoid: Ïƒ(z) = 1 / (1 + e^(-z))

        Rango: (0, 1)
        Uso: Capa de salida para clasificaciÃ³n binaria
        Problema: Vanishing gradient para |z| grande
        """
        z = np.clip(z, -500, 500)  # Previene overflow en exp() con valores extremos
        return 1 / (1 + np.exp(-z))  # FÃ³rmula matemÃ¡tica de la sigmoide

    @staticmethod  # MÃ©todo estÃ¡tico: la derivada depende solo de la activaciÃ³n `a` ya calculada
    def sigmoid_derivative(a: np.ndarray) -> np.ndarray:  # Derivada de sigmoide: usada en backprop para propagar gradientes
        """Ïƒ'(z) = Ïƒ(z) Â· (1 - Ïƒ(z)) = a Â· (1 - a)"""  # Docstring: recuerda identidad; no afecta el valor devuelto
        return a * (1 - a)  # Derivada simplificada usando salida ya calculada

    @staticmethod  # MÃ©todo estÃ¡tico: ReLU no requiere estado interno
    def relu(z: np.ndarray) -> np.ndarray:  # ReLU: activa solo valores positivos; es estÃ¡ndar en capas ocultas
        """
        ReLU: f(z) = max(0, z)

        Rango: [0, âˆ)
        Uso: Capas ocultas (default moderno)
        Ventaja: No vanishing gradient para z > 0
        Problema: "Dying ReLU" si z < 0 siempre
        """
        return np.maximum(0, z)  # ImplementaciÃ³n directa de ReLU

    @staticmethod  # MÃ©todo estÃ¡tico: derivada depende de z (pre-activaciÃ³n) para crear la mÃ¡scara
    def relu_derivative(z: np.ndarray) -> np.ndarray:  # Derivada de ReLU: 1 en z>0, 0 en z<=0 (define dÃ³nde fluye el gradiente)
        """ReLU'(z) = 1 si z > 0, 0 si z â‰¤ 0"""  # Docstring: especifica la regla de la derivada; ayuda a depuraciÃ³n
        return (z > 0).astype(float)  # Convierte booleano a float (1.0 o 0.0)

    @staticmethod  # MÃ©todo estÃ¡tico: tanh tampoco requiere estado
    def tanh(z: np.ndarray) -> np.ndarray:  # Tanh: alternativa centrada en 0; puede usarse en capas ocultas
        """
        Tanh: f(z) = (e^z - e^(-z)) / (e^z + e^(-z))

        Rango: (-1, 1)
        Uso: Alternativa a sigmoid (centrado en 0)
        """
        return np.tanh(z)  # Usa implementaciÃ³n NumPy optimizada

    @staticmethod  # MÃ©todo estÃ¡tico: derivada depende de la salida `a=tanh(z)` para evitar recomputar tanh
    def tanh_derivative(a: np.ndarray) -> np.ndarray:  # Derivada de tanh: usada en backprop; decrece cerca de saturaciÃ³n
        """tanh'(z) = 1 - tanhÂ²(z) = 1 - aÂ²"""  # Docstring: identidad matemÃ¡tica base para el cÃ¡lculo
        return 1 - a ** 2  # Derivada usando identidad matemÃ¡tica

    @staticmethod  # MÃ©todo estÃ¡tico: softmax opera por fila/eje y no requiere estado
    def softmax(z: np.ndarray) -> np.ndarray:  # Softmax: convierte logits en distribuciÃ³n de probabilidad multiclase (suma 1)
        """
        Softmax: softmax(z)áµ¢ = e^(záµ¢) / Î£â±¼ e^(zâ±¼)

        Rango: (0, 1), suma = 1
        Uso: Capa de salida para clasificaciÃ³n multiclase
        Output: probabilidades de cada clase
        """
        # Restar mÃ¡ximo para estabilidad numÃ©rica (previene overflow en exp)
        z_shifted = z - np.max(z, axis=-1, keepdims=True)  # Centra logits restando el mÃ¡ximo por fila: no cambia softmax y evita overflow
        exp_z = np.exp(z_shifted)  # Calcula exponenciales de valores estabilizados
        return exp_z / np.sum(exp_z, axis=-1, keepdims=True)  # Normaliza para que suma = 1


# Demo de funciones de activaciÃ³n
z = np.array([-2, -1, 0, 1, 2])  # Valores de prueba
act = Activations()  # Instancia clase de activaciones

print("z:", z)  # Muestra valores originales
print("sigmoid:", act.sigmoid(z))  # Muestra sigmoid aplicada
print("relu:", act.relu(z))  # Muestra ReLU aplicada
print("tanh:", act.tanh(z))  # Muestra tanh aplicada
print("softmax:", act.softmax(z))  # Muestra softmax aplicada
```

### 1.3 El Problema XOR

```python
"""
XOR: La limitaciÃ³n del PerceptrÃ³n Simple

XOR truth table:
    x1  x2  |  y
    0   0   |  0
    0   1   |  1
    1   0   |  1
    1   1   |  0

Un perceptrÃ³n simple NO puede resolver XOR porque:
- XOR no es linealmente separable
- No existe una lÃ­nea que separe las clases

SoluciÃ³n: Red multicapa (MLP)
- Una capa oculta puede aprender features intermedias
- CombinaciÃ³n de features no lineales resuelve XOR
"""

# Datos XOR - problema clÃ¡sico no linealmente separable
X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Entradas binarias
y_xor = np.array([0, 1, 1, 0])  # Salidas XOR

# Un perceptrÃ³n simple no puede aprender esto
# Necesitamos una red con al menos una capa oculta
```

---

## ğŸ’» Parte 2: Forward Propagation

### 2.1 Arquitectura MLP

```python
"""
MLP - Multilayer Perceptron

Arquitectura tÃ­pica:
    Input Layer â†’ Hidden Layer(s) â†’ Output Layer

Ejemplo para clasificaciÃ³n binaria:
    x (n_features) â†’ h (n_hidden) â†’ y (1)

Forward Pass:
    zâ‚ = Wâ‚x + bâ‚        (capa 1: lineal)
    aâ‚ = Ïƒ(zâ‚)           (capa 1: activaciÃ³n)
    zâ‚‚ = Wâ‚‚aâ‚ + bâ‚‚       (capa 2: lineal)
    aâ‚‚ = Ïƒ(zâ‚‚)           (capa 2: activaciÃ³n = output)

Dimensiones:
    x: (n_features,)
    Wâ‚: (n_hidden, n_features)
    bâ‚: (n_hidden,)
    zâ‚, aâ‚: (n_hidden,)
    Wâ‚‚: (n_output, n_hidden)
    bâ‚‚: (n_output,)
    zâ‚‚, aâ‚‚: (n_output,)
"""
```

### 2.2 ImplementaciÃ³n Forward Pass

```python
import numpy as np  # Importa NumPy: operaciones vectorizadas y funciones matemÃ¡ticas para el forward pass
from typing import List, Dict  # Importa tipos: documenta estructuras (lista de capas, cache) sin afectar runtime

class Layer:  # Capa densa: implementa forward (z=Wx+b) y aplica una activaciÃ³n
    """Una capa de la red neuronal."""  # Docstring de clase: documenta responsabilidad; se ejecuta como literal de string

    def __init__(self, input_size: int, output_size: int, activation: str = 'relu'):  # Inicializa parÃ¡metros y tipo de activaciÃ³n
        """
        Args:
            input_size: nÃºmero de entradas
            output_size: nÃºmero de neuronas
            activation: 'relu', 'sigmoid', 'tanh', 'softmax', 'linear'
        """
        self.input_size = input_size  # Guarda dimensiÃ³n de entrada: Ãºtil para entender shapes y depurar
        self.output_size = output_size  # Guarda dimensiÃ³n de salida: nÃºmero de neuronas de la capa
        self.activation = activation  # Guarda activaciÃ³n: controla la no linealidad aplicada en forward

        # InicializaciÃ³n Xavier/He
        if activation == 'relu':  # Selecciona He init para ReLU (varianza estable para activaciones)
            # He initialization para ReLU
            std = np.sqrt(2.0 / input_size)  # Std He: sqrt(2/fan_in)
        else:  # Para activaciones suaves (tanh/sigmoid) suele usarse Xavier para evitar saturaciÃ³n
            # Xavier initialization
            std = np.sqrt(1.0 / input_size)  # Std Xavier simplificado: sqrt(1/fan_in)

        self.W = np.random.randn(output_size, input_size) * std  # Pesos: shape (out,in) con escala de init
        self.b = np.zeros(output_size)  # Bias: vector (out,) inicializado a cero

        # Cache para backprop
        self.cache = {}  # Diccionario de cache: guarda x/z/a del forward para usar luego en backward

    def forward(self, x: np.ndarray) -> np.ndarray:  # Forward: computa activaciÃ³n de la capa para un input
        """
        Forward pass de una capa.

        z = Wx + b
        a = activation(z)
        """
        self.cache['x'] = x  # Guarda input: necesario para gradientes de W en backprop

        # TransformaciÃ³n lineal
        z = self.W @ x + self.b  # Pre-activaciÃ³n: (out,in)@(in,) + (out,) => (out,)
        self.cache['z'] = z  # Guarda z: Ãºtil para derivadas (ReLU) y depuraciÃ³n

        # ActivaciÃ³n
        if self.activation == 'relu':  # ReLU: comÃºn en capas ocultas
            a = np.maximum(0, z)  # max(0,z) elemento a elemento
        elif self.activation == 'sigmoid':  # Sigmoid: Ãºtil como salida binaria
            a = 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Sigmoid estable: clip evita overflow
        elif self.activation == 'tanh':  # tanh: activaciÃ³n centrada en 0
            a = np.tanh(z)  # Aplica tanh elemento a elemento
        elif self.activation == 'softmax':  # Softmax: salida multiclase
            z_shifted = z - np.max(z)  # EstabilizaciÃ³n: resta mÃ¡ximo para prevenir overflow
            exp_z = np.exp(z_shifted)  # Exponencia logits estabilizados
            a = exp_z / np.sum(exp_z)  # Normaliza para obtener probabilidades que suman 1
        else:  # linear
            a = z  # Identidad: sin no linealidad

        self.cache['a'] = a  # Guarda activaciÃ³n: Ãºtil para derivadas (sigmoid/tanh) y capa siguiente
        return a  # Devuelve la salida de la capa


class NeuralNetwork:  # Red multicapa: compone varias Layer y realiza forward secuencial
    """Red Neuronal Multicapa."""  # Docstring de clase: describe el contenedor de capas; no afecta el resultado del forward

    def __init__(self, layer_sizes: List[int], activations: List[str]):  # Construye la red a partir de tamaÃ±os y activaciones
        """
        Args:
            layer_sizes: [input_size, hidden1, hidden2, ..., output_size]
            activations: ['relu', 'relu', ..., 'sigmoid'] para cada capa
        """
        assert len(activations) == len(layer_sizes) - 1  # Invariante: una activaciÃ³n por capa (excepto input)

        self.layers = []  # Lista de capas en orden: output de una alimenta la siguiente
        for i in range(len(layer_sizes) - 1):  # Itera pares consecutivos (in->out)
            layer = Layer(layer_sizes[i], layer_sizes[i+1], activations[i])  # Crea capa i con su activaciÃ³n
            self.layers.append(layer)  # Agrega la capa a la red

    def forward(self, x: np.ndarray) -> np.ndarray:  # Forward de la red: propaga la entrada por todas las capas
        """Forward pass a travÃ©s de todas las capas."""  # Docstring de mÃ©todo: describe la funciÃ³n; es una cadena literal en runtime
        a = x  # ActivaciÃ³n inicial: la entrada del modelo
        for layer in self.layers:  # Recorre capas en orden forward
            a = layer.forward(a)  # Propaga activaciÃ³n a travÃ©s de la capa
        return a  # Devuelve salida final

    def predict(self, X: np.ndarray) -> np.ndarray:  # PredicciÃ³n batch: aplica forward y convierte a clases
        """PredicciÃ³n para mÃºltiples muestras."""  # Docstring de mÃ©todo: explica uso de predict en batch
        predictions = []  # Acumula predicciones por muestra
        for x in X:  # Itera muestras del batch
            output = self.forward(x)  # Forward por muestra
            if len(output) == 1:  # Caso binario: una sola salida
                predictions.append(1 if output[0] > 0.5 else 0)  # Umbral 0.5 para sigmoid
            else:  # Caso multiclase: vector de scores/probabilidades
                predictions.append(np.argmax(output))  # Selecciona Ã­ndice del mÃ¡ximo
        return np.array(predictions)  # Devuelve ndarray para usar en mÃ©tricas


# Demo
net = NeuralNetwork(  # Instancia red de demostraciÃ³n (sin entrenamiento) para probar el forward
    layer_sizes=[2, 4, 1],  # 2 inputs â†’ 4 hidden â†’ 1 output
    activations=['relu', 'sigmoid']  # ReLU en oculta, sigmoid en salida
)  # Cierra construcciÃ³n de la red demo

# Forward pass
x = np.array([0.5, 0.3])  # Input de ejemplo: vector 2D
output = net.forward(x)  # Ejecuta forward: salida depende de pesos aleatorios
print(f"Input: {x}")  # Imprime input para referencia
print(f"Output: {output}")  # Imprime output de la red (sin entrenar)
```

---

## ğŸ’» Parte 3: Backpropagation

### 3.0 Backpropagation â€” Nivel: intermedio/avanzado

**PropÃ³sito:** este bloque te lleva de â€œsÃ© que backprop existeâ€ a **poder derivarlo, implementarlo y depurarlo** bajo condiciones tipo examen.

#### Objetivos de aprendizaje (medibles)

Al terminar este bloque podrÃ¡s:

- **Recordar** la notaciÃ³n estÃ¡ndar de una capa (`z = Wx + b`, `a = Ï†(z)`) y el rol de cada variable.
- **Explicar** por quÃ© backprop es simplemente *regla de la cadena aplicada a un grafo computacional*.
- **Aplicar** backprop para calcular `âˆ‚L/âˆ‚W` y `âˆ‚L/âˆ‚b` en una red MLP de 2 capas.
- **Analizar** fallas tÃ­picas (signos, shapes, overflow) usando pruebas de sanidad.
- **Evaluar** si tu implementaciÃ³n es correcta con un *overfit test* y (cuando aplique) *gradient checking*.
- **Crear** una implementaciÃ³n mÃ­nima (NumPy) de forward + backward y entrenarla en un toy dataset.

#### MotivaciÃ³n / por quÃ© importa

Backpropagation es el mecanismo que hace posible que redes con millones de parÃ¡metros se ajusten a datos. En prÃ¡ctica:

- **VisiÃ³n (CV):** CNNs y modelos de clasificaciÃ³n/segmentaciÃ³n se entrenan con backprop.
- **NLP:** aunque los Transformers no se implementan aquÃ­, el entrenamiento sigue siendo backprop sobre un grafo computacional.
- **Industria:** cuando un entrenamiento â€œno aprendeâ€, casi siempre el diagnÃ³stico comienza revisando gradientes, estabilidad numÃ©rica y shapes.

#### Prerrequisitos y nivel de entrada

- **CÃ¡lculo:** derivadas, derivadas parciales, regla de la cadena.
- **Ãlgebra lineal:** multiplicaciÃ³n matriz-vector, transpuesta.
- **Probabilidad / pÃ©rdidas:** cross-entropy como pÃ©rdida para clasificaciÃ³n.

Mini-recordatorio (enlaces directos):

- [GLOSARIO: Chain Rule](GLOSARIO.md#chain-rule)
- [GLOSARIO: Gradient](GLOSARIO.md#gradient)
- [GLOSARIO: Backpropagation](GLOSARIO.md#backpropagation)
- [GLOSARIO: Binary Cross-Entropy](GLOSARIO.md#binary-cross-entropy)

#### Resumen ejecutivo (big idea)

Backpropagation calcula gradientes **de manera eficiente** reutilizando resultados intermedios del forward pass. En vez de derivar a mano una expresiÃ³n enorme, modelas el cÃ¡lculo como un **grafo** de operaciones simples (sumas, productos, activaciones). Luego aplicas la regla de la cadena localmente y propagas â€œresponsabilidad del errorâ€ desde la salida hasta los parÃ¡metros.

La idea operacional es:

- Haces un **forward pass** guardando `x`, `z`, `a` de cada capa.
- Calculas la pÃ©rdida `L`.
- Empiezas en la salida con un gradiente inicial y haces un **backward pass** capa por capa:
  - `Î´ = âˆ‚L/âˆ‚z` (el â€œerrorâ€ local)
  - `âˆ‚L/âˆ‚W = Î´ âŠ— x` y `âˆ‚L/âˆ‚b = Î´`
  - propagas hacia atrÃ¡s: `âˆ‚L/âˆ‚x = Wáµ€ Î´`

#### VisualizaciÃ³n crÃ­tica: el grafo computacional de Backprop (hacer clic mental)

Para entender backprop, no mires fÃ³rmulas planas: mira el grafo.

Una neurona simple:

`L(a)  â†  a = Ïƒ(z)  â†  z = wÂ·x + b`

El gradiente fluye rÃ­o arriba (de derecha a izquierda):

1) **Llegada del error:** recibes `âˆ‚L/âˆ‚a`.
2) **Compuerta sigmoide:** multiplicas por la derivada local `Ïƒ'(z)`.
3) **SeÃ±al en z:**

`Î´ = âˆ‚L/âˆ‚z = (âˆ‚L/âˆ‚a) Â· Ïƒ'(z)`

4) **BifurcaciÃ³n lineal (`z = wÂ·x + b`):**

- Hacia `w`: `âˆ‚L/âˆ‚w = Î´ Â· x`
- Hacia `b`: `âˆ‚L/âˆ‚b = Î´`
- Hacia `x`: `âˆ‚L/âˆ‚x = Î´ Â· w`

Regla mnemotÃ©cnica:

- Gradiente del **peso** = **error local (`Î´`) Ã— entrada (`x`)**
- Gradiente hacia atrÃ¡s = **error local (`Î´`) Ã— peso (`w`)**

#### Mapa del contenido y tiempo estimado

- **IntuiciÃ³n + vocabulario:** 20â€“35 min
- **FormalizaciÃ³n (notaciÃ³n + shapes):** 30â€“45 min
- **DerivaciÃ³n guiada (2 capas):** 45â€“75 min
- **Worked example numÃ©rico (paso a paso):** 45â€“60 min
- **ImplementaciÃ³n prÃ¡ctica (NumPy) + pruebas de sanidad:** 2â€“4 h

#### NÃºcleo: explicaciÃ³n progresiva por capas

##### a) IntuiciÃ³n / metÃ¡fora

Piensa en una red como una fÃ¡brica con varias estaciones. La salida estÃ¡ mal (pÃ©rdida alta) y quieres saber **cuÃ¡nto contribuyÃ³ cada perilla** (peso) al error. Backprop es un procedimiento para *repartir la culpa* desde el error final hacia atrÃ¡s, estaciÃ³n por estaciÃ³n.

##### b) Conceptos clave (glosario mÃ­nimo)

- **Forward pass:** computar `z` y `a` desde la entrada hasta la salida.
- **Loss `L`:** nÃºmero que mide â€œquÃ© tan malâ€ predice el modelo.
- **Gradiente:** vector de derivadas que indica cÃ³mo cambia `L` si mueves parÃ¡metros.
- **Delta `Î´`:** gradiente local `âˆ‚L/âˆ‚z` en una capa (la seÃ±al que se propaga hacia atrÃ¡s).

##### c) FormalizaciÃ³n (fÃ³rmulas + shapes)

Para una capa totalmente conectada:

- `z = Wx + b`
- `a = Ï†(z)`

Shapes recomendados (para evitar errores silenciosos):

- `x`: `(n_in,)` o `(n_in, 1)`
- `W`: `(n_out, n_in)`
- `b`: `(n_out,)` o `(n_out, 1)`
- `z, a`: `(n_out,)` o `(n_out, 1)`

##### d) DemostraciÃ³n / derivaciÃ³n (idea central)

En cada capa usas regla de la cadena:

- `âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚W`
- `âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚b`
- `âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚x`

Y como `z = Wx + b`:

- `âˆ‚z/âˆ‚W` depende de `x`
- `âˆ‚z/âˆ‚b = 1`
- `âˆ‚z/âˆ‚x = W`

Esto produce el patrÃ³n computacional:

```
dL_da  â†’  (multiplicar por Ï†'(z))  â†’  Î´ = dL_dz
                       â”‚
                       â”œâ”€â”€ dL_dW = Î´ âŠ— x
                       â”œâ”€â”€ dL_db = Î´
                       â””â”€â”€ dL_dx = Wáµ€ Î´
```

##### e) Ejemplo resuelto (worked example) paso a paso

Objetivo del ejemplo: una red **2-2-1** (2 entradas, 2 ocultas, 1 salida) con sigmoid en salida para clasificaciÃ³n binaria. El entregable es poder escribir:

- forward: `z1, a1, z2, a2`
- backward: `Î´2, dW2, db2, Î´1, dW1, db1`

GuÃ­a de trabajo (sin nÃºmeros para que puedas rellenar tÃº):

1. **Forward**
   - `z1 = W1 x + b1`
   - `a1 = Ï†(z1)`
   - `z2 = W2 a1 + b2`
   - `a2 = Ïƒ(z2)`
2. **Loss**
   - `L = BCE(y, a2)`
3. **Backward**
   - Para sigmoid + BCE (caso tÃ­pico): `Î´2 = a2 - y`
   - `dW2 = Î´2 âŠ— a1`
   - `db2 = Î´2`
   - `Î´1 = (W2áµ€ Î´2) âŠ™ Ï†'(z1)`
   - `dW1 = Î´1 âŠ— x`
   - `db1 = Î´1`

Ejemplo numÃ©rico completo (forward y backward, con nÃºmeros):

Definimos:

- Entrada: `x = [1.0, -2.0]`
- Etiqueta: `y = 1`
- ActivaciÃ³n oculta: `Ï† = ReLU`
- ActivaciÃ³n salida: `Ïƒ` (sigmoid)

ParÃ¡metros:

- `W1 = [[0.1, -0.2], [0.4, 0.3]]`, `b1 = [0.0, 0.1]`
- `W2 = [[-0.3, 0.2]]`, `b2 = [0.05]`

1) Forward

- `z1 = W1x + b1`
  - `z1_1 = 0.1Â·1 + (-0.2)Â·(-2) + 0.0 = 0.5`
  - `z1_2 = 0.4Â·1 + 0.3Â·(-2) + 0.1 = -0.1`
  - `z1 = [0.5, -0.1]`
- `a1 = ReLU(z1) = [0.5, 0.0]`

- `z2 = W2a1 + b2 = (-0.3)Â·0.5 + 0.2Â·0.0 + 0.05 = -0.10`
- `a2 = Ïƒ(z2) â‰ˆ 0.4750`

2) Loss (Binary Cross-Entropy)

- `L = -log(a2) â‰ˆ -log(0.4750) â‰ˆ 0.744`

3) Backward

- Para sigmoid + BCE: `Î´2 = a2 - y â‰ˆ 0.4750 - 1 = -0.5250`

- Gradientes en salida:
  - `dW2 = Î´2 âŠ— a1 = [-0.5250Â·0.5, -0.5250Â·0.0] â‰ˆ [-0.2625, 0.0]`
  - `db2 = Î´2 â‰ˆ -0.5250`

- PropagaciÃ³n a la capa oculta:
  - `dL/da1 = W2áµ€ Î´2 = [-0.3, 0.2]áµ€ Â· (-0.5250) â‰ˆ [0.1575, -0.1050]`
  - `ReLU'(z1) = [1, 0]` (porque `z1_1>0` y `z1_2<0`)
  - `Î´1 = dL/da1 âŠ™ ReLU'(z1) â‰ˆ [0.1575, 0.0]`

- Gradientes en primera capa:
  - `dW1 = Î´1 âŠ— x`
    - para neurona 1: `[0.1575Â·1.0, 0.1575Â·(-2.0)] â‰ˆ [0.1575, -0.3150]`
    - para neurona 2: `[0, 0]`
  - `db1 = Î´1 â‰ˆ [0.1575, 0.0]`

Chequeo mental:

- Los gradientes â€œse apaganâ€ donde `ReLU'(z)=0`.
- `dW` siempre tiene la misma shape que `W`.

#### Protocolo (Semana 18): grafo computacional + shapes explÃ­citos (antes de programar `backward()`)

Antes de escribir cualquier `backward()`, fija dos cosas:

- **Tu grafo computacional** (quÃ© nodos existen y quiÃ©n depende de quiÃ©n).
- **Tus shapes** (para que cada gradiente tenga una shape Ãºnica y verificable).

##### 1) Elige una convenciÃ³n y no la mezcles (recomendado: batch-first 2D)

- `X`: `(n, d_in)`
- `W`: `(d_in, d_out)`
- `b`: `(d_out,)` (se â€œbroadcasteaâ€ a `(n, d_out)`)
- `Z = XW + b`: `(n, d_out)`
- Activaciones `A`: `(n, d_out)`

Evita mezclar `(d,)` y `(d,1)` a menos que decidas usar columna-vectores en TODO.

##### 2) Red de 2 capas: shapes del forward que debes poder escribir de memoria

Red (batch):

- `Z1 = XW1 + b1`, `A1 = relu(Z1)`
- `Z2 = A1W2 + b2`, `P = sigmoid(Z2)`

Tabla de shapes:

| SÃ­mbolo | Significado | Shape |
|---|---|---|
| `X` | batch de entrada | `(n, d_in)` |
| `W1` | pesos capa 1 | `(d_in, d_h)` |
| `b1` | bias capa 1 | `(d_h,)` |
| `Z1`, `A1` | pre/post activaciÃ³n | `(n, d_h)` |
| `W2` | pesos capa 2 | `(d_h, d_out)` |
| `b2` | bias capa 2 | `(d_out,)` |
| `Z2`, `P` | logits / probabilidades | `(n, d_out)` |
| `y` | targets | `(n, d_out)` |

##### 3) Invariantes de gradientes (no negociables)

Si `Z = XW + b` con las shapes batch-first:

- `dW` **debe** tener la misma shape que `W`.
- `db` **debe** tener la misma shape que `b`.
- `dX` **debe** tener la misma shape que `X`.

Para la red de 2 capas:

| Gradiente | Shape |
|---|---|
| `dZ2` | `(n, d_out)` |
| `dW2 = A1.T @ dZ2` | `(d_h, d_out)` |
| `db2 = sum(dZ2, axis=0)` | `(d_out,)` |
| `dA1 = dZ2 @ W2.T` | `(n, d_h)` |
| `dZ1 = dA1 * relu'(Z1)` | `(n, d_h)` |
| `dW1 = X.T @ dZ1` | `(d_in, d_h)` |
| `db1 = sum(dZ1, axis=0)` | `(d_h,)` |

##### 4) Protocolo de depuraciÃ³n (antes de â€œtocar hyperparamsâ€)

- Agrega `assert` de shapes.
- Haz **gradient checking** en 1â€“3 coordenadas.
- Haz un **overfit test** en un dataset mini: si no memoriza, es bug.

##### 4.1 CÃ¡psula: Shape checks (decorator + asserts)

Regla prÃ¡ctica: si una funciÃ³n consume tensores/arrays, valida **shapes** al inicio (y, si aplica, valida la salida). Esto reduce bugs silenciosos en `forward()`/`backward()`.

```python
import numpy as np  # Importa NumPy: provee np.asarray, generaciÃ³n de datos aleatorios y operaciones vectorizadas usadas en el ejemplo
from typing import Any, Callable, Dict, Optional, Sequence, Tuple  # Importa tipos: se usan para anotar shapes/firmas del decorator (solo documentaciÃ³n/IDE; no cambia el runtime)

def assert_shape(x: np.ndarray, shape: Sequence[Optional[int]], name: str = "x") -> np.ndarray:  # Valida que `x` tenga la dimensionalidad/shape esperada (con `None` como comodÃ­n)
    x = np.asarray(x)  # Fuerza conversiÃ³n a ndarray: normaliza inputs (listas/tuplas) y garantiza que `ndim/shape` existan
    assert x.ndim == len(shape), f"{name}.ndim={x.ndim}, expected={len(shape)}"  # Verifica #dims: si falla, se detiene con AssertionError explicando el mismatch
    for i, (got, exp) in enumerate(zip(x.shape, shape)):  # Itera por dimensiÃ³n i: compara la shape real vs la esperada dimensiÃ³n-a-dimensiÃ³n
        if exp is not None:  # `None` significa â€œno validar esta dimensiÃ³nâ€ (Ãºtil para batch variable)
            assert got == exp, f"{name}.shape[{i}]={got}, expected={exp}"  # Verifica dimensiÃ³n i: si falla, se corta temprano evitando bugs silenciosos de broadcasting
    return x  # Devuelve el mismo array (ya normalizado): permite encadenar validaciÃ³n dentro de pipelines/funciones

def shape_check(  # Factory de decorator: construye un wrapper que valida shapes de kwargs (y opcionalmente la salida)
    spec: Dict[str, Sequence[Optional[int]]],  # EspecificaciÃ³n: mapping nombre_argumento -> shape esperada (con None como comodÃ­n)
    out: Optional[Sequence[Optional[int]]] = None,  # Shape esperada de la salida (si se pasa): Ãºtil para validar invariantes post-forward
) -> Callable[[Callable[..., Any]], Callable[..., Any]]:  # Devuelve un decorator que, al aplicarse, produce una funciÃ³n wrapped con asserts
    def deco(fn: Callable[..., Any]) -> Callable[..., Any]:  # Recibe la funciÃ³n objetivo y devuelve una versiÃ³n instrumentada con checks
        def wrapper(*args: Any, **kwargs: Any) -> Any:  # Wrapper: intercepta llamada para validar inputs/salida sin modificar la lÃ³gica interna
            for k, shp in spec.items():  # Recorre las claves especificadas en `spec` (solo valida las entradas que se esperan)
                if k in kwargs:  # Valida Ãºnicamente si el argumento fue pasado por keyword (este helper estÃ¡ diseÃ±ado para kwargs)
                    assert_shape(kwargs[k], shp, name=k)  # Aplica assert_shape al argumento: si no coincide, falla antes de ejecutar el cÃ¡lculo
            y = fn(*args, **kwargs)  # Ejecuta la funciÃ³n original con los mismos args/kwargs: no altera el resultado, solo lo captura
            if out is not None:  # Si se definiÃ³ un shape esperado de salida, habilita chequeo posterior
                assert_shape(y, out, name="out")  # Verifica shape del output: detecta errores de dimensiones inmediatamente tras el forward
            return y  # Devuelve el resultado original: el wrapper es transparente salvo por los asserts

        return wrapper  # Retorna la funciÃ³n decorada: es la que reemplazarÃ¡ a `fn` en tiempo de import/definiciÃ³n

    return deco  # Retorna el decorator configurado con `spec/out`: permite reutilizar la misma regla en mÃºltiples funciones

@shape_check({"X": (None, 3), "W": (3, 4), "b": (4,)}, out=(None, 4))  # Valida shapes de inputs/outputs: detecta bugs de dimensiones antes de que propaguen
def dense_forward(X: np.ndarray, W: np.ndarray, b: np.ndarray) -> np.ndarray:  # Forward de capa densa: aplica transformaciÃ³n afÃ­n Z = XW + b (batch-first)
    return X @ W + b  # Multiplica (n,3)@(3,4)->(n,4) y suma bias (4,) por broadcasting: produce logits/activaciones pre-no-lineales

X = np.random.randn(5, 3)  # Crea un batch de 5 muestras con 3 features: ejemplo que cumple la spec (None,3)
W = np.random.randn(3, 4)  # Crea matriz de pesos (3->4): compatible con X para producto matricial
b = np.random.randn(4)  # Crea bias de salida (4,): se sumarÃ¡ a cada fila del batch vÃ­a broadcasting
Z = dense_forward(X=X, W=W, b=b)  # Ejecuta forward validado por decorator: asserts corren antes/despuÃ©s y luego se calcula Z
assert Z.shape == (5, 4)  # Sanity check final: confirma que la salida cumple la shape esperada (batch=5, d_out=4)
```

##### 4.2 CÃ¡psula: InicializaciÃ³n (Xavier vs He/Kaiming)

Regla prÃ¡ctica (MLP):

- Activaciones tipo `tanh/sigmoid` suelen ir mejor con **Xavier/Glorot**.
- Activaciones tipo `ReLU` suelen ir mejor con **He/Kaiming**.

```python
import numpy as np  # Importa NumPy: se usa para RNG, sqrt y generar matrices de pesos con distribuciÃ³n normal
from typing import Literal, Optional  # Importa tipos: restringe `mode` a valores vÃ¡lidos y hace `seed` opcional (anotaciones)

def init_linear(  # Inicializa pesos de una capa lineal controlando la varianza segÃºn la activaciÃ³n (Xavier vs He/Kaiming)
    fan_in: int,  # NÃºmero de unidades de entrada: determina la escala recomendada de inicializaciÃ³n para evitar exploding/vanishing
    fan_out: int,  # NÃºmero de unidades de salida: determina la shape final de W (fan_in, fan_out)
    mode: Literal["xavier", "kaiming"] = "xavier",  # Selecciona esquema: Xavier (tanh/sigmoid) o Kaiming (ReLU)
    seed: Optional[int] = None,  # Semilla opcional: si se pasa, la inicializaciÃ³n serÃ¡ reproducible
) -> np.ndarray:  # Devuelve matriz de pesos W con shape (fan_in, fan_out)
    rng = np.random.default_rng(seed)  # Crea generador RNG moderno: evita global state de np.random y permite reproducibilidad por seed

    if mode == "kaiming":  # He/Kaiming: recomendado para ReLU porque mantiene varianza al pasar por la no-linealidad rectificada
        std = np.sqrt(2.0 / fan_in)  # Desv. estÃ¡ndar: sqrt(2/fan_in) para compensar que ReLU â€œapagaâ€ ~mitad de activaciones
    else:  # Xavier/Glorot: recomendado para tanh/sigmoid (mÃ¡s simÃ©tricas), busca preservar varianza entre capas
        std = np.sqrt(1.0 / fan_in)  # Desv. estÃ¡ndar: sqrt(1/fan_in) (forma simplificada) para mantener escala estable en forward/backward

    W = rng.standard_normal((fan_in, fan_out)) * std  # Muestra N(0,1) y escala por std: produce pesos con varianza controlada
    return W  # Devuelve pesos: se usarÃ¡n en la capa lineal; una mala escala puede causar saturaciÃ³n o gradientes inestables

d_in, d_out = 784, 128  # Dimensiones ejemplo (MNIST->capa oculta): 784 entradas (28x28) y 128 unidades de salida
W_relu = init_linear(d_in, d_out, mode="kaiming", seed=0)  # Inicializa pesos para red con ReLU: usa He/Kaiming
W_tanh = init_linear(d_in, d_out, mode="xavier", seed=0)  # Inicializa pesos para red con tanh/sigmoid: usa Xavier/Glorot
assert W_relu.shape == (d_in, d_out)  # Sanity check: verifica que la shape de W coincide con (fan_in, fan_out)
assert W_tanh.shape == (d_in, d_out)  # Sanity check: confirma lo mismo para la inicializaciÃ³n Xavier
```

##### f) ImplementaciÃ³n prÃ¡ctica (laboratorio)

Checklist mÃ­nimo de implementaciÃ³n (sin â€œmagiaâ€):

- una clase/capa que guarde `x`, `z`, `a` en cache
- un `backward()` que devuelva `dL_dx`, `dL_dW`, `dL_db`
- un training loop que muestre una curva de pÃ©rdida descendente

Protocolos de ejecuciÃ³n (integraciÃ³n v4/v5):

- **v4.0 (Semana 18):** antes de programar, completar `study_tools/DRYRUN_BACKPROPAGATION.md`.
- **v5.0 (validaciÃ³n):** si el entrenamiento no converge, hacer:
  - *Overfit on small batch* (este mÃ³dulo ya lo incluye mÃ¡s abajo).
  - si el error persiste, revisar *gradient checking* (ver checklist general en `CHECKLIST.md`).

##### g) Variantes, limitaciones y casos frontera

- **Softmax + Cross-Entropy:** el gradiente de salida tambiÃ©n se simplifica a `y_pred - y_true` (cuando `y_true` es one-hot).
- **Sigmoid en capas ocultas:** riesgo de *vanishing gradients* si `|z|` crece.
- **ReLU:** riesgo de *dying ReLU* (neurona que queda en 0 siempre).
- **Estabilidad numÃ©rica:** usar `clip`, restar `max(z)` en softmax, y `eps` en logs.

#### Visuales (para estudiar y recordar)

Grafo computacional mÃ­nimo (una capa):

```
x â”€â”€â–º (Wx + b) â”€â”€â–º z â”€â”€â–º Ï†(z) â”€â”€â–º a â”€â”€â–º L
         â–²                    â–²
         â”‚                    â”‚
         W,b                  Ï†'
```

#### Diagrama de flujo: forward (verde) / backward (rojo)

```
FORWARD (verde)
x â†’ z1=W1x+b1 â†’ a1=Ï†(z1) â†’ z2=W2a1+b2 â†’ a2 â†’ L

BACKWARD (rojo)
L â†’ dL/da2 â†’ Î´2=dL/dz2 â†’ dW2,db2 â†’ Î´1 â†’ dW1,db1
```

Regla prÃ¡ctica para implementarlo:

- **Forward:** guarda en cache `x, z, a` por capa.
- **Backward:** empieza por el Ãºltimo `Î´` y propaga hacia atrÃ¡s con `Wáµ€`.

#### Fallas tÃ­picas (con visual): Vanishing Gradient vs Dying ReLU

**1) Vanishing gradient (sigmoid/tanh en capas ocultas)**

Si `|z|` es grande, `Ïƒ(z)` se satura y `Ïƒ'(z) â‰ˆ 0`.
En backprop, multiplicas muchas derivadas pequeÃ±as:

```
Î´1 = (W2áµ€ Î´2) âŠ™ Ï†'(z1)
Î´0 = (W1áµ€ Î´1) âŠ™ Ï†'(z0)
...

si Ï†'(z) â‰ˆ 0 en varias capas â†’ Î´ se vuelve ~0
```

SÃ­ntomas:

- loss baja muy lento
- pesos de capas tempranas casi no cambian

MitigaciÃ³n (en este nivel):

- usa ReLU en ocultas (o inicializaciones cuidadosas)
- normaliza features

**2) Dying ReLU**

ReLU: `Ï†(z)=max(0,z)` y `Ï†'(z)=0` si `z<0`.

Si una neurona queda siempre con `z<0`, su gradiente se vuelve 0 y â€œmuereâ€:

```
z < 0  â†’  a = 0
Ï†'(z)=0  â†’  Î´ = Î´_next âŠ™ 0 = 0
```

SÃ­ntomas:

- muchas activaciones exactamente 0
- algunas neuronas nunca â€œrevivenâ€

MitigaciÃ³n (en este nivel):

- baja learning rate
- inicializa pesos con escalas razonables
- considera LeakyReLU (conceptual)

#### Actividades activas (aprendizaje activo)

- **Retrieval practice (5â€“10 min):** sin mirar notas, escribe las 6 ecuaciones: `Î´2`, `dW2`, `db2`, `Î´1`, `dW1`, `db1`.
- **Interleaving:** alterna ejercicios de backprop con ejercicios de shapes (recomendado: `study_tools/DRILL_DIMENSIONES_NUMPY.md`).
- **GeneraciÃ³n:** crea tu propio mini-ejemplo con una red 3-3-1 y verifica a mano una iteraciÃ³n.

#### EvaluaciÃ³n (formativa y sumativa)

- **Quiz conceptual:**
  - Â¿QuÃ© representa `Î´` y por quÃ© es Ãºtil?
  - Â¿Por quÃ© `Î´2 = a2 - y` en sigmoid+BCE?
- **Prueba prÃ¡ctica:** tu red debe:
  - resolver XOR, y
  - pasar el *overfit test* sobre un minibatch.

#### Cheat sheet (repaso rÃ¡pido)

- `z = Wx + b`
- `a = Ï†(z)`
- `Î´ = âˆ‚L/âˆ‚z = (âˆ‚L/âˆ‚a) âŠ™ Ï†'(z)`
- `âˆ‚L/âˆ‚W = Î´ âŠ— x`
- `âˆ‚L/âˆ‚b = Î´`
- `âˆ‚L/âˆ‚x = Wáµ€ Î´`

#### Errores comunes y FAQs

- **(Shapes)** confundir `(n,)` con `(n,1)` y obtener gradientes transpuestos.
- **(Signos)** usar `y - y_pred` en lugar de `y_pred - y` y â€œsubirâ€ la loss.
- **(Softmax)** implementar softmax sin restar el mÃ¡ximo â†’ overflow.
- **(Debug)** si la red no puede memorizar 4 puntos de XOR, *no* es â€œfalta de datosâ€; es un bug.

#### Recursos complementarios (orientados a prÃ¡ctica)

- [RECURSOS.md](RECURSOS.md)
- `study_tools/DRYRUN_BACKPROPAGATION.md`
- `study_tools/EXAMEN_ADMISION_SIMULADO.md`
- [3Blue1Brown: Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)

### ConsolidaciÃ³n (Backpropagation)

#### Errores comunes

- **Shapes mal:** confundir `(n,)` con `(n,1)` y obtener transpuestas inesperadas.
- **Signo del gradiente:** si actualizas con `+ lr * grad`, subes la loss.
- **No cachear:** si no guardas `x, z, a`, terminas recomputando o usando valores incorrectos.
- **ExplosiÃ³n numÃ©rica:** logits grandes â†’ `exp` overflow â†’ `nan`.

#### Debugging / validaciÃ³n (v5)

- **Overfit on small batch:** si no puede memorizar 4 puntos (XOR), asume bug.
- Revisa `nan/inf`:
  - `np.exp` sin `clip`
  - `np.log` sin `eps`
- Registra hallazgos en `study_tools/DIARIO_ERRORES.md`.
- Protocolos completos:
  - [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)
  - [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md)

#### Reto Feynman (tablero blanco)

Explica en 5 lÃ­neas o menos:

1) Â¿QuÃ© es `Î´` y por quÃ© es la seÃ±al que â€œviaja hacia atrÃ¡sâ€?
2) Â¿Por quÃ© `dW = Î´ âŠ— x` tiene sentido dimensionalmente?
3) Â¿CÃ³mo distinguirÃ­as vanishing gradient vs dying ReLU en logs/activaciones?

---

## ğŸ’» Parte 4: Optimizadores

### 4.1 SGD (Stochastic Gradient Descent)

```python
class SGD:  # Optimizador SGD bÃ¡sico: aplica descenso por gradiente con learning rate fijo (sin momentum ni adaptividad)
    """Vanilla Stochastic Gradient Descent."""  # Docstring: describe el algoritmo; es un literal de string y no cambia la actualizaciÃ³n

    def __init__(self, learning_rate: float = 0.01):  # Constructor: almacena el learning rate que se usarÃ¡ en cada paso
        self.lr = learning_rate  # Guarda lr: escala el update; valores extremos causan divergencia o aprendizaje lento

    def update(self, layer, dW: np.ndarray, db: np.ndarray):  # Update in-place de parÃ¡metros del layer usando gradientes dW/db
        layer.W -= self.lr * dW  # Pesos: W <- W - lr*dW (descenso por gradiente)
        layer.b -= self.lr * db  # Bias: b <- b - lr*db
```

### 4.2 SGD con Momentum

```python
class SGDMomentum:  # Define SGD con momentum: mantiene una â€œvelocidadâ€ (EMA del gradiente) para suavizar y acelerar el descenso
    """
    SGD con Momentum.

    v_t = Î²Â·v_{t-1} + (1-Î²)Â·âˆ‡L
    Î¸ = Î¸ - lrÂ·v_t

    Momentum ayuda a:
    - Acelerar convergencia
    - Escapar de mÃ­nimos locales
    - Reducir oscilaciones
    """

    def __init__(self, learning_rate: float = 0.01, momentum: float = 0.9):  # Inicializa hiperparÃ¡metros y el estado (velocidades) por capa
        self.lr = learning_rate  # Guarda learning rate: escala el tamaÃ±o de paso al aplicar la velocidad a los parÃ¡metros
        self.momentum = momentum  # Guarda Î² (momentum): controla cuÃ¡nto del â€œpasadoâ€ se conserva en la velocidad (suavizado)
        self.velocities = {}  # Diccionario layer_id -> {'W': vW, 'b': vb}: buffers persistentes para aplicar momentum por parÃ¡metro

    def update(self, layer, dW: np.ndarray, db: np.ndarray, layer_id: int):  # Aplica un paso de actualizaciÃ³n con momentum a W/b del layer
        if layer_id not in self.velocities:  # Inicializa buffers si es la primera vez que se actualiza este layer (por id estable)
            self.velocities[layer_id] = {  # Crea estructura de velocidad: se mantiene entre iteraciones para acumular gradientes suavizados
                'W': np.zeros_like(dW),  # vW inicial en 0: mismo shape que dW para poder acumular EMA de gradientes de pesos
                'b': np.zeros_like(db)  # vb inicial en 0: mismo shape que db para acumular EMA de gradientes de bias
            }  # Fin de inicializaciÃ³n: si no se hace, el primer update no tendrÃ­a historial y habrÃ­a KeyError

        v = self.velocities[layer_id]  # Recupera referencia a los buffers del layer: se actualizarÃ¡n in-place para persistir entre pasos

        # Actualizar velocidad
        v['W'] = self.momentum * v['W'] + (1 - self.momentum) * dW  # Actualiza velocidad W: EMA del gradiente; reduce oscilaciÃ³n en ravines
        v['b'] = self.momentum * v['b'] + (1 - self.momentum) * db  # Actualiza velocidad b: mismo principio para bias

        # Actualizar parÃ¡metros
        layer.W -= self.lr * v['W']  # Actualiza pesos usando velocidad: paso efectivo incorpora historial (momentum)
        layer.b -= self.lr * v['b']  # Actualiza bias: se mantiene consistente con update de W
```

### 4.3 Adam Optimizer

```python
class Adam:  # Define optimizador Adam: mantiene promedios mÃ³viles (1er y 2do momento) por parÃ¡metro para pasos adaptativos
    """
    Adam: Adaptive Moment Estimation.

    Combina:
    - Momentum (primer momento)
    - RMSprop (segundo momento)

    m_t = Î²â‚Â·m_{t-1} + (1-Î²â‚)Â·g_t       (momentum)
    v_t = Î²â‚‚Â·v_{t-1} + (1-Î²â‚‚)Â·g_tÂ²      (velocidad adaptativa)
    mÌ‚_t = m_t / (1 - Î²â‚^t)              (correcciÃ³n de bias)
    vÌ‚_t = v_t / (1 - Î²â‚‚^t)
    Î¸ = Î¸ - lr Â· mÌ‚_t / (âˆšvÌ‚_t + Îµ)
    """

    def __init__(  # Inicializa hiperparÃ¡metros y estados internos del optimizador (m, v, y contador de paso t)
        self,  # Referencia a la instancia: permite guardar hiperparÃ¡metros y buffers entre actualizaciones
        learning_rate: float = 0.001,  # Paso base (lr): escala la magnitud del update; muy alto puede divergir, muy bajo aprende lento
        beta1: float = 0.9,  # Decaimiento del 1er momento (momentum): controla suavizado de gradientes en `m`
        beta2: float = 0.999,  # Decaimiento del 2do momento (RMS): controla suavizado de gradiente^2 en `v`
        epsilon: float = 1e-8  # TÃ©rmino numÃ©rico: evita divisiÃ³n por cero cuando sqrt(v_hat) es muy pequeÃ±o
    ):  # Cierra firma: al instanciarse una vez, estos valores quedan fijos para todo el entrenamiento
        self.lr = learning_rate  # Guarda lr: se reutiliza en cada update para escalar el paso
        self.beta1 = beta1  # Guarda Î²1: controla cuÃ¡nto â€œrecuerdaâ€ el 1er momento el pasado
        self.beta2 = beta2  # Guarda Î²2: controla cuÃ¡nto â€œrecuerdaâ€ el 2do momento el pasado
        self.epsilon = epsilon  # Guarda Îµ: estabiliza la divisiÃ³n en la regla de actualizaciÃ³n
        self.m = {}  # Diccionario de 1er momento por layer_id: cada entrada guarda arrays para 'W' y 'b'
        self.v = {}  # Diccionario de 2do momento por layer_id: acumula promedio de gradiente al cuadrado
        self.t = 0  # Paso global: se usa para correcciÃ³n de bias (Î²^t) en momentos iniciales

    def update(self, layer, dW: np.ndarray, db: np.ndarray, layer_id: int):  # Aplica un paso de Adam a los parÃ¡metros del `layer` usando gradientes dW/db
        if layer_id not in self.m:  # Inicializa estados si es la primera vez que se actualiza este layer_id
            self.m[layer_id] = {'W': np.zeros_like(dW), 'b': np.zeros_like(db)}  # m=0: mismo shape que gradientes para acumular 1er momento
            self.v[layer_id] = {'W': np.zeros_like(dW), 'b': np.zeros_like(db)}  # v=0: mismo shape que gradientes para acumular 2do momento

        self.t += 1  # Incrementa paso: importante para correcciÃ³n de bias; si no se incrementa, m_hat/v_hat quedan mal escalados
        m, v = self.m[layer_id], self.v[layer_id]  # Recupera buffers del layer: referencias mutables para actualizar in-place

        # Actualizar momentos
        m['W'] = self.beta1 * m['W'] + (1 - self.beta1) * dW  # 1er momento (W): EMA del gradiente; suaviza ruido y acelera en direcciones consistentes
        m['b'] = self.beta1 * m['b'] + (1 - self.beta1) * db  # 1er momento (b): mismo cÃ¡lculo para bias
        v['W'] = self.beta2 * v['W'] + (1 - self.beta2) * dW**2  # 2do momento (W): EMA de gradiente^2; aproxima varianza para normalizar paso
        v['b'] = self.beta2 * v['b'] + (1 - self.beta2) * db**2  # 2do momento (b): mismo cÃ¡lculo para bias

        # CorrecciÃ³n de bias
        m_hat_W = m['W'] / (1 - self.beta1**self.t)  # Corrige bias en m_W: al inicio m estÃ¡ sesgado hacia 0 por inicializaciÃ³n en cero
        m_hat_b = m['b'] / (1 - self.beta1**self.t)  # Corrige bias en m_b: misma idea para bias
        v_hat_W = v['W'] / (1 - self.beta2**self.t)  # Corrige bias en v_W: evita subestimar magnitud al principio
        v_hat_b = v['b'] / (1 - self.beta2**self.t)  # Corrige bias en v_b: misma idea para bias

        # Actualizar parÃ¡metros
        layer.W -= self.lr * m_hat_W / (np.sqrt(v_hat_W) + self.epsilon)  # Update W: paso adaptativo por coordenada (divide por RMS) + Îµ para estabilidad
        layer.b -= self.lr * m_hat_b / (np.sqrt(v_hat_b) + self.epsilon)  # Update b: mismo update para bias; requiere que `np` estÃ© en el namespace
```

---

## ğŸ¯ Ejercicios por tema (progresivos) + Soluciones

Reglas:

- **Intenta primero** sin mirar la soluciÃ³n.
- **Timebox sugerido:** 30â€“75 min por ejercicio.
- **Ã‰xito mÃ­nimo:** tu soluciÃ³n debe pasar los `assert`.

---

### Ejercicio 7.1: Activaciones y derivadas (chequeo numÃ©rico)

#### Enunciado

1) **BÃ¡sico**

- Implementa `sigmoid(z)` y `relu(z)`.

2) **Intermedio**

- Implementa derivadas: `sigmoid'(z)` y `relu'(z)`.

3) **Avanzado**

- Verifica `sigmoid'(z)` con diferencias finitas centrales.

#### SoluciÃ³n

```python
import numpy as np  # Importa NumPy: se usa para asarray/clip/exp/maximum y para generar datos aleatorios del chequeo numÃ©rico

def sigmoid(z: np.ndarray) -> np.ndarray:  # Sigmoide: transforma logits reales en valores (0,1) de forma elemento-a-elemento
    z = np.asarray(z, dtype=float)  # Normaliza entrada a ndarray float: evita dtype entero y garantiza operaciones vectorizadas estables
    z = np.clip(z, -500, 500)  # Recorta extremos para evitar overflow/underflow en exp(-z) cuando |z| es grande
    return 1.0 / (1.0 + np.exp(-z))  # Calcula Ïƒ(z)=1/(1+e^{-z}); devuelve array con misma shape que z


def sigmoid_deriv(z: np.ndarray) -> np.ndarray:  # Derivada de sigmoide respecto a z: necesaria para backprop cuando activaciÃ³n es sigmoid
    a = sigmoid(z)  # Reutiliza la salida de sigmoid: permite usar identidad Ïƒ'(z)=Ïƒ(z)(1-Ïƒ(z)) sin recomputar exp manualmente
    return a * (1.0 - a)  # Calcula Ïƒ'(z): si esto estÃ¡ mal, el gradiente tendrÃ¡ signo/magnitud errÃ³neos y el entrenamiento fallarÃ¡


def relu(z: np.ndarray) -> np.ndarray:  # ReLU: pone a 0 los valores negativos y deja pasar los positivos; estÃ¡ndar en capas ocultas
    return np.maximum(0.0, np.asarray(z, dtype=float))  # Convierte a float y aplica max(0,z) vectorizado; devuelve misma shape que z


def relu_deriv(z: np.ndarray) -> np.ndarray:  # Derivada de ReLU: mÃ¡scara binaria (1 donde z>0, 0 donde z<=0)
    z = np.asarray(z, dtype=float)  # Normaliza z: asegura comparaciÃ³n numÃ©rica consistente y broadcasting esperado
    return (z > 0.0).astype(float)  # Convierte booleano a float: produce gradiente 1/0 para multiplicar en backprop


def num_derivative(f, z: np.ndarray, h: float = 1e-6) -> np.ndarray:  # Derivada numÃ©rica central: aproxima f'(z) usando diferencias finitas
    return (f(z + h) - f(z - h)) / (2.0 * h)  # FÃ³rmula central: (f(z+h)-f(z-h))/(2h); mÃ¡s precisa que forward diff pero sensible a h


np.random.seed(0)  # Fija semilla global: hace reproducible el vector de prueba `z` y, por tanto, el resultado del test
z = np.random.randn(10)  # Genera 10 valores gaussianos: sirve como input genÃ©rico para comparar derivada numÃ©rica vs analÃ­tica
g_num = num_derivative(sigmoid, z)  # Calcula derivada numÃ©rica de sigmoid en z: referencia â€œaproximadaâ€ para validar implementaciÃ³n
g_ana = sigmoid_deriv(z)  # Calcula derivada analÃ­tica implementada: debe coincidir con la numÃ©rica dentro de tolerancias
assert np.allclose(g_num, g_ana, rtol=1e-5, atol=1e-6)  # Sanity check: si falla, hay bug en sigmoid_deriv o inestabilidad numÃ©rica
```

<details open>
<summary><strong>Complemento pedagÃ³gico â€” Ejercicio 7.1: Activaciones y derivadas (chequeo numÃ©rico)</strong></summary>

#### 1) Metadatos
- **ID (opcional):** `M07-E07_1`
- **DuraciÃ³n estimada:** 20â€“45 min
- **Nivel:** Intermedio

#### 2) Objetivos
- Entender la diferencia entre **activaciÃ³n** `f(z)` y **derivada** `f'(z)`.
- Validar una derivada con **diferencias finitas centrales**.

#### 3) Errores comunes
- Usar diferencias hacia delante (mÃ¡s error) en lugar de centrales.
- Elegir `h` demasiado grande (sesgo) o demasiado pequeÃ±o (error numÃ©rico).
- No â€œclipearâ€ `z` en sigmoid y obtener `inf/NaN`.

#### 4) Nota docente
- Pide que el alumno explique por quÃ© el chequeo numÃ©rico es una prueba de sanidad (no una demostraciÃ³n formal).
</details>

---

### Ejercicio 7.2: Forward de una capa densa (batch) + shapes

#### Enunciado

1) **BÃ¡sico**

- Implementa `dense_forward(X, W, b)` con `X:(n,d_in)`, `W:(d_in,d_out)`, `b:(d_out,)`.

2) **Intermedio**

- Verifica shapes de salida `Z:(n,d_out)`.

3) **Avanzado**

- Verifica que coincide con una implementaciÃ³n con loop (para un caso pequeÃ±o).

#### SoluciÃ³n

```python
import numpy as np  # Importa NumPy: se usa para RNG, matrices, producto @, zeros_like y comparaciones numÃ©ricas (allclose)

def dense_forward(X: np.ndarray, W: np.ndarray, b: np.ndarray) -> np.ndarray:  # Forward de capa densa: Z = XW + b (batch-first)
    return X @ W + b  # Multiplica (n,d_in)@(d_in,d_out)->(n,d_out) y suma bias (d_out,) por broadcasting en el eje batch


np.random.seed(1)  # Fija semilla global: hace reproducibles los datos/parametrizaciÃ³n del ejemplo
n, d_in, d_out = 5, 3, 4  # Define shapes: batch=5, input_dim=3, output_dim=4 (contrato bÃ¡sico de capa densa)
X = np.random.randn(n, d_in)  # Genera batch de entradas: shape (n,d_in)
W = np.random.randn(d_in, d_out)  # Genera matriz de pesos: shape (d_in,d_out) compatible con X @ W
b = np.random.randn(d_out)  # Genera bias: shape (d_out,) se sumarÃ¡ a cada fila de Z vÃ­a broadcasting

Z = dense_forward(X, W, b)  # Calcula salida vectorizada: referencia â€œcorrectaâ€ (sin loops explÃ­citos)
assert Z.shape == (n, d_out)  # Invariante de shape: si falla, hay error en el contrato de dimensiones o en el broadcasting de b

Z_loop = np.zeros_like(Z)  # Inicializa buffer para versiÃ³n con loop: mismo shape/dtype que Z para comparar resultados
for i in range(n):  # Recorre cada ejemplo del batch: implementa el mismo cÃ¡lculo pero de forma escalar por fila
    Z_loop[i] = X[i] @ W + b  # Calcula Z para la fila i: (d_in,)@(d_in,d_out)->(d_out,) y suma bias

assert np.allclose(Z, Z_loop)  # Sanity check: versiÃ³n vectorizada y versiÃ³n con loop deben coincidir (tolerancias numÃ©ricas)
```

<details open>
<summary><strong>Complemento pedagÃ³gico â€” Ejercicio 7.2: Forward denso (batch) y contratos de shape</strong></summary>

#### 1) Metadatos
- **ID (opcional):** `M07-E07_2`
- **DuraciÃ³n estimada:** 20â€“40 min
- **Nivel:** Intermedio

#### 2) Idea clave
- En convenciÃ³n batch-first, `X @ W + b` responde a:
  - `X:(n,d_in)`, `W:(d_in,d_out)`, `b:(d_out,)` â†’ `Z:(n,d_out)`.

#### 3) Errores comunes
- Poner `W` como `(d_out,d_in)` y luego forzar traspuestas por â€œarreglo rÃ¡pidoâ€.
- Confundir `axis` al sumar bias (debe broadcast a la segunda dimensiÃ³n).

#### 4) Nota docente
- Pide que el alumno escriba los shapes de memoria antes de correr el cÃ³digo.
</details>

---

### Ejercicio 7.3: Softmax estable + Cross-Entropy (multiclase)

#### Enunciado

1) **BÃ¡sico**

- Implementa `logsumexp` y `softmax` estable.

2) **Intermedio**

- Implementa `categorical_cross_entropy` para `y_true` one-hot.

3) **Avanzado**

- Verifica:
  - `softmax(z)` suma 1.
  - CCE baja cuando aumenta la probabilidad de la clase correcta.

#### SoluciÃ³n

```python
import numpy as np  # Importa NumPy: se usa para operaciones vectorizadas (max/sum/exp/log), conversiÃ³n a arrays y asserts numÃ©ricos

def logsumexp(z: np.ndarray, axis: int = -1, keepdims: bool = False) -> np.ndarray:  # Calcula log(sum(exp(z))) de forma estable (evita overflow) a lo largo de `axis`
    z = np.asarray(z, dtype=float)  # Convierte a ndarray float: normaliza el tipo y asegura que exp/log funcionen con precisiÃ³n estable
    m = np.max(z, axis=axis, keepdims=True)  # Extrae el mÃ¡ximo por eje: se usa para â€œcentrarâ€ logits sin cambiar el resultado (invariante por suma)
    out = m + np.log(np.sum(np.exp(z - m), axis=axis, keepdims=True))  # Implementa identidad estable: logsumexp(z)=m+log(sum(exp(z-m)))
    return out if keepdims else np.squeeze(out, axis=axis)  # Mantiene o elimina dimensiÃ³n reducida: `keepdims` controla broadcasting posterior


def softmax(z: np.ndarray, axis: int = -1) -> np.ndarray:  # Calcula softmax estable: devuelve probabilidades que suman 1 a lo largo de `axis`
    z = np.asarray(z, dtype=float)  # Normaliza logits a ndarray float: evita sorpresas de dtype/broadcasting
    lse = logsumexp(z, axis=axis, keepdims=True)  # Calcula logsumexp estable con keepdims: permite restar con broadcasting (misma rank)
    return np.exp(z - lse)  # softmax(z)=exp(z-logsumexp(z)): estable y garantiza normalizaciÃ³n (sum=1) salvo error numÃ©rico mÃ­nimo


def categorical_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-15) -> float:  # CCE para one-hot: penaliza baja prob. asignada a la clase correcta
    y_true = np.asarray(y_true, dtype=float)  # Convierte labels one-hot a float: asegura multiplicaciÃ³n/log coherentes
    y_pred = np.asarray(y_pred, dtype=float)  # Convierte predicciones a float: deben ser probabilidades (o aproximaciÃ³n) por clase
    y_pred = np.clip(y_pred, eps, 1.0)  # Evita log(0): recorta p en [eps,1]; sin esto, la loss puede volverse inf/nan
    return float(-np.mean(np.sum(y_true * np.log(y_pred), axis=1)))  # CCE=-E[sum_k y_k log p_k]; con one-hot queda -E[log p_clase]


z = np.array([[10.0, 0.0, -10.0]])  # Logits de ejemplo (1x3): gran separaciÃ³n para probar estabilidad y ranking de probabilidades
p = softmax(z)  # Convierte logits a probabilidades: debe asignar casi toda la masa a la clase de logit mÃ¡ximo (10.0)
assert np.isclose(np.sum(p), 1.0)  # Invariante softmax: las probabilidades suman ~1 (tolerancia numÃ©rica)
assert np.argmax(p) == 0  # Invariante de ranking: la clase 0 tiene el mayor logit, por lo tanto debe tener la mayor probabilidad

y_true = np.array([[1.0, 0.0, 0.0]])  # Target one-hot: la clase correcta es la 0 (prob=1 en Ã­ndice 0)
loss_good = categorical_cross_entropy(y_true, np.array([[0.9, 0.05, 0.05]]))  # Caso â€œbuenoâ€: alta prob. en clase correcta -> loss baja
loss_bad = categorical_cross_entropy(y_true, np.array([[0.4, 0.3, 0.3]]))  # Caso â€œmaloâ€: menos masa en clase correcta -> loss mÃ¡s alta
assert loss_good < loss_bad  # Sanity check: CCE debe penalizar mÃ¡s cuando baja la probabilidad de la clase correcta
```

<details open>
<summary><strong>Complemento pedagÃ³gico â€” Ejercicio 7.3: Softmax estable + Cross-Entropy</strong></summary>

#### 1) Metadatos
- **ID (opcional):** `M07-E07_3`
- **DuraciÃ³n estimada:** 30â€“60 min
- **Nivel:** Intermedio

#### 2) Idea clave
- Estabilidad: `softmax(z) = exp(z - logsumexp(z))` evita overflow.
- Para clasificaciÃ³n, lo importante es **comparar probabilidades** sin caer en `NaN`.

#### 3) Errores comunes
- Hacer `exp(z)` directamente con logits grandes.
- Olvidar `eps` al hacer `log(y_pred)`.
- Confundir CCE para `y_true` one-hot con BCE binaria.

#### 4) Nota docente
- Pide que el alumno explique por quÃ© restar el mÃ¡ximo no cambia el resultado de softmax.
</details>

---

### Ejercicio 7.4: Backprop de 2 capas (gradiente) + gradient checking

#### Enunciado

Red (batch):

- `Z1 = XW1 + b1`, `A1 = relu(Z1)`
- `Z2 = A1W2 + b2`, `P = sigmoid(Z2)`
- Loss BCE: `L = -mean(y log(P) + (1-y) log(1-P))`

1) **BÃ¡sico**

- Implementa forward + loss.

2) **Intermedio**

- Implementa backward: gradientes `dW1, db1, dW2, db2`.

3) **Avanzado**

- Verifica una coordenada de `dW2` con diferencias centrales.

#### SoluciÃ³n

```python
import numpy as np  # Importa NumPy: operaciones vectorizadas para forward/backward y generaciÃ³n de datos

def sigmoid(z: np.ndarray) -> np.ndarray:  # Sigmoid estable para salida binaria/probabilidades
    z = np.clip(z, -500, 500)  # Clipping: evita overflow en exp para |z| grande
    return 1.0 / (1.0 + np.exp(-z))  # Ïƒ(z)=1/(1+e^-z)


def relu(z: np.ndarray) -> np.ndarray:  # ReLU: no linealidad comÃºn en capas ocultas
    return np.maximum(0.0, z)  # Aplica max(0,z) elemento a elemento


def relu_deriv(z: np.ndarray) -> np.ndarray:  # Derivada de ReLU (subgradiente)
    return (z > 0.0).astype(float)  # 1 si z>0, 0 si z<=0


def bce(y: np.ndarray, p: np.ndarray, eps: float = 1e-15) -> float:  # Binary Cross-Entropy para targets {0,1}
    p = np.clip(p, eps, 1.0 - eps)  # Clipping: evita log(0) que produce inf/NaN
    return float(-np.mean(y * np.log(p) + (1.0 - y) * np.log(1.0 - p)))  # BCE media sobre el batch


def forward(X, W1, b1, W2, b2):  # Forward de red 2-capas: lineal+ReLU y lineal+sigmoid
    Z1 = X @ W1 + b1  # Pre-activaciÃ³n 1: (n,d_in)@(d_in,d_h)+(d_h,) -> (n,d_h)
    A1 = relu(Z1)  # ActivaciÃ³n oculta: aplica ReLU
    Z2 = A1 @ W2 + b2  # Pre-activaciÃ³n 2: (n,d_h)@(d_h,1)+(1,) -> (n,1)
    P = sigmoid(Z2)  # Probabilidad predicha: sigmoid sobre logits
    cache = (X, Z1, A1, Z2, P)  # Cachea tensores para backward sin recomputar
    return P, cache  # Devuelve predicciÃ³n y cache para backprop


def loss_fn(X, y, W1, b1, W2, b2):  # FunciÃ³n de pÃ©rdida: forward + BCE
    P, _ = forward(X, W1, b1, W2, b2)  # Calcula probabilidades con forward
    return bce(y, P)  # EvalÃºa BCE sobre el batch


def backward(y, cache, W2):  # Backward de la red: calcula gradientes de W1/b1/W2/b2
    X, Z1, A1, Z2, P = cache  # Desempaqueta cache: variables del forward
    n = X.shape[0]  # TamaÃ±o de batch: normaliza gradientes (media)
    # BCE with sigmoid output: dZ2 = (P - y) / n
    dZ2 = (P - y) / n  # Para BCE+sigmoid: dZ2=(P-y)/n (batch mean)
    dW2 = A1.T @ dZ2  # Gradiente W2: (d_h,n)@(n,1) -> (d_h,1)
    db2 = np.sum(dZ2, axis=0)  # Gradiente b2: suma sobre batch -> (1,)
    dA1 = dZ2 @ W2.T  # Propaga a activaciÃ³n oculta: (n,1)@(1,d_h) -> (n,d_h)
    dZ1 = dA1 * relu_deriv(Z1)  # Aplica derivada ReLU: enmascara gradiente donde Z1<=0
    dW1 = X.T @ dZ1  # Gradiente W1: (d_in,n)@(n,d_h) -> (d_in,d_h)
    db1 = np.sum(dZ1, axis=0)  # Gradiente b1: suma sobre batch -> (d_h,)
    return dW1, db1, dW2, db2  # Devuelve gradientes para actualizaciÃ³n/chequeo


np.random.seed(0)  # Semilla fija: reproducibilidad del grad-check
n, d_in, d_h = 8, 3, 5  # Dimensiones: batch=8, input=3, hidden=5
X = np.random.randn(n, d_in)  # Datos de entrada aleatorios: shape (n,d_in)
y = (np.random.rand(n, 1) < 0.5).astype(float)  # Labels binarios aleatorios: shape (n,1)
W1 = np.random.randn(d_in, d_h) * 0.1  # Pesos 1: init pequeÃ±o para estabilidad
b1 = np.zeros(d_h)  # Bias 1: vector (d_h,)
W2 = np.random.randn(d_h, 1) * 0.1  # Pesos 2: shape (d_h,1)
b2 = np.zeros(1)  # Bias 2: vector (1,)

P, cache = forward(X, W1, b1, W2, b2)  # Forward: obtiene probabilidades y cache
dW1, db1, dW2, db2 = backward(y, cache, W2)  # Backward: calcula gradientes analÃ­ticos

# Gradient check on one W2 coordinate
i, j = 2, 0  # Coordenada de W2 a chequear: Ã­ndice (fila,col)
h = 1e-6  # Paso pequeÃ±o para diferencias finitas centrales
E = np.zeros_like(W2)  # Matriz base para perturbar una coordenada de W2
E[i, j] = 1.0  # Marca la coordenada (i,j) a perturbar
L_plus = loss_fn(X, y, W1, b1, W2 + h * E, b2)  # Loss con W2(i,j)+h
L_minus = loss_fn(X, y, W1, b1, W2 - h * E, b2)  # Loss con W2(i,j)-h
g_num = (L_plus - L_minus) / (2.0 * h)  # Gradiente numÃ©rico (diferencia central)
assert np.isclose(dW2[i, j], g_num, rtol=1e-4, atol=1e-6)  # Verifica gradiente analÃ­tico vs numÃ©rico
```

<details open>
<summary><strong>Complemento pedagÃ³gico â€” Ejercicio 7.4: Backprop + gradient checking</strong></summary>

#### 1) Metadatos
- **ID (opcional):** `M07-E07_4`
- **DuraciÃ³n estimada:** 60â€“120 min
- **Nivel:** Avanzado

#### 2) Invariante principal
- Para `Z = XW + b` (batch-first):
  - `dW` tiene shape de `W`, `db` de `b`, `dX` de `X`.

#### 3) Gradient checking (mÃ­nimo viable)
- Chequea 1 coordenada (o pocas) de un gradiente grande (`dW2`) con diferencias centrales.
- Ajusta `h` y tolerancias si estÃ¡s en float64 vs float32.

#### 4) Errores comunes
- Olvidar dividir por `n` en la loss (o en `dZ2`) y â€œmoverâ€ el bug de lugar.
- Mezclar `y` como `(n,)` con `P` como `(n,1)`.

#### 5) Nota docente
- Pide que el alumno explique por quÃ© un Ãºnico chequeo no garantiza que TODO el gradiente estÃ© correcto.
</details>

---

### Ejercicio 7.5: Overfit test (sanity check obligatorio)

#### Enunciado

1) **BÃ¡sico**

- Construye un dataset tiny (8â€“16 ejemplos) linealmente separable.

2) **Intermedio**

- Entrena Logistic Regression (GD) y verifica que la pÃ©rdida baja.

3) **Avanzado**

- Verifica que logra accuracy alta (por ejemplo, > 95%).

#### SoluciÃ³n

```python
import numpy as np  # Importa NumPy: se usa para vectores/matrices, RNG, funciones exp/log y operaciones de Ã¡lgebra lineal

def sigmoid(z: np.ndarray) -> np.ndarray:  # Define sigmoide: mapea logits reales a probabilidades en (0,1) elemento-a-elemento
    z = np.clip(z, -500, 500)  # Recorta logits para evitar overflow/underflow numÃ©rico en exp(-z) cuando |z| es grande
    return 1.0 / (1.0 + np.exp(-z))  # Calcula Ïƒ(z)=1/(1+e^{-z}); produce salida con misma shape que z


def bce(y: np.ndarray, p: np.ndarray, eps: float = 1e-15) -> float:  # Define Binary Cross-Entropy: mide discrepancia entre labels y probabilidades (mÃ¡s baja es mejor)
    p = np.clip(p, eps, 1.0 - eps)  # Estabiliza logs evitando log(0): fuerza p a (eps,1-eps) para evitar inf/nan
    return float(-np.mean(y * np.log(p) + (1.0 - y) * np.log(1.0 - p)))  # Promedia BCE por ejemplo y devuelve escalar Python (float)


np.random.seed(1)  # Semilla fija: reproducibilidad del overfit test (mismo dataset)
n = 16  # TamaÃ±o del dataset tiny: 16 ejemplos (8 positivos, 8 negativos)
X_pos = np.random.randn(n // 2, 2) + np.array([2.0, 2.0])  # Clase positiva: gaussiana centrada en (2,2)
X_neg = np.random.randn(n // 2, 2) + np.array([-2.0, -2.0])  # Clase negativa: gaussiana centrada en (-2,-2)
X = np.vstack([X_pos, X_neg])  # Concatena ejemplos: shape (n,2)
y = np.vstack([np.ones((n // 2, 1)), np.zeros((n // 2, 1))])  # Labels: 1 para pos, 0 para neg (shape (n,1))

w = np.zeros((2, 1))  # Pesos de logistic regression: shape (2,1)
b = 0.0  # Bias escalar
lr = 0.2  # Learning rate: suficientemente alto para converger en pocas iteraciones

loss0 = None  # Guarda la loss inicial (t=0) para comparar progreso
for t in range(400):  # Loop de entrenamiento: gradient descent batch para logistic regression
    logits = X @ w + b  # Logits: (n,2)@(2,1)+(scalar) -> (n,1)
    p = sigmoid(logits)  # Probabilidades predichas: shape (n,1)
    loss = bce(y, p)  # Loss actual: BCE sobre dataset
    if loss0 is None:  # Captura loss inicial una sola vez
        loss0 = loss  # Guarda baseline para validar que al final disminuye
    # gradients
    dz = (p - y) / n  # Gradiente wrt logits: (p-y)/n para BCE+sigmoid
    dw = X.T @ dz  # Gradiente wrt w: (2,n)@(n,1) -> (2,1)
    db = float(np.sum(dz))  # Gradiente wrt b: suma sobre batch (escalar)
    w -= lr * dw  # Update w: descenso por gradiente
    b -= lr * db  # Update b: descenso por gradiente

loss_end = bce(y, sigmoid(X @ w + b))  # Loss final: deberÃ­a ser <= loss0 si aprende
pred = (sigmoid(X @ w + b) >= 0.5).astype(int)  # PredicciÃ³n binaria: umbral 0.5 sobre probabilidad
acc = float(np.mean(pred == y.astype(int)))  # Accuracy final: proporciÃ³n de aciertos

assert loss_end <= loss0  # Invariante: la optimizaciÃ³n debe reducir la pÃ©rdida
assert acc > 0.95  # Invariante: en dataset separable deberÃ­a lograr alta accuracy
```

<details open>
<summary><strong>Complemento pedagÃ³gico â€” Ejercicio 7.5: Overfit test (sanity check)</strong></summary>

#### 1) Metadatos
- **ID (opcional):** `M07-E07_5`
- **DuraciÃ³n estimada:** 30â€“60 min
- **Nivel:** Intermedio

#### 2) Regla de oro
- Si tu modelo no puede **memorizar** un dataset tiny, asume bug (no â€œmala suerteâ€).

#### 3) Errores comunes
- Learning rate demasiado bajo (parece bug, pero sÃ³lo no se mueve).
- Dataset no separable o etiquetas con shape inconsistente.
- Error en el gradiente (signo, normalizaciÃ³n por `n`, broadcasting de `b`).

#### 4) Nota docente
- Pide que el alumno haga el mismo test con 2â€“3 seeds y compare estabilidad.
</details>

---

### Ejercicio 7.6: Optimizadores en una funciÃ³n cuadrÃ¡tica (SGD vs Adam)

#### Enunciado

Minimiza `f(w) = (w - 3)^2`.

1) **BÃ¡sico**

- Implementa SGD.

2) **Intermedio**

- Implementa Adam.

3) **Avanzado**

- Verifica que ambos se acercan a `wâ‰ˆ3` y que Adam no diverge.

#### SoluciÃ³n

```python
import numpy as np  # Importa NumPy: se usa para sqrt y para mantener consistencia numÃ©rica en Adam (np.sqrt)

def grad_f(w: float) -> float:  # Define el gradiente de f(w)=(w-3)^2: derivada analÃ­tica para usar en SGD/Adam
    return 2.0 * (w - 3.0)  # d/dw (w-3)^2 = 2(w-3): si esto estuviera mal, el optimizador convergerÃ­a al punto equivocado


def sgd(w0: float, lr: float, steps: int) -> float:  # Implementa SGD 1D: aplica descenso por gradiente con paso constante
    w = float(w0)  # Convierte el inicial a float nativo: garantiza aritmÃ©tica escalar y evita tipos raros (p.ej., np scalar)
    for _ in range(steps):  # Itera un nÃºmero fijo de pasos: cada iteraciÃ³n aplica una actualizaciÃ³n usando el gradiente actual
        w -= lr * grad_f(w)  # Update SGD: w <- w - lr * g(w); el signo es crÃ­tico (si fuera +, diverge)
    return w  # Devuelve el w final: aproximaciÃ³n al mÃ­nimo (idealmente cercano a 3)


def adam(w0: float, lr: float, steps: int, beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8) -> float:  # Implementa Adam 1D: momentos + normalizaciÃ³n por RMS con correcciÃ³n de bias
    w = float(w0)  # Estado del parÃ¡metro: se actualiza in-place en cada paso de optimizaciÃ³n
    m = 0.0  # Primer momento (EMA del gradiente): actÃºa como momentum, suavizando ruido
    v = 0.0  # Segundo momento (EMA del gradiente^2): estima escala/varianza para ajustar el paso
    t = 0  # Contador de tiempo: necesario para correcciÃ³n de bias (1 - beta^t) en los primeros pasos
    for _ in range(steps):  # Ejecuta N pasos: en problemas reales, esto serÃ­a por batch/iteraciÃ³n de entrenamiento
        t += 1  # Avanza el tiempo: si se omite, m_hat/v_hat quedan mal corregidos y el paso se sesga
        g = grad_f(w)  # Calcula gradiente actual en w: direcciÃ³n local de mÃ¡xima subida (queremos bajar)
        m = beta1 * m + (1 - beta1) * g  # Actualiza 1er momento: EMA del gradiente (promedio con decaimiento)
        v = beta2 * v + (1 - beta2) * (g ** 2)  # Actualiza 2do momento: EMA de g^2 (magnitud tÃ­pica del gradiente)
        m_hat = m / (1 - beta1 ** t)  # CorrecciÃ³n de bias en m: compensa inicializaciÃ³n en cero, importante cuando t es pequeÃ±o
        v_hat = v / (1 - beta2 ** t)  # CorrecciÃ³n de bias en v: evita subestimar la escala del gradiente al inicio
        w -= lr * m_hat / (np.sqrt(v_hat) + eps)  # Update Adam: paso adaptativo por RMS; eps evita divisiÃ³n por cero/inestabilidad
    return w  # Devuelve el w final: deberÃ­a acercarse al mÃ­nimo en w=3 si el update estÃ¡ bien implementado


w_sgd = sgd(w0=10.0, lr=0.1, steps=50)  # Ejecuta SGD desde w0=10: espera converger hacia 3 con lr moderado
w_adam = adam(w0=10.0, lr=0.2, steps=50)  # Ejecuta Adam desde w0=10: suele tolerar lr mayor por normalizaciÃ³n adaptativa

assert abs(w_sgd - 3.0) < 1e-2  # Sanity check: SGD debe terminar suficientemente cerca del Ã³ptimo w=3
assert abs(w_adam - 3.0) < 1e-2  # Sanity check: Adam tambiÃ©n debe converger; si falla, hay bug en momentos/correcciÃ³n/update
```

<details open>
<summary><strong>Complemento pedagÃ³gico â€” Ejercicio 7.6: SGD vs Adam (intuiciÃ³n)</strong></summary>

#### 1) Metadatos
- **ID (opcional):** `M07-E07_6`
- **DuraciÃ³n estimada:** 30â€“60 min
- **Nivel:** Intermedio

#### 2) Idea clave
- SGD usa el gradiente â€œtal cualâ€.
- Adam introduce momentos (media y varianza) y suele ser mÃ¡s estable en problemas mal condicionados.

#### 3) Errores comunes
- Olvidar correcciÃ³n de bias (`m_hat`, `v_hat`).
- Elegir `lr` de Adam igual que el de SGD sin validar.

#### 4) Nota docente
- Pide que el alumno grafique `w_t` para comparar trayectorias.
</details>

---

### Ejercicio 7.7: Gradient clipping (evitar exploding gradients)

#### Enunciado

1) **BÃ¡sico**

- Implementa clipping por norma: si `||g|| > max_norm`, entonces `g <- g * (max_norm/||g||)`.

2) **Intermedio**

- Verifica que tras clipping la norma es `<= max_norm`.

3) **Avanzado**

- Verifica que si la norma ya es pequeÃ±a, el gradiente no cambia.

#### SoluciÃ³n

```python
import numpy as np  # Importa NumPy: se usa para asarray, norma L2 (linalg.norm), arrays de prueba y allclose

def clip_by_norm(g: np.ndarray, max_norm: float) -> np.ndarray:  # Clipping por norma: re-escala g para que ||g|| <= max_norm (si excede)
    g = np.asarray(g, dtype=float)  # Normaliza entrada a ndarray float: asegura que la norma y escalado sean numÃ©ricamente consistentes
    n = np.linalg.norm(g)  # Calcula norma L2: mide magnitud global del gradiente (no por componente)
    if n == 0.0:  # Caso borde: gradiente cero (no hay direcciÃ³n de descenso);
        return g  # Retorna sin cambio: evita divisiÃ³n por cero y preserva semÃ¡ntica (0 sigue siendo 0)
    if n <= max_norm:  # Si ya estÃ¡ bajo el umbral, no se debe tocar (evita introducir sesgo innecesario)
        return g  # Retorna el gradiente original: clipping sÃ³lo actÃºa cuando hay riesgo de pasos gigantes
    return g * (max_norm / n)  # Re-escala manteniendo direcciÃ³n: multiplica por factor <1 para que la nueva norma sea exactamente max_norm


g_big = np.array([3.0, 4.0])  # norm=5
g_clip = clip_by_norm(g_big, max_norm=1.0)  # Aplica clipping: al ser ||g||=5>1, el resultado debe tener norma ~1
assert np.linalg.norm(g_clip) <= 1.0 + 1e-12  # Verifica invariante: tras clipping, la norma no debe exceder el umbral (con tolerancia)

g_small = np.array([0.3, 0.4])  # norm=0.5
g_keep = clip_by_norm(g_small, max_norm=1.0)  # Aplica clipping: como ||g||=0.5<=1, no debe modificar el gradiente
assert np.allclose(g_small, g_keep)  # Verifica que no hay cambio numÃ©rico: clipping no debe afectar gradientes ya pequeÃ±os
```

<details open>
<summary><strong>Complemento pedagÃ³gico â€” Ejercicio 7.7: Gradient clipping</strong></summary>

#### 1) Metadatos
- **ID (opcional):** `M07-E07_7`
- **DuraciÃ³n estimada:** 20â€“45 min
- **Nivel:** Intermedio

#### 2) Idea clave
- Clipping por norma no â€œarreglaâ€ el gradiente: sÃ³lo evita pasos gigantes.

#### 3) Errores comunes
- Hacer clipping por componente (otra tÃ©cnica) pensando que es lo mismo.
- No manejar el caso `||g||=0`.

#### 4) Nota docente
- Pide que el alumno explique por quÃ© clipping puede estabilizar RNN/transformers (conceptual).
</details>

---

### Ejercicio 7.8: ConvoluciÃ³n - cÃ¡lculo de output shape (padding/stride)

#### Enunciado

1) **BÃ¡sico**

- Implementa `conv2d_out(H, W, KH, KW, stride, padding)` para una conv sin dilataciÃ³n.

2) **Intermedio**

- Verifica el caso MNIST: `28x28` con kernel `5x5`, `stride=1`, `padding=0` â†’ `24x24`.

3) **Avanzado**

- Verifica un caso con padding: `28x28`, `5x5`, `stride=1`, `padding=2` â†’ `28x28`.

#### SoluciÃ³n

```python
import numpy as np  # Importa NumPy: se usa para validaciÃ³n numÃ©rica en asserts y para mantener consistencia con el resto del mÃ³dulo

def conv2d_out(H: int, W: int, KH: int, KW: int, stride: int = 1, padding: int = 0):  # Output shape (sin dilataciÃ³n): fÃ³rmula estÃ¡ndar de conv para cada eje
    H_out = (H + 2 * padding - KH) // stride + 1  # Altura de salida: floor((H+2P-KH)/S)+1
    W_out = (W + 2 * padding - KW) // stride + 1  # Ancho de salida: floor((W+2P-KW)/S)+1
    return int(H_out), int(W_out)  # Devuelve (H_out,W_out): se usa para asserts y para dimensionar tensores

assert conv2d_out(28, 28, 5, 5, stride=1, padding=0) == (24, 24)  # Caso MNIST sin padding: 28-5+1=24
assert conv2d_out(28, 28, 5, 5, stride=1, padding=2) == (28, 28)  # Caso con padding=2 (aprox â€œsameâ€ para KH=5): mantiene 28
```

<details open>
<summary><strong>Complemento pedagÃ³gico â€” Ejercicio 7.8: Output shape de conv (stride/padding)</strong></summary>

#### 1) Metadatos
- **ID (opcional):** `M07-E07_8`
- **DuraciÃ³n estimada:** 20â€“45 min
- **Nivel:** Intermedio

#### 2) Idea clave
- FÃ³rmula sin dilataciÃ³n: `H_out = (H + 2P - KH)//S + 1` (igual para `W_out`).
- Si no cuadra, normalmente el error estÃ¡ en `padding` o en entero vs float.

#### 3) Errores comunes
- Olvidar que `padding` aplica a ambos lados (por eso `2P`).
- Usar `/` en vez de `//` y obtener floats.

#### 4) Nota docente
- Pide que el alumno derive la fÃ³rmula a partir de â€œcuÃ¡ntas posiciones cabe el kernelâ€.
</details>

---

## ğŸ“¦ Entregable del MÃ³dulo

### `neural_network.py`

```python
"""
Neural Network Module

ImplementaciÃ³n desde cero de:
- MLP (Multilayer Perceptron)
- Backpropagation
- Optimizadores (SGD, Momentum, Adam)
- Funciones de activaciÃ³n

Autor: [Tu nombre]
MÃ³dulo: 06 - Deep Learning
"""

import numpy as np  # Importa NumPy: base de operaciones vectorizadas (matmul, exp, clip, etc.)
from typing import List, Tuple, Optional  # Importa tipos: documentaciÃ³n estÃ¡tica de firmas, no afecta runtime


# ============================================================
# ACTIVACIONES
# ============================================================

def sigmoid(z):  # Sigmoide: convierte logits a (0,1) aplicando una no linealidad suave (se usa tÃ­picamente en salida binaria)
    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Sigmoid estable: clip evita overflow en exp para |z| grande

def sigmoid_deriv(a):  # Derivada de sigmoide en funciÃ³n de la activaciÃ³n: Ãºtil en backprop para obtener da/dz sin recomputar exp
    return a * (1 - a)  # Derivada de sigmoid en funciÃ³n de la activaciÃ³n: Ïƒ'(z)=a(1-a)

def relu(z):  # ReLU: activa solo valores positivos (max(0,z)); estÃ¡ndar en capas ocultas por estabilidad de gradiente
    return np.maximum(0, z)  # ReLU: pasa valores positivos y anula negativos (no linealidad)

def relu_deriv(z):  # Derivada de ReLU: mÃ¡scara 1/0 segÃºn z>0; controla por dÃ³nde fluye el gradiente en backprop
    return (z > 0).astype(float)  # Derivada de ReLU: 1 si z>0, 0 si z<=0 (subgradiente)

def tanh_deriv(a):  # Derivada de tanh en funciÃ³n de la activaciÃ³n: tanh'(z)=1-a^2, usada en backprop
    return 1 - a**2  # Derivada de tanh en funciÃ³n de la activaciÃ³n: tanh'(z)=1-a^2

def softmax(z):  # Softmax: normaliza logits a distribuciÃ³n (suma 1); se usa en salida multiclase
    exp_z = np.exp(z - np.max(z))  # Softmax (estabilizado restando max): reduce overflow en exp
    return exp_z / np.sum(exp_z)  # Normaliza para que la suma sea 1 (distribuciÃ³n de probabilidad)


# ============================================================
# CAPA
# ============================================================

class Layer:  # Define una capa densa simple: aplica W@x+b seguido de una activaciÃ³n y guarda cache para backward
    def __init__(self, input_size: int, output_size: int, activation: str = 'relu'):  # Inicializa pesos/bias y configura activaciÃ³n
        self.activation = activation  # Guarda nombre de activaciÃ³n: define forward/backward de la capa
        scale = np.sqrt(2.0 / input_size) if activation == 'relu' else np.sqrt(1.0 / input_size)  # He para ReLU, Xavier simple para resto
        self.W = np.random.randn(output_size, input_size) * scale  # Pesos: (out,in) escalados para estabilidad inicial
        self.b = np.zeros(output_size)  # Bias: vector (out,) inicializado a cero
        self.cache = {}  # Cache: guarda x/z/a del forward para usar en backward sin recomputar

    def forward(self, x: np.ndarray) -> np.ndarray:  # Forward: computa z=W@x+b y aplica la activaciÃ³n
        self.cache['x'] = x  # Guarda input: se necesita para dW en backward (outer product con delta)
        z = self.W @ x + self.b  # Pre-activaciÃ³n: combinaciÃ³n lineal (out,in)@(in,) + (out,) -> (out,)
        self.cache['z'] = z  # Guarda z: derivada depende de z (ReLU) o se usa para depuraciÃ³n

        if self.activation == 'relu':  # Rama ReLU: tÃ­pica en capas ocultas
            a = relu(z)  # Aplica ReLU elemento a elemento
        elif self.activation == 'sigmoid':  # Rama sigmoid: tÃ­pica en salida binaria
            a = sigmoid(z)  # Convierte logits a probabilidad (0,1)
        elif self.activation == 'tanh':  # Rama tanh: no linealidad centrada en 0
            a = np.tanh(z)  # Aplica tanh elemento a elemento
        elif self.activation == 'softmax':  # Rama softmax: salida multiclase como distribuciÃ³n
            a = softmax(z)  # Normaliza logits a probabilidades
        else:  # Rama lineal/identidad: sin no linealidad
            a = z  # Identidad: Ãºtil en regresiÃ³n o como logits antes de softmax externa

        self.cache['a'] = a  # Guarda activaciÃ³n: se usa en backward (sigmoid/tanh derivan de a)
        return a  # Devuelve salida de la capa para alimentar la siguiente

    def backward(self, dL_da: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:  # Backward: propaga gradiente y produce dW/db
        z, x, a = self.cache['z'], self.cache['x'], self.cache['a']  # Recupera forward cacheado para calcular derivadas

        if self.activation == 'sigmoid':  # Derivada de sigmoid se expresa con la activaciÃ³n a
            da_dz = sigmoid_deriv(a)  # da/dz para sigmoid
        elif self.activation == 'relu':  # Derivada de ReLU depende de z (signo)
            da_dz = relu_deriv(z)  # da/dz para ReLU
        elif self.activation == 'tanh':  # Derivada de tanh se expresa con la activaciÃ³n a
            da_dz = tanh_deriv(a)  # da/dz para tanh
        else:  # ActivaciÃ³n lineal: derivada 1
            da_dz = np.ones_like(z)  # da/dz=1 para identidad (misma shape que z)

        delta = dL_da * da_dz  # Regla de la cadena: dL/dz = dL/da * da/dz (elementwise)
        dL_dW = np.outer(delta, x)  # Gradiente de W: outer(delta(out,), x(in,)) -> (out,in)
        dL_db = delta  # Gradiente de b: dL/db = dL/dz (por neurona), sin suma porque es single-sample
        dL_dx = self.W.T @ delta  # Gradiente hacia atrÃ¡s: (in,out)@(out,) -> (in,)

        return dL_dx, dL_dW, dL_db  # Devuelve gradientes: input, pesos, bias (para propagaciÃ³n y optimizaciÃ³n)


# ============================================================
# OPTIMIZADORES
# ============================================================

class SGD:  # Optimizer SGD â€œvanillaâ€: actualiza parÃ¡metros restando lr * gradiente en cada paso
    def __init__(self, lr=0.01):  # Constructor SGD: fija la tasa de aprendizaje
        self.lr = lr  # Guarda learning rate: escala del update en cada step

    def step(self, layers, gradients):  # Aplica un paso de SGD a una lista de capas
        for layer, (dW, db) in zip(layers, gradients):  # Recorre capas y sus gradientes alineados
            layer.W -= self.lr * dW  # Update SGD: W <- W - lr * dW
            layer.b -= self.lr * db  # Update SGD: b <- b - lr * db


class Adam:  # Optimizer Adam: mantiene momentos (m,v) y aplica correcciÃ³n de bias para updates adaptativos por parÃ¡metro
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):  # Inicializa hiperparÃ¡metros de Adam
        self.lr, self.beta1, self.beta2, self.eps = lr, beta1, beta2, eps  # Guarda lr y decays de momentos + epsilon numÃ©rico
        self.m, self.v, self.t = {}, {}, 0  # Estado por capa: momentos m/v y contador de pasos t

    def step(self, layers, gradients):  # Paso Adam: actualiza cada capa con momentos y bias correction
        self.t += 1  # Incrementa timestep: necesario para correcciÃ³n de sesgo (bias correction)
        for i, (layer, (dW, db)) in enumerate(zip(layers, gradients)):  # Itera capas con Ã­ndice para almacenar estado
            if i not in self.m:  # Inicializa estado si es la primera vez que se ve esta capa
                self.m[i] = {'W': np.zeros_like(dW), 'b': np.zeros_like(db)}  # m: primer momento (media mÃ³vil del gradiente)
                self.v[i] = {'W': np.zeros_like(dW), 'b': np.zeros_like(db)}  # v: segundo momento (media mÃ³vil del gradiente^2)

            self.m[i]['W'] = self.beta1 * self.m[i]['W'] + (1 - self.beta1) * dW  # Actualiza m(W): EMA del gradiente
            self.m[i]['b'] = self.beta1 * self.m[i]['b'] + (1 - self.beta1) * db  # Actualiza m(b): EMA del gradiente
            self.v[i]['W'] = self.beta2 * self.v[i]['W'] + (1 - self.beta2) * dW**2  # Actualiza v(W): EMA del gradiente^2
            self.v[i]['b'] = self.beta2 * self.v[i]['b'] + (1 - self.beta2) * db**2  # Actualiza v(b): EMA del gradiente^2

            m_hat_W = self.m[i]['W'] / (1 - self.beta1**self.t)  # Bias correction de m(W): corrige arranque en 0
            m_hat_b = self.m[i]['b'] / (1 - self.beta1**self.t)  # Bias correction de m(b)
            v_hat_W = self.v[i]['W'] / (1 - self.beta2**self.t)  # Bias correction de v(W)
            v_hat_b = self.v[i]['b'] / (1 - self.beta2**self.t)  # Bias correction de v(b)

            layer.W -= self.lr * m_hat_W / (np.sqrt(v_hat_W) + self.eps)  # Update Adam W: step adaptativo por componente
            layer.b -= self.lr * m_hat_b / (np.sqrt(v_hat_b) + self.eps)  # Update Adam b: mismo update para bias


# ============================================================
# RED NEURONAL
# ============================================================

class NeuralNetwork:  # Red feedforward (MLP): compone capas, ejecuta forward/backward y entrena con SGD/Adam
    def __init__(self, layer_sizes: List[int], activations: List[str]):  # Construye una red feedforward a partir de tamaÃ±os
        self.layers = [Layer(layer_sizes[i], layer_sizes[i+1], activations[i])  # Crea Layer i con fan-in/out y activaciÃ³n
                       for i in range(len(layer_sizes)-1)]  # Itera pares consecutivos de tamaÃ±os para construir todas las capas
        self.loss_history = []  # Historial de pÃ©rdida por Ã©poca: Ãºtil para depuraciÃ³n (convergencia)

    def forward(self, x: np.ndarray) -> np.ndarray:  # Forward de la red: aplica forward secuencial de cada capa
        for layer in self.layers:  # Recorre capas en orden: la salida de una es entrada de la siguiente
            x = layer.forward(x)  # Propaga activaciones: actualiza x con la salida de la capa
        return x  # Devuelve la salida final (probabilidad/logits segÃºn Ãºltima activaciÃ³n)

    def backward(self, y_true: np.ndarray) -> List[Tuple]:  # Backprop: calcula gradientes de parÃ¡metros en todas las capas
        y_pred = self.layers[-1].cache['a']  # Usa activaciÃ³n del Ãºltimo forward: evita recalcular predicciÃ³n
        dL_da = y_pred - y_true  # Gradiente inicial (MSE simplificada): cambia si cambias la funciÃ³n de pÃ©rdida

        gradients = []  # Lista de gradientes por capa (dW, db) en orden forward
        for layer in reversed(self.layers):  # Recorre capas de atrÃ¡s hacia adelante (regla de la cadena)
            dL_da, dW, db = layer.backward(dL_da)  # Backward capa: devuelve gradiente para capa anterior y sus dW/db
            gradients.insert(0, (dW, db))  # Inserta al inicio para alinear con self.layers (misma indexaciÃ³n)
        return gradients  # Devuelve gradientes listos para el optimizador

    def fit(self, X, y, epochs=1000, lr=0.1, optimizer='sgd', verbose=True):  # Entrenamiento por SGD/Adam (muestra a muestra)
        opt = Adam(lr) if optimizer == 'adam' else SGD(lr)  # Selecciona optimizador segÃºn string: cambia dinÃ¡mica de convergencia

        for epoch in range(epochs):  # Loop principal de entrenamiento: una iteraciÃ³n por Ã©poca
            total_loss = 0  # Acumulador de pÃ©rdida total de la Ã©poca (para promedio/monitoreo)
            for xi, yi in zip(X, y):  # Itera dataset ejemplo a ejemplo (SGD puro, no mini-batch)
                yi_arr = np.atleast_1d(yi)  # Asegura y como vector: evita errores si yi es escalar
                output = self.forward(xi)  # Forward: predicciÃ³n actual con parÃ¡metros actuales

                # BCE loss
                output_clip = np.clip(output, 1e-15, 1-1e-15)  # Clipping: evita log(0) -> inf/NaN en BCE
                loss = -np.sum(yi_arr * np.log(output_clip) + (1-yi_arr) * np.log(1-output_clip))  # BCE binaria por muestra
                total_loss += loss  # Suma pÃ©rdidas: luego se promedia por nÃºmero de muestras

                gradients = self.backward(yi_arr)  # Backprop: calcula gradientes de todas las capas
                opt.step(self.layers, gradients)  # Update: aplica optimizador a parÃ¡metros usando los gradientes

            self.loss_history.append(total_loss / len(X))  # Guarda pÃ©rdida media de la Ã©poca para trazado/diagnÃ³stico
            if verbose and epoch % (epochs//10) == 0:  # Loggea ~10 veces (ojo: epochs//10 debe ser >0)
                print(f"Epoch {epoch}: Loss = {self.loss_history[-1]:.4f}")  # Imprime pÃ©rdida: ayuda a detectar estancamiento

    def predict(self, X: np.ndarray) -> np.ndarray:  # PredicciÃ³n binaria: umbraliza la salida de forward
        return np.array([1 if self.forward(x)[0] > 0.5 else 0 for x in X])  # 0.5 como umbral estÃ¡ndar para sigmoid

    def score(self, X: np.ndarray, y: np.ndarray) -> float:  # Accuracy: mÃ©trica simple para clasificaciÃ³n binaria
        return np.mean(self.predict(X) == y)  # ProporciÃ³n de aciertos (promedio de booleanos)


# ============================================================
# TESTS
# ============================================================

if __name__ == "__main__":  # Entry point: ejecuta un test rÃ¡pido cuando se corre este archivo como script
    print("=== Test: XOR Problem ===")  # Banner: indica inicio del test de XOR
    X = np.array([[0,0], [0,1], [1,0], [1,1]])  # Dataset XOR (4 ejemplos): no linealmente separable
    y = np.array([0, 1, 1, 0])  # Etiquetas XOR: 1 si bits difieren, 0 si son iguales

    net = NeuralNetwork([2, 4, 1], ['tanh', 'sigmoid'])  # Red 2â†’4â†’1: suficiente para aprender XOR con no linealidad
    net.fit(X, y, epochs=5000, lr=0.5, verbose=True)  # Entrena muchas Ã©pocas: en dataset pequeÃ±o debe converger

    print("\nPredicciones:")  # Encabezado: muestra predicciones finales tras entrenamiento
    for xi, yi in zip(X, y):  # Itera ejemplos para inspecciÃ³n manual de outputs
        pred = net.forward(xi)[0]  # Forward sobre un ejemplo: toma componente 0 porque salida es (1,)
        print(f"{xi} -> {pred:.4f} (target: {yi})")  # Imprime predicciÃ³n vs target para ver si memoriza XOR

    print(f"\nAccuracy: {net.score(X, y):.2%}")  # Accuracy final: deberÃ­a acercarse a 100% si aprendiÃ³
    print("\nâœ“ Test XOR completado!")  # Mensaje final: indica que terminÃ³ el bloque de pruebas
```

---

## ğŸ’» Parte 5: CNNs - Redes Convolucionales (Semana 19)

> âš ï¸ **Nota:** En este mÃ³dulo implementas **solo el forward pass** de una CNN simple en NumPy (para dominar dimensiones). El entrenamiento completo de una CNN se hace con **PyTorch** (sin implementar backward manual de CNN).

### Protocolo D (visualizaciÃ³n generativa): convoluciÃ³n sobre una imagen real

Para que â€œconvoluciÃ³nâ€ no sea solo una fÃ³rmula, ejecuta el script:

- [`visualizations/viz_convolution.py`](../visualizations/viz_convolution.py)

Uso recomendado (con una imagen propia):

```bash
python3 visualizations/viz_convolution.py /ruta/a/tu_imagen.png
```

QuÃ© debes observar:

- el **Sobel X** responde fuerte a bordes verticales
- la **magnitud** combina bordes en varias direcciones

Entregable sugerido: captura de *input vs feature map* + explicaciÃ³n en 5 lÃ­neas de quÃ© patrÃ³n detecta el filtro.

### 5.1 Â¿Por QuÃ© CNNs para ImÃ¡genes?

```
PROBLEMA CON MLP PARA IMÃGENES:

Imagen MNIST: 28x28 = 784 pÃ­xeles
MLP fully connected a capa de 256 neuronas:
  â†’ 784 Ã— 256 = 200,704 parÃ¡metros (Â¡solo primera capa!)

Imagen HD: 1920x1080x3 = 6,220,800 pÃ­xeles
  â†’ Imposible conectar todo con todo

SOLUCIÃ“N: CONVOLUCIÃ“N
- Procesar regiones locales (no toda la imagen)
- Compartir pesos (el mismo filtro en toda la imagen)
- Detectar patrones sin importar su posiciÃ³n
```

### 5.2 La OperaciÃ³n de ConvoluciÃ³n

```python
import numpy as np  # Importa NumPy: se usa para arrays, zeros, sum y construir el ejemplo de imagen/kernel

def convolve2d_simple(image: np.ndarray, kernel: np.ndarray) -> np.ndarray:  # Define convoluciÃ³n 2D â€œvalidâ€ (sin padding) para entender el mecanismo
    """
    ConvoluciÃ³n 2D simplificada (para entender el concepto).

    La convoluciÃ³n desliza un kernel (filtro) sobre la imagen
    y calcula el producto punto en cada posiciÃ³n.

    Args:
        image: Imagen de entrada (H, W)
        kernel: Filtro (kH, kW), tÃ­picamente 3x3 o 5x5

    Returns:
        Feature map (H-kH+1, W-kW+1)
    """
    H, W = image.shape  # Extrae alto/ancho de la imagen: define el espacio sobre el que el kernel puede deslizarse
    kH, kW = kernel.shape  # Extrae alto/ancho del kernel: define el tamaÃ±o de la ventana local que se multiplica por la imagen

    # TamaÃ±o del output (sin padding)
    out_H = H - kH + 1  # Alto del feature map (valid): cantidad de posiciones verticales posibles del kernel
    out_W = W - kW + 1  # Ancho del feature map (valid): cantidad de posiciones horizontales posibles del kernel

    output = np.zeros((out_H, out_W))  # Inicializa salida en 0: aquÃ­ se acumularÃ¡ el producto punto regiÃ³nÂ·kernel en cada posiciÃ³n

    for i in range(out_H):  # Recorre filas de la salida: i indica el desplazamiento vertical del kernel sobre la imagen
        for j in range(out_W):  # Recorre columnas de la salida: j indica el desplazamiento horizontal del kernel sobre la imagen
            # Extraer regiÃ³n de la imagen
            region = image[i:i+kH, j:j+kW]  # Toma ventana local (kH,kW): la porciÃ³n de imagen bajo el kernel en esta posiciÃ³n
            # Producto punto con el kernel
            output[i, j] = np.sum(region * kernel)  # Multiplica elemento a elemento y suma: implementa correlaciÃ³n/convoluciÃ³n simplificada

    return output  # Devuelve el feature map: respuesta del filtro para cada posiciÃ³n (sin padding)


# Ejemplo: DetecciÃ³n de bordes verticales
image = np.array([  # Define una â€œimagenâ€ toy: matriz 4x6 con un borde vertical (cambio de 0 a 1) en la mitad derecha
    [0, 0, 0, 1, 1, 1],  # Fila 0: patrÃ³n de borde vertical (izquierda oscura, derecha clara)
    [0, 0, 0, 1, 1, 1],  # Fila 1: repite patrÃ³n para que el filtro detecte borde consistente
    [0, 0, 0, 1, 1, 1],  # Fila 2: repite patrÃ³n
    [0, 0, 0, 1, 1, 1],  # Fila 3: repite patrÃ³n
])  # Cierra el array: dtype se infiere; aquÃ­ son enteros 0/1

# Kernel Sobel para bordes verticales
sobel_vertical = np.array([  # Define kernel Sobel vertical: responde fuerte donde hay cambios en la direcciÃ³n x (vertical edges)
    [-1, 0, 1],  # Fila superior: diferencia izquierda-derecha (detecta gradiente horizontal)
    [-2, 0, 2],  # Fila central: mayor peso en el centro para robustez
    [-1, 0, 1]  # Fila inferior: completa el patrÃ³n simÃ©trico del filtro
])  # Cierra el kernel: shape (3,3), se aplicarÃ¡ en cada regiÃ³n 3x3 de la imagen

edges = convolve2d_simple(image, sobel_vertical)  # Aplica convoluciÃ³n: produce mapa de activaciones donde el borde vertical es mÃ¡s intenso
print("Feature map (bordes verticales):")  # Imprime etiqueta: facilita interpretar la salida en consola
print(edges)  # Imprime el feature map numÃ©rico: valores altos/magnitudes indican detecciÃ³n del borde
```

### 5.3 Conceptos Clave de CNNs

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VOCABULARIO CNN                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  KERNEL (FILTRO)                                                â”‚
â”‚  â”œâ”€â”€ Matriz pequeÃ±a (3x3, 5x5) que detecta patrones             â”‚
â”‚  â”œâ”€â”€ Los valores del kernel son APRENDIDOS (backprop)           â”‚
â”‚  â””â”€â”€ Diferentes kernels detectan diferentes features            â”‚
â”‚                                                                 â”‚
â”‚  STRIDE                                                         â”‚
â”‚  â”œâ”€â”€ CuÃ¡ntos pÃ­xeles se mueve el kernel en cada paso            â”‚
â”‚  â”œâ”€â”€ stride=1: mueve 1 pÃ­xel (output grande)                    â”‚
â”‚  â””â”€â”€ stride=2: mueve 2 pÃ­xeles (output mÃ¡s pequeÃ±o)             â”‚
â”‚                                                                 â”‚
â”‚  PADDING                                                        â”‚
â”‚  â”œâ”€â”€ AÃ±adir ceros alrededor de la imagen                        â”‚
â”‚  â”œâ”€â”€ 'valid': sin padding (output mÃ¡s pequeÃ±o)                  â”‚
â”‚  â””â”€â”€ 'same': padding para mantener tamaÃ±o                       â”‚
â”‚                                                                 â”‚
â”‚  POOLING                                                        â”‚
â”‚  â”œâ”€â”€ Reduce dimensiones (downsampling)                          â”‚
â”‚  â”œâ”€â”€ Max Pooling: toma el mÃ¡ximo de cada regiÃ³n                 â”‚
â”‚  â””â”€â”€ Average Pooling: toma el promedio                          â”‚
â”‚                                                                 â”‚
â”‚  FEATURE MAP                                                    â”‚
â”‚  â””â”€â”€ Output de aplicar un filtro (lo que "ve" el filtro)        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.4 CÃ¡lculo de Dimensiones (Importante para ExÃ¡menes)

```python
def output_size(input_size: int, kernel_size: int,  # TamaÃ±o por eje (conv/pooling): Ãºtil para H_out o W_out en problemas de examen
                stride: int = 1, padding: int = 0) -> int:  # Usa divisiÃ³n piso (//) para obtener un entero vÃ¡lido
    """
    FÃ³rmula para calcular tamaÃ±o del output de convoluciÃ³n.

    output_size = floor((input + 2*padding - kernel) / stride) + 1
    """
    return (input_size + 2 * padding - kernel_size) // stride + 1  # Aplica floor((in+2P-K)/S)+1: si no cuadra, revisa padding/stride


# Ejemplos tÃ­picos de examen:
print("=== Ejercicios de dimensiones ===")  # Encabezado: imprime separador para ver los resultados de los casos en consola

# Ejemplo 1: MNIST sin padding
# Input: 28x28, Kernel: 5x5, Stride: 1, Padding: 0
out = output_size(28, 5, stride=1, padding=0)  # Esperado 24: 28-5+1
print(f"MNIST 28x28, kernel 5x5, stride 1: output = {out}x{out}")  # 24x24

# Ejemplo 2: Con padding 'same'
# Para mantener tamaÃ±o con kernel 3x3, necesitas padding=1
out = output_size(28, 3, stride=1, padding=1)  # Esperado 28: padding=1 compensa kernel 3x3 con stride 1
print(f"MNIST 28x28, kernel 3x3, padding 1: output = {out}x{out}")  # 28x28

# Ejemplo 3: Max Pooling 2x2 stride 2
out = output_size(24, 2, stride=2, padding=0)  # Esperado 12: pooling 2 con stride 2 reduce a la mitad
print(f"24x24, pooling 2x2 stride 2: output = {out}x{out}")  # 12x12

```


### 5.5 Arquitectura TÃ­pica de CNN

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ARQUITECTURA LENET-5 (ClÃ¡sica para MNIST)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Input: 28x28x1 (imagen grayscale)                              â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [CONV 5x5, 6 filtros] â†’ 24x24x6                                â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [ReLU]                                                         â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [MaxPool 2x2] â†’ 12x12x6                                        â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [CONV 5x5, 16 filtros] â†’ 8x8x16                                â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [ReLU]                                                         â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [MaxPool 2x2] â†’ 4x4x16 = 256 neuronas                          â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [Flatten] â†’ 256                                                â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [FC 120] â†’ 120                                                 â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [FC 84] â†’ 84                                                   â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [FC 10 + Softmax] â†’ 10 clases (dÃ­gitos 0-9)                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.6 Max Pooling

```python
def max_pool2d(x: np.ndarray, pool_size: int = 2) -> np.ndarray:  # Define max pooling 2D: reduce resoluciÃ³n tomando el mÃ¡ximo por ventana (downsampling)
    """
    Max Pooling 2D.

    Reduce dimensiones tomando el mÃ¡ximo de cada regiÃ³n.
    Hace la red mÃ¡s robusta a pequeÃ±as traslaciones.

    Args:
        x: Feature map (H, W)
        pool_size: TamaÃ±o de la ventana (tÃ­picamente 2)

    Returns:
        Pooled output (H//pool_size, W//pool_size)
    """
    H, W = x.shape  # Extrae dimensiones de entrada: altura/ancho del feature map (asume 2D)
    out_H, out_W = H // pool_size, W // pool_size  # Calcula dimensiones de salida (stride=pool_size): reduce por factor entero

    output = np.zeros((out_H, out_W))  # Inicializa salida: guardarÃ¡ el mÃ¡ximo de cada regiÃ³n (out_H,out_W)

    for i in range(out_H):  # Itera filas de salida: cada i corresponde a una ventana vertical de `pool_size` pÃ­xeles
        for j in range(out_W):  # Itera columnas de salida: cada j corresponde a una ventana horizontal de `pool_size` pÃ­xeles
            region = x[i*pool_size:(i+1)*pool_size,  # Extrae ventana en filas: desde i*pool_size hasta (i+1)*pool_size (no inclusivo)
                      j*pool_size:(j+1)*pool_size]  # Extrae ventana en columnas: define el bloque local cuyo mÃ¡ximo representarÃ¡ la regiÃ³n
            output[i, j] = np.max(region)  # Agrega mÃ¡ximo de la regiÃ³n: implementa invarianza parcial a traslaciones pequeÃ±as

    return output  # Devuelve mapa pooled: reduce tamaÃ±o y conserva activaciones mÃ¡s salientes por regiÃ³n


# Ejemplo
feature_map = np.array([  # Define feature map toy 4x4: valores sencillos para verificar visualmente el max pooling
    [1, 3, 2, 4],  # Fila 0: contiene mÃ¡ximo 4 en la esquina derecha
    [5, 6, 1, 2],  # Fila 1: contiene mÃ¡ximo 6 que deberÃ­a sobrevivir al pooling en la ventana superior izquierda
    [3, 2, 1, 0],  # Fila 2: valores decrecientes para probar pooling en la parte inferior izquierda
    [1, 2, 3, 4]  # Fila 3: contiene mÃ¡ximo 4 en la ventana inferior derecha
])  # Cierra el array: shape (4,4) compatible con pool_size=2

pooled = max_pool2d(feature_map, pool_size=2)  # Ejecuta max pooling 2x2: reduce 4x4 -> 2x2 tomando mÃ¡ximos por bloque
print("Original 4x4:")  # Imprime etiqueta del input: facilita lectura del ejemplo en consola
print(feature_map)  # Muestra matriz original: permite comparar contra salida pooled
print("\nMax Pooled 2x2:")  # Imprime etiqueta con salto de lÃ­nea: separa visualmente input y output
print(pooled)  # [[6, 4], [3, 4]]
```

### 5.7 Por QuÃ© Funcionan las CNNs

```
INTUICIÃ“N:

1. CAPAS INICIALES: Detectan features simples
   - Bordes horizontales, verticales, diagonales
   - Cambios de color, texturas

2. CAPAS MEDIAS: Combinan features simples
   - Esquinas, curvas, patrones

3. CAPAS PROFUNDAS: Features de alto nivel
   - Partes de objetos (ojos, ruedas, letras)

4. CAPAS FINALES: Objetos completos
   - "Esto es un 7", "Esto es un gato"

VENTAJAS CLAVE:
â”œâ”€â”€ Parameter sharing: mismo filtro en toda la imagen
â”œâ”€â”€ Sparse connectivity: cada output depende de regiÃ³n local
â”œâ”€â”€ Translation invariance: detecta patrones sin importar posiciÃ³n
â””â”€â”€ Hierarchical features: de simple a complejo
```

### 5.8 Recursos para Profundizar en CNNs

| Recurso | DescripciÃ³n |
|---------|-------------|
| [3B1B - But what is a convolution?](https://www.youtube.com/watch?v=KuXjwB4LzSA) | IntuiciÃ³n visual |
| [CS231n Stanford](http://cs231n.stanford.edu/) | Curso completo de CNNs |
| Deep Learning Book, Cap. 9 | TeorÃ­a formal |

---

## ğŸ“ DerivaciÃ³n AnalÃ­tica: Backpropagation a Mano (v3.2)

> ğŸ“ **SimulaciÃ³n de Examen:** *"Derive las ecuaciones de backpropagation para una red de 2 capas"*. Este es un clÃ¡sico de exÃ¡menes de posgrado.

### Red de 2 Capas: DerivaciÃ³n Completa

**Arquitectura:**
- Input: $x$ (vector de features)
- Capa 1: $z_1 = W_1 x + b_1$, $a_1 = \sigma(z_1)$
- Capa 2: $z_2 = W_2 a_1 + b_2$, $\hat{y} = \sigma(z_2)$
- Loss: $L = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$

#### Paso 1: Gradiente de la Capa de Salida

$$\frac{\partial L}{\partial z_2} = \hat{y} - y = \delta_2$$

(Resultado elegante gracias a la combinaciÃ³n sigmoid + cross-entropy)

$$\frac{\partial L}{\partial W_2} = \delta_2 \cdot a_1^T$$

$$\frac{\partial L}{\partial b_2} = \delta_2$$

#### Paso 2: Propagar el Error Hacia AtrÃ¡s (Capa Oculta)

$$\frac{\partial L}{\partial a_1} = W_2^T \delta_2$$

$$\frac{\partial L}{\partial z_1} = \frac{\partial L}{\partial a_1} \odot \sigma'(z_1) = W_2^T \delta_2 \odot a_1 \odot (1 - a_1) = \delta_1$$

$$\frac{\partial L}{\partial W_1} = \delta_1 \cdot x^T$$

$$\frac{\partial L}{\partial b_1} = \delta_1$$

#### Resumen: Las 4 Ecuaciones de Backprop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ECUACIONES DE BACKPROPAGATION                               â”‚
â”‚                                                             â”‚
â”‚ 1. Î´_L = âˆ‡_a L âŠ™ Ïƒ'(z_L)     Error en capa final           â”‚
â”‚                                                             â”‚
â”‚ 2. Î´_l = (W_{l+1}^T Î´_{l+1}) âŠ™ Ïƒ'(z_l)   Propagar atrÃ¡s    â”‚
â”‚                                                             â”‚
â”‚ 3. âˆ‚L/âˆ‚W_l = Î´_l Â· a_{l-1}^T   Gradiente de pesos           â”‚
â”‚                                                             â”‚
â”‚ 4. âˆ‚L/âˆ‚b_l = Î´_l              Gradiente de bias             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tu Entregable

Escribe en un documento (Markdown o LaTeX):
1. DerivaciÃ³n completa de backprop para red de 2 capas
2. Por quÃ© $\delta_L = \hat{y} - y$ cuando usamos sigmoid + cross-entropy
3. Diagrama de grafo computacional mostrando el flujo de gradientes

---

## ğŸ§ª Overfit on Small Batch: Debugging de Redes Neuronales (v3.3)

> âš ï¸ **CRÃTICO:** Esta es la tÃ©cnica #1 de debugging en Deep Learning. Si tu red no puede hacer overfitting en 10 ejemplos, tiene un bug.

### El Principio

```
REGLA DE ORO DEL DEBUGGING EN DL:

Una red neuronal DEBE poder memorizar un dataset pequeÃ±o.

Si entrenas con:
- 10 ejemplos
- Muchas Ã©pocas (1000+)
- Sin regularizaciÃ³n

El loss DEBE llegar a ~0.00 (o muy cercano).

Si NO llega a 0 â†’ TU IMPLEMENTACIÃ“N TIENE UN BUG
```

### Por QuÃ© Funciona

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OVERFIT TEST                                                â”‚
â”‚                                                             â”‚
â”‚ Dataset pequeÃ±o (10 ejemplos):                              â”‚
â”‚ - Capacidad de la red >> complejidad del dataset            â”‚
â”‚ - La red puede "memorizar" cada ejemplo perfectamente       â”‚
â”‚ - Loss debe â†’ 0 si backprop funciona                        â”‚
â”‚                                                             â”‚
â”‚ Si loss NO baja:                                            â”‚
â”‚ - Gradiente mal calculado                                   â”‚
â”‚ - Learning rate incorrecto                                  â”‚
â”‚ - Arquitectura rota (dimensiones)                           â”‚
â”‚ - Bug en forward o backward pass                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Script: `overfit_test.py` (Entregable Obligatorio v3.3)

```python
"""
Overfit Test - ValidaciÃ³n de Redes Neuronales
Si tu red no puede hacer overfit en 10 ejemplos, estÃ¡ rota.

Autor: [Tu nombre]
MÃ³dulo: 07 - Deep Learning
"""
import numpy as np  # Importa NumPy: se usa para arrays, MSE y operaciones numÃ©ricas del test
from typing import List, Tuple  # Importa tipos: documenta la firma (retorna passed e histÃ³rico de loss)


def overfit_test(  # Test diagnÃ³stico: fuerza al modelo a memorizar un dataset mÃ­nimo para validar backprop
    model,  # Modelo a evaluar: debe implementar .forward(), .backward() y .update() segÃºn este runner
    X_small: np.ndarray,  # Features del dataset pequeÃ±o: tÃ­picamente (n_samples, n_features)
    y_small: np.ndarray,  # Labels del dataset pequeÃ±o: shape compatible con output del modelo
    epochs: int = 2000,  # NÃºmero de Ã©pocas: debe ser alto para dar margen a que la loss baje a target
    target_loss: float = 0.01,  # Umbral de aprobaciÃ³n: si la loss final es < target_loss, consideramos que memoriza
    verbose: bool = True  # Controla prints: Ãºtil para debugging sin afectar el cÃ¡lculo
) -> Tuple[bool, List[float]]:  # Retorna (passed, loss_history) para automatizar validaciÃ³n y diagnÃ³stico
    """
    Test de overfitting: la red debe memorizar un dataset pequeÃ±o.

    Args:
        model: Tu red neuronal (debe tener .fit() y .forward())
        X_small: Dataset pequeÃ±o (10-20 ejemplos)
        y_small: Labels del dataset
        epochs: Ã‰pocas de entrenamiento
        target_loss: Loss objetivo (default: 0.01)
        verbose: Mostrar progreso

    Returns:
        (passed, loss_history)
    """
    if verbose:  # Si verbose estÃ¡ activo, mostramos banner y parÃ¡metros del test para facilitar debugging
        print("=" * 60)  # Separador visual: mejora legibilidad del log
        print("OVERFIT TEST: Â¿Puede tu red memorizar 10 ejemplos?")  # Mensaje: define el objetivo del test
        print("=" * 60)  # Cierra el banner superior
        print(f"Dataset size: {len(y_small)}")  # Reporta tamaÃ±o del dataset: ayuda a asegurar que es realmente â€œpequeÃ±oâ€
        print(f"Epochs: {epochs}")  # Reporta Ã©pocas: si es bajo, el test puede fallar por falta de entrenamiento
        print(f"Target loss: {target_loss}")  # Reporta umbral: criterio de Ã©xito/fracaso
        print("-" * 60)  # Separador antes de comenzar loop de entrenamiento

    # Entrenar
    loss_history = []  # Guarda la loss media por Ã©poca: permite ver si converge y detectar estancamientos
    for epoch in range(epochs):  # Loop de Ã©pocas: repetimos varias pasadas sobre el dataset pequeÃ±o
        # Forward pass para todos los ejemplos
        total_loss = 0.0  # Acumula pÃ©rdida de la Ã©poca: se promediarÃ¡ al final
        for i in range(len(y_small)):  # Itera cada ejemplo del dataset: entrenamiento muestra a muestra
            output = model.forward(X_small[i])  # Forward: predicciÃ³n actual para el ejemplo i
            loss = np.mean((output - y_small[i]) ** 2)  # MSE: loss simple para comprobar que el gradiente aprende
            total_loss += loss  # Suma loss por ejemplo: permite calcular la media por Ã©poca

            # Backward y update (asumiendo que model tiene estos mÃ©todos)
            model.backward(y_small[i])  # Backward: calcula gradientes internos usando el target del ejemplo i
            model.update(learning_rate=0.1)  # Update: aplica un paso de optimizaciÃ³n con LR fijo (ajustable)

        avg_loss = total_loss / len(y_small)  # Promedio de la Ã©poca: mÃ©trica comparable entre Ã©pocas
        loss_history.append(avg_loss)  # Guarda histÃ³rico: Ãºtil para graficar y para criterio final

        if verbose and epoch % 500 == 0:  # Log cada 500 Ã©pocas: balance entre visibilidad y ruido
            print(f"Epoch {epoch:4d}: Loss = {avg_loss:.6f}")  # Reporta loss: ayuda a ver si desciende hacia target

    final_loss = loss_history[-1]  # Loss final: se usa como criterio de aprobaciÃ³n del test
    passed = final_loss < target_loss  # CondiciÃ³n de Ã©xito: si puede memorizar, el gradiente y updates probablemente estÃ¡n bien

    if verbose:  # Imprime el diagnÃ³stico final: ayuda a decidir si hay bug o si solo falta tuning
        print("-" * 60)  # Separador: delimita fin del entrenamiento
        print(f"Final Loss: {final_loss:.6f}")  # Reporta la loss final alcanzada por el modelo
        if passed:  # Rama Ã©xito: la red pudo memorizar el dataset pequeÃ±o
            print("âœ“ PASSED: Tu red puede hacer overfitting")  # Indicador: criterio de overfit cumplido
            print("  â†’ El forward y backward pass funcionan correctamente")  # InterpretaciÃ³n: gradiente/update parecen correctos
        else:  # Rama fallo: no memorizÃ³, tÃ­picamente hay bug o hiperparÃ¡metros mal elegidos
            print("âœ— FAILED: Tu red NO puede hacer overfitting")  # Indicador: criterio de overfit no cumplido
            print("  â†’ Revisa tu implementaciÃ³n de backprop")  # Sugerencia principal: backprop suele ser el culpable
            print("  Posibles causas:")  # Lista de causas comunes para orientar el debugging
            print("  - Gradiente mal calculado")  # Error tÃ­pico: derivadas incorrectas o signos invertidos
            print("  - Learning rate muy bajo")  # Si LR es demasiado bajo, la loss puede bajar muy lento
            print("  - Bug en forward pass")  # Si forward estÃ¡ mal, backward tambiÃ©n serÃ¡ incorrecto
            print("  - Dimensiones incorrectas")  # Shapes incorrectas rompen el gradiente o el update

    return passed, loss_history  # Devuelve resultado + curva: permite asserts y anÃ¡lisis de convergencia


# ============================================================
# EJEMPLO: Test con XOR (debe pasar)
# ============================================================

def test_xor_overfit():  # Demo: prueba el overfit_test con el dataset XOR para validar el runner
    """Test: Una red pequeÃ±a debe resolver XOR perfectamente."""  # Docstring: criterio de Ã©xito del test (memorizar XOR en toy dataset)
    print("\n" + "=" * 60)  # Banner: separa visualmente el test del resto del output
    print("TEST: Overfit on XOR Problem")  # Mensaje: indica que se estÃ¡ probando overfit en XOR
    print("=" * 60)  # Cierra banner

    # XOR dataset (4 ejemplos)
    X = np.array([  # Inputs XOR: todas las combinaciones posibles de 2 bits
        [0, 0],  # Caso 00
        [0, 1],  # Caso 01
        [1, 0],  # Caso 10
        [1, 1]  # Caso 11
    ], dtype=np.float64)  # Fuerza float64: estabilidad numÃ©rica y consistencia en operaciones

    y = np.array([  # Targets XOR: 1 si bits difieren, 0 si son iguales
        [0],  # XOR(0,0)=0
        [1],  # XOR(0,1)=1
        [1],  # XOR(1,0)=1
        [0]  # XOR(1,1)=0
    ], dtype=np.float64)  # Misma precisiÃ³n que X: evita casts implÃ­citos

    # Crear red simple (2 -> 8 -> 1)
    # NOTA: Reemplaza esto con tu clase NeuralNetwork
    class SimpleNet:  # Red mÃ­nima de 2 capas (2â†’8â†’1): suficiente capacidad para memorizar XOR
        def __init__(self):  # Inicializa parÃ¡metros y cache para backprop
            np.random.seed(42)  # Semilla fija: hace reproducible el resultado (misma inicializaciÃ³n)
            self.W1 = np.random.randn(8, 2) * 0.5  # Pesos capa 1: (hidden=8, in=2) escalados para evitar saturaciÃ³n
            self.b1 = np.zeros((8, 1))  # Bias capa 1: (8,1) para broadcasting con z1
            self.W2 = np.random.randn(1, 8) * 0.5  # Pesos capa 2: (out=1, hidden=8)
            self.b2 = np.zeros((1, 1))  # Bias salida: (1,1)

            # Cache para backprop
            self.cache = {}  # Guarda tensores intermedios del forward: necesarios para el backward

        def sigmoid(self, z):  # ActivaciÃ³n sigmoid estable: evita overflow en exp
            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Ïƒ(z)=1/(1+e^-z) con clipping de z

        def forward(self, x):  # Forward: computa predicciÃ³n pasando por 2 capas con sigmoid
            x = x.reshape(-1, 1)  # Asegura vector columna (2,1): requerido para shapes de matmul
            z1 = self.W1 @ x + self.b1  # Logits capa 1: (8,2)@(2,1)+(8,1) -> (8,1)
            a1 = self.sigmoid(z1)  # ActivaciÃ³n oculta: introduce no linealidad
            z2 = self.W2 @ a1 + self.b2  # Logit salida: (1,8)@(8,1)+(1,1) -> (1,1)
            a2 = self.sigmoid(z2)  # Salida: probabilidad en (0,1)

            self.cache = {'x': x, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2}  # Cachea intermedios para backprop
            return a2.flatten()  # Devuelve vector 1D: compatible con el cÃ¡lculo de MSE del runner

        def backward(self, y_true):  # Backward: calcula gradientes dW/db para ambas capas
            y_true = np.array(y_true).reshape(-1, 1)  # Normaliza target a columna (1,1) para restas
            a2 = self.cache['a2']  # Recupera activaciÃ³n de salida
            a1 = self.cache['a1']  # Recupera activaciÃ³n oculta
            x = self.cache['x']  # Recupera input (columna)

            # Gradientes
            dz2 = a2 - y_true  # Error en salida (simplificado): sirve como delta para MSE/gradiente aproximado
            self.dW2 = dz2 @ a1.T  # Gradiente W2: (1,1)@(1,8) -> (1,8)
            self.db2 = dz2  # Gradiente b2: (1,1)

            da1 = self.W2.T @ dz2  # Propaga delta hacia capa oculta: (8,1)@(1,1)->(8,1)
            dz1 = da1 * a1 * (1 - a1)  # Delta oculta: multiplica por derivada de sigmoid Ïƒ'(z)=a(1-a)
            self.dW1 = dz1 @ x.T  # Gradiente W1: (8,1)@(1,2)->(8,2)
            self.db1 = dz1  # Gradiente b1: (8,1)

        def update(self, learning_rate):  # Update: aplica descenso por gradiente con el LR dado
            self.W1 -= learning_rate * self.dW1  # Actualiza W1
            self.b1 -= learning_rate * self.db1  # Actualiza b1
            self.W2 -= learning_rate * self.dW2  # Actualiza W2
            self.b2 -= learning_rate * self.db2  # Actualiza b2

    # Ejecutar test
    model = SimpleNet()  # Instancia la red simple: se usarÃ¡ para validar overfit en XOR
    passed, history = overfit_test(model, X, y, epochs=2000, target_loss=0.01)  # Ejecuta el runner: deberÃ­a llegar a loss < 0.01

    # Verificar predicciones finales
    print("\nPredicciones finales:")  # Encabezado: muestra predicciones despuÃ©s del entrenamiento
    for i in range(len(X)):  # Itera cada ejemplo de XOR
        pred = model.forward(X[i])  # Predice usando el modelo entrenado
        print(f"  Input: {X[i]} â†’ Pred: {pred[0]:.3f} (Target: {y[i][0]})")  # Compara predicciÃ³n vs target

    return passed  # Devuelve si pasÃ³: permite integrarlo en asserts/pytest o validaciÃ³n manual


if __name__ == "__main__":  # Entry point: permite ejecutar este archivo como script
    test_xor_overfit()  # Lanza el test XOR para verificar el overfit_test end-to-end
```

### Checklist de Debugging con Overfit Test

| SÃ­ntoma | DiagnÃ³stico | SoluciÃ³n |
|---------|-------------|----------|
| Loss no baja | Gradiente = 0 o NaN | Verificar derivadas con grad_check |
| Loss baja muy lento | Learning rate muy bajo | Aumentar LR (probar 0.1, 0.5, 1.0) |
| Loss oscila mucho | Learning rate muy alto | Reducir LR |
| Loss sube | Signos invertidos en gradiente | Revisar forward/backward |
| Loss = NaN | Overflow en exp/softmax | Usar versiones numÃ©ricamente estables |

---

## ğŸ¯ El Reto del Tablero Blanco (MetodologÃ­a Feynman)

Explica en **mÃ¡ximo 5 lÃ­neas** sin jerga tÃ©cnica:

1. **Â¿QuÃ© es backpropagation?**
   > Pista: Piensa en "culpar" a cada peso por el error.

2. **Â¿Por quÃ© ReLU es mejor que sigmoid en capas ocultas?**
   > Pista: Piensa en quÃ© pasa con el gradiente de sigmoid cuando z es muy grande o muy pequeÃ±o.

3. **Â¿QuÃ© hace una convoluciÃ³n en una imagen?**
   > Pista: Piensa en "deslizar una lupa" buscando un patrÃ³n especÃ­fico.

4. **Â¿Por quÃ© usamos pooling?**
   > Pista: Piensa en "resumir" una regiÃ³n y hacerla mÃ¡s pequeÃ±a.

---

## âœ… Checklist de FinalizaciÃ³n (v3.3)

### Conocimiento
- [ ] Entiendo la analogÃ­a neurona biolÃ³gica â†’ neurona artificial
- [ ] ImplementÃ© sigmoid, ReLU, tanh, softmax y sus derivadas
- [ ] Entiendo por quÃ© XOR no es linealmente separable
- [ ] ImplementÃ© forward pass para MLP
- [ ] Entiendo la Chain Rule aplicada a backpropagation
- [ ] ImplementÃ© backward pass calculando gradientes
- [ ] ImplementÃ© SGD, SGD+Momentum y Adam
- [ ] Mi red resuelve el problema XOR

### CNNs (TeorÃ­a)
- [ ] Entiendo quÃ© es convoluciÃ³n, stride, padding y pooling
- [ ] Puedo calcular dimensiones de output de una CNN
- [ ] Conozco la arquitectura LeNet-5

### CNNs (PrÃ¡ctica)
- [ ] ImplementÃ© forward pass (NumPy) de convoluciÃ³n + pooling para una arquitectura tipo LeNet
- [ ] EntrenÃ© una CNN equivalente con PyTorch usando `scripts/train_cnn_pytorch.py`

### Sequence Modeling (Light)
- [ ] EjecutÃ© `scripts/simple_rnn_forward.py` y verifiquÃ© shapes `(batch,time,features)`
- [ ] Puedo explicar quÃ© cambia al variar `batch`, `time` y `hidden`

### Entregables de CÃ³digo
- [ ] `neural_network.py` con tests pasando
- [ ] `mypy src/` pasa sin errores
- [ ] `pytest tests/` pasa sin errores

### Overfit Test (v3.3 - Obligatorio)
- [ ] **`overfit_test.py` implementado**
- [ ] **Mi red hace overfit en XOR (loss < 0.01)**
- [ ] Si el test falla, debuggeÃ© con grad_check

### DerivaciÃ³n AnalÃ­tica (Obligatorio)
- [ ] DerivÃ© las ecuaciones de backprop a mano
- [ ] Documento con derivaciÃ³n completa (Markdown o LaTeX)
- [ ] Diagrama de grafo computacional

### MetodologÃ­a Feynman
- [ ] Puedo explicar backpropagation en 5 lÃ­neas sin jerga
- [ ] Puedo explicar ReLU vs sigmoid en 5 lÃ­neas
- [ ] Puedo explicar convoluciÃ³n en 5 lÃ­neas
- [ ] Puedo explicar pooling en 5 lÃ­neas

---

## ğŸ”— NavegaciÃ³n

| Anterior | Ãndice | Siguiente |
|----------|--------|-----------|
| [06_UNSUPERVISED_LEARNING](06_UNSUPERVISED_LEARNING.md) | [00_INDICE](00_INDICE.md) | [08_PROYECTO_MNIST](08_PROYECTO_MNIST.md) |
