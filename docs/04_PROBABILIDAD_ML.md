# MÃ³dulo 04: Probabilidad Esencial para Machine Learning

> **Semana 8 | Prerequisito para entender Loss Functions y GMM**
> **FilosofÃ­a: Solo la probabilidad que necesitas para la LÃ­nea 1**

---

## ğŸ¯ Objetivo del MÃ³dulo

Dominar los **conceptos mÃ­nimos de probabilidad** necesarios para:

1. Entender **Logistic Regression** como modelo probabilÃ­stico
2. Comprender **Cross-Entropy Loss** y por quÃ© funciona
3. Prepararte para **Gaussian Mixture Models (GMM)** en Unsupervised
4. Entender **Softmax** como distribuciÃ³n de probabilidad

> âš ï¸ **Nota:** Este NO es el curso completo de Probabilidad (LÃ­nea 2). Es solo lo esencial para ML.

---

<a id="m04-0"></a>

## ğŸ§­ CÃ³mo usar este mÃ³dulo (modo 0â†’100)

**PropÃ³sito:** conectar probabilidad con lo que realmente usarÃ¡s en el Pathway:

- pÃ©rdidas (cross-entropy) como *negative log-likelihood*
- clasificaciÃ³n probabilÃ­stica (logistic/softmax)
- gaussianas como base de modelos generativos (GMM)
- estabilidad numÃ©rica (evitar `NaN`)

### Objetivos de aprendizaje (medibles)

Al terminar el mÃ³dulo podrÃ¡s:

- **Explicar** `P(A|B)` y el teorema de Bayes con un ejemplo de clasificaciÃ³n.
- **Aplicar** el punto de vista de MLE: â€œelegir parÃ¡metros que hacen los datos mÃ¡s probablesâ€.
- **Derivar** por quÃ© minimizar cross-entropy equivale a maximizar log-likelihood (binaria y multiclase).
- **Implementar** softmax y log-softmax de forma numÃ©ricamente estable (log-sum-exp).
- **Diagnosticar** fallos tÃ­picos: `log(0)`, overflow/underflow, probabilidades que no suman 1.

### Prerrequisitos

- De `MÃ³dulo 01`: NumPy (vectorizaciÃ³n, `axis`, broadcasting).
- De `MÃ³dulo 03`: Chain Rule y gradiente (para entender el salto a `MÃ³dulo 05/07`).

Enlaces rÃ¡pidos:

- [RECURSOS.md](RECURSOS.md)
- [GLOSARIO: Binary Cross-Entropy](GLOSARIO.md#binary-cross-entropy)
- [GLOSARIO: Softmax](GLOSARIO.md#softmax)
- [GLOSARIO: Chain Rule](GLOSARIO.md#chain-rule)

### IntegraciÃ³n con Plan v4/v5

- [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)
- [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md)
- Registro de errores: `study_tools/DIARIO_ERRORES.md`
- EvaluaciÃ³n (rÃºbrica): [study_tools/RUBRICA_v1.md](../study_tools/RUBRICA_v1.md) (scope `M04` en `rubrica.csv`; incluye PB-8)

### Recursos (cuÃ¡ndo usarlos)

| Prioridad | Recurso | CuÃ¡ndo usarlo en este mÃ³dulo | Para quÃ© |
|----------|---------|------------------------------|----------|
| **Obligatorio** | `study_tools/DIARIO_ERRORES.md` | Cada vez que aparezca `NaN`, `inf`, `log(0)` u overflow/underflow | Registrar el caso y crear un â€œfixâ€ reproducible |
| **Obligatorio** | [StatQuest - Maximum Likelihood](https://www.youtube.com/watch?v=XepXtl9YKwc) | Antes (o durante) la secciÃ³n de MLE y cross-entropy | Alinear intuiciÃ³n de â€œmaximizar verosimilitudâ€ |
| **Complementario** | [3Blue1Brown - Bayes Theorem](https://www.youtube.com/watch?v=HZGCoVF3YvM) | Cuando Bayes se sienta â€œfÃ³rmula sin sentidoâ€ (dÃ­a 3-4) | Visualizar prior/likelihood/posterior |
| **Complementario** | [Mathematics for ML (book)](https://mml-book.github.io/) | Al implementar Gaussiana multivariada y covarianza | Refuerzo de notaciÃ³n y derivaciones |
| **Opcional** | [RECURSOS.md](RECURSOS.md) | Al terminar el mÃ³dulo (para planificar LÃ­nea 2 o profundizar) | Elegir rutas de estudio sin romper el foco de LÃ­nea 1 |

### Mapa conceptual (quÃ© conecta con quÃ©)

- **MLE â†’ Cross-Entropy:** sustenta Logistic Regression (MÃ³dulo 05) y BCE/CCE en Deep Learning (MÃ³dulo 07).
- **Gaussiana multivariada:** es el â€œÃ¡tomoâ€ de GMM (MÃ³dulo 06).
- **Softmax + Log-Sum-Exp:** evita inestabilidad numÃ©rica en clasificaciÃ³n multiclase (MÃ³dulo 05/07).

---

## ğŸ“š Contenido

### DÃ­a 1-2: Fundamentos de Probabilidad

#### 1.1 Probabilidad BÃ¡sica

```text
P(A) = casos favorables / casos totales

Propiedades:
- 0 â‰¤ P(A) â‰¤ 1
- P(Î©) = 1 (espacio muestral)
- P(âˆ…) = 0 (evento imposible)
```

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 1.1: Probabilidad BÃ¡sica</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Probabilidad como â€œregla de conteoâ€ + axiomas mÃ­nimos
- **ID (opcional):** `M04-T01_1`
- **DuraciÃ³n estimada:** 45â€“90 min
- **Nivel:** BÃ¡sico
- **Dependencias:** M01 (manejo bÃ¡sico de notaciÃ³n y nÃºmeros)

#### 2) Objetivos
- Calcular `P(A)` en ejemplos discretos simples y verificar que `0 â‰¤ P(A) â‰¤ 1`.
- Explicar quÃ© son `Î©`, `âˆ…` y por quÃ© `P(Î©)=1`.

#### 3) Relevancia
- En ML casi todo termina siendo â€œprobabilidadâ€ o â€œlog-probabilidadâ€ (pÃ©rdidas como NLL).

#### 4) Mapa conceptual mÃ­nimo
- **Espacio muestral (`Î©`)** â†’ posibles resultados.
- **Evento (`A`)** âŠ† `Î©` â†’ subconjunto de resultados.
- **Probabilidad** â†’ nÃºmero en [0,1] que cuantifica quÃ© tan â€œfrecuenteâ€ es el evento.

#### 5) Definiciones esenciales
- `Î©`: conjunto de resultados posibles.
- `A`: evento.
- `P(A)`: probabilidad del evento.

#### 6) ExplicaciÃ³n didÃ¡ctica
- Regla de sanidad: si te da `P(A)>1` o negativa, tu modelado estÃ¡ mal.

#### 7) Ejemplo modelado
- Dado un dado justo: `P(A=â€œsale parâ€) = 3/6 = 0.5`.

#### 8) PrÃ¡ctica guiada
- Escribe 3 eventos distintos en un dado (por ejemplo `{1}`, `{1,2,3}`, `{2,4,6}`) y calcula `P`.

#### 9) PrÃ¡ctica independiente
- Baraja estÃ¡ndar: calcula `P(A=â€œcarta rojaâ€)` y `P(B=â€œcorazÃ³nâ€)`.

#### 10) AutoevaluaciÃ³n
- Â¿Por quÃ© `P(âˆ…)=0` es consistente con la idea de â€œcasos favorables/casos totalesâ€?

#### 11) Errores comunes
- Confundir â€œprobabilidadâ€ con â€œconteoâ€ sin normalizar por el total.
- Olvidar definir el espacio muestral antes de calcular probabilidades.

#### 12) RetenciÃ³n
- (dÃ­a 2) define `Î©`, `A` y escribe las 3 propiedades bÃ¡sicas (rango, `P(Î©)`, `P(âˆ…)`).

#### 13) DiferenciaciÃ³n
- Avanzado: interpreta probabilidad como frecuencia relativa lÃ­mite (intuiciÃ³n frequentista).

#### 14) Recursos
- StatQuest (intro de probabilidad) / cualquier texto de probabilidad bÃ¡sica.

#### 15) Nota docente
- Exigir siempre: â€œÂ¿CuÃ¡l es `Î©`?â€ antes de aceptar un `P(A)`.
</details>

#### 1.2 Probabilidad Condicional

```text
P(A|B) = P(A âˆ© B) / P(B)

"Probabilidad de A dado que B ocurriÃ³"
```

**Ejemplo en ML:**
- P(spam | contiene "gratis") = Â¿QuÃ© tan probable es spam si el email dice "gratis"?

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 1.2: Probabilidad Condicional</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Condicionar = restringir el universo a â€œB ocurriÃ³â€
- **ID (opcional):** `M04-T01_2`
- **DuraciÃ³n estimada:** 60â€“120 min
- **Nivel:** BÃ¡sicoâ€“Intermedio
- **Dependencias:** 1.1

#### 2) Objetivos
- Interpretar `P(A|B)` en lenguaje natural (â€œprobabilidad de A dado Bâ€).
- Usar `P(A|B)=P(Aâˆ©B)/P(B)` y reconocer cuÃ¡ndo aplica (si `P(B)>0`).

#### 3) Relevancia
- ClasificaciÃ³n probabilÃ­stica en ML se formula como `P(clase|datos)`.

#### 4) Mapa conceptual mÃ­nimo
- **IntersecciÃ³n** `Aâˆ©B`: ambos ocurren.
- **CondiciÃ³n** `|B`: nos quedamos solo con los casos donde B ocurre.

#### 5) Definiciones esenciales
- `P(Aâˆ©B)`: probabilidad conjunta.
- `P(A|B)`: probabilidad condicional.

#### 6) ExplicaciÃ³n didÃ¡ctica
- IntuiciÃ³n: al condicionar, el denominador cambia; ya no divides entre â€œtodoâ€, sino entre â€œlos casos con Bâ€.

#### 7) Ejemplo modelado
- Si en un dataset el 10% son spam, pero si contiene â€œgratisâ€ el 80% son spam, entonces `P(spam|gratis)=0.8`.

#### 8) PrÃ¡ctica guiada
- Construye una tabla 2Ã—2 (spam/ham vs contiene gratis/no) y calcula `P(spam|gratis)`.

#### 9) PrÃ¡ctica independiente
- Da un ejemplo donde `P(A|B) > P(A)` y explica por quÃ© no es contradictorio.

#### 10) AutoevaluaciÃ³n
- Â¿QuÃ© ocurre si `P(B)=0`? Â¿Por quÃ© la definiciÃ³n falla?

#### 11) Errores comunes
- Confundir `P(A|B)` con `P(B|A)`.
- Olvidar que `P(Aâˆ©B)` no es `P(A)P(B)` a menos que haya independencia.

#### 12) RetenciÃ³n
- (dÃ­a 2) escribe la fÃ³rmula de `P(A|B)` y un ejemplo en una frase.

#### 13) DiferenciaciÃ³n
- Avanzado: conecta con â€œactualizaciÃ³n de creenciasâ€ (preview a Bayes).

#### 14) Recursos
- SecciÃ³n de probabilidad condicional en cualquier material de probabilidad.

#### 15) Nota docente
- Pedir al alumno que primero responda verbalmente (â€œÂ¿quÃ© significa dado B?â€) antes de calcular.
</details>

#### 1.3 Independencia

```text
A y B son independientes si:
P(A âˆ© B) = P(A) Â· P(B)

Equivalente a:
P(A|B) = P(A)
```

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 1.3: Independencia</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Independencia: â€œsaber B no cambia Aâ€
- **ID (opcional):** `M04-T01_3`
- **DuraciÃ³n estimada:** 60â€“120 min
- **Nivel:** Intermedio
- **Dependencias:** 1.1, 1.2

#### 2) Objetivos
- Reconocer equivalencias: `P(Aâˆ©B)=P(A)P(B)` y `P(A|B)=P(A)`.
- Evaluar con ejemplos si una suposiciÃ³n de independencia es razonable.

#### 3) Relevancia
- Naive Bayes se sostiene sobre una suposiciÃ³n fuerte de independencia condicional.

#### 4) Mapa conceptual mÃ­nimo
- **Dependencia**: informaciÃ³n sobre B cambia tu probabilidad de A.
- **Independencia**: no cambia.

#### 5) Definiciones esenciales
- A y B independientes si `P(A|B)=P(A)` (cuando `P(B)>0`).

#### 6) ExplicaciÃ³n didÃ¡ctica
- La independencia casi nunca es exacta en datos reales; se usa como aproximaciÃ³n Ãºtil.

#### 7) Ejemplo modelado
- En una moneda justa: eventos â€œsale caraâ€ y â€œsale cruzâ€ en el mismo tiro no aplican (mutuamente excluyentes), ojo: no es independencia.

#### 8) PrÃ¡ctica guiada
- Da un ejemplo de eventos independientes (dos tiros de moneda) y uno claramente dependiente.

#### 9) PrÃ¡ctica independiente
- Explica por quÃ© â€œmutuamente excluyenteâ€ no implica â€œindependienteâ€.

#### 10) AutoevaluaciÃ³n
- Â¿QuÃ© valor deberÃ­a tener `P(Aâˆ©B)` si A y B son independientes?

#### 11) Errores comunes
- Confundir independencia con exclusiÃ³n mutua.
- Asumir independencia sin justificar (y luego sorprenderse por resultados malos en Naive Bayes).

#### 12) RetenciÃ³n
- (dÃ­a 2) memoriza una equivalencia: `P(Aâˆ©B)=P(A)P(B)`.

#### 13) DiferenciaciÃ³n
- Avanzado: independencia condicional `P(A,B|C)=P(A|C)P(B|C)` (preview a Naive Bayes).

#### 14) Recursos
- Lecturas de independencia y diagramas de Venn.

#### 15) Nota docente
- Pedir al alumno que traduzca a lenguaje natural: â€œsaber B no me da info sobre Aâ€.
</details>

---

### DÃ­a 3-4: Teorema de Bayes (CrÃ­tico para ML)

#### 2.1 La FÃ³rmula

```text
            P(B|A) Â· P(A)
P(A|B) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
               P(B)

Donde:
- P(A|B) = Posterior (lo que queremos calcular)
- P(B|A) = Likelihood (verosimilitud)
- P(A)   = Prior (conocimiento previo)
- P(B)   = Evidence (normalizador)
```

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 2.1: Teorema de Bayes (la fÃ³rmula)</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Bayes = reordenar condicionales (posterior = likelihoodÂ·prior / evidence)
- **ID (opcional):** `M04-T02_1`
- **DuraciÃ³n estimada:** 60â€“120 min
- **Nivel:** Intermedio
- **Dependencias:** 1.2 (condicional), 1.3 (independencia como contraste)

#### 2) Objetivos
- Identificar los 4 tÃ©rminos: posterior, likelihood, prior, evidence.
- Aplicar Bayes en un ejemplo tipo clasificaciÃ³n y explicar quÃ© significa cada tÃ©rmino.

#### 3) Relevancia
- Mucho ML supervisado puede verse como inferencia: estimar `P(clase|datos)`.

#### 4) Mapa conceptual mÃ­nimo
- **Prior**: lo que creÃ­as antes.
- **Likelihood**: quÃ© tan compatibles son los datos con la clase.
- **Posterior**: lo que crees despuÃ©s de ver datos.
- **Evidence**: normalizador para que sume 1.

#### 5) Definiciones esenciales
- `P(A|B) = P(B|A)P(A) / P(B)`.

#### 6) ExplicaciÃ³n didÃ¡ctica
- Para comparar clases, muchas veces basta el numerador `P(datos|clase)P(clase)` (posterior sin normalizar).

#### 7) Ejemplo modelado
- Spam: `P(spam|palabras) âˆ P(palabras|spam)Â·P(spam)`.

#### 8) PrÃ¡ctica guiada
- Define un prior `P(spam)` y dos likelihoods y calcula quÃ© clase gana (sin normalizar).

#### 9) PrÃ¡ctica independiente
- Crea un ejemplo con una enfermedad rara: prior pequeÃ±o, likelihood grande; discute el resultado.

#### 10) AutoevaluaciÃ³n
- Â¿QuÃ© rol cumple `P(B)`? Â¿Por quÃ© no depende de A?

#### 11) Errores comunes
- Confundir posterior con likelihood.
- Mezclar `P(A|B)` con `P(B|A)`.

#### 12) RetenciÃ³n
- (dÃ­a 2) escribe de memoria: posterior = likelihood Ã— prior / evidence.

#### 13) DiferenciaciÃ³n
- Avanzado: conecta con Naive Bayes (producto de likelihoods por feature en log).

#### 14) Recursos
- 3Blue1Brown Bayes (visual), StatQuest Bayes (intuiciÃ³n).

#### 15) Nota docente
- Pedir â€œtraducciÃ³n verbalâ€ de cada tÃ©rmino antes de hacer nÃºmeros.
</details>

#### 2.2 InterpretaciÃ³n para ML

```text
              P(datos|clase) Â· P(clase)
P(clase|datos) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                      P(datos)

Ejemplo: ClasificaciÃ³n de spam
- P(spam|palabras) = P(palabras|spam) Â· P(spam) / P(palabras)
```

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 2.2: InterpretaciÃ³n de Bayes para ML</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Bayes como clasificador: comparar posteriors (a veces sin normalizar)
- **ID (opcional):** `M04-T02_2`
- **DuraciÃ³n estimada:** 60â€“120 min
- **Nivel:** Intermedio
- **Dependencias:** 2.1, 1.2

#### 2) Objetivos
- Reescribir un problema de clasificaciÃ³n como `argmax_c P(c|x)`.
- Explicar por quÃ© `P(x)` puede omitirse al comparar clases (misma evidencia).

#### 3) Relevancia
- Este marco conecta directamente con Logistic Regression/Softmax: â€œsalidas como probabilidadesâ€.

#### 4) Mapa conceptual mÃ­nimo
- **Modelo generativo (tipo Bayes/Naive Bayes):** modela `P(x|c)` y `P(c)`.
- **Inferencia:** obtiene `P(c|x)`.

#### 5) Definiciones esenciales
- **Posterior sin normalizar:** `score(c) = P(x|c)Â·P(c)`.
- **DecisiÃ³n MAP:** elegir la clase con mayor posterior.

#### 6) ExplicaciÃ³n didÃ¡ctica
- Si solo quieres la clase, no necesitas `P(x)`; si quieres probabilidades calibradas, sÃ­.

#### 7) Ejemplo modelado
- Spam vs ham: compara `P(palabras|spam)P(spam)` contra `P(palabras|ham)P(ham)`.

#### 8) PrÃ¡ctica guiada
- Usa dos priors distintos (spam raro vs frecuente) y observa cÃ³mo cambia la decisiÃ³n.

#### 9) PrÃ¡ctica independiente
- Explica un caso donde el likelihood gana pero el prior lo revierte (o viceversa).

#### 10) AutoevaluaciÃ³n
- Â¿CuÃ¡ndo te importa `P(x)`? (pista: cuando quieres una probabilidad real, no solo ranking)

#### 11) Errores comunes
- Confundir â€œlikelihoodâ€ con â€œposteriorâ€.
- Creer que omitir `P(x)` es â€œincorrectoâ€ en clasificaciÃ³n (no lo es para argmax).

#### 12) RetenciÃ³n
- (dÃ­a 2) memoriza: `P(c|x) âˆ P(x|c)P(c)`.

#### 13) DiferenciaciÃ³n
- Avanzado: en vez de multiplicar, usa logs: `log P(x|c) + log P(c)`.

#### 14) Recursos
- StatQuest: Bayes classifier / Naive Bayes.

#### 15) Nota docente
- Pedir al alumno que seÃ±ale quÃ© tÃ©rmino es â€œmodeloâ€ (`P(x|c)`) y cuÃ¡l es â€œcreencia previaâ€ (`P(c)`).
</details>

#### 2.3 ImplementaciÃ³n en Python

```python
import numpy as np

def bayes_classifier(x: np.ndarray,
                     likelihood_spam: float,
                     likelihood_ham: float,
                     prior_spam: float = 0.3) -> str:
    """
    Clasificador Bayesiano simple.

    Args:
        x: CaracterÃ­sticas del email (simplificado)
        likelihood_spam: P(x|spam)
        likelihood_ham: P(x|ham)
        prior_spam: P(spam) - conocimiento previo

    Returns:
        'spam' o 'ham'
    """
    prior_ham = 1 - prior_spam

    # Posterior (sin normalizar, solo comparamos)
    posterior_spam = likelihood_spam * prior_spam
    posterior_ham = likelihood_ham * prior_ham

    return 'spam' if posterior_spam > posterior_ham else 'ham'


# Ejemplo: Email con palabra "gratis"
# P("gratis"|spam) = 0.8, P("gratis"|ham) = 0.1
result = bayes_classifier(
    x=None,  # simplificado
    likelihood_spam=0.8,
    likelihood_ham=0.1,
    prior_spam=0.3
)
print(f"ClasificaciÃ³n: {result}")  # spam
```

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 2.3: ImplementaciÃ³n de Bayes en Python</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** De la fÃ³rmula a cÃ³digo: calcular scores y decidir
- **ID (opcional):** `M04-T02_3`
- **DuraciÃ³n estimada:** 60â€“120 min
- **Nivel:** Intermedio
- **Dependencias:** 2.1, 2.2

#### 2) Objetivos
- Implementar un clasificador Bayesiano mÃ­nimo y explicar cada variable.
- Separar â€œcÃ¡lculo de scoreâ€ de â€œdecisiÃ³n finalâ€ (`argmax`).

#### 3) Relevancia
- Te entrena a convertir fÃ³rmulas en implementaciones legibles (habilidad clave para ML desde cero).

#### 4) Mapa conceptual mÃ­nimo
- **Inputs:** likelihoods + priors.
- **Procesamiento:** score por clase.
- **Output:** clase ganadora.

#### 5) Definiciones esenciales
- `posterior_spam âˆ likelihood_spam * prior_spam`.

#### 6) ExplicaciÃ³n didÃ¡ctica
- En problemas reales, multiplicar muchas probabilidades causa underflow â†’ usar log-sum (preview).

#### 7) Ejemplo modelado
- El ejemplo usa â€œposterior sin normalizarâ€ para comparar clases.

#### 8) PrÃ¡ctica guiada
- Extiende el cÃ³digo para que devuelva tambiÃ©n el score de ambas clases.

#### 9) PrÃ¡ctica independiente
- Cambia priors y likelihoods y escribe 3 casos donde el resultado cambie.

#### 10) AutoevaluaciÃ³n
- Â¿Por quÃ© no aparece `P(datos)` en el cÃ³digo?

#### 11) Errores comunes
- Tratar `x` como si se usara cuando el ejemplo lo deja simplificado.
- Mezclar probabilidades con porcentajes (0.8 vs 80).

#### 12) RetenciÃ³n
- (dÃ­a 2) escribe una funciÃ³n que compare 2 clases usando `score = likelihood*prior`.

#### 13) DiferenciaciÃ³n
- Avanzado: reescribe el clasificador en log-espacio: `log_score = log_likelihood + log_prior`.

#### 14) Recursos
- Numpy docs: `np.log`, manejo de underflow/overflow.

#### 15) Nota docente
- Pedir que el alumno comente (en voz) quÃ© representa cada parÃ¡metro: prior vs likelihood.
</details>

#### 2.4 Naive Bayes (ConexiÃ³n con Supervised Learning)

```python
def naive_bayes_predict(X: np.ndarray,
                        class_priors: np.ndarray,
                        feature_probs: dict) -> np.ndarray:
    """
    Naive Bayes asume independencia entre features:
    P(x1, x2, ..., xn | clase) = P(x1|clase) Â· P(x2|clase) Â· ... Â· P(xn|clase)

    Esta "ingenuidad" simplifica mucho el cÃ¡lculo.
    """
    n_samples = X.shape[0]
    n_classes = len(class_priors)

    log_posteriors = np.zeros((n_samples, n_classes))

    for c in range(n_classes):
        # Log para evitar underflow con muchas features
        log_prior = np.log(class_priors[c])
        log_likelihood = np.sum(np.log(feature_probs[c][X]), axis=1)
        log_posteriors[:, c] = log_prior + log_likelihood

    return np.argmax(log_posteriors, axis=1)
```

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 2.4: Naive Bayes</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Naive Bayes: independencia condicional para escalar a muchas features
- **ID (opcional):** `M04-T02_4`
- **DuraciÃ³n estimada:** 60â€“150 min
- **Nivel:** Intermedio
- **Dependencias:** 1.3 (independencia), 2.1 (Bayes)

#### 2) Objetivos
- Explicar la suposiciÃ³n: `P(x1,â€¦,xn|c) = Î _i P(xi|c)`.
- Entender por quÃ© se usa log: `log Î  = Î£ log` (evitar underflow).

#### 3) Relevancia
- Es un baseline fuerte en texto y problemas discretos; enseÃ±a buenas prÃ¡cticas numÃ©ricas.

#### 4) Mapa conceptual mÃ­nimo
- **Modelo:** aprende `P(xi|c)` por feature y `P(c)`.
- **PredicciÃ³n:** suma log-likelihoods + log-prior.

#### 5) Definiciones esenciales
- `log_posterior(c|x) = log P(c) + Î£_i log P(x_i|c)`.

#### 6) ExplicaciÃ³n didÃ¡ctica
- â€œNaiveâ€ no significa inÃºtil: significa *suposiciÃ³n simplificadora* para poder multiplicar muchos tÃ©rminos.

#### 7) Ejemplo modelado
- En texto (bag-of-words): cada palabra aporta un tÃ©rmino de log-likelihood.

#### 8) PrÃ¡ctica guiada
- Implementa una versiÃ³n binaria con 2 clases y 3 features discretas y verifica con un mini dataset.

#### 9) PrÃ¡ctica independiente
- Discute un caso donde la independencia condicional es claramente falsa (features redundantes) y quÃ© esperas que pase.

#### 10) AutoevaluaciÃ³n
- Â¿Por quÃ© `np.log` transforma multiplicaciones en sumas y por quÃ© eso ayuda en cÃ³mputo?

#### 11) Errores comunes
- No suavizar probabilidades â†’ `log(0)`.
- Confundir `P(x|c)` con `P(c|x)`.

#### 12) RetenciÃ³n
- (dÃ­a 2) memoriza el score: `log_prior + sum(log_likelihoods)`.

#### 13) DiferenciaciÃ³n
- Avanzado: introduce Laplace smoothing (Î±) para evitar ceros.

#### 14) Recursos
- StatQuest Naive Bayes; notas de smoothing.

#### 15) Nota docente
- Pedir una demostraciÃ³n de underflow: multiplicar 100 probabilidades ~0.01 y ver que colapsa sin log.
</details>

---

## ğŸ§© Micro-CapÃ­tulo Maestro: Maximum Likelihood Estimation (MLE) â€” Nivel: Avanzado

### 1) IntuiciÃ³n (la metÃ¡fora del detective)

Imagina que eres un detective que llega a una escena del crimen (tus **datos** `X`).

- Tienes una lista de sospechosos (tus **modelos**).
- Cada sospechoso tiene un comportamiento ajustable por perillas (tus **parÃ¡metros** `Î¸`).

MLE pregunta:

> **Â¿QuÃ© valores de `Î¸` hacen MÃS PROBABLE que estos datos especÃ­ficos hayan ocurrido?**

Importante:

- No estamos diciendo â€œquÃ© parÃ¡metro es mÃ¡s probableâ€ (eso serÃ­a un enfoque Bayesiano).
- Estamos diciendo â€œquÃ© parÃ¡metro le da la mayor probabilidad a los datos que YA vimosâ€.

### 2) FormalizaciÃ³n (likelihood y log-likelihood)

Sea `X = {x1, x2, ..., xn}` un conjunto de datos i.i.d.

La **likelihood** es:

`L(Î¸ | X) = P(X | Î¸) = Î _{i=1}^{n} P(x_i | Î¸)`

Como multiplicar muchos nÃºmeros pequeÃ±os causa underflow, usamos log:

`â„“(Î¸) = log L(Î¸|X) = Î£_{i=1}^{n} log P(x_i | Î¸)`

Como `log` es monÃ³tona creciente, maximizar `L` y maximizar `â„“` es equivalente:

`Î¸_MLE = argmax_Î¸ â„“(Î¸)`

### 3) DerivaciÃ³n clave: de MLE a MSE (RegresiÃ³n Lineal)

La idea conceptual: cuando usas **MSE**, estÃ¡s asumiendo implÃ­citamente un modelo de ruido.

SupÃ³n que tu regresiÃ³n lineal es:

`y = XÎ² + Îµ` con `Îµ ~ N(0, ÏƒÂ² I)`

Entonces la probabilidad de observar `y` dado `Î²` es Gaussiana:

`P(y | X, Î²) âˆ exp( - (1/(2ÏƒÂ²)) ||y - XÎ²||Â² )`

Tomando log-likelihood y tirando constantes que no dependen de `Î²`:

`â„“(Î²) = const - (1/(2ÏƒÂ²)) ||y - XÎ²||Â²`

Maximizar `â„“(Î²)` equivale a minimizar `||y - XÎ²||Â²`.

ConclusiÃ³n:

- Minimizar **SSE/MSE** es exactamente hacer **MLE** bajo ruido Gaussiano.
- Esta conexiÃ³n es el puente directo hacia **Statistical Estimation** (LÃ­nea 2).

### 4) ConexiÃ³n LÃ­nea 2: estimadores, sesgo y varianza (intuiciÃ³n)

En LÃ­nea 2, la palabra clave es **estimador**: una regla que convierte datos en un parÃ¡metro.

- Un **estimador** es una funciÃ³n: `\hat{Î¸} = g(X)`.
- **Sesgo (bias):** si `E[\hat{Î¸}]` no coincide con el valor real `Î¸`.
- **Varianza:** cuÃ¡nto cambia `\hat{Î¸}` si repites el muestreo.

Regla mental:

- **MÃ¡s bias** suele dar **menos varianza**.
- **Menos bias** suele dar **mÃ¡s varianza**.

Esto reaparece en ML como *bias-variance tradeoff*.

### 5) TeorÃ­a de Estimadores (lo que te evalÃºan en proyectos/examen)

AquÃ­ pasamos de la intuiciÃ³n a una formalizaciÃ³n que aparece mucho en evaluaciÃ³n.

#### 5.1 Sesgo, varianza y MSE (descomposiciÃ³n clave)

Si quieres estimar un parÃ¡metro real `Î¸` con un estimador `\hat{Î¸}`, el error cuadrÃ¡tico medio es:

`MSE(\hat{Î¸}) = E[(\hat{Î¸} - Î¸)^2]`

La identidad importante es:

`MSE(\hat{Î¸}) = Var(\hat{Î¸}) + Bias(\hat{Î¸})^2`

Donde:

- `Bias(\hat{Î¸}) = E[\hat{Î¸}] - Î¸`
- `Var(\hat{Î¸}) = E[(\hat{Î¸} - E[\hat{Î¸}])^2]`

Lectura mental:

- Puedes reducir MSE bajando varianza, aunque suba un poco el sesgo.
- O puedes â€œperseguir cero sesgoâ€ y pagar con alta varianza.

Esto es exactamente el *bias-variance trade-off* en ML (por ejemplo, regularizar o simplificar modelos).

#### 5.2 Unbiased vs consistente (2 propiedades distintas)

- **Unbiased (insesgado):** `E[\hat{Î¸}] = Î¸`.
- **Consistente:** cuando `n â†’ âˆ`, `\hat{Î¸} â†’ Î¸` (en un sentido probabilÃ­stico).

Un estimador puede ser sesgado y aun asÃ­ consistente (y a veces es preferible si reduce varianza para `n` finito).

#### 5.3 ConexiÃ³n directa con regularizaciÃ³n (puente a ML)

Ejemplo mental:

- **Ridge / L2** introduce sesgo (empuja coeficientes hacia 0).
- A cambio suele reducir varianza (soluciÃ³n mÃ¡s estable ante ruido y colinealidad).

En tÃ©rminos de la descomposiciÃ³n:

- sube `Bias^2`
- baja `Var`

Si el total baja, mejora el `MSE` esperado fuera de muestra.

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Micro-CapÃ­tulo: Maximum Likelihood Estimation (MLE)</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** MLE como filosofÃ­a unificadora: de â€œajustar perillasâ€ a pÃ©rdidas en ML
- **ID (opcional):** `M04-MICRO-MLE`
- **DuraciÃ³n estimada:** 120â€“180 min
- **Nivel:** Intermedioâ€“Avanzado
- **Dependencias:** 1.1â€“2.4 (probabilidad + Bayes), M03 (gradiente/chain rule como preview)

#### 2) Objetivos
- Explicar quÃ© maximiza MLE (verosimilitud de datos observados) y por quÃ© se usa log-likelihood.
- Conectar MLE con pÃ©rdidas: MSE â†” Gaussiana, BCE/CCE â†” Bernoulli/Categorical.
- Interpretar sesgo/varianza/MSE como puente a regularizaciÃ³n.

#### 3) Relevancia
- Te da el â€œpor quÃ©â€ de cross-entropy: no es un truco, es NLL.

#### 4) Mapa conceptual mÃ­nimo
- **Modelo** `P(D|Î¸)` â†’ define cÃ³mo â€œgeneraâ€ datos.
- **Likelihood** `L(Î¸|D)` â†’ probabilidad de D dado Î¸.
- **Log-likelihood** `â„“(Î¸)` â†’ suma (estable) en vez de producto.
- **Entrenamiento** â†’ minimizar `-â„“(Î¸)`.

#### 5) Definiciones esenciales
- `Î¸_MLE = argmax_Î¸ P(D|Î¸)`.
- `â„“(Î¸)=Î£ log P(x_i|Î¸)`.

#### 6) ExplicaciÃ³n didÃ¡ctica
- â€œMLE elige la perilla que hace que tus datos se vean menos sorprendentes bajo el modeloâ€.

#### 7) Ejemplo modelado
- Moneda Bernoulli: `p_MLE` = proporciÃ³n de caras.

#### 8) PrÃ¡ctica guiada
- Repite el worked example cambiando la secuencia de datos y verifica que `p_MLE` cambia como frecuencia.

#### 9) PrÃ¡ctica independiente
- Explica por quÃ© maximizar log-likelihood y maximizar likelihood dan el mismo argmax.

#### 10) AutoevaluaciÃ³n
- Â¿QuÃ© diferencia hay entre â€œparÃ¡metro mÃ¡s probableâ€ (Bayes) y â€œparÃ¡metro que hace los datos mÃ¡s probablesâ€ (MLE)?

#### 11) Errores comunes
- Confundir `P(Î¸|D)` con `P(D|Î¸)`.
- Olvidar que log convierte producto en suma (y por quÃ© ayuda numÃ©ricamente).

#### 12) RetenciÃ³n
- (dÃ­a 2) escribe: `Î¸_MLE = argmax_Î¸ Î£ log p(x_i|Î¸)`.

#### 13) DiferenciaciÃ³n
- Avanzado: conectar con MAP (regularizaciÃ³n como prior) (preview).

#### 14) Recursos
- StatQuest: Maximum Likelihood.

#### 15) Nota docente
- Pedir al alumno que diga â€œquÃ© asume el modeloâ€ antes de escribir `P(D|Î¸)`.
</details>

## ğŸ§© Micro-CapÃ­tulo Maestro: IntroducciÃ³n a Markov Chains â€” Nivel: Intermedio

### 1) Concepto

Una cadena de Markov es un sistema que salta entre estados.

Propiedad de Markov (â€œfalta de memoriaâ€):

`P(S_{t+1} | S_t, S_{t-1}, ...) = P(S_{t+1} | S_t)`

### 2) RepresentaciÃ³n matricial (puente con Ãlgebra Lineal)

Si tienes 3 estados (Sol, Nube, Lluvia), defines una matriz de transiciÃ³n `P` (3Ã—3) donde cada fila suma 1.

Si `Ï€_t` es un vector fila (1Ã—3) con la distribuciÃ³n â€œhoyâ€, entonces:

`Ï€_{t+1} = Ï€_t P`

Y en `k` pasos:

`Ï€_{t+k} = Ï€_t P^k`

### 3) Reto mental: estacionariedad = eigenvector

Si repites multiplicaciones, muchas cadenas convergen a una distribuciÃ³n estacionaria `Ï€*` tal que:

`Ï€* = Ï€* P`

Eso significa (en la perspectiva correcta) que `Ï€*` es un **eigenvector** asociado al **eigenvalue 1**.

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Micro-CapÃ­tulo: IntroducciÃ³n a Markov Chains</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Markov Chains como dinÃ¡mica lineal sobre distribuciones (Ï€_{t+1}=Ï€_t P)
- **ID (opcional):** `M04-MICRO-MARKOV`
- **DuraciÃ³n estimada:** 90â€“150 min
- **Nivel:** Intermedio
- **Dependencias:** M02 (multiplicaciÃ³n de matrices, eigenvectors), probabilidad bÃ¡sica (distribuciones)

#### 2) Objetivos
- Interpretar `P(S_{t+1}|S_t)` como â€œmemoria de 1 pasoâ€.
- Usar `Ï€_{t+1}=Ï€_t P` y verificar que `Ï€_t` sigue sumando 1.
- Explicar la condiciÃ³n de estacionariedad `Ï€*=Ï€*P`.

#### 3) Relevancia
- Conecta probabilidad con Ã¡lgebra lineal; reaparece en modelos secuenciales y Monte Carlo (LÃ­nea 2).

#### 4) Mapa conceptual mÃ­nimo
- **Estados** â†’ categorÃ­as discretas.
- **Matriz P** â†’ transiciones (filas suman 1).
- **DistribuciÃ³n Ï€** â†’ vector de probabilidades.
- **EvoluciÃ³n temporal** â†’ multiplicaciones repetidas.

#### 5) Definiciones esenciales
- Matriz estocÃ¡stica por filas: cada fila suma 1.
- DistribuciÃ³n estacionaria: `Ï€* = Ï€*P`.

#### 6) ExplicaciÃ³n didÃ¡ctica
- Piensa en `Ï€` como â€œmezclaâ€ de estados; multiplicar por P redistribuye masa.

#### 7) Ejemplo modelado
- 2 estados con `P=[[0.9,0.1],[0.2,0.8]]`: interpreta cada fila como â€œdesde dÃ³nde vienesâ€.

#### 8) PrÃ¡ctica guiada
- Elige un `Ï€_0` y calcula `Ï€_1`, `Ï€_2` a mano.

#### 9) PrÃ¡ctica independiente
- Encuentra (conceptualmente) `Ï€*` resolviendo `Ï€*=Ï€*P` + suma=1.

#### 10) AutoevaluaciÃ³n
- Â¿Por quÃ© el eigenvalue asociado a `Ï€*` es 1?

#### 11) Errores comunes
- Confundir si `Ï€` es vector fila o columna (y dÃ³nde multiplicar P).
- Usar una P donde filas no suman 1.

#### 12) RetenciÃ³n
- (dÃ­a 7) escribe `Ï€_{t+1}=Ï€_tP` y explica en una frase quÃ© hace.

#### 13) DiferenciaciÃ³n
- Avanzado: discutir condiciones de convergencia (ergodicidad) (solo conceptual).

#### 14) Recursos
- Material introductorio de Markov Chains + conexiÃ³n con eigenvectors.

#### 15) Nota docente
- Obligar â€œsanity checkâ€: despuÃ©s de multiplicar, verificar suma=1.
</details>

---

### DÃ­a 5: DistribuciÃ³n Gaussiana (Normal)

#### 3.1 La DistribuciÃ³n MÃ¡s Importante en ML

```text
                    1              (x - Î¼)Â²
f(x) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Â· exp(- â”€â”€â”€â”€â”€â”€â”€â”€â”€)
       Ïƒ Â· âˆš(2Ï€)                   2ÏƒÂ²

ParÃ¡metros:
- Î¼ (mu): Media (centro de la campana)
- Ïƒ (sigma): DesviaciÃ³n estÃ¡ndar (ancho)
- ÏƒÂ² (sigmaÂ²): Varianza
```

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 3.1: DistribuciÃ³n Gaussiana (definiciÃ³n)</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** PDF Gaussiana: forma, parÃ¡metros y lectura correcta
- **ID (opcional):** `M04-T03_1`
- **DuraciÃ³n estimada:** 60â€“120 min
- **Nivel:** Intermedio
- **Dependencias:** 1.1 (probabilidad), nociÃ³n de funciÃ³n exponencial/log

#### 2) Objetivos
- Identificar quÃ© controla `Î¼` (desplazamiento) y `Ïƒ`/`ÏƒÂ²` (dispersiÃ³n).
- Distinguir â€œdensidadâ€ `f(x)` de â€œprobabilidadâ€ (Ã¡rea bajo la curva).

#### 3) Relevancia
- La Gaussiana es el Ã¡tomo de modelos generativos (GMM) y del supuesto de ruido que conecta con MSE.

#### 4) Mapa conceptual mÃ­nimo
- **PDF** `f(x)` describe densidad.
- **ParÃ¡metros**: `Î¼` centra, `Ïƒ` escala.
- **Probabilidad**: integral de `f(x)` sobre un intervalo.

#### 5) Definiciones esenciales
- `X ~ N(Î¼, ÏƒÂ²)`.
- `f(x)` es densidad (puede ser >1), pero el Ã¡rea total integra a 1.

#### 6) ExplicaciÃ³n didÃ¡ctica
- Error clÃ¡sico: interpretar `f(0.5)=0.3` como â€œ30% de probabilidad en x=0.5â€ (en continuas eso es falso).

#### 7) Ejemplo modelado
- â€œCampanaâ€ estÃ¡ndar: `N(0,1)`.

#### 8) PrÃ¡ctica guiada
- Describe quÃ© pasa si duplicas `Ïƒ`: el pico baja y la curva se ensancha.

#### 9) PrÃ¡ctica independiente
- Explica quÃ© significa â€œ2 desviaciones estÃ¡ndarâ€ alrededor de la media en tÃ©rminos cualitativos.

#### 10) AutoevaluaciÃ³n
- Â¿Por quÃ© `P(X = x) = 0` en una variable continua aunque `f(x)` sea positiva?

#### 11) Errores comunes
- Confundir `Ïƒ` con `ÏƒÂ²`.
- Confundir densidad con probabilidad.

#### 12) RetenciÃ³n
- (dÃ­a 2) escribe la forma general de la PDF y nombra sus parÃ¡metros.

#### 13) DiferenciaciÃ³n
- Avanzado: conecta con log-likelihood de una Gaussiana (preview a MLE).

#### 14) Recursos
- SecciÃ³n â€œNormal distributionâ€ (cualquier referencia de probabilidad).

#### 15) Nota docente
- Exigir que el alumno diga: â€œdensidad â‰  probabilidad; probabilidad = Ã¡reaâ€.
</details>

#### 3.2 Por QuÃ© es Importante

1. **Muchos fenÃ³menos naturales** siguen esta distribuciÃ³n
2. **Teorema del LÃ­mite Central:** promedios de cualquier distribuciÃ³n â†’ Normal
3. **GMM usa Gaussianas** para modelar clusters
4. **InicializaciÃ³n de pesos** en redes neuronales

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 3.2: Por quÃ© la Gaussiana importa en ML</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Normal como â€œdefaultâ€ estadÃ­stico: TLC, ruido y modelos
- **ID (opcional):** `M04-T03_2`
- **DuraciÃ³n estimada:** 45â€“90 min
- **Nivel:** Intermedio
- **Dependencias:** 3.1

#### 2) Objetivos
- Explicar 3 usos tÃ­picos: ruido Gaussiano â†” MSE, GMM, inicializaciÃ³n.
- Conectar el TLC con â€œpromedios tienden a normalâ€.

#### 3) Relevancia
- Entender esto evita que la Normal se sienta como â€œfÃ³rmula que memorizasâ€ sin uso.

#### 4) Mapa conceptual mÃ­nimo
- **TLC** â†’ por quÃ© aparece en promedios.
- **Ruido** `Îµ~N(0,ÏƒÂ²)` â†’ por quÃ© MSE es natural.
- **GMM** â†’ mezcla de gaussianas para clustering.

#### 5) Definiciones esenciales
- TLC (enunciado informal): suma/promedio de muchas variables â†’ aproximadamente normal.

#### 6) ExplicaciÃ³n didÃ¡ctica
- Muchos modelos lineales asumen ruido Gaussiano: no porque sea â€œverdad absolutaâ€, sino porque da un modelo tractable.

#### 7) Ejemplo modelado
- RegresiÃ³n lineal con ruido: minimizas SSE/MSE como MLE Gaussiano (puente a DÃ­a 6).

#### 8) PrÃ¡ctica guiada
- Da un ejemplo cotidiano donde â€œmuchas fuentes pequeÃ±as de variaciÃ³nâ€ sugiere normalidad.

#### 9) PrÃ¡ctica independiente
- Explica por quÃ© en pesos de NN se usan gaussianas pequeÃ±as (inicializaciÃ³n) y quÃ© pasa si son muy grandes.

#### 10) AutoevaluaciÃ³n
- Â¿QuÃ© aspecto de la normal explica que valores extremos sean raros (colas)?

#### 11) Errores comunes
- Creer que â€œtodo es normalâ€ sin validar.
- Confundir â€œdistribuciÃ³n de datosâ€ con â€œdistribuciÃ³n de ruidoâ€.

#### 12) RetenciÃ³n
- (dÃ­a 2) enumera 3 conexiones: MSE, GMM, inicializaciÃ³n.

#### 13) DiferenciaciÃ³n
- Avanzado: discusiÃ³n de heavy tails y por quÃ© a veces Laplace/Student-t es mejor.

#### 14) Recursos
- StatQuest: Normal distribution / Central Limit Theorem.

#### 15) Nota docente
- Pedir una justificaciÃ³n: â€œÂ¿quÃ© hipÃ³tesis hace que MSE tenga sentido?â€.
</details>

#### 3.3 ImplementaciÃ³n

```python
import numpy as np  # NumPy: arrays, operaciones vectorizadas y funciones matemÃ¡ticas (exp, sqrt)

def gaussian_pdf(x: np.ndarray, mu: float, sigma: float) -> np.ndarray:  # PDF univariada: f(x) de N(Î¼, ÏƒÂ²)
    """
    Probability Density Function de la Gaussiana.

    Args:
        x: Puntos donde evaluar
        mu: Media
        sigma: DesviaciÃ³n estÃ¡ndar

    Returns:
        Densidad de probabilidad en cada punto
    """
    coefficient = 1 / (sigma * np.sqrt(2 * np.pi))  # Coeficiente de normalizaciÃ³n: 1/(Ïƒâˆš(2Ï€))
    exponent = -((x - mu) ** 2) / (2 * sigma ** 2)  # Exponente: - (x-Î¼)Â² / (2ÏƒÂ²) (forma estÃ¡ndar)
    return coefficient * np.exp(exponent)  # EvaluaciÃ³n final: coef * exp(exponente) (vectorizado)


# VisualizaciÃ³n
import matplotlib.pyplot as plt  # Matplotlib: grÃ¡ficos para construir intuiciÃ³n visual

x = np.linspace(-5, 5, 1000)  # Eje 1D de evaluaciÃ³n (1000 puntos para curva suave)

# Diferentes Gaussianas
plt.figure(figsize=(10, 6))  # Crea un lienzo con tamaÃ±o controlado
plt.plot(x, gaussian_pdf(x, mu=0, sigma=1), label='Î¼=0, Ïƒ=1 (estÃ¡ndar)')  # Curva â€œcampanaâ€ estÃ¡ndar
plt.plot(x, gaussian_pdf(x, mu=0, sigma=2), label='Î¼=0, Ïƒ=2 (mÃ¡s ancha)')  # Aumentar Ïƒ ensancha y baja el pico
plt.plot(x, gaussian_pdf(x, mu=2, sigma=1), label='Î¼=2, Ïƒ=1 (desplazada)')  # Cambiar Î¼ desplaza la curva
plt.legend()  # Muestra leyenda con labels
plt.title('Distribuciones Gaussianas')  # TÃ­tulo descriptivo
plt.xlabel('x')  # Etiqueta del eje x
plt.ylabel('f(x)')  # Etiqueta del eje y (densidad)
plt.grid(True)  # Rejilla para lectura mÃ¡s fÃ¡cil
plt.savefig('gaussian_distributions.png')  # Guarda imagen (Ãºtil para reportes)
```

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 3.3: ImplementaciÃ³n de la PDF Gaussiana (univariada)</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Implementar PDF: normalizaciÃ³n, vectorizaciÃ³n y sanity checks
- **ID (opcional):** `M04-T03_3`
- **DuraciÃ³n estimada:** 60â€“120 min
- **Nivel:** Intermedio
- **Dependencias:** 3.1

#### 2) Objetivos
- Implementar `gaussian_pdf` sin errores de forma y con vectorizaciÃ³n.
- Identificar el rol del coeficiente y del exponente.

#### 3) Relevancia
- Te entrena para implementar funciones de densidad y luego reutilizarlas en log-likelihood/EM.

#### 4) Mapa conceptual mÃ­nimo
- **Coeficiente** `1/(Ïƒâˆš(2Ï€))` normaliza.
- **Exponente** penaliza distancia al centro.
- **VectorizaciÃ³n**: evaluar muchos x de una vez.

#### 5) Definiciones esenciales
- `Ïƒ>0` (si `Ïƒ<=0` el modelo no tiene sentido).

#### 6) ExplicaciÃ³n didÃ¡ctica
- Sanity check numÃ©rico: la curva debe ser no negativa y â€œparecer campanaâ€.

#### 7) Ejemplo modelado
- ComparaciÃ³n de distintas `Î¼` y `Ïƒ` para construir intuiciÃ³n visual.

#### 8) PrÃ¡ctica guiada
- AÃ±ade una verificaciÃ³n: `assert np.all(gaussian_pdf(x,mu,sigma) >= 0)`.

#### 9) PrÃ¡ctica independiente
- (conceptual) Â¿QuÃ© deberÃ­a pasar con el pico cuando `Ïƒ` se hace muy pequeÃ±o?

#### 10) AutoevaluaciÃ³n
- Â¿QuÃ© parte del cÃ³digo cambia si reemplazas `Ïƒ` por `ÏƒÂ²` como parÃ¡metro?

#### 11) Errores comunes
- Overflow/underflow en `exp` cuando `Ïƒ` es muy pequeÃ±o o `|x-Î¼|` grande.
- Olvidar que `sigma` es desviaciÃ³n estÃ¡ndar (no varianza).

#### 12) RetenciÃ³n
- (dÃ­a 2) escribe la funciÃ³n en pseudo-cÃ³digo: coef Ã— exp(exponente).

#### 13) DiferenciaciÃ³n
- Avanzado: implementar `log_gaussian_pdf` estable y comparar.

#### 14) Recursos
- Numpy `np.exp`, estabilidad numÃ©rica.

#### 15) Nota docente
- Pedir al alumno que explique quÃ© controla `Î¼` y quÃ© controla `Ïƒ` viendo los plots.
</details>

#### 3.4 Gaussiana Multivariada (Para GMM)

```python
def multivariate_gaussian_pdf(x: np.ndarray,  # x:(d,) vector de caracterÃ­sticas (una muestra)
                               mu: np.ndarray,  # mu:(d,) vector de medias
                               cov: np.ndarray) -> float:  # cov:(d,d) matriz de covarianza
    """
    Gaussiana multivariada para vectores.

    Args:
        x: Vector de caracterÃ­sticas (d,)
        mu: Vector de medias (d,)
        cov: Matriz de covarianza (d, d)

    Returns:
        Densidad de probabilidad
    """
    d = len(mu)  # d: dimensiÃ³n del espacio (nÃºmero de features)
    diff = x - mu  # diff:(d,) centra el punto restando la media

    # Determinante e inversa de la covarianza
    det_cov = np.linalg.det(cov)  # |Î£|: controla el â€œvolumenâ€ de la elipse gaussiana
    inv_cov = np.linalg.inv(cov)  # Î£^{-1}: aparece en la forma cuadrÃ¡tica (Mahalanobis)

    # Coeficiente de normalizaciÃ³n
    coefficient = 1 / (np.sqrt((2 * np.pi) ** d * det_cov))  # 1 / sqrt((2Ï€)^d |Î£|)

    # Exponente (forma cuadrÃ¡tica)
    exponent = -0.5 * diff.T @ inv_cov @ diff  # -(1/2)(x-Î¼)^T Î£^{-1} (x-Î¼)

    return coefficient * np.exp(exponent)  # Devuelve densidad (escala) * exp(exponente)


# Ejemplo 2D
mu = np.array([0, 0])  # Î¼:(2,) media en 2D
cov = np.array([[1, 0.5],  # Î£[0,0]=var(x1), Î£[0,1]=cov(x1,x2)
                [0.5, 1]])  # CorrelaciÃ³n positiva: elipses rotadas respecto a los ejes

x = np.array([0.5, 0.5])  # Punto a evaluar (una muestra)
prob = multivariate_gaussian_pdf(x, mu, cov)  # Escalar: densidad en ese punto
print(f"P(x=[0.5, 0.5]) = {prob:.4f}")  # Imprime densidad (ojo: no es probabilidad discreta)
```

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 3.4: Gaussiana Multivariada</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Multivariada: covarianza, elipses y Mahalanobis
- **ID (opcional):** `M04-T03_4`
- **DuraciÃ³n estimada:** 90â€“150 min
- **Nivel:** Intermedioâ€“Avanzado
- **Dependencias:** M02 (det/inv, formas cuadrÃ¡ticas), 3.1

#### 2) Objetivos
- Interpretar el rol de `Î£` (covarianza) como escala + correlaciÃ³n.
- Reconocer la forma cuadrÃ¡tica `(x-Î¼)^T Î£^{-1} (x-Î¼)` como â€œdistancia elÃ­pticaâ€.

#### 3) Relevancia
- Es el nÃºcleo matemÃ¡tico de GMM y de muchas tÃ©cnicas estadÃ­sticas.

#### 4) Mapa conceptual mÃ­nimo
- `Î¼` fija el centro.
- `Î£` fija la elipse (forma/orientaciÃ³n).
- `|Î£|` controla volumen.

#### 5) Definiciones esenciales
- Covarianza vÃ¡lida: simÃ©trica y PSD (idealmente PD para invertir).

#### 6) ExplicaciÃ³n didÃ¡ctica
- Si `Î£` tiene covarianzas fuera de la diagonal, la elipse rota.

#### 7) Ejemplo modelado
- Caso 2D con correlaciÃ³n positiva (`0.5`) para ver rotaciÃ³n.

#### 8) PrÃ¡ctica guiada
- Cambia `cov` a diagonal y compara con el caso correlacionado.

#### 9) PrÃ¡ctica independiente
- Explica quÃ© pasa si `det_cov` es casi 0 (covarianza casi singular).

#### 10) AutoevaluaciÃ³n
- Â¿Por quÃ© aparece `Î£^{-1}` en lugar de `Î£` en el exponente?

#### 11) Errores comunes
- Invertir `Î£` singular (numÃ©ricamente inestable).
- Confundir densidad con probabilidad.

#### 12) RetenciÃ³n
- (dÃ­a 7) escribe la forma: coeficiente Ã— exp(-0.5 * Mahalanobis).

#### 13) DiferenciaciÃ³n
- Avanzado: usar Cholesky para estabilidad en lugar de `inv`/`det` directos.

#### 14) Recursos
- Material de GMM / multivariate normal.

#### 15) Nota docente
- Pedir al alumno que dibuje cÃ³mo cambia la elipse al variar covarianza.
</details>

---

### DÃ­a 6: Maximum Likelihood Estimation (MLE)

#### 4.0 MLE â†’ Cross-Entropy (la conexiÃ³n que te piden en exÃ¡menes)

**Idea:** si un modelo produce probabilidades `P(y|x, Î¸)`, entrenar por MLE significa:

- maximizar `Î áµ¢ P(yáµ¢|xáµ¢, Î¸)`

Por estabilidad numÃ©rica y conveniencia, trabajamos con log:

- maximizar `Î£áµ¢ log P(yáµ¢|xáµ¢, Î¸)`

Y como optimizadores minimizan, entrenamos minimizando:

- `-Î£áµ¢ log P(yáµ¢|xáµ¢, Î¸)`  (negative log-likelihood)

Ese tÃ©rmino es exactamente la **cross-entropy** que usas en:

- Logistic Regression (BCE) en `MÃ³dulo 05`
- clasificaciÃ³n multiclase (CCE) en `MÃ³dulo 07`

**Cheat sheet:**

- **MLE:** maximizar likelihood
- **Entrenamiento:** minimizar negative log-likelihood
- **En clasificaciÃ³n:** eso se llama cross-entropy

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 4.0: MLE â†’ Cross-Entropy (la conexiÃ³n que te piden en exÃ¡menes)</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** De maximizar likelihood a minimizar cross-entropy (NLL)
- **ID (opcional):** `M04-T04_0`
- **DuraciÃ³n estimada:** 45â€“90 min
- **Nivel:** Intermedio
- **Dependencias:** 2.1â€“2.3 (probabilidad condicional/Bayes), nociÃ³n de logaritmo

#### 2) Objetivos
- Conectar el producto de probabilidades con suma de log-probabilidades.
- Explicar por quÃ© optimizamos **NLL** (negative log-likelihood) en vez de maximizar likelihood.
- Reconocer que en clasificaciÃ³n la NLL se escribe como **cross-entropy**.

#### 3) Relevancia
- Esta equivalencia es el â€œpuenteâ€ entre probabilidad y entrenamiento: explica por quÃ© la loss tÃ­pica en clasificaciÃ³n es cross-entropy.

#### 4) Mapa conceptual mÃ­nimo
- **Likelihood** `P(D|Î¸)` (producto)
- **Log-likelihood** `log P(D|Î¸)` (suma)
- **NLL** `-log P(D|Î¸)` (minimizaciÃ³n)
- **Cross-entropy** (forma estÃ¡ndar de la NLL en clasificaciÃ³n)

#### 5) Definiciones esenciales
- **Likelihood:** probabilidad de observar los datos si el modelo tuviera parÃ¡metros `Î¸`.
- **NLL:** `-Î£ log P(yáµ¢|xáµ¢,Î¸)`; es una loss no negativa (en promedio) que penaliza probabilidades pequeÃ±as asignadas a la etiqueta correcta.

#### 6) ExplicaciÃ³n didÃ¡ctica
- El producto `Î  P(yáµ¢|xáµ¢,Î¸)` se vuelve numÃ©ricamente pequeÃ±o; el log lo transforma en suma y evita underflow.
- Cambiar de â€œmaximizarâ€ a â€œminimizarâ€ es solo conveniencia (los optimizadores tÃ­picos minimizan).

#### 7) Ejemplo modelado
- Si el modelo asigna `P(y=correcto|x)=0.01`, entonces `-log(0.01)` es grande: el entrenamiento â€œsienteâ€ fuerte ese error.

#### 8) PrÃ¡ctica guiada
- Reescribe el objetivo para un dataset de 3 muestras y verifica el paso:
  - `max Î  páµ¢` â†’ `max Î£ log páµ¢` â†’ `min -Î£ log páµ¢`.

#### 9) PrÃ¡ctica independiente
- Describe quÃ© pasa con la NLL si duplicas el dataset (mismas muestras dos veces). Â¿Por quÃ© se suele usar promedio `1/m`?

#### 10) AutoevaluaciÃ³n
- Â¿Por quÃ© `log` convierte productos en sumas y por quÃ© eso ayuda a optimizar?

#### 11) Errores comunes
- Confundir **cross-entropy** con accuracy: una es funciÃ³n suave optimizable, la otra no.
- Olvidar el signo: minimizar `-log(p)` equivale a maximizar `log(p)`.

#### 12) RetenciÃ³n
- Regla mnemÃ³nica: **MLE â‡’ max log-likelihood â‡’ min NLL â‡’ cross-entropy (clasificaciÃ³n)**.

#### 13) DiferenciaciÃ³n
- Avanzado: compara NLL con label smoothing (cÃ³mo cambia la penalizaciÃ³n cuando `y` no es one-hot perfecto).

#### 14) Recursos
- FunciÃ³n `log` y propiedades: `log(ab)=log(a)+log(b)`.

#### 15) Nota docente
- Pedir al alumno que explique â€œpor quÃ© el log es un truco numÃ©rico y algebraico a la vezâ€.
</details>

---

### ExtensiÃ³n EstratÃ©gica (LÃ­nea 2): Statistical Estimation

#### MLE como filosofÃ­a: â€œajustar perillasâ€

MLE no es solo una fÃ³rmula: es una forma de pensar.

- Tienes un modelo con parÃ¡metros `Î¸` (las â€œperillasâ€).
- Ya viste datos `D`.
- Pregunta: Â¿quÃ© valores de `Î¸` hacen que `D` sea lo mÃ¡s probable posible?

Formalmente:

```text
Î¸_MLE = argmax_Î¸ P(D | Î¸)
```

Como `P(D|Î¸)` suele ser un producto grande, usamos log:

```text
Î¸_MLE = argmax_Î¸ log P(D | Î¸)
```

Esto es el puente directo a **Statistical Estimation** (LÃ­nea 2): estimadores, sesgo, varianza, y por quÃ© â€œpromedioâ€ aparece en tantos lados.

#### Worked example: Moneda (Bernoulli) â†’ estimador MLE

Modelo:

- `X_i ~ Bernoulli(p)` donde `p = P(cara)`.

Datos:

- `D = {x_1, ..., x_n}` con `x_i âˆˆ {0,1}`.

Likelihood:

```text
P(D | p) = Î _i p^{x_i} (1-p)^{(1-x_i)}
```

Log-likelihood:

```text
â„“(p) = Î£_i [x_i log p + (1-x_i) log(1-p)]
```

Derivar y hacer 0 (intuiciÃ³n: el mÃ¡ximo ocurre cuando la â€œprobabilidad del modeloâ€ coincide con la frecuencia observada):

```text
dâ„“/dp = Î£_i [x_i/p - (1-x_i)/(1-p)] = 0
```

SoluciÃ³n:

```text
p_MLE = (1/n) Î£_i x_i
```

InterpretaciÃ³n: el MLE de `p` es simplemente la **proporciÃ³n de caras**. Este patrÃ³n (media muestral) reaparece en gaussianas y en muchos estimadores.

#### 4.1 La Idea Central

```text
MLE: Encontrar los parÃ¡metros Î¸ que maximizan la probabilidad
     de observar los datos que tenemos.

Î¸_MLE = argmax P(datos | Î¸)
            Î¸
```

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 4.1: La Idea Central</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** QuÃ© significa â€œajustar Î¸ para explicar los datosâ€
- **ID (opcional):** `M04-T04_1`
- **DuraciÃ³n estimada:** 30â€“60 min
- **Nivel:** BÃ¡sicoâ€“Intermedio
- **Dependencias:** 4.0

#### 2) Objetivos
- Interpretar `argmax_Î¸ P(datos|Î¸)` como â€œbuscar el Î¸ que hace los datos mÃ¡s probablesâ€.
- Identificar quÃ© es **dato**, quÃ© es **parÃ¡metro** y quÃ© es **modelo**.

#### 3) Relevancia
- Esta idea aparece en regresiÃ³n logÃ­stica, Naive Bayes, gaussianas, GMM y en general en modelos probabilÃ­sticos.

#### 4) Mapa conceptual mÃ­nimo
- **Modelo** `P(x|Î¸)` / `P(y|x,Î¸)`
- **Datos** `D={xáµ¢,yáµ¢}`
- **ParÃ¡metros** `Î¸`
- **Objetivo** `argmax` (o `argmin` NLL)

#### 5) Definiciones esenciales
- `argmax`: devuelve el valor del parÃ¡metro que maximiza una funciÃ³n.
- i.i.d. (supuesto tÃ­pico): cada muestra aporta un factor multiplicativo a la likelihood.

#### 6) ExplicaciÃ³n didÃ¡ctica
- Piensa en `Î¸` como â€œperillasâ€ del generador de datos: MLE elige las perillas que hacen â€œcreÃ­bleâ€ el dataset observado.

#### 7) Ejemplo modelado
- Moneda: `Î¸=p`; si observas muchas caras, el `p` que mejor explica el dato es alto.

#### 8) PrÃ¡ctica guiada
- Identifica `Î¸` en:
  - Bernoulli (`p`),
  - Gaussiana (`Î¼,Ïƒ`),
  - Softmax (`W`).

#### 9) PrÃ¡ctica independiente
- Escribe en una lÃ­nea quÃ© maximiza MLE para un modelo `P(y|x,Î¸)`.

#### 10) AutoevaluaciÃ³n
- Â¿QuÃ© cambia si los datos no fueran independientes?

#### 11) Errores comunes
- Mezclar `P(Î¸|datos)` (Bayes) con `P(datos|Î¸)` (MLE).

#### 12) RetenciÃ³n
- Frase clave: **MLE mira datosâ†’Î¸ (quÃ© Î¸ explica mejor lo observado)**.

#### 13) DiferenciaciÃ³n
- Avanzado: contrasta MLE con MAP (`argmax P(Î¸|D)`), aunque ambos suelen acabar en minimizar una loss.

#### 14) Recursos
- Repasar diferencia entre prior, likelihood y posterior.

#### 15) Nota docente
- VerbalizaciÃ³n obligatoria: â€œÂ¿quÃ© estoy maximizando exactamente y respecto a quÃ© variable?â€
</details>

#### 4.2 Por QuÃ© es Fundamental

- **Logistic Regression** usa MLE para encontrar los pesos
- **Cross-Entropy Loss** viene de maximizar likelihood
- **GMM** usa MLE (via EM algorithm)

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 4.2: Por QuÃ© es Fundamental</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Por quÃ© MLE estÃ¡ â€œdebajoâ€ de pÃ©rdidas y modelos comunes
- **ID (opcional):** `M04-T04_2`
- **DuraciÃ³n estimada:** 30â€“60 min
- **Nivel:** Intermedio
- **Dependencias:** 4.0â€“4.1

#### 2) Objetivos
- Identificar al menos 3 lugares del stack ML donde MLE aparece implÃ­citamente.
- Conectar *modelado probabilÃ­stico* con *funciÃ³n de pÃ©rdida*.

#### 3) Relevancia
- Te permite â€œleerâ€ una loss como una suposiciÃ³n probabilÃ­stica (quÃ© distribuciÃ³n estÃ¡s asumiendo).

#### 4) Mapa conceptual mÃ­nimo
- **Modelo probabilÃ­stico** â†’ **log-likelihood** â†’ **NLL** â†’ **gradiente/optimizaciÃ³n**

#### 5) Definiciones esenciales
- **Estimador:** regla que produce un parÃ¡metro `\hat{Î¸}` desde datos.
- **Loss probabilÃ­stica:** una loss que puede interpretarse como NLL bajo un modelo.

#### 6) ExplicaciÃ³n didÃ¡ctica
- Cuando eliges cross-entropy, eliges implÃ­citamente â€œel dato `y` sigue una distribuciÃ³n categÃ³rica parametrizada por el modeloâ€.

#### 7) Ejemplo modelado
- RegresiÃ³n:
  - Si asumes ruido Gaussiano, la NLL se parece a MSE.
  - Si asumes Bernoulli/categÃ³rica, la NLL se vuelve BCE/CCE.

#### 8) PrÃ¡ctica guiada
- Para cada bullet del tema (LogReg, Cross-Entropy, GMM), completa la frase:
  - â€œLa loss es la NLL de una distribuciÃ³n ____â€.

#### 9) PrÃ¡ctica independiente
- Â¿QuÃ© suposiciÃ³n probabilÃ­stica hay detrÃ¡s de usar MSE como loss?

#### 10) AutoevaluaciÃ³n
- Â¿Por quÃ© â€œmaximizar likelihoodâ€ y â€œminimizar NLLâ€ son el mismo objetivo?

#### 11) Errores comunes
- Creer que MLE â€œsoloâ€ es una tÃ©cnica estadÃ­stica: en ML moderno es una forma estÃ¡ndar de derivar losses.

#### 12) RetenciÃ³n
- FÃ³rmula mental: **modelar `P(y|x)` â‡’ entrenar = maximizar `P(y|x,Î¸)`**.

#### 13) DiferenciaciÃ³n
- Avanzado: discute cuÃ¡ndo preferir MAP/regularizaciÃ³n como â€œpriorâ€ implÃ­cito.

#### 14) Recursos
- Lectura corta: interpretaciÃ³n probabilÃ­stica de MSE/BCE/CCE.

#### 15) Nota docente
- Mini-debate: â€œÂ¿una loss define un modelo o un modelo define una loss?â€
</details>

#### 4.3 MLE para Gaussiana

```python
def mle_gaussian(data: np.ndarray) -> tuple[float, float]:
    """
    Estimar parÃ¡metros de Gaussiana con MLE.

    Para una Gaussiana, los estimadores MLE son:
    - Î¼_MLE = media muestral
    - ÏƒÂ²_MLE = varianza muestral (con n, no n-1)

    Args:
        data: Muestras observadas

    Returns:
        (mu_mle, sigma_mle)
    """
    n = len(data)

    # MLE de la media
    mu_mle = np.mean(data)

    # MLE de la varianza (dividir por n, no n-1)
    sigma_squared_mle = np.sum((data - mu_mle) ** 2) / n
    sigma_mle = np.sqrt(sigma_squared_mle)

    return mu_mle, sigma_mle


# Ejemplo: Generar datos y estimar
np.random.seed(42)
true_mu, true_sigma = 5.0, 2.0
samples = np.random.normal(true_mu, true_sigma, size=1000)

estimated_mu, estimated_sigma = mle_gaussian(samples)
print(f"ParÃ¡metros reales: Î¼={true_mu}, Ïƒ={true_sigma}")
print(f"MLE estimados:     Î¼={estimated_mu:.3f}, Ïƒ={estimated_sigma:.3f}")
```

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 4.3: MLE para Gaussiana</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Media muestral y varianza con `n` (no `n-1`) como MLE
- **ID (opcional):** `M04-T04_3`
- **DuraciÃ³n estimada:** 60â€“120 min
- **Nivel:** Intermedio
- **Dependencias:** 3.1â€“3.3 (Gaussiana univariada) + 4.1

#### 2) Objetivos
- Diferenciar varianza MLE (`/n`) de varianza insesgada (`/(n-1)`).
- Implementar estimadores MLE para `Î¼` y `Ïƒ` y validar con datos simulados.

#### 3) Relevancia
- Esta derivaciÃ³n aparece en EM/GMM y en cualquier modelo que use gaussianas (ruido, priors, etc.).

#### 4) Mapa conceptual mÃ­nimo
- **AsunciÃ³n:** `xáµ¢ ~ N(Î¼,ÏƒÂ²)`
- **Objetivo:** `argmax log P(D|Î¼,Ïƒ)`
- **Resultado:** `Î¼=mean(x)` y `ÏƒÂ²=mean((x-Î¼)Â²)`

#### 5) Definiciones esenciales
- `ÏƒÂ²_MLE = (1/n) Î£ (xáµ¢-Î¼)Â²`.
- Estimador insesgado: usa `1/(n-1)` (otra propiedad, objetivo distinto).

#### 6) ExplicaciÃ³n didÃ¡ctica
- MLE optimiza â€œquÃ© parÃ¡metros hacen mÃ¡s probable el datasetâ€, no â€œque el estimador sea insesgadoâ€.

#### 7) Ejemplo modelado
- Con `n=1000`, `\hat{Î¼}` y `\hat{Ïƒ}` deberÃ­an acercarse a los parÃ¡metros reales por ley de los grandes nÃºmeros.

#### 8) PrÃ¡ctica guiada
- Agrega checks:
  - `assert estimated_sigma > 0`.
  - `assert abs(estimated_mu-true_mu) < 0.2` (con `n` grande).

#### 9) PrÃ¡ctica independiente
- Repite con `n=10` y observa la variabilidad de `\hat{Ïƒ}`.

#### 10) AutoevaluaciÃ³n
- Â¿Por quÃ© `/(n-1)` no sale de MLE cuando maximizas likelihood?

#### 11) Errores comunes
- Usar `np.std(data, ddof=1)` y decir que es MLE (eso es insesgado, no MLE).
- Confundir `Ïƒ` con `ÏƒÂ²` en el retorno.

#### 12) RetenciÃ³n
- Regla: **MLE de media = promedio; MLE de varianza = promedio de cuadrados centrados**.

#### 13) DiferenciaciÃ³n
- Avanzado: deriva la log-likelihood de la Gaussiana y ubica dÃ³nde aparece el tÃ©rmino `log Ïƒ`.

#### 14) Recursos
- Numpy: `np.mean`, `np.sum`, `np.sqrt`.

#### 15) Nota docente
- Pregunta guiadora: â€œÂ¿quÃ© propiedad estÃ¡s optimizando: likelihood o sesgo?â€
</details>

#### 4.4 ConexiÃ³n con Cross-Entropy Loss

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 4.4: ConexiÃ³n con Cross-Entropy Loss</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Cross-entropy como NLL: la forma â€œestÃ¡ndarâ€ de escribir MLE en clasificaciÃ³n
- **ID (opcional):** `M04-T04_4`
- **DuraciÃ³n estimada:** 30â€“60 min
- **Nivel:** Intermedio
- **Dependencias:** 4.0 + 2.1 (probabilidades condicionadas)

#### 2) Objetivos
- Escribir explÃ­citamente la NLL en binario y multiclase.
- Identificar la â€œclase correctaâ€ como el tÃ©rmino que se queda en la suma cuando `y` es one-hot.

#### 3) Relevancia
- Esta conexiÃ³n explica por quÃ© la loss tiene logs y por quÃ© penaliza con fuerza probabilidades pequeÃ±as.

#### 4) Mapa conceptual mÃ­nimo
- `P(y|x,Î¸)` â†’ `log P(y|x,Î¸)` â†’ `-log P(y|x,Î¸)`
- One-hot `y` â€œseleccionaâ€ la clase correcta en `Î£ y_k log(p_k)`

#### 5) Definiciones esenciales
- **Cross-entropy (multiclase):** `H(y,p)= -Î£_k y_k log(p_k)`.
- Si `y` es one-hot, entonces `H(y,p) = -log(p_clase_correcta)`.

#### 6) ExplicaciÃ³n didÃ¡ctica
- No hay â€œmagiaâ€: el log aparece por MLE y por estabilidad numÃ©rica.

#### 7) Ejemplo modelado
- Si `p_correcta=0.9`, pÃ©rdida â‰ˆ `0.105`; si `p_correcta=0.01`, pÃ©rdida â‰ˆ `4.605`.

#### 8) PrÃ¡ctica guiada
- Calcula `-log(p_correcta)` para `pâˆˆ{0.9,0.5,0.1,0.01}` y ordÃ©nalos.

#### 9) PrÃ¡ctica independiente
- Explica por quÃ© una predicciÃ³n â€œmuy segura y equivocadaâ€ recibe mucha penalizaciÃ³n.

#### 10) AutoevaluaciÃ³n
- Â¿QuÃ© pasa con la loss si el modelo siempre predice `p_correcta=1/K`?

#### 11) Errores comunes
- Calcular `np.log(softmax(z))` de forma ingenua y sufrir underflow/NaN (ver dÃ­a 7).

#### 12) RetenciÃ³n
- Frase: **cross-entropy = costo de sorprenderte al ver la etiqueta verdadera**.

#### 13) DiferenciaciÃ³n
- Avanzado: conecta con KL: `H(y,p)=H(y)+KL(y||p)` (cuando `y` es distribuciÃ³n).

#### 14) Recursos
- RevisiÃ³n: propiedades de `log` y estabilidad numÃ©rica.

#### 15) Nota docente
- Pedir que el alumno derive la forma one-hot â†’ `-log(p_correcta)` en 3 lÃ­neas.
</details>

#### 4.5 MLE para multiclase (Softmax + Categorical Cross-Entropy)

Para `K` clases, `y` es one-hot y el modelo produce probabilidades con softmax:

- `p = softmax(z)` donde `z = XW` son logits

Likelihood (por muestra):

- `P(y|x) = Î _k p_k^{y_k}`

Log-likelihood:

- `log P(y|x) = Î£_k y_k log(p_k)`

Negative log-likelihood promedio:

- `L = -(1/m) Î£áµ¢ Î£_k y_{ik} log(p_{ik})`

Eso es exactamente **Categorical Cross-Entropy**.

```python
def cross_entropy_from_mle():
    """
    DemostraciÃ³n de que Cross-Entropy viene de MLE.

    Para clasificaciÃ³n binaria con Bernoulli:
    P(y|x, Î¸) = p^y Â· (1-p)^(1-y)

    Donde p = Ïƒ(Î¸áµ€x) (predicciÃ³n del modelo)

    Log-likelihood:
    log P(y|x, Î¸) = yÂ·log(p) + (1-y)Â·log(1-p)

    Maximizar likelihood = Minimizar negative log-likelihood
    = Minimizar Cross-Entropy!
    """
    # Ejemplo numÃ©rico
    y_true = np.array([1, 0, 1, 1, 0])
    y_pred = np.array([0.9, 0.1, 0.8, 0.7, 0.2])  # Probabilidades

    # Cross-Entropy (negative log-likelihood promedio)
    epsilon = 1e-15  # Para evitar log(0)
    ce = -np.mean(
        y_true * np.log(y_pred + epsilon) +
        (1 - y_true) * np.log(1 - y_pred + epsilon)
    )

    print(f"Cross-Entropy Loss: {ce:.4f}")
    return ce

cross_entropy_from_mle()
```

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 4.5: MLE para multiclase (Softmax + Categorical Cross-Entropy)</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** De `Î  p_k^{y_k}` a `-Î£ y_k log(p_k)` (y por quÃ© eso es entrenable)
- **ID (opcional):** `M04-T04_5`
- **DuraciÃ³n estimada:** 60â€“120 min
- **Nivel:** Intermedio
- **Dependencias:** 4.0 + nociÃ³n de one-hot + softmax (dÃ­a 7)

#### 2) Objetivos
- Derivar la log-likelihood multiclase usando one-hot.
- Interpretar la CCE como â€œcastigoâ€ a la probabilidad asignada a la clase correcta.
- Reconocer el rol de `epsilon` como protecciÃ³n de `log(0)`.

#### 3) Relevancia
- Esta es la base de entrenamiento para redes neuronales multiclase y modelos lineales con softmax.

#### 4) Mapa conceptual mÃ­nimo
- **Logits** `z` â†’ **Softmax** `p` â†’ **Log-prob** `log(p)` â†’ **CCE/NLL**

#### 5) Definiciones esenciales
- One-hot: `y_kâˆˆ{0,1}`, `Î£_k y_k = 1`.
- CCE por muestra: `L = -Î£_k y_k log(p_k)`.

#### 6) ExplicaciÃ³n didÃ¡ctica
- El producto `Î _k p_k^{y_k}` â€œseleccionaâ€ exactamente la probabilidad de la clase verdadera.
- El log convierte ese producto en suma (y vuelve diferenciable y mÃ¡s estable el entrenamiento).

#### 7) Ejemplo modelado
- Para `K=3`, si la clase verdadera es 2, la loss es `-log(p_2)`.

#### 8) PrÃ¡ctica guiada
- Construye un `y` one-hot y un vector `p` y verifica a mano que:
  - `-Î£ y_k log(p_k)` coincide con `-log(p_clase_correcta)`.

#### 9) PrÃ¡ctica independiente
- Explica por quÃ© se promedia en batch (`1/m`) y no se usa suma sin normalizar.

#### 10) AutoevaluaciÃ³n
- Â¿QuÃ© problema numÃ©rico aparece si `p_k` llega a 0 exacto?

#### 11) Errores comunes
- Usar softmax + log de manera ingenua y obtener `-inf/NaN`.
- Confundir `logits` (sin normalizar) con probabilidades.

#### 12) RetenciÃ³n
- Regla: **CCE = NLL de una categÃ³rica parametrizada por softmax**.

#### 13) DiferenciaciÃ³n
- Avanzado: describe por quÃ© en prÃ¡ctica se prefiere â€œCE desde logitsâ€ con `log_softmax`.

#### 14) Recursos
- Estabilidad numÃ©rica: Log-Sum-Exp trick (dÃ­a 7).

#### 15) Nota docente
- Pedir que el alumno identifique, en una implementaciÃ³n, dÃ³nde se aplica `max(z)` para estabilizar.
</details>

## ğŸŒ± ExtensiÃ³n EstratÃ©gica (LÃ­nea 2): Markov Chains (intro conceptual)

> Esta secciÃ³n es conceptual: no vas a implementar Markov Chains en LÃ­nea 1, pero sÃ­ necesitas que la idea te resulte familiar cuando entres al curso de **Discrete-Time Markov Chains and Monte Carlo Methods**.

### Idea central: estados y transiciones

Una cadena de Markov modela un sistema que â€œsaltaâ€ entre **estados**.

- Hoy estÃ¡s en un estado `S_t`.
- MaÃ±ana estÃ¡s en `S_{t+1}`.
- Lo importante: `P(S_{t+1} | S_t)` depende solo del estado actual (memoria de 1 paso).

### Matriz de transiciÃ³n (conexiÃ³n con Ãlgebra Lineal)

Definimos una matriz `P` donde:

- `P[i, j] = P(estado j | estado i)`
- Cada fila suma 1 (matriz estocÃ¡stica por filas)

Si `Ï€_t` es un vector fila con la distribuciÃ³n de probabilidad sobre estados en el tiempo `t`, entonces:

```text
Ï€_{t+1} = Ï€_t P
```

Esto conecta directamente con `MÃ³dulo 02`: es **multiplicaciÃ³n de matrices** aplicada a probabilidades.

### Ejemplo mÃ­nimo (2 estados)

Estados: `A` y `B`.

```text
P = [[0.9, 0.1],
     [0.2, 0.8]]
```

InterpretaciÃ³n:

- Si estÃ¡s en `A`, te quedas en `A` con 0.9, pasas a `B` con 0.1.
- Si estÃ¡s en `B`, pasas a `A` con 0.2, te quedas en `B` con 0.8.

### Estacionariedad (semilla para LÃ­nea 2)

Una distribuciÃ³n estacionaria `Ï€*` satisface:

```text
Ï€* = Ï€* P
```

En otras palabras: es un **autovector** (eigenvector) asociado al eigenvalue `1` (visto desde la perspectiva correcta). Esto vuelve a conectar Markov Chains con eigenvalues/eigenvectors.

---

### DÃ­a 7: Softmax como DistribuciÃ³n de Probabilidad

#### 5.1 De Logits a Probabilidades

```text
                     exp(záµ¢)
softmax(z)áµ¢ = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
               Î£â±¼ exp(zâ±¼)

Propiedades:
- Cada salida âˆˆ (0, 1)
- Suma de salidas = 1 (distribuciÃ³n vÃ¡lida)
- Preserva el orden (mayor logit â†’ mayor probabilidad)
```

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 5.1: De Logits a Probabilidades</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Softmax como distribuciÃ³n: de scores a probabilidades comparables
- **ID (opcional):** `M04-T05_1`
- **DuraciÃ³n estimada:** 45â€“90 min
- **Nivel:** Intermedio
- **Dependencias:** 4.5 (CCE desde MLE), Ã¡lgebra bÃ¡sica de exponentes

#### 2) Objetivos
- Explicar quÃ© son **logits** y por quÃ© no son probabilidades.
- Interpretar softmax como una normalizaciÃ³n positiva que suma 1.
- Reconocer invariancia por desplazamiento: `softmax(z)=softmax(z+c)`.

#### 3) Relevancia
- Softmax es la salida estÃ¡ndar en clasificaciÃ³n multiclase y conecta directamente con la CCE.

#### 4) Mapa conceptual mÃ­nimo
- **Logits** `z` â†’ `exp(z)` â†’ **normalizaciÃ³n** `Î£ exp(z)` â†’ **probabilidades**

#### 5) Definiciones esenciales
- **Logit:** score sin normalizar (puede ser cualquier real).
- **DistribuciÃ³n vÃ¡lida:** entradas en `(0,1)` y suma 1.

#### 6) ExplicaciÃ³n didÃ¡ctica
- `exp` asegura positividad; dividir por la suma fuerza â€œcompetenciaâ€ entre clases.

#### 7) Ejemplo modelado
- Si una clase sube su logit, su probabilidad sube y las demÃ¡s bajan para mantener suma 1.

#### 8) PrÃ¡ctica guiada
- Verifica (a mano) que `softmax([0,0]) = [0.5,0.5]`.

#### 9) PrÃ¡ctica independiente
- Demuestra en 2 lÃ­neas la invariancia: `softmax(z)=softmax(z-c)` para cualquier constante `c`.

#### 10) AutoevaluaciÃ³n
- Â¿QuÃ© sucede si sumas 100 a todos los logits? Â¿Cambia el resultado?

#### 11) Errores comunes
- Interpretar logits como probabilidades.
- Olvidar que softmax depende de las diferencias relativas entre logits.

#### 12) RetenciÃ³n
- Regla: **softmax convierte scores relativos en probabilidades que compiten**.

#### 13) DiferenciaciÃ³n
- Avanzado: explora el efecto de la temperatura `softmax(z/T)`.

#### 14) Recursos
- RelaciÃ³n con CCE: `L = -log p(clase correcta)`.

#### 15) Nota docente
- Pregunta rÃ¡pida: â€œsi una probabilidad sube, Â¿quÃ© debe pasar con las otras y por quÃ©?â€

</details>

#### 5.2 El Problema de Estabilidad NumÃ©rica (v3.3)

```text
âš ï¸ PROBLEMA: exp() puede causar overflow/underflow

Ejemplo peligroso:
    z = [1000, 1001, 1002]
    exp(z) = [inf, inf, inf]  â†’ NaN en softmax!

Ejemplo underflow:
    z = [-1000, -1001, -1002]
    exp(z) = [0, 0, 0]  â†’ 0/0 = NaN!
```

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 5.2: El Problema de Estabilidad NumÃ©rica</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Por quÃ© `exp` rompe y cÃ³mo reconocer overflow/underflow
- **ID (opcional):** `M04-T05_2`
- **DuraciÃ³n estimada:** 30â€“60 min
- **Nivel:** Intermedio
- **Dependencias:** 5.1

#### 2) Objetivos
- Identificar sÃ­ntomas: `inf`, `0`, `NaN` en softmax.
- Explicar por quÃ© `inf/inf` y `0/0` aparecen.
- Justificar la necesidad de un truco algebraico (no â€œparcheâ€).

#### 3) Relevancia
- Este error es comÃºn en entrenamiento real y puede arruinar gradients (loss NaN).

#### 4) Mapa conceptual mÃ­nimo
- logits grandes â†’ `exp(z)` overflow â†’ `inf` â†’ `inf/inf` â†’ `NaN`
- logits muy negativos â†’ `exp(z)` underflow â†’ `0` â†’ `0/0` â†’ `NaN`

#### 5) Definiciones esenciales
- **Overflow:** nÃºmero demasiado grande para representarse (â†’ `inf`).
- **Underflow:** nÃºmero tan pequeÃ±o que se aproxima a 0.

#### 6) ExplicaciÃ³n didÃ¡ctica
- Softmax es sensible al rango numÃ©rico por el `exp`. El objetivo es mantener exponentes en un rango seguro.

#### 7) Ejemplo modelado
- `z=[1000,1001,1002]` es un caso â€œconceptualmente fÃ¡cilâ€ (deberÃ­a ganar la Ãºltima clase) pero numÃ©ricamente peligroso.

#### 8) PrÃ¡ctica guiada
- Â¿CuÃ¡l de estos casos produce `inf` y cuÃ¡l produce `0`?
  - `exp(1000)`, `exp(-1000)`.

#### 9) PrÃ¡ctica independiente
- Explica por quÃ© aunque el resultado final de softmax estÃ© en `(0,1)`, el cÃ¡lculo intermedio puede romper.

#### 10) AutoevaluaciÃ³n
- Â¿QuÃ© dos operaciones generan `NaN` tÃ­picamente en este contexto?

#### 11) Errores comunes
- â€œSolucionarâ€ con `epsilon` dentro de `exp` (no resuelve overflow).

#### 12) RetenciÃ³n
- SeÃ±al roja: **si ves logits con magnitud ~1e3, softmax naive es sospechoso**.

#### 13) DiferenciaciÃ³n
- Avanzado: discute por quÃ© el problema empeora con batch grande y/o modelos profundos.

#### 14) Recursos
- IEEE-754, lÃ­mites de `float64/float32` (intuitivo: `exp(88)` ya es enorme en `float32`).

#### 15) Nota docente
- Pide al alumno que describa el fallo como â€œoperaciÃ³n indefinidaâ€ (`inf/inf`, `0/0`).

</details>

#### 5.3 Log-Sum-Exp Trick (Estabilidad NumÃ©rica)

```text
TRUCO: softmax(z) = softmax(z - max(z))

DemostraciÃ³n:
    softmax(z - c)áµ¢ = exp(záµ¢ - c) / Î£â±¼ exp(zâ±¼ - c)
                    = exp(záµ¢)Â·exp(-c) / Î£â±¼ exp(zâ±¼)Â·exp(-c)
                    = exp(záµ¢) / Î£â±¼ exp(zâ±¼)
                    = softmax(z)áµ¢

Al restar max(z), todos los exponentes son â‰¤ 0, evitando overflow.
```

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 5.3: Log-Sum-Exp Trick</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Shift por `max(z)` para hacer `exp` seguro sin cambiar softmax
- **ID (opcional):** `M04-T05_3`
- **DuraciÃ³n estimada:** 45â€“90 min
- **Nivel:** Intermedio
- **Dependencias:** 5.2

#### 2) Objetivos
- Probar que restar una constante no cambia softmax.
- Entender por quÃ© usar `max(z)` es una elecciÃ³n Ã³ptima simple.
- Reconocer el patrÃ³n â€œlog-sum-expâ€ como herramienta general.

#### 3) Relevancia
- Es la base de implementaciones estables de softmax/log-softmax y cross-entropy desde logits.

#### 4) Mapa conceptual mÃ­nimo
- invariancia por shift â†’ elegir `c=max(z)` â†’ exponentes â‰¤ 0 â†’ sin overflow

#### 5) Definiciones esenciales
- **Shift/centrado:** `z' = z - c`.
- **log-sum-exp:** `log(Î£ exp(z))` computado de forma estable.

#### 6) ExplicaciÃ³n didÃ¡ctica
- Restar `max(z)` hace que el mayor exponente sea `exp(0)=1` y el resto `â‰¤1`.

#### 7) Ejemplo modelado
- Si `z=[1000,1001,1002]`, entonces `z'=[-2,-1,0]` (seguro) y softmax no cambia.

#### 8) PrÃ¡ctica guiada
- Repite la demostraciÃ³n de invariancia para `softmax(z-c)` con sÃ­mbolos.

#### 9) PrÃ¡ctica independiente
- Â¿Por quÃ© no basta con restar un nÃºmero fijo como 100? Â¿QuÃ© hace especial a `max(z)`?

#### 10) AutoevaluaciÃ³n
- Â¿QuÃ© garantiza que `exp(z')` no overflow si `max(z')=0`?

#### 11) Errores comunes
- Restar el `max` sin `keepdims=True` y romper shapes en batch.

#### 12) RetenciÃ³n
- Mantra: **softmax es invariante a shift; usa `max` para estabilidad**.

#### 13) DiferenciaciÃ³n
- Avanzado: conecta con `log_softmax(z)=z-logsumexp(z)`.

#### 14) Recursos
- BÃºsqueda: â€œlogsumexp trickâ€ (patrÃ³n general en modelos probabilÃ­sticos).

#### 15) Nota docente
- Pide al alumno que identifique dÃ³nde aparece la misma idea en `log_softmax`.

</details>

#### 5.4 ImplementaciÃ³n NumÃ©ricamente Estable

```python
import numpy as np  # NumPy: necesario para exp/log/max/sum en softmax estable

def softmax(z: np.ndarray) -> np.ndarray:
    """
    Softmax numÃ©ricamente estable usando Log-Sum-Exp trick.

    Truco: Restar el mÃ¡ximo para evitar overflow en exp()
    softmax(z) = softmax(z - max(z))

    Args:
        z: Logits (scores antes de activaciÃ³n)

    Returns:
        Probabilidades que suman 1
    """
    # Log-Sum-Exp trick: restar el mÃ¡ximo
    z_stable = z - np.max(z, axis=-1, keepdims=True)  # Shift: ancla numÃ©rica por fila (mantiene invariancia)

    exp_z = np.exp(z_stable)  # exp() seguro: valores â‰¤ 0 evitan overflow
    return exp_z / np.sum(exp_z, axis=-1, keepdims=True)  # Normaliza para que sumen 1 (distribuciÃ³n)


def log_softmax(z: np.ndarray) -> np.ndarray:
    """
    Log-Softmax estable (Ãºtil para Cross-Entropy).

    log(softmax(z)) calculado de forma estable.
    Evita calcular softmax primero y luego log (pierde precisiÃ³n).
    """
    z_stable = z - np.max(z, axis=-1, keepdims=True)  # Mismo shift: reduce rango numÃ©rico
    log_sum_exp = np.log(np.sum(np.exp(z_stable), axis=-1, keepdims=True))  # log(sum(exp(z_stable))) por fila
    return z_stable - log_sum_exp  # log_softmax = z - logsumexp(z)


def categorical_cross_entropy_from_logits(y_true: np.ndarray, logits: np.ndarray) -> float:
    """
    Cross-entropy estable usando logits directamente.

    Evita calcular softmax explÃ­cito.
    Ãštil cuando entrenas modelos y quieres estabilidad.
    """
    log_probs = log_softmax(logits)  # Convierte logits a log-probabilidades estables
    return -np.mean(np.sum(y_true * log_probs, axis=1))  # NLL promedio: -E[log p(clase correcta)]


# ============================================================
# DEMOSTRACIÃ“N: Por quÃ© el trick es necesario
# ============================================================

def demo_numerical_stability():
    """Muestra por quÃ© necesitamos el Log-Sum-Exp trick."""

    # Caso peligroso: logits muy grandes
    z_dangerous = np.array([1000.0, 1001.0, 1002.0])  # Logits extremos: exp() desborda sin protecciÃ³n

    # Sin el trick (INCORRECTO)
    def softmax_naive(z):
        exp_z = np.exp(z)  # Â¡Overflow! exp(1000) -> inf
        return exp_z / np.sum(exp_z)  # inf/inf -> NaN (resultado no es una distribuciÃ³n vÃ¡lida)

    # Con el trick (CORRECTO)
    def softmax_stable(z):
        z_stable = z - np.max(z)  # Restar max: invariancia de softmax pero con estabilidad
        exp_z = np.exp(z_stable)  # Ahora exp() es seguro (valores â‰¤ 0)
        return exp_z / np.sum(exp_z)  # Normaliza a suma 1

    print("Logits peligrosos:", z_dangerous)
    print()

    # Naive (falla)
    import warnings
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")  # Ignora warning esperado por overflow (demo)
        result_naive = softmax_naive(z_dangerous)  # Resultado ingenuo (suele contener NaN)
        print(f"Softmax NAIVE: {result_naive}")  # Imprime el vector (para ver NaN/inf)
        print(f"  â†’ Suma: {np.sum(result_naive)} (deberÃ­a ser 1.0)")  # Verifica que no normaliza bien

    # Estable (funciona)
    result_stable = softmax_stable(z_dangerous)  # Resultado estable: finito y normalizado
    print(f"\nSoftmax ESTABLE: {result_stable}")  # Imprime el vector estable
    print(f"  â†’ Suma: {np.sum(result_stable):.6f} âœ“")  # Suma ~1 confirma distribuciÃ³n vÃ¡lida

demo_numerical_stability()


# Ejemplo: ClasificaciÃ³n multiclase (dÃ­gitos 0-9)
logits = np.array([2.0, 1.0, 0.1, -1.0, 3.0, 0.5, -0.5, 1.5, 0.0, -2.0])
probs = softmax(logits)

print("\nLogits â†’ Probabilidades:")
for i, (l, p) in enumerate(zip(logits, probs)):
    print(f"  Clase {i}: logit={l:+.1f} â†’ prob={p:.3f}")
print(f"\nSuma de probabilidades: {np.sum(probs):.6f}")
print(f"Clase predicha: {np.argmax(probs)}")

```

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 5.4: ImplementaciÃ³n NumÃ©ricamente Estable</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Implementar `softmax`/`log_softmax` sin NaN (y por quÃ© funciona)
- **ID (opcional):** `M04-T05_4`
- **DuraciÃ³n estimada:** 60â€“120 min
- **Nivel:** Intermedio
- **Dependencias:** 5.2â€“5.3

#### 2) Objetivos
- Implementar softmax estable con `z - max(z)`.
- Entender por quÃ© `log_softmax` es preferible a `np.log(softmax(z))`.
- Verificar propiedades: probabilidades finitas y suma 1.

#### 3) Relevancia
- Esta es una de las fuentes mÃ¡s comunes de `loss=NaN` en entrenamiento real (overflow/underflow en `exp`).

#### 4) Mapa conceptual mÃ­nimo
- logits `z` â†’ shift `z-max(z)` â†’ `exp` seguro â†’ normalizar â†’ softmax
- logits `z` â†’ `log_softmax(z)=z-logsumexp(z)` â†’ CE estable

#### 5) Definiciones esenciales
- **Shift invariante:** restar una constante a todos los logits no cambia softmax.
- **log-softmax:** log-probabilidades computadas sin pasar por probabilidades intermedias inestables.

#### 6) ExplicaciÃ³n didÃ¡ctica
- Restar `max(z)` â€œcentraâ€ la fila para que el mayor exponente sea `exp(0)=1` y el resto `â‰¤1`.

#### 7) Ejemplo modelado
- El demo con logits grandes muestra que la versiÃ³n naive puede producir `inf/inf â†’ NaN`, mientras que la estable no.

#### 8) PrÃ¡ctica guiada
- AÃ±ade checks:
  - `assert np.all(np.isfinite(softmax(z)))`
  - `assert np.allclose(np.sum(softmax(z)), 1.0)` (vector) o por fila (batch).

#### 9) PrÃ¡ctica independiente
- Implementa soporte batch `(n_samples, n_classes)` y verifica que `axis=-1` es el correcto.

#### 10) AutoevaluaciÃ³n
- Â¿Por quÃ© `argmax(softmax(z)) == argmax(z)` aunque cambien los valores?

#### 11) Errores comunes
- Olvidar `keepdims=True` y romper broadcasting.
- Normalizar sobre el eje incorrecto.

#### 12) RetenciÃ³n
- Regla: **si ves `exp`, piensa en estabilidad y en restar `max`**.

#### 13) DiferenciaciÃ³n
- Avanzado: compara el comportamiento en `float32` vs `float64`.

#### 14) Recursos
- PatrÃ³n: â€œlog-sum-exp trickâ€ (idea general en modelos probabilÃ­sticos).

#### 15) Nota docente
- Pide al alumno explicar el fallo del naive como â€œoperaciÃ³n indefinidaâ€ (`inf/inf`, `0/0`).
</details>

#### 5.5 Categorical Cross-Entropy (Multiclase)

```python
def categorical_cross_entropy(y_true: np.ndarray,
                               y_pred: np.ndarray) -> float:
    """
    Loss para clasificaciÃ³n multiclase.

    Args:
        y_true: One-hot encoded labels (n_samples, n_classes)
        y_pred: Probabilidades softmax (n_samples, n_classes)

    Returns:
        Loss promedio
    """
    epsilon = 1e-15
    # Solo cuenta la clase correcta (donde y_true=1)
    return -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=1))


# Ejemplo
y_true = np.array([
    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],  # Clase 4
    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # Clase 0
])

y_pred = np.array([
    softmax(np.array([0, 0, 0, 0, 5, 0, 0, 0, 0, 0])),  # Confiado en 4
    softmax(np.array([3, 1, 0, 0, 0, 0, 0, 0, 0, 0])),  # Confiado en 0
])

loss = categorical_cross_entropy(y_true, y_pred)
print(f"Categorical Cross-Entropy: {loss:.4f}")

```

<details open>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” SecciÃ³n 5.5: Categorical Cross-Entropy (Multiclase)</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Implementar CCE con one-hot y entender quÃ© suma realmente
- **ID (opcional):** `M04-T05_5`
- **DuraciÃ³n estimada:** 45â€“90 min
- **Nivel:** Intermedio
- **Dependencias:** 4.5 + 5.4

#### 2) Objetivos
- Implementar CCE con protecciÃ³n numÃ©rica (`epsilon`).
- Entender por quÃ©, con one-hot, la loss selecciona la probabilidad de la clase correcta.
- Conectar CCE con NLL/MLE: minimizar CCE â‰¡ maximizar likelihood categÃ³rica.

#### 3) Relevancia
- CCE es la funciÃ³n de pÃ©rdida estÃ¡ndar en clasificaciÃ³n multiclase con softmax.

#### 4) Mapa conceptual mÃ­nimo
- one-hot `y` â†’ selecciona clase correcta â†’ `-log(p_correcta)` â†’ promedio en batch

#### 5) Definiciones esenciales
- **One-hot:** vector con un 1 en la clase correcta y 0 en las demÃ¡s.
- **`epsilon`:** evita `log(0)` cuando `p` llega a 0 por redondeo.

#### 6) ExplicaciÃ³n didÃ¡ctica
- El tÃ©rmino `np.sum(y_true * log(p), axis=1)` actÃºa como â€œselectorâ€ de la clase correcta.

#### 7) Ejemplo modelado
- Si `p_correcta` pasa de `0.9` a `0.1`, la loss sube fuertemente (penaliza confianza equivocada).

#### 8) PrÃ¡ctica guiada
- Calcula a mano una muestra: `L=-log(p_correcta)` y valida con el print del cÃ³digo.

#### 9) PrÃ¡ctica independiente
- Implementa la versiÃ³n con Ã­ndices (`y_true` como clase entera) y compara resultados.

#### 10) AutoevaluaciÃ³n
- Â¿Por quÃ© `epsilon` arregla `log(0)` pero no corrige overflow que ocurre antes en softmax naive?

#### 11) Errores comunes
- Pasar logits a una CE que espera probabilidades.
- No verificar que `y_pred` suma 1 por fila.

#### 12) RetenciÃ³n
- FÃ³rmula: **CCE = -promedio(log(probabilidad de la clase correcta))**.

#### 13) DiferenciaciÃ³n
- Avanzado: discute label smoothing y cÃ³mo cambia la suma `Î£ y_k log(p_k)`.

#### 14) Recursos
- ConexiÃ³n directa con el tema 4.5 (NLL y MLE).

#### 15) Nota docente
- Pregunta de control: â€œÂ¿quÃ© lÃ­nea hace que solo cuente la clase correcta?â€
</details>

## ğŸ¯ Ejercicios por tema (progresivos) + Soluciones

Reglas:

- **Intenta primero** sin mirar la soluciÃ³n.
- **Timebox sugerido:** 15â€“30 min por ejercicio.
- **Ã‰xito mÃ­nimo:** tu soluciÃ³n debe pasar los `assert`.

---

### Ejercicio 4.1: Probabilidad condicional (P(A|B)) y consistencia

#### Enunciado

1) **BÃ¡sico**

- Dado un conjunto de conteos de eventos, calcula `P(A)`, `P(B)` y `P(A âˆ© B)`.

2) **Intermedio**

- Calcula `P(A|B) = P(Aâˆ©B)/P(B)` y verifica que estÃ¡ en `[0,1]`.

3) **Avanzado**

- Verifica que `P(Aâˆ©B) = P(A|B)Â·P(B)`.

#### SoluciÃ³n

```python
import numpy as np

# SimulaciÃ³n con conteos (dataset pequeÃ±o)
n = 100
count_A = 40
count_B = 50
count_A_and_B = 20

P_A = count_A / n
P_B = count_B / n
P_A_and_B = count_A_and_B / n

P_A_given_B = P_A_and_B / P_B

assert 0.0 <= P_A <= 1.0
assert 0.0 <= P_B <= 1.0
assert 0.0 <= P_A_given_B <= 1.0
assert np.isclose(P_A_and_B, P_A_given_B * P_B)
```

---

### Ejercicio 4.2: Bayes en modo clasificador (posterior sin normalizar)

#### Enunciado

1) **BÃ¡sico**

- Implementa el cÃ¡lculo de posterior sin normalizar:
  - `score_spam = P(x|spam)Â·P(spam)`
  - `score_ham = P(x|ham)Â·P(ham)`

2) **Intermedio**

- Normaliza y obtÃ©n `P(spam|x)` y `P(ham|x)`.

3) **Avanzado**

- Verifica que las probabilidades normalizadas suman 1.

#### SoluciÃ³n

```python
import numpy as np

P_spam = 0.3
P_ham = 1.0 - P_spam

P_x_given_spam = 0.8
P_x_given_ham = 0.1

score_spam = P_x_given_spam * P_spam
score_ham = P_x_given_ham * P_ham

Z = score_spam + score_ham
P_spam_given_x = score_spam / Z
P_ham_given_x = score_ham / Z

assert np.isclose(P_spam_given_x + P_ham_given_x, 1.0)
assert P_spam_given_x > P_ham_given_x
```

---

### Ejercicio 4.3: Independencia (test empÃ­rico)

#### Enunciado

1) **BÃ¡sico**

- Simula dos variables binarias independientes `A` y `B`.

2) **Intermedio**

- Estima `P(A)`, `P(B)`, `P(Aâˆ©B)` y verifica `P(Aâˆ©B) â‰ˆ P(A)P(B)`.

3) **Avanzado**

- Simula un caso dependiente y verifica que la igualdad se rompe.

#### SoluciÃ³n

```python
import numpy as np

np.random.seed(0)
n = 20000

# Independientes
A = (np.random.rand(n) < 0.4)
B = (np.random.rand(n) < 0.5)

P_A = A.mean()
P_B = B.mean()
P_A_and_B = (A & B).mean()

assert abs(P_A_and_B - (P_A * P_B)) < 0.01

# Dependientes: B es casi A
B_dep = (A | (np.random.rand(n) < 0.05))
P_B_dep = B_dep.mean()
P_A_and_B_dep = (A & B_dep).mean()

assert abs(P_A_and_B_dep - (P_A * P_B_dep)) > 0.02
```

---

### Ejercicio 4.4: MLE de Bernoulli ("fracciÃ³n de heads")

#### Enunciado

1) **BÃ¡sico**

- Genera muestras Bernoulli con `p_true`.

2) **Intermedio**

- Implementa el estimador MLE `p_hat = mean(x)`.

3) **Avanzado**

- Verifica que `p_hat` se aproxima a `p_true` con suficientes muestras.

#### SoluciÃ³n

```python
import numpy as np

np.random.seed(1)
p_true = 0.7
n = 5000
x = (np.random.rand(n) < p_true).astype(float)

p_hat = float(np.mean(x))
assert abs(p_hat - p_true) < 0.02
```

---

### Ejercicio 4.5: PDF Gaussiana univariada (sanity check)

#### Enunciado

1) **BÃ¡sico**

- Implementa la PDF de una normal `N(Î¼,ÏƒÂ²)`.

2) **Intermedio**

- Verifica que para `N(0,1)` en `x=0` la densidad â‰ˆ `0.39894228`.

3) **Avanzado**

- Verifica que `pdf(x)` es simÃ©trica: `pdf(a) == pdf(-a)` cuando `Î¼=0`.

#### SoluciÃ³n

```python
import numpy as np

def gaussian_pdf(x: np.ndarray, mu: float, sigma: float) -> np.ndarray:
    x = np.asarray(x, dtype=float)
    sigma = float(sigma)
    assert sigma > 0
    z = (x - mu) / sigma
    return (1.0 / (np.sqrt(2.0 * np.pi) * sigma)) * np.exp(-0.5 * z**2)


val0 = gaussian_pdf(np.array([0.0]), mu=0.0, sigma=1.0)[0]
assert np.isclose(val0, 0.39894228, atol=1e-4)

a = 1.7
assert np.isclose(
    gaussian_pdf(np.array([a]), 0.0, 1.0)[0],
    gaussian_pdf(np.array([-a]), 0.0, 1.0)[0],
    rtol=1e-12,
    atol=1e-12,
)
```

---

### Ejercicio 4.6: Gaussiana multivariada (2D) + covarianza vÃ¡lida

#### Enunciado

1) **BÃ¡sico**

- Implementa la densidad `N(Î¼, Î£)` en 2D.

2) **Intermedio**

- Para `Î¼=0` y `Î£=I`, verifica que `pdf(0) = 1/(2Ï€)`.

3) **Avanzado**

- Verifica que `Î£` es definida positiva (eigenvalores > 0) antes de invertir.

4) **Bonus (elipse de covarianza)**

- Para una matriz de covarianza no diagonal, genera puntos en la elipse de covarianza 2D para una escala `k` (por ejemplo, `k=2`) usando descomposiciÃ³n en eigenvalues/eigenvectors, y verifica que satisfacen `(x-Î¼)^T Î£^{-1} (x-Î¼) â‰ˆ k^2`.

#### SoluciÃ³n

```python
import numpy as np

def multivariate_gaussian_pdf(x: np.ndarray, mu: np.ndarray, cov: np.ndarray) -> float:
    x = np.asarray(x, dtype=float)
    mu = np.asarray(mu, dtype=float)
    cov = np.asarray(cov, dtype=float)
    d = x.shape[0]

    assert mu.shape == (d,)
    assert cov.shape == (d, d)
    assert np.allclose(cov, cov.T)
    eigvals = np.linalg.eigvals(cov)
    assert np.all(eigvals > 0)

    diff = x - mu
    inv = np.linalg.inv(cov)
    det = np.linalg.det(cov)
    norm = 1.0 / (np.sqrt(((2.0 * np.pi) ** d) * det))
    expo = -0.5 * float(diff.T @ inv @ diff)
    return float(norm * np.exp(expo))


mu = np.array([0.0, 0.0])
cov = np.eye(2)
pdf0 = multivariate_gaussian_pdf(np.array([0.0, 0.0]), mu, cov)
assert np.isclose(pdf0, 1.0 / (2.0 * np.pi), atol=1e-6)
assert pdf0 > 0.0

def covariance_ellipse_points(mu: np.ndarray, cov: np.ndarray, k: float = 2.0, n: int = 200) -> np.ndarray:
    mu = np.asarray(mu, dtype=float)
    cov = np.asarray(cov, dtype=float)
    assert mu.shape == (2,)
    assert cov.shape == (2, 2)
    assert np.allclose(cov, cov.T)

    eigvals, eigvecs = np.linalg.eigh(cov)
    assert np.all(eigvals > 0)

    t = np.linspace(0.0, 2.0 * np.pi, n, endpoint=False)
    circle = np.stack([np.cos(t), np.sin(t)], axis=0)

    transform = eigvecs @ np.diag(np.sqrt(eigvals))
    pts = (mu.reshape(2, 1) + (k * transform @ circle)).T
    return pts


mu2 = np.array([0.0, 0.0])
cov2 = np.array([
    [2.0, 1.2],
    [1.2, 1.0],
], dtype=float)
pts = covariance_ellipse_points(mu2, cov2, k=2.0, n=180)
inv2 = np.linalg.inv(cov2)

q = np.einsum('...i,ij,...j->...', pts - mu2, inv2, pts - mu2)
assert np.allclose(q, 4.0, atol=1e-6)
```

---

### Ejercicio 4.6B: VisualizaciÃ³n (Gaussiana 2D variando covarianza) (OBLIGATORIO)

#### Enunciado

Construye una visualizaciÃ³n que haga **visible** la covarianza:

1) **BÃ¡sico**

- Crea un grid 2D y grafica contornos (`contour`) de `N(Î¼, Î£)`.

2) **Intermedio**

- Compara al menos 3 covarianzas:
  - isotrÃ³pica (`Î£ = I`)
  - elÃ­ptica (varianzas distintas)
  - correlacionada (tÃ©rminos fuera de la diagonal)

3) **Avanzado**

- Sobre cada plot, dibuja la **elipse de covarianza** para `k=2` y verifica que sus puntos cumplen `(x-Î¼)^T Î£^{-1} (x-Î¼) â‰ˆ k^2`.

#### SoluciÃ³n

```python
import numpy as np  # NumPy: grid 2D, Ã¡lgebra lineal y evaluaciÃ³n vectorizada
import matplotlib.pyplot as plt  # Matplotlib: contornos 2D y trazado de elipses


def multivariate_gaussian_pdf_grid(xx: np.ndarray, yy: np.ndarray, mu: np.ndarray, cov: np.ndarray) -> np.ndarray:
    # xx, yy: grids 2D (H,W) tÃ­picamente creados con np.meshgrid
    xx = np.asarray(xx, dtype=float)  # Asegura dtype float para evitar ints en exp/log
    yy = np.asarray(yy, dtype=float)  # Mismo contrato: (H,W)
    mu = np.asarray(mu, dtype=float)  # mu:(2,) media 2D
    cov = np.asarray(cov, dtype=float)  # cov:(2,2) covarianza

    assert mu.shape == (2,)  # Sanidad: trabajamos en 2D
    assert cov.shape == (2, 2)  # Sanidad: covarianza 2D
    assert np.allclose(cov, cov.T)  # Debe ser simÃ©trica

    eigvals = np.linalg.eigvalsh(cov)  # Eigenvalues reales para matriz simÃ©trica (mÃ¡s estable)
    assert np.all(eigvals > 0.0)  # Covarianza debe ser definida positiva (invertible)

    inv = np.linalg.inv(cov)  # Î£^{-1} para la forma cuadrÃ¡tica
    det = np.linalg.det(cov)  # |Î£| para el coeficiente de normalizaciÃ³n

    pos = np.dstack([xx, yy])  # pos:(H,W,2) apila coordenadas (x,y) en el Ãºltimo eje
    diff = pos - mu.reshape(1, 1, 2)  # diff:(H,W,2) resta Î¼ por broadcasting

    quad = np.einsum('...i,ij,...j->...', diff, inv, diff)  # (x-Î¼)^T Î£^{-1} (x-Î¼) para cada punto del grid
    expo = -0.5 * quad  # Exponente de la Gaussiana

    norm = 1.0 / (2.0 * np.pi * np.sqrt(det))  # NormalizaciÃ³n en 2D: 1 / (2Ï€ sqrt(|Î£|))
    pdf = norm * np.exp(expo)  # pdf:(H,W) densidad evaluada en el grid

    return pdf  # Devuelve matriz 2D lista para contour/contourf


def covariance_ellipse_points(mu: np.ndarray, cov: np.ndarray, k: float = 2.0, n: int = 200) -> np.ndarray:
    # Esta funciÃ³n genera puntos sobre la elipse: (x-Î¼)^T Î£^{-1} (x-Î¼) = k^2
    mu = np.asarray(mu, dtype=float)  # mu:(2,) asegura float
    cov = np.asarray(cov, dtype=float)  # cov:(2,2) asegura float

    assert mu.shape == (2,)  # Solo soportamos 2D para visualizaciÃ³n
    assert cov.shape == (2, 2)  # Covarianza 2D
    assert np.allclose(cov, cov.T)  # SimetrÃ­a

    eigvals, eigvecs = np.linalg.eigh(cov)  # DescomposiciÃ³n simÃ©trica: cov = Q Î› Q^T
    assert np.all(eigvals > 0.0)  # PD: eigenvalues positivos

    t = np.linspace(0.0, 2.0 * np.pi, n, endpoint=False)  # ParÃ¡metro angular para un cÃ­rculo unitario
    circle = np.stack([np.cos(t), np.sin(t)], axis=0)  # circle:(2,n) cÃ­rculo unitario

    transform = eigvecs @ np.diag(np.sqrt(eigvals))  # TransformaciÃ³n que mapea cÃ­rculo -> elipse base (k=1)
    pts = (mu.reshape(2, 1) + (k * transform @ circle)).T  # pts:(n,2) traslada por Î¼ y escala por k

    return pts  # Puntos listos para plt.plot(pts[:,0], pts[:,1])


mu = np.array([0.0, 0.0], dtype=float)  # Î¼:(2,) centramos en el origen para comparar solo Î£

covs = [
    np.eye(2, dtype=float),  # Î£1: isotrÃ³pica (cÃ­rculo)
    np.array([[3.0, 0.0], [0.0, 1.0]], dtype=float),  # Î£2: elÃ­ptica (varianza distinta por eje)
    np.array([[2.0, 1.2], [1.2, 1.0]], dtype=float),  # Î£3: correlacionada (tÃ©rmino fuera de diagonal)
]  # Lista de covarianzas a comparar

labels = [
    "Î£ = I (isotrÃ³pica)",  # Texto para subplot 1
    "Î£ = diag(3,1) (elÃ­ptica)",  # Texto para subplot 2
    "Î£ con correlaciÃ³n (elipse rotada)",  # Texto para subplot 3
]  # Etiquetas

grid = np.linspace(-4.0, 4.0, 250)  # Rejilla 1D para construir el grid 2D
xx, yy = np.meshgrid(grid, grid)  # xx,yy:(H,W) coordenadas del plano

fig, axes = plt.subplots(1, 3, figsize=(15, 4), constrained_layout=True)  # 1 fila, 3 columnas

for ax, cov, title in zip(axes, covs, labels):  # Iteramos por cada Î£ y su eje
    Z = multivariate_gaussian_pdf_grid(xx, yy, mu, cov)  # Z:(H,W) densidad en el plano

    ax.contour(xx, yy, Z, levels=10)  # Contornos: lÃ­neas de igual densidad

    pts = covariance_ellipse_points(mu, cov, k=2.0, n=240)  # pts:(n,2) elipse k=2
    ax.plot(pts[:, 0], pts[:, 1])  # Dibuja la elipse encima de los contornos

    inv = np.linalg.inv(cov)  # Î£^{-1} para verificar la ecuaciÃ³n cuadrÃ¡tica
    q = np.einsum('...i,ij,...j->...', pts - mu, inv, pts - mu)  # q:(n,) valor de (x-Î¼)^T Î£^{-1} (x-Î¼)
    assert np.allclose(q, 4.0, atol=1e-6)  # Debe ser â‰ˆ k^2 = 4 si la elipse es correcta

    ax.set_title(title)  # TÃ­tulo por subplot
    ax.set_aspect('equal', 'box')  # Aspect ratio 1:1 para que la elipse no se distorsione
    ax.set_xlabel('x1')  # Eje x
    ax.set_ylabel('x2')  # Eje y

plt.savefig('gaussian_covariance_contours.png', dpi=160)  # Guarda la figura (Ãºtil para reportes)
```

---

### Ejercicio 4.7: Log-Sum-Exp y log-softmax estable (OBLIGATORIO)

#### Enunciado

1) **BÃ¡sico**

- Implementa `logsumexp(z)` de forma estable (restando `max(z)`).

2) **Intermedio**

- Implementa `log_softmax(z) = z - logsumexp(z)`.

3) **Avanzado**

- Verifica que `sum(exp(log_softmax(z))) == 1` y que no hay `inf` con logits grandes.

#### SoluciÃ³n

```python
import numpy as np  # NumPy: arrays, exp/log y validaciÃ³n numÃ©rica

def logsumexp(z: np.ndarray) -> float:
    z = np.asarray(z, dtype=float)  # Asegura float para que exp/log sean numÃ©ricamente consistentes
    m = np.max(z)  # m = max(z) sirve como â€œanclaâ€ para evitar overflow en exp
    return float(m + np.log(np.sum(np.exp(z - m))))  # Log-Sum-Exp: m + log(sum(exp(z-m)))


def log_softmax(z: np.ndarray) -> np.ndarray:
    z = np.asarray(z, dtype=float)  # Asegura float y copia segura
    return z - logsumexp(z)  # log_softmax(z) = z - log(sum(exp(z)))


z = np.array([1000.0, 0.0, -1000.0])  # Logits extremos para estresar estabilidad numÃ©rica
lsm = log_softmax(z)  # lsm:(3,) log-probabilidades estables
probs = np.exp(lsm)  # Convertimos a probabilidades (deben ser finitas)
assert np.isfinite(lsm).all()  # No debe haber NaN/inf en log-probabilidades
assert np.isfinite(probs).all()  # No debe haber NaN/inf en probabilidades
assert np.isclose(np.sum(probs), 1.0)  # Las probabilidades deben sumar 1
```

#### SoluciÃ³n (NaN trap: naive vs estable + verificaciÃ³n) (OBLIGATORIO)

```python
import numpy as np  # NumPy: exp/log y validaciÃ³n numÃ©rica
import warnings  # warnings: suprimir warnings esperados en el caso naÃ¯ve (overflow)


def softmax_naive(z: np.ndarray) -> np.ndarray:  # ImplementaciÃ³n ingenua (propensa a overflow/underflow)
    z = np.asarray(z, dtype=float)  # Asegura float para que exp opere en floats
    exp_z = np.exp(z)  # Â¡Peligro! exp(1000) -> inf (overflow)
    return exp_z / np.sum(exp_z)  # Normaliza (pero si hay inf/0 puede producir NaN)


def softmax_stable(z: np.ndarray) -> np.ndarray:  # Softmax estable: aplica el Log-Sum-Exp trick
    z = np.asarray(z, dtype=float)  # Convierte a float (contrato)
    z_shift = z - np.max(z)  # Restar max(z) no cambia softmax pero evita overflow
    exp_z = np.exp(z_shift)  # Ahora exp() recibe valores <= 0 (seguro)
    return exp_z / np.sum(exp_z)  # Normaliza para que sum(p)=1


z_big = np.array([1000.0, 1001.0, 1002.0])  # Logits peligrosos (magnitudes enormes)

with warnings.catch_warnings():  # Contexto para que el notebook/terminal no se llene de warnings
    warnings.simplefilter("ignore")  # Suprimimos RuntimeWarning por overflow (esperado aquÃ­)
    p_naive = softmax_naive(z_big)  # Resultado ingenuo (tÃ­picamente NaN)

naive_ok = np.isfinite(p_naive).all() and np.isclose(np.sum(p_naive), 1.0)  # Criterio de â€œdistribuciÃ³n vÃ¡lidaâ€
assert not naive_ok  # Debe fallar: aquÃ­ demostramos el NaN/inf trap

p_stable = softmax_stable(z_big)  # Softmax estable (debe funcionar)
assert np.isfinite(p_stable).all()  # No debe haber NaN/inf
assert np.isclose(np.sum(p_stable), 1.0)  # Debe sumar 1
assert np.argmax(p_stable) == np.argmax(z_big)  # Debe preservar el orden de logits
```

---

### Ejercicio 4.8: Softmax estable (invariancia a constantes)

#### Enunciado

1) **BÃ¡sico**

- Implementa softmax estable: `exp(z-max)/sum(exp(z-max))`.

2) **Intermedio**

- Verifica que suma 1.

3) **Avanzado**

- Verifica invariancia: `softmax(z) == softmax(z + c)`.

#### SoluciÃ³n

```python
import numpy as np

def softmax(z: np.ndarray) -> np.ndarray:
    z = np.asarray(z, dtype=float)
    z_shift = z - np.max(z)
    expz = np.exp(z_shift)
    return expz / np.sum(expz)


z = np.array([2.0, 1.0, 0.0])
p = softmax(z)
assert np.isclose(np.sum(p), 1.0)

c = 100.0
p2 = softmax(z + c)
assert np.allclose(p, p2)
assert np.argmax(p) == np.argmax(z)
```

---

### Ejercicio 4.9: Binary Cross-Entropy estable (evitar log(0))

#### Enunciado

1) **BÃ¡sico**

- Implementa BCE: `-mean(y log(p) + (1-y) log(1-p))`.

2) **Intermedio**

- Usa `clip`/`epsilon` para evitar `log(0)`.

3) **Avanzado**

- Verifica:
  - BCE cerca de 0 para predicciones casi perfectas.
  - BCE â‰ˆ `-log(0.9)` cuando `y=1` y `p=0.9`.

#### SoluciÃ³n

```python
import numpy as np

def binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-15) -> float:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    y_pred = np.clip(y_pred, eps, 1.0 - eps)
    return float(-np.mean(y_true * np.log(y_pred) + (1.0 - y_true) * np.log(1.0 - y_pred)))


y_true = np.array([1.0, 0.0, 1.0, 0.0])
y_pred_good = np.array([0.999, 0.001, 0.999, 0.001])
assert binary_cross_entropy(y_true, y_pred_good) < 0.01

assert np.isclose(binary_cross_entropy(np.array([1.0]), np.array([0.9])), -np.log(0.9), atol=1e-12)
```

---

### Ejercicio 4.10: Categorical Cross-Entropy (multiclase) + one-hot

#### Enunciado

1) **BÃ¡sico**

- Implementa CCE: `-mean(sum(y_true * log(y_pred)))`.

2) **Intermedio**

- Asegura que `y_pred` no contiene ceros (epsilon).

3) **Avanzado**

- Verifica que el loss baja cuando aumenta la probabilidad de la clase correcta.

#### SoluciÃ³n

```python
import numpy as np

def categorical_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-15) -> float:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    y_pred = np.clip(y_pred, eps, 1.0)
    return float(-np.mean(np.sum(y_true * np.log(y_pred), axis=1)))


y_true = np.array([[0, 1, 0], [1, 0, 0]], dtype=float)
y_pred_bad = np.array([[0.34, 0.33, 0.33], [0.34, 0.33, 0.33]], dtype=float)
y_pred_good = np.array([[0.05, 0.90, 0.05], [0.90, 0.05, 0.05]], dtype=float)

loss_bad = categorical_cross_entropy(y_true, y_pred_bad)
loss_good = categorical_cross_entropy(y_true, y_pred_good)
assert loss_good < loss_bad
```

---

### (Bonus) Ejercicio 4.11: Cadena de Markov (matriz de transiciÃ³n)

#### Enunciado

1) **BÃ¡sico**

- Define una matriz de transiciÃ³n `P` (filas suman 1).

2) **Intermedio**

- Propaga una distribuciÃ³n `Ï€_{t+1} = Ï€_t P` y verifica que sigue siendo distribuciÃ³n.

3) **Avanzado**

- Encuentra una distribuciÃ³n estacionaria aproximada iterando muchas veces y verifica `Ï€ â‰ˆ Ï€P`.

4) **Bonus (potencias de matrices)**

- Verifica que iterar `Ï€_{t+1} = Ï€_t P` por `k` pasos coincide con `Ï€_t P^k` usando `np.linalg.matrix_power`.

#### SoluciÃ³n

```python
import numpy as np

P = np.array([
    [0.9, 0.1],
    [0.2, 0.8],
], dtype=float)
assert np.allclose(P.sum(axis=1), 1.0)

k = 50
pi0 = np.array([1.0, 0.0])
pi = pi0.copy()
for _ in range(k):
    pi = pi @ P
    assert np.isclose(np.sum(pi), 1.0)
    assert np.all(pi >= 0)

pi_power = pi0 @ np.linalg.matrix_power(P, k)
assert np.allclose(pi, pi_power, atol=1e-12)

pi_star = pi.copy()
assert np.allclose(pi_star, pi_star @ P, atol=1e-6)
```

## ğŸ”¨ Entregables del MÃ³dulo

### E1: `probability.py`

```python
"""
MÃ³dulo de probabilidad esencial para ML.
Implementaciones desde cero con NumPy.
"""

import numpy as np
from typing import Tuple

def gaussian_pdf(x: np.ndarray, mu: float, sigma: float) -> np.ndarray:
    """Densidad de probabilidad Gaussiana univariada."""
    pass

def multivariate_gaussian_pdf(x: np.ndarray,
                               mu: np.ndarray,
                               cov: np.ndarray) -> float:
    """Densidad de probabilidad Gaussiana multivariada."""
    pass

def mle_gaussian(data: np.ndarray) -> Tuple[float, float]:
    """EstimaciÃ³n MLE de parÃ¡metros de Gaussiana."""
    pass

def softmax(z: np.ndarray) -> np.ndarray:
    """FunciÃ³n softmax numÃ©ricamente estable."""
    pass

def cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Binary cross-entropy loss."""
    pass

def categorical_cross_entropy(y_true: np.ndarray,
                               y_pred: np.ndarray) -> float:
    """Categorical cross-entropy loss para multiclase."""
    pass
```

### E2: Tests

```python
# tests/test_probability.py
import numpy as np
import pytest
from src.probability import (
    gaussian_pdf, mle_gaussian, softmax,
    cross_entropy, categorical_cross_entropy
)

def test_gaussian_pdf_standard():
    """PDF de Gaussiana estÃ¡ndar en x=0 debe ser ~0.3989."""
    result = gaussian_pdf(np.array([0.0]), mu=0, sigma=1)
    expected = 1 / np.sqrt(2 * np.pi)  # ~0.3989
    assert np.isclose(result[0], expected, rtol=1e-5)

def test_softmax_sums_to_one():
    """Softmax debe sumar 1."""
    z = np.random.randn(10)
    probs = softmax(z)
    assert np.isclose(np.sum(probs), 1.0)

def test_softmax_preserves_order():
    """Mayor logit â†’ mayor probabilidad."""
    z = np.array([1.0, 2.0, 3.0])
    probs = softmax(z)
    assert probs[2] > probs[1] > probs[0]

def test_mle_gaussian_accuracy():
    """MLE debe recuperar parÃ¡metros con suficientes datos."""
    np.random.seed(42)
    true_mu, true_sigma = 10.0, 3.0
    data = np.random.normal(true_mu, true_sigma, size=10000)

    est_mu, est_sigma = mle_gaussian(data)

    assert np.isclose(est_mu, true_mu, rtol=0.05)
    assert np.isclose(est_sigma, true_sigma, rtol=0.05)

def test_cross_entropy_perfect_prediction():
    """CE debe ser ~0 para predicciones perfectas."""
    y_true = np.array([1, 0, 1])
    y_pred = np.array([0.999, 0.001, 0.999])

    loss = cross_entropy(y_true, y_pred)
    assert loss < 0.01
```

---

## ğŸ“Š Resumen Visual

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PROBABILIDAD PARA ML - MAPA CONCEPTUAL                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  TEOREMA DE BAYES                                               â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€â–º Naive Bayes Classifier (MÃ³dulo 05)                   â”‚
â”‚       â””â”€â”€â–º IntuiciÃ³n de posterior vs prior                      â”‚
â”‚                                                                 â”‚
â”‚  DISTRIBUCIÃ“N GAUSSIANA                                         â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€â–º GMM en Unsupervised (MÃ³dulo 06)                      â”‚
â”‚       â”œâ”€â”€â–º InicializaciÃ³n de pesos en DL (MÃ³dulo 07)            â”‚
â”‚       â””â”€â”€â–º NormalizaciÃ³n de datos                               â”‚
â”‚                                                                 â”‚
â”‚  MAXIMUM LIKELIHOOD (MLE)                                       â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€â–º Cross-Entropy Loss (Logistic Regression)             â”‚
â”‚       â”œâ”€â”€â–º Categorical CE (Softmax + Multiclase)                â”‚
â”‚       â””â”€â”€â–º EM Algorithm en GMM                                  â”‚
â”‚                                                                 â”‚
â”‚  SOFTMAX                                                        â”‚
â”‚       â”‚                                                         â”‚
â”‚       â””â”€â”€â–º Capa de salida en clasificaciÃ³n multiclase           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”— Conexiones con Otros MÃ³dulos

| Concepto | DÃ³nde se usa |
|----------|--------------|
| Teorema de Bayes | Naive Bayes en MÃ³dulo 05 |
| Gaussiana | GMM en MÃ³dulo 06, inicializaciÃ³n en MÃ³dulo 07 |
| MLE | DerivaciÃ³n de Cross-Entropy en MÃ³dulo 05 |
| Softmax | Capa de salida en MÃ³dulo 07 |
| Cross-Entropy | Loss function principal en MÃ³dulo 05 y 07 |

---

## ğŸ§© ConsolidaciÃ³n (errores comunes + debugging v5 + reto Feynman)

### Errores comunes

- **Confundir PDF con probabilidad:** en continuas, `f(x)` es densidad; la probabilidad requiere integrar en un intervalo.
- **`log(0)` en cross-entropy:** siempre usa `epsilon` o `np.clip`.
- **Overflow/underflow en `exp`:** aplica log-sum-exp / log-softmax.
- **MLE â€œmÃ¡gicoâ€:** si no puedes explicar por quÃ© aparece la media, repite el worked example Bernoulli.

### Debugging / validaciÃ³n (v5)

- Cuando algo explote con `nan/inf`, revisa:
  - `np.log` sobre valores 0
  - `np.exp` sobre logits grandes
  - normalizaciÃ³n incorrecta en probabilidades (que no suman 1)
- Registra hallazgos en `study_tools/DIARIO_ERRORES.md`.
- Protocolos completos:
  - [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)
  - [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md)

### Reto Feynman (tablero blanco)

Explica en 5 lÃ­neas o menos:

1) Â¿Por quÃ© maximizar likelihood es equivalente a minimizar negative log-likelihood?
2) Â¿Por quÃ© el MLE de una moneda es â€œproporciÃ³n de carasâ€?
3) Â¿QuÃ© significa `Ï€_{t+1} = Ï€_t P` y por quÃ© es Ã¡lgebra lineal?

## âœ… Checklist del MÃ³dulo

- [ ] Puedo explicar el Teorema de Bayes con un ejemplo
- [ ] SÃ© calcular la PDF de una Gaussiana a mano
- [ ] Entiendo por quÃ© MLE da Cross-Entropy como loss
- [ ] ImplementÃ© softmax numÃ©ricamente estable
- [ ] Puedo derivar el MLE de una Bernoulli (moneda) y explicarlo
- [ ] Puedo explicar quÃ© es una Markov Chain y quÃ© representa una matriz de transiciÃ³n
- [ ] Los tests de `probability.py` pasan

---

## ğŸ“– Recursos Adicionales

### Videos
- [3Blue1Brown - Bayes Theorem](https://www.youtube.com/watch?v=HZGCoVF3YvM)
- [StatQuest - Maximum Likelihood](https://www.youtube.com/watch?v=XepXtl9YKwc)
- [StatQuest - Gaussian Distribution](https://www.youtube.com/watch?v=rzFX5NWojp0)

### Lecturas
- Mathematics for ML, Cap. 6 (Probability)
- Pattern Recognition and ML (Bishop), Cap. 1-2

---

> ğŸ’¡ **Nota Final:** Este mÃ³dulo sigue siendo compacto comparado con un curso completo de probabilidad/estadÃ­stica, pero aquÃ­ ya tienes el nÃºcleo de LÃ­nea 1 y una â€œsemillaâ€ intencional para LÃ­nea 2 (estimaciÃ³n y Markov Chains).

---

**[â† MÃ³dulo 03: CÃ¡lculo](03_CALCULO_MULTIVARIANTE.md)** | **[MÃ³dulo 05: Supervised Learning â†’](05_SUPERVISED_LEARNING.md)**
