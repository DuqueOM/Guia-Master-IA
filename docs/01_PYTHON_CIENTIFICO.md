# MÃ³dulo 01 - Python CientÃ­fico + Pandas

> **ğŸ¯ Objetivo:** Dominar Pandas para datos + NumPy para matemÃ¡ticas
> **Fase:** 1 - Fundamentos | **Semanas 1-2**
> **Prerrequisitos:** Python bÃ¡sico (variables, funciones, listas, loops)

---

<a id="m01-0"></a>

## ğŸ§­ CÃ³mo usar este mÃ³dulo (modo 0â†’100)

**PropÃ³sito:** que pases de â€œsÃ© Python bÃ¡sicoâ€ a **poder trabajar con datos reales y producir arrays listos para modelos** (lo que usarÃ¡s en TODO el Pathway).

### Objetivos de aprendizaje (medibles)

Al terminar este mÃ³dulo podrÃ¡s:

- **Aplicar** Pandas para cargar, explorar y limpiar datasets reales.
- **Convertir** datasets a `np.ndarray` con shapes correctos para ML (`X` y `y`).
- **Explicar** quÃ© es vectorizaciÃ³n y por quÃ© NumPy elimina loops.
- **Diagnosticar** los errores de shapes mÃ¡s comunes (`(n,)` vs `(n,1)`, broadcasting silencioso, vistas vs copias).

### Prerrequisitos

- Python bÃ¡sico (loops, funciones, listas, diccionarios).

Enlaces rÃ¡pidos:

- [GLOSARIO: NumPy](GLOSARIO.md#numpy)
- [GLOSARIO: Broadcasting](GLOSARIO.md#broadcasting)
- [GLOSARIO: Vectorization](GLOSARIO.md#vectorization)
- [RECURSOS.md](RECURSOS.md)

### IntegraciÃ³n con Plan v4/v5

- Drill diario de shapes: `study_tools/DRILL_DIMENSIONES_NUMPY.md`
- Registro de errores: `study_tools/DIARIO_ERRORES.md`
- EvaluaciÃ³n (rÃºbrica): [study_tools/RUBRICA_v1.md](../study_tools/RUBRICA_v1.md) (scope `M01` en `rubrica.csv`)
- Protocolo completo:
  - [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)
  - [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md)

### Recursos (cuÃ¡ndo usarlos)

| Prioridad | Recurso | CuÃ¡ndo usarlo en este mÃ³dulo | Para quÃ© |
|----------|---------|------------------------------|----------|
| **Obligatorio** | [Pandas Getting Started](https://pandas.pydata.org/docs/getting_started/) | Semana 1, antes de empezar con `DataFrame/Series` y limpieza | Referencia oficial para flujo tÃ­pico de carga/EDA/limpieza |
| **Obligatorio** | [NumPy Documentation (absolute beginners)](https://numpy.org/doc/stable/user/absolute_beginners.html) | Semana 2, cuando aparezcan `ndarray`, `dtype`, `reshape`, `axis`, broadcasting | Fuente oficial para resolver dudas de shapes/axis |
| **Obligatorio** | `study_tools/DRILL_DIMENSIONES_NUMPY.md` | Cada vez que te equivoques en un shape / antes del checklist de salida | Automatizar intuiciÃ³n de shapes |
| **Complementario** | [Real Python - NumPy](https://realpython.com/numpy-tutorial/) | DespuÃ©s de completar broadcasting + vectorizaciÃ³n (Semana 2) | Consolidar patrones idiomÃ¡ticos con ejemplos prÃ¡cticos |
| **Opcional** | [RECURSOS.md](RECURSOS.md) | Al terminar el mÃ³dulo (para planificar refuerzo) | Elegir rutas de profundizaciÃ³n sin dispersarte |

### Criterio de salida (cuÃ¡ndo puedes avanzar)

- Puedes preparar un `X` y `y` desde un CSV sin errores de dtype/shape.
- Puedes explicar `axis=0` vs `axis=1` y predecir shapes sin ejecutar.
- Puedes demostrar speedup vectorizado (benchmark) y justificarlo.

## ğŸ§  Â¿Por QuÃ© Este MÃ³dulo?

### El Problema con Python Puro para ML

```python
# âŒ AsÃ­ NO se hace en Machine Learning
def dot_product_slow(a: list, b: list) -> float:
    """Producto punto con loop - LENTO."""
    result = 0
    for i in range(len(a)):
        result += a[i] * b[i]
    return result

# Para vectores de 1 millÃ³n de elementos:
# Tiempo: ~200ms
```

```python
# âœ… AsÃ­ SÃ se hace en Machine Learning
import numpy as np

def dot_product_fast(a: np.ndarray, b: np.ndarray) -> float:
    """Producto punto vectorizado - RÃPIDO."""
    return np.dot(a, b)

# Para vectores de 1 millÃ³n de elementos:
# Tiempo: ~2ms (100x mÃ¡s rÃ¡pido)
```

### ConexiÃ³n con el Pathway

En los cursos de CU Boulder:
- **Supervised Learning:** Multiplicaciones de matrices para regresiÃ³n
- **Unsupervised Learning:** PCA requiere descomposiciÃ³n de matrices
- **Deep Learning:** Forward/backward pass son operaciones matriciales

**Sin NumPy, no puedes hacer ML eficiente.**

---

## ğŸ“š Contenido del MÃ³dulo

### Semana 1: Pandas + NumPy BÃ¡sico

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DÃA 1: Pandas - DataFrame y Series                             â”‚
â”‚  DÃA 2: Pandas - Carga de CSVs (read_csv, head, info)           â”‚
â”‚  DÃA 3: Pandas - Limpieza (dropna, fillna, dtypes)              â”‚
â”‚  DÃA 4: NumPy - Arrays y dtypes                                 â”‚
â”‚  DÃA 5: NumPy - Indexing y Slicing                              â”‚
â”‚  DÃA 6: Pandas â†’ NumPy (df.values, df.to_numpy())               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Semana 2: NumPy Vectorizado

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DÃA 1: Broadcasting                                            â”‚
â”‚  DÃA 2: Producto matricial (@, np.dot, np.matmul) + reshape/flatten â”‚
â”‚  DÃA 3: OOP para ML (v5.1): class Tensor (__init__, __add__, @) â”‚
â”‚  DÃA 4: Agregaciones y operaciones con ejes                     â”‚
â”‚  DÃA 5: Random y generaciÃ³n de datos sintÃ©ticos                 â”‚
â”‚  DÃA 6: Entregable: Pipeline Pandas â†’ NumPy                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» Conceptos Clave

### 0. Pandas Esencial (DÃ­as 1-3)

#### Â¿Por QuÃ© Pandas?

En el mundo real de ML, los datos vienen en CSVs sucios, no en arrays NumPy perfectos. Antes de aplicar cualquier algoritmo necesitas:

1. **Cargar datos** desde archivos
2. **Explorar** estructura y tipos
3. **Limpiar** valores faltantes y errores
4. **Convertir** a NumPy para el modelo

```python
import pandas as pd
import numpy as np

# ========== CARGA DE DATOS ==========
# Cargar CSV
df = pd.read_csv('data/iris.csv')

# Primeras filas
print(df.head())

# InformaciÃ³n del DataFrame
print(df.info())
#  Column         Non-Null Count  Dtype
# ---  ------         --------------  -----
#  0   sepal_length   150 non-null    float64
#  1   sepal_width    150 non-null    float64
#  2   petal_length   150 non-null    float64
#  3   petal_width    150 non-null    float64
#  4   species        150 non-null    object

# EstadÃ­sticas bÃ¡sicas
print(df.describe())
```

#### Limpieza de Datos

```python
import pandas as pd

# Crear DataFrame con datos sucios
df = pd.DataFrame({
    'edad': [25, 30, None, 45, 50],
    'salario': [50000, 60000, 70000, None, 90000],
    'ciudad': ['Madrid', 'Barcelona', 'Madrid', 'Sevilla', None]
})

# ========== DETECTAR NULOS ==========
print(df.isnull().sum())
# edad       1
# salario    1
# ciudad     1

# ========== ELIMINAR FILAS CON NULOS ==========
df_clean = df.dropna()  # Elimina filas con cualquier nulo
print(f"Filas despuÃ©s de dropna: {len(df_clean)}")  # 2

# ========== RELLENAR NULOS ==========
df_filled = df.copy()
df_filled['edad'] = df_filled['edad'].fillna(df_filled['edad'].mean())
df_filled['salario'] = df_filled['salario'].fillna(df_filled['salario'].median())
df_filled['ciudad'] = df_filled['ciudad'].fillna('Desconocido')

print(df_filled)
```

#### SelecciÃ³n y Filtrado

```python
import pandas as pd

df = pd.read_csv('data/iris.csv')

# ========== SELECCIONAR COLUMNAS ==========
# Una columna (Serie)
sepal_length = df['sepal_length']

# MÃºltiples columnas (DataFrame)
features = df[['sepal_length', 'sepal_width']]

# ========== FILTRAR FILAS ==========
# CondiciÃ³n simple
setosa = df[df['species'] == 'setosa']

# MÃºltiples condiciones
large_setosa = df[(df['species'] == 'setosa') & (df['sepal_length'] > 5)]

# ========== LOC e ILOC ==========
# loc: por etiquetas
df.loc[0:5, ['sepal_length', 'species']]

# iloc: por posiciÃ³n (como NumPy)
df.iloc[0:5, 0:2]
```

#### De Pandas a NumPy (DÃ­a 6)

```python
import pandas as pd
import numpy as np

df = pd.read_csv('data/iris.csv')

# ========== SEPARAR FEATURES Y TARGET ==========
# Features (X) - todas las columnas numÃ©ricas
X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].to_numpy()
print(f"X shape: {X.shape}")  # (150, 4)
print(f"X dtype: {X.dtype}")  # float64

# Target (y) - convertir categorÃ­as a nÃºmeros
y = df['species'].map({'setosa': 0, 'versicolor': 1, 'virginica': 2}).to_numpy()
print(f"y shape: {y.shape}")  # (150,)

# ========== VERIFICAR ==========
print(f"Tipo X: {type(X)}")  # <class 'numpy.ndarray'>
print(f"Tipo y: {type(y)}")  # <class 'numpy.ndarray'>

# Ahora X e y estÃ¡n listos para algoritmos de ML
```

<details>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 0: Pandas Esencial (DÃ­as 1â€“3)</strong></summary>

#### 1) Metadatos (organizaciÃ³n)
- **TÃ­tulo:** Pandas esencial para preparar datasets reales
- **ID (opcional):** `M01-T00`
- **DuraciÃ³n estimada:** 2â€“4 horas
- **Nivel:** Intro
- **Dependencias:** Python bÃ¡sico, lectura de CSV, listas/diccionarios

#### 2) Objetivos de aprendizaje (medibles)
- Al terminar, el estudiante podrÃ¡ **cargar** un CSV en un `DataFrame`, **inspeccionar** su esquema (`info/describe`) y **detectar** valores faltantes **en â‰¤ 10 min**.
- Al terminar, el estudiante podrÃ¡ **limpiar** valores faltantes con `dropna/fillna` y **justificar** la elecciÃ³n (media/mediana/categorÃ­a) **en un caso dado**.
- Al terminar, el estudiante podrÃ¡ **construir** `X` y `y` como `np.ndarray` con shapes correctos (`X.shape=(n,d)`, `y.shape=(n,)`) y **verificar** `dtype`.

#### 3) Relevancia y contexto
- **Â¿Por quÃ© importa?** En ML real, el 80% del trabajo inicial es preparar datos: si `dtype/NaN/shape` estÃ¡n mal, el modelo â€œfallaâ€ o (peor) aprende basura.
- **DÃ³nde encaja:** Esto es el â€œpuenteâ€ a todos los mÃ³dulos (regresiÃ³n/logÃ­stica/PCA/redes). Antes: Python bÃ¡sico. DespuÃ©s: NumPy vectorizado y Ã¡lgebra lineal.

#### 4) Mapa conceptual / conceptos clave
- **Archivo (CSV/Parquet)**
  - **â†’ `DataFrame`** (tabla con columnas tipadas)
    - **â†’ EDA rÃ¡pido** (`head/info/describe`)
      - **â†’ Limpieza** (NaN, dtypes, outliers bÃ¡sicos)
        - **â†’ `X` (features) y `y` (target)**
          - **â†’ `to_numpy()` + validaciÃ³n de shapes**

#### 5) Definiciones, notaciÃ³n y â€œfÃ³rmulasâ€ esenciales
- **`DataFrame`:** estructura tabular (filas/columnas) con columnas tipadas y operaciones vectorizadas.
- **NotaciÃ³n (la usaremos toda la guÃ­a):**
  - `n` = nÃºmero de muestras (filas)
  - `d` = nÃºmero de features (columnas)
  - `X` = matriz de features, `X.shape == (n, d)`
  - `y` = target, tÃ­picamente `y.shape == (n,)`
- **ImputaciÃ³n (idea):**
  - NumÃ©ricas: media/mediana (segÃºn outliers)
  - CategÃ³ricas: categorÃ­a â€œDesconocidoâ€ o moda

#### 6) ExplicaciÃ³n didÃ¡ctica (2 niveles)
- **IntuiciÃ³n (2â€“3 frases):** Pandas es una â€œmesa de trabajoâ€ para convertir datos sucios en una matriz `X` limpia. El objetivo no es memorizar mÃ©todos, sino ejecutar un pipeline estable: cargar â†’ revisar â†’ limpiar â†’ separar â†’ convertir.
- **Paso a paso (pipeline mÃ­nimo):**
  - 1. `read_csv` (cargar)
  - 2. `head/info/describe` (entender esquema)
  - 3. `isnull().sum()` (medir faltantes)
  - 4. imputar/eliminar (decisiÃ³n explÃ­cita)
  - 5. separar `y` primero y luego construir `X`
  - 6. `to_numpy()` + checks de `dtype/shape`
- **Visual recomendado:**
  - tabla de `df.info()` + heatmap de faltantes (seaborn) + histograma por columna numÃ©rica

#### 7) Ejemplos modelados (worked examples)
- **Ejemplo 1 (sencillo):** Iris â†’ `X` con 4 columnas numÃ©ricas, `y` codificada a {0,1,2}.
- **Ejemplo 2 (realista):** dataset con columna de fecha + categorÃ­as:
  - parsear fechas con `pd.to_datetime`
  - `get_dummies` para categorÃ­as (si aplica)
  - verificar que `X.dtype` sea numÃ©rico antes de convertir.
- **Contraejemplo (quÃ© NO hacer):** convertir todo el `DataFrame` a NumPy sin filtrar columnas y terminar con `dtype=object`.

#### 8) PrÃ¡ctica guiada (scaffolded)
- **Ejercicio A (con pista):** carga un CSV y lista columnas por tipo (`df.select_dtypes`).
  - Pista: `df.select_dtypes(include=["number"])`
- **Ejercicio B (con pista):** calcula porcentaje de NaN por columna y decide estrategia.
  - Pista: `df.isnull().mean().sort_values(ascending=False)`
- **Ejercicio C (con pista):** crea `X,y` y verifica shapes.
  - Pista: `assert X.ndim == 2` y `assert y.ndim == 1`

#### 9) PrÃ¡ctica independiente / transferencia
- **Mini-proyecto:** toma un dataset tabular (Titanic / Wine / Housing), define target, limpia faltantes, y entrega `X,y` listos.
- **Entrega:** notebook o script con:
  - impresiÃ³n de `df.info()`
  - estrategia de limpieza (explicada en 5â€“10 lÃ­neas)
  - `assert` de shapes/dtypes
- **RÃºbrica (3 criterios):** precisiÃ³n (shapes/dtypes), aplicaciÃ³n (pipeline completo), comunicaciÃ³n (justificaciÃ³n breve)

#### 10) EvaluaciÃ³n formativa (rÃ¡pida)
- **Mini-quiz (5):**
  - 1. Â¿QuÃ© diferencia prÃ¡ctica hay entre `df.values` y `df.to_numpy()`?
  - 2. Â¿QuÃ© te dice `df.info()` que `df.head()` no te dice?
  - 3. Â¿CuÃ¡ndo usarÃ­as mediana en lugar de media para imputar?
  - 4. Â¿QuÃ© shapes esperas para `X` y `y` en un problema supervisado?
  - 5. Â¿Por quÃ© `dtype=object` es una seÃ±al de alerta para ML?

#### 11) Errores comunes y correcciÃ³n
- **Error:** â€œ`X` contiene stringsâ€ â†’ **CorrecciÃ³n:** filtra/convierte columnas y valida `select_dtypes`.
- **Error:** leakage por incluir el target en `X` â†’ **CorrecciÃ³n:** separar `y` primero y luego construir `X`.
- **Error:** imputar sin medir NaN â†’ **CorrecciÃ³n:** reportar `isnull().sum()` / proporciÃ³n.
- **Error:** `SettingWithCopyWarning` â†’ **CorrecciÃ³n:** usar `.loc[...]` y/o `.copy()`.
- **Error:** no validar shapes â†’ **CorrecciÃ³n:** `assert X.shape[0] == y.shape[0]`.

#### 12) RetenciÃ³n (spaced retrieval)
- **Preguntas (dÃ­a 2):** Â¿QuÃ© hace `axis=0` en medias por columna? Â¿QuÃ© shape devuelve?
- **Preguntas (dÃ­a 7):** Â¿CÃ³mo detectas `dtype=object` y quÃ© harÃ­as?
- **Checklist rÃ¡pido:** `info â†’ nulls â†’ split â†’ to_numpy â†’ asserts`.

#### 13) DiferenciaciÃ³n
- **Principiante:** repetir el pipeline en 2 datasets distintos (uno â€œlimpioâ€, uno â€œsucioâ€).
- **Avanzado:** diseÃ±ar una funciÃ³n `prepare_xy(df, feature_cols, target_col)` con asserts y tests mÃ­nimos.

#### 14) Recursos y materiales
- Pandas: Getting Started (oficial)
- Cheatsheet corta: `df.head/info/describe/isnull/dropna/fillna/loc/iloc/to_numpy`

#### 15) Notas para el docente
- **Tiempo sugerido:** 15 min (carga + schema) + 30 min (limpieza) + 20 min (X/y) + 15 min (debug de errores).
- **Preguntas socrÃ¡ticas:** â€œÂ¿QuÃ© evidencia tienes de que `X` es numÃ©rico?â€ â€œÂ¿QuÃ© asumiste al imputar con media?â€
</details>

---

### 1. Arrays vs Listas

#### IntuiciÃ³n: â€œmemoria contiguaâ€ (NumPy) vs â€œcajas dispersasâ€ (listas)

Piensa en una **lista de Python** como una fila de cajitas que guardan **referencias** a objetos; esos objetos pueden estar **dispersos** por la memoria. NumPy, en cambio, busca representar un `ndarray` como un **bloque contiguo** de nÃºmeros del mismo tipo (homogÃ©neos). Esa decisiÃ³n habilita:

- **VectorizaciÃ³n real:** bucles internos en C (muy optimizados).
- **Mejor uso de cachÃ© CPU:** leer datos contiguos es mÃ¡s rÃ¡pido.
- **Menos overhead:** no hay â€œun objeto por nÃºmeroâ€.

Mini-diagrama mental:

```
Lista (referencias):  [ * ] -> obj1   [ * ] -> obj2   [ * ] -> obj3   ...
                       |              |              |
                      mem@A          mem@Z          mem@K

NumPy (contiguo):     [ 1.0 ][ 2.0 ][ 3.0 ][ 4.0 ] ...  (mismo dtype)
```

```python
import numpy as np  # Importa NumPy para demostrar cÃ³mo axis afecta agregaciones

# Lista de Python
lista = [1, 2, 3, 4, 5]

# Array de NumPy
array = np.array([1, 2, 3, 4, 5])

# Diferencias clave:
# 1. Tipo homogÃ©neo (todos los elementos del mismo tipo)
# 2. TamaÃ±o fijo despuÃ©s de creaciÃ³n
# 3. Operaciones vectorizadas
# 4. Almacenamiento contiguo en memoria
```

<details>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 1: Arrays vs Listas</strong></summary>

#### 1) Metadatos (organizaciÃ³n)
- **TÃ­tulo:** Por quÃ© `ndarray` existe (memoria, dtype y vectorizaciÃ³n)
- **ID (opcional):** `M01-T01`
- **DuraciÃ³n estimada:** 30â€“60 min
- **Nivel:** Intro
- **Dependencias:** listas de Python, loops, tipos numÃ©ricos

#### 2) Objetivos de aprendizaje (medibles)
- Al terminar, el estudiante podrÃ¡ **explicar** (sin jerga) por quÃ© NumPy es mÃ¡s rÃ¡pido que listas para operaciones numÃ©ricas **en â‰¤ 2 min**.
- Al terminar, el estudiante podrÃ¡ **predecir** el `dtype` resultante al crear un `np.array(...)` a partir de distintos inputs (ints/floats/mezcla) **en 5 casos**.
- Al terminar, el estudiante podrÃ¡ **elegir** entre lista vs `ndarray` dado un problema (cÃ³mputo numÃ©rico vs estructuras heterogÃ©neas) y **justificar** la elecciÃ³n.

#### 3) Relevancia y contexto
- **Â¿Por quÃ© importa?** Tus modelos (regresiÃ³n/logÃ­stica/redes) son multiplicaciones y sumas masivas: hacerlo con listas te hace lento y propenso a errores.
- **DÃ³nde encaja:** fundamento del â€œpor quÃ©â€ de vectorizaciÃ³n, broadcasting, y producto matricial.

#### 4) Mapa conceptual / conceptos clave
- **Lista (Python):** referencias a objetos â†’ heterogÃ©nea â†’ overhead por elemento
- **`ndarray` (NumPy):** buffer contiguo + `dtype` homogÃ©neo â†’ operaciones en C/BLAS
- **Consecuencia:** speedup + menos bugs al operar por bloques

#### 5) Definiciones y notaciÃ³n
- **`dtype`:** tipo numÃ©rico almacenado en el buffer (p.ej. `float64`, `int64`).
- **`shape`:** tupla con dimensiones (p.ej. `(n,)`, `(n, d)`).
- **â€œVectorizaciÃ³nâ€:** operar sobre arrays completos evitando loops en Python.

#### 6) ExplicaciÃ³n didÃ¡ctica (2 niveles)
- **IntuiciÃ³n:** una lista guarda â€œpunterosâ€; un `ndarray` guarda nÃºmeros contiguos. Leer memoria contigua y aplicar rutinas en C/BLAS suele ser 10xâ€“100x mÃ¡s rÃ¡pido.
- **Paso a paso:**
  - 1. lista: `for` + operaciÃ³n por elemento
  - 2. NumPy: operaciÃ³n sobre el array completo
  - 3. validaciÃ³n: comparar tiempos y verificar que el resultado coincide
- **Visual recomendado:** grÃ¡fico de tiempo (ms) vs tamaÃ±o del vector (log-scale).

#### 7) Ejemplos modelados
- **Ejemplo 1:** sumar un escalar a 1e6 nÃºmeros (lista vs array).
- **Ejemplo 2:** producto punto (lista vs `np.dot`).
- **Contraejemplo:** crear un array con mezcla de nÃºmeros y strings â†’ `dtype=object` â†’ pierdes vectorizaciÃ³n.

#### 8) PrÃ¡ctica guiada
- **Ejercicio A:** crea `np.array([1,2,3])` y `np.array([1,2,3.0])`. Â¿QuÃ© `dtype` obtienes?
  - Pista: imprime `arr.dtype`.
- **Ejercicio B:** mide tiempo de `sum([..])` vs `np.sum(np.array(..))` para `n=10^5`.
  - Pista: usa `time.time()` o `time.perf_counter()`.
- **Ejercicio C:** explica por escrito (5 lÃ­neas) cuÃ¡ndo una lista sigue siendo mejor opciÃ³n.

#### 9) Transferencia
- **Actividad:** dado un â€œdatasetâ€ en lista de diccionarios, conviÃ©rtelo a `DataFrame` y luego a `ndarray` numÃ©rico.
- **Criterio:** no debe quedar `dtype=object` en `X`.

#### 10) EvaluaciÃ³n formativa
- 1. Â¿QuÃ© significa que NumPy sea â€œhomogÃ©neoâ€?
- 2. Â¿QuÃ© desventaja tiene `dtype=object`?
- 3. Â¿Por quÃ© el loop en Python es costoso?

#### 11) Errores comunes
- **Error:** asumir que `np.array(lista)` siempre crea floats â†’ **CorrecciÃ³n:** inspeccionar `dtype`.
- **Error:** mezclar tipos â†’ **CorrecciÃ³n:** normalizar/convertir antes.

#### 12) RetenciÃ³n
- **Pregunta (dÃ­a 2):** define `dtype` y por quÃ© importa.
- **Pregunta (dÃ­a 7):** da un ejemplo donde lista > NumPy.

#### 13) DiferenciaciÃ³n
- **Principiante:** repetir el benchmark con 3 tamaÃ±os (1e3, 1e5, 1e6).
- **Avanzado:** explicar (alto nivel) el rol de BLAS y por quÃ© `np.dot` suele estar optimizado.

#### 14) Recursos
- NumPy â€œabsolute beginnersâ€ (oficial)

#### 15) Notas para el docente
- **Check for understanding:** pedir que predigan `dtype` antes de imprimirlo.
</details>

### 2. CreaciÃ³n de Arrays

```python
import numpy as np

# Desde lista
a = np.array([1, 2, 3])

# Arrays especiales
zeros = np.zeros((3, 4))        # Matriz 3x4 de ceros
ones = np.ones((2, 3))          # Matriz 2x3 de unos
identity = np.eye(4)            # Matriz identidad 4x4
random = np.random.randn(3, 3)  # Matriz 3x3 valores normales

# Secuencias
rango = np.arange(0, 10, 2)     # [0, 2, 4, 6, 8]
linspace = np.linspace(0, 1, 5) # [0, 0.25, 0.5, 0.75, 1]

print(f"Shape de zeros: {zeros.shape}")  # (3, 4)
print(f"Dtype de zeros: {zeros.dtype}")  # float64
```

<details>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 2: CreaciÃ³n de Arrays</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Crear `ndarray` correctamente (shape, dtype, inicializaciÃ³n)
- **ID (opcional):** `M01-T02`
- **DuraciÃ³n estimada:** 30â€“45 min
- **Nivel:** Intro
- **Dependencias:** Tema 1 (Arrays vs Listas), nociÃ³n de `shape` y `dtype`

#### 2) Objetivos de aprendizaje (medibles)
- Al terminar, el estudiante podrÃ¡ **crear** arrays 1D/2D con `np.array/zeros/ones/eye` y **verificar** `shape` y `dtype` **en â‰¤ 5 min**.
- Al terminar, el estudiante podrÃ¡ **elegir** entre `arange` y `linspace` dado un objetivo (paso fijo vs #puntos) y **justificar** la elecciÃ³n.

#### 3) Relevancia y contexto
- **Â¿Por quÃ© importa?** En ML, crear tensores de parÃ¡metros (`W`, `b`) y datos sintÃ©ticos es rutina; si creas mal shapes/dtypes, los bugs aparecen tarde.
- **DÃ³nde encaja:** base para broadcasting, producto matricial y generaciÃ³n de datos.

#### 4) Mapa conceptual / conceptos clave
- `shape` â†’ `dtype` â†’ constructor (`zeros/ones/eye/array`) â†’ verificaciÃ³n (`assert`)

#### 5) Definiciones
- **Inicializadores:** `zeros/ones/eye` crean arrays con estructuras Ãºtiles para Ã¡lgebra lineal y debugging.
- **Generadores:** `arange/linspace` construyen secuencias controladas (ojo con floats).

#### 6) ExplicaciÃ³n didÃ¡ctica (2 niveles)
- **Resumen:** decide primero la forma y el tipo; luego el constructor.
- **Paso a paso:**
  - 1. define el `shape`
  - 2. define `dtype` (en ML, casi siempre float)
  - 3. crea
  - 4. valida `shape/dtype`

#### 7) Ejemplos modelados
- **Ejemplo 1:** `np.zeros((n, d), dtype=float)` como â€œplaceholderâ€ de datos.
- **Ejemplo 2:** `np.eye(k)` para confirmar identidad.
- **Contraejemplo:** usar `np.arange(0, 1, 0.1)` esperando incluir exactamente 1.0.

#### 8) PrÃ¡ctica guiada
- **Ejercicio A:** crea una matriz `(3,4)` de unos con dtype float.
- **Ejercicio B:** crea 11 puntos entre 0 y 1 (incluyendo extremos).
- **Ejercicio C:** crea `w` de shape `(d,)` y `b` escalar y verifica tipos.

#### 9) Transferencia
- **Actividad:** genera un dataset sintÃ©tico `X` con shape `(200, 2)` y verifica `X.ndim == 2`.

#### 10) EvaluaciÃ³n formativa
- Â¿CuÃ¡ndo prefieres `linspace` sobre `arange`?
- Â¿Por quÃ© conviene fijar `dtype=float`?

#### 11) Errores comunes
- confundir `(n,)` vs `(n,1)` al crear vectores
- terminar con ints por no fijar dtype

#### 12) RetenciÃ³n
- (dÃ­a 2) Â¿quÃ© shape devuelve `np.eye(3)`?
- (dÃ­a 7) diferencia prÃ¡ctica `arange` vs `linspace`.

#### 13) DiferenciaciÃ³n
- **Principiante:** escribir `shape/dtype` esperado antes de ejecutar 5 creaciones.
- **Avanzado:** explicar el problema de floats con `arange`.

#### 14) Recursos
- NumPy â€œabsolute beginnersâ€ (oficial)

#### 15) Nota para el docente
- Pedir predicciÃ³n de `shape/dtype` antes de imprimir.
</details>

### 3. Indexing y Slicing

```python
import numpy as np

# Crear matriz 2D
matrix = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
])

# Acceso a elementos
print(matrix[0, 0])      # 1 (fila 0, columna 0)
print(matrix[1, 2])      # 6 (fila 1, columna 2)

# Slicing
print(matrix[0, :])      # [1, 2, 3] (toda la fila 0)
print(matrix[:, 1])      # [2, 5, 8] (toda la columna 1)
print(matrix[0:2, 1:3])  # [[2, 3], [5, 6]] (submatriz)

# Indexing booleano
print(matrix[matrix > 5])  # [6, 7, 8, 9]
```

<details>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 3: Indexing y Slicing</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Extraer submatrices sin loops (y sin errores de vistas/copias)
- **ID (opcional):** `M01-T03`
- **DuraciÃ³n estimada:** 45â€“60 min
- **Nivel:** Intro
- **Dependencias:** Tema 2 (creaciÃ³n de arrays) + nociÃ³n de `shape`

#### 2) Objetivos de aprendizaje (medibles)
- Al terminar, el estudiante podrÃ¡ **extraer** filas, columnas y submatrices y **predecir** el `shape` resultante **en â‰¥ 8 de 10 casos**.
- Al terminar, el estudiante podrÃ¡ **usar** indexing booleano para filtrar valores y **explicar** por quÃ© el resultado suele ser 1D.

#### 3) Relevancia y contexto
- **Â¿Por quÃ© importa?** SelecciÃ³n de features, minibatches y splits train/val depende de indexar bien; fallar aquÃ­ produce bugs silenciosos.

#### 4) Mapa conceptual / conceptos clave
- indexado por enteros â†’ slicing â†’ masks booleanas â†’ vistas/copias

#### 5) Definiciones
- **Slice:** rango semiabierto `[inicio:fin)`.
- **Mask booleana:** array de `True/False` que filtra elementos.

#### 6) ExplicaciÃ³n didÃ¡ctica (2 niveles)
- **Resumen:** primero predice el shape, luego indexa y valida.
- **Paso a paso:**
  - 1. escribe el shape esperado
  - 2. aplica el slicing/indexing
  - 3. valida con `assert`

#### 7) Ejemplos modelados
- **Ejemplo 1:** `X[0]` vs `X[0:1]` (diferencia de shapes).
- **Ejemplo 2:** `X[:, j]` (columna) y por quÃ© queda `(n,)`.
- **Contraejemplo:** modificar un slice pensando que es copia (conecta con la secciÃ³n de debugging del mÃ³dulo).

#### 8) PrÃ¡ctica guiada
- **Ejercicio A:** toma un minibatch `X[:32]`.
- **Ejercicio B:** filtra valores en un vector `v` que estÃ©n en `[a,b]`.
- **Ejercicio C:** extrae una submatriz y predice su shape antes.

#### 9) Transferencia
- **Actividad:** implementa split 80/20 por slicing y valida shapes de `X_train/X_val`.

#### 10) EvaluaciÃ³n formativa
- Â¿Por quÃ© `X[X>0]` suele ser 1D?
- Diferencia entre `X[0]` y `X[0:1]`.

#### 11) Errores comunes
- olvidar parÃ©ntesis al usar `&` en masks
- asumir que slices son copias

#### 12) RetenciÃ³n
- (dÃ­a 2) define â€œslice semiabiertoâ€.
- (dÃ­a 7) Â¿cuÃ¡ndo usar `.copy()`?

#### 13) DiferenciaciÃ³n
- **Principiante:** 10 predicciones de shape.
- **Avanzado:** verificar vistas/copias con `np.shares_memory`.

#### 14) Recursos
- NumPy indexing (oficial)

#### 15) Nota para el docente
- Pedir â€œshape antes de ejecutarâ€ + asserts.
</details>

### 4. Broadcasting

#### Worked Example: `(3, 1) + (1, 3)` paso a paso

Objetivo: entender **por quÃ©** funciona sin loops.

1) Define dos arrays con una dimensiÃ³n â€œde tamaÃ±o 1â€:

- `A.shape = (3, 1)` (columna)
- `B.shape = (1, 3)` (fila)

2) Regla clave: si en una dimensiÃ³n uno de los tamaÃ±os es `1`, NumPy puede **â€œestirarâ€** esa dimensiÃ³n para igualar al otro.

3) Resultado final: ambos se ven como `(3, 3)` y se suman elemento a elemento.

```python
import numpy as np

A = np.array([[1], [2], [3]])        # shape: (3, 1)
B = np.array([[10, 20, 30]])         # shape: (1, 3)

# Broadcasting:
# A se repite horizontalmente 3 veces
# B se repite verticalmente 3 veces
C = A + B                             # shape: (3, 3)

print("A:\n", A)
print("B:\n", B)
print("C = A + B:\n", C)
```

```python
import numpy as np

# Broadcasting: operar arrays de diferentes shapes

# Escalar + Array
a = np.array([1, 2, 3])
print(a + 10)  # [11, 12, 13]

# Vector + Matriz (broadcasting automÃ¡tico)
matrix = np.array([
    [1, 2, 3],
    [4, 5, 6]
])
vector = np.array([10, 20, 30])

# El vector se "expande" para coincidir con la matriz
print(matrix + vector)
# [[11, 22, 33],
#  [14, 25, 36]]

# Regla de broadcasting:
# Las dimensiones deben ser iguales O una de ellas debe ser 1
```

<details>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 4: Broadcasting</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** Reglas de broadcasting (sumas/restas por columna/fila)
- **ID (opcional):** `M01-T04`
- **DuraciÃ³n estimada:** 60â€“90 min
- **Nivel:** Intermedio
- **Dependencias:** Tema 2 (shape) y Tema 3 (indexing)

#### 2) Objetivos de aprendizaje (medibles)
- Al terminar, el estudiante podrÃ¡ **predecir** el shape resultante de operaciones tÃ­picas (`(n,d)+(d,)`, `(n,1)+(1,d)`) **en â‰¥ 8 de 10 casos**.
- Al terminar, el estudiante podrÃ¡ **normalizar** por columna `X_norm = (X - mean) / (std + eps)` conservando shape `(n,d)`.

#### 3) Relevancia y contexto
- **Â¿Por quÃ© importa?** Broadcasting es base de normalizaciÃ³n, bias en redes (`+b`) y fÃ³rmulas vectorizadas; tambiÃ©n es fuente #1 de bugs silenciosos.

#### 4) Mapa conceptual / conceptos clave
- compatibilidad de shapes (alineaciÃ³n por el final) â†’ dimensiÃ³n 1 â€œestirableâ€ â†’ `keepdims=True` para control

#### 5) DefiniciÃ³n
- **Broadcasting:** regla que permite operar arrays de distinto shape expandiendo virtualmente dimensiones de tamaÃ±o 1 sin copiar datos.

#### 6) ExplicaciÃ³n didÃ¡ctica (2 niveles)
- **Resumen:** compara dimensiones desde el final; si son iguales o alguna es 1, son compatibles.
- **Paso a paso:**
  - 1. alinea shapes por la derecha
  - 2. valida compatibilidad por eje
  - 3. el resultado toma el mÃ¡ximo por eje

#### 7) Ejemplos modelados
- **Ejemplo 1:** `(3,1)+(1,3) -> (3,3)`.
- **Ejemplo 2:** `X (n,d) - mean (d,)` para centrar.
- **Contraejemplo:** usar `np.mean(X)` (escalar) cuando querÃ­as `axis=0` (vector de size d).

#### 8) PrÃ¡ctica guiada
- **Ejercicio A:** suma un vector `(d,)` a cada fila de `X (n,d)`.
- **Ejercicio B:** calcula `mu = X.mean(axis=0, keepdims=True)` y resta `X - mu`.
- **Ejercicio C:** implementa z-score por columna con `eps`.

#### 9) Transferencia
- **Actividad:** implementa `predict_linear(X,w,b)` que soporte batch: `(n,d)@(d,) + b`.
- **Criterio:** salida con shape `(n,)`.

#### 10) EvaluaciÃ³n formativa
- Â¿QuÃ© significa â€œalineaciÃ³n por la derechaâ€?
- Â¿QuÃ© hace `keepdims=True` y por quÃ© ayuda?

#### 11) Errores comunes
- olvidar `axis=` en agregaciones
- broadcasting â€œfuncionaâ€ pero estÃ¡ mal (promedio global)

#### 12) RetenciÃ³n
- (dÃ­a 2) predice shapes de 5 operaciones.
- (dÃ­a 7) describe un bug de broadcasting y cÃ³mo lo detectarÃ­as.

#### 13) DiferenciaciÃ³n
- **Principiante:** practicar `(n,d)+(d,)` y `(n,d)-(d,)`.
- **Avanzado:** justificar por quÃ© `X - X.mean(axis=0)` funciona.

#### 14) Recursos
- NumPy broadcasting rules (oficial)

#### 15) Nota para el docente
- Repetir la regla: â€œshape antes de ejecutarâ€ + asserts.
</details>

### 5. Agregaciones y Ejes

#### VisualizaciÃ³n: Â¿quÃ© â€œcolapsaâ€ cada eje?

Regla prÃ¡ctica:

- `axis=0` **colapsa filas** â†’ te queda â€œuna salida por columnaâ€
- `axis=1` **colapsa columnas** â†’ te queda â€œuna salida por filaâ€

Ejemplo con una matriz `2x3`:

```
X = [[1, 2, 3],
     [4, 5, 6]]

sum(axis=0) = [1+4, 2+5, 3+6] = [5, 7, 9]
sum(axis=1) = [1+2+3, 4+5+6] = [6, 15]
```

```python
import numpy as np

matrix = np.array([
    [1, 2, 3],
    [4, 5, 6]
])

# Agregaciones globales
print(np.sum(matrix))   # 21 (suma de todos)
print(np.mean(matrix))  # 3.5 (promedio de todos)
print(np.std(matrix))   # 1.707... (desviaciÃ³n estÃ¡ndar)

# Agregaciones por eje
# axis=0: colapsar filas (operar columnas)
print(np.sum(matrix, axis=0))  # [5, 7, 9]

# axis=1: colapsar columnas (operar filas)
print(np.sum(matrix, axis=1))  # [6, 15]

# VisualizaciÃ³n de ejes:
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ axis=0 â†“    â”‚
# â”‚ [1, 2, 3]   â”‚ â†’ axis=1
# â”‚ [4, 5, 6]   â”‚ â†’ axis=1
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

<details>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 5: Agregaciones y Ejes</strong></summary>

#### 1) Metadatos (1â€“2 lÃ­neas)
- **TÃ­tulo:** Agregaciones con `axis` (sum/mean/std) y control de shapes
- **ID (opcional):** `M01-T05`
- **DuraciÃ³n estimada:** 60â€“90 min
- **Nivel:** Intermedio
- **Dependencias:** Tema 4 (Broadcasting), nociÃ³n de `shape` y batch `(n,d)`

#### 2) Objetivos de aprendizaje (medibles)
- Al terminar, el estudiante podrÃ¡ **calcular** agregaciones globales y por eje (`axis=0/1`) y **predecir** el shape resultante **en â‰¥ 8 de 10 casos**.
- Al terminar, el estudiante podrÃ¡ **usar** `keepdims=True` para conservar dimensiones y **justificar** cuÃ¡ndo conviene (para broadcasting explÃ­cito) **en un ejemplo**.

#### 3) Relevancia y contexto
- **Â¿Por quÃ© importa?** NormalizaciÃ³n por columna, cÃ¡lculo de mÃ©tricas y centrado de datos dependen de agregaciones correctas; un `axis` mal puesto produce errores silenciosos.
- **DÃ³nde encaja:** DespuÃ©s de broadcasting, antes de producto matricial y entrenamiento (donde normalizas features y evalÃºas resultados).

#### 4) Mapa conceptual / lista de conceptos clave
- `X.shape=(n,d)`
  - agregaciÃ³n global â†’ escalar
  - `axis=0` â†’ salida por columna (`(d,)` o `(1,d)`)
  - `axis=1` â†’ salida por fila (`(n,)` o `(n,1)`)
  - `keepdims=True` â†’ habilita broadcasting controlado

#### 5) Definiciones, notaciÃ³n y fÃ³rmulas esenciales
- **`axis`:** dimensiÃ³n que se reduce/colapsa.
- **`keepdims=True`:** conserva la dimensiÃ³n reducida con tamaÃ±o 1 (facilita broadcasting).
- **NotaciÃ³n:** `XâˆˆR^{nÃ—d}`, `Î¼ = mean(X, axis=0)`.

#### 6) ExplicaciÃ³n didÃ¡ctica (2 niveles)
- **IntuiciÃ³n (2â€“3 frases):** `axis` te dice â€œquÃ© direcciÃ³n colapsasâ€. Si colapsas filas (`axis=0`), te queda una salida por columna. Si colapsas columnas (`axis=1`), te queda una salida por fila.
- **Paso a paso:**
  - 1. escribe el shape de entrada
  - 2. decide si quieres resultado â€œpor columnaâ€ o â€œpor filaâ€
  - 3. elige `axis`
  - 4. si vas a restar/dividir a `X`, evalÃºa `keepdims=True`
- **Visual recomendado:** matriz dibujada con flechas `axis=0` (â†“) y `axis=1` (â†’).

#### 7) Ejemplos modelados (worked examples)
- **Ejemplo 1 â€” sencillo:** `X.sum(axis=0)` y `X.sum(axis=1)` y verificaciÃ³n de shapes.
- **Ejemplo 2 â€” realista:** estandarizaciÃ³n por columna con `mu = X.mean(axis=0, keepdims=True)`.
- **Contraejemplo:** usar `X.mean()` (global) cuando se querÃ­a media por feature.

#### 8) PrÃ¡ctica guiada (scaffolded)
- **Ejercicio 1:** dado `X (4,3)`, calcula `X.mean(axis=0)` y predice shape.
- **Ejercicio 2:** usa `keepdims=True` y comprueba que `X - mu` conserva shape.
- **Ejercicio 3:** calcula `std` por columna, agrega `eps` y normaliza.
- **Ejercicio 4:** verifica con `assert` que la media por columna de `X_norm` es ~0.

#### 9) PrÃ¡ctica independiente / transferencia
- **Mini-proyecto:** implementa `standardize_columns(X, eps=1e-8)` que retorne `X_norm, mu, std`.
- **Entrega:** funciÃ³n + asserts de shape + breve explicaciÃ³n del uso de `axis`.
- **RÃºbrica corta:** precisiÃ³n (axis/shapes), aplicaciÃ³n (normaliza bien), comunicaciÃ³n (explica keepdims).

#### 10) EvaluaciÃ³n formativa (rÃ¡pida)
- **Mini-quiz (5):**
  - 1. Â¿QuÃ© devuelve `X.mean(axis=0)` si `X` es `(n,d)`?
  - 2. Â¿QuÃ© hace `keepdims=True`?
  - 3. Diferencia entre `axis=0` y `axis=1`.
  - 4. Â¿Por quÃ© `X.mean()` puede ser un bug?
  - 5. Â¿QuÃ© shape debe tener `mu` para restarse a `X` sin ambigÃ¼edad?

#### 11) Errores comunes y estrategias correctivas
- **Error:** confundir `axis=0`/`axis=1` â†’ **CorrecciÃ³n:** â€œaxis=0 reduce filas; axis=1 reduce columnasâ€.
- **Error:** usar agregaciÃ³n global â†’ **CorrecciÃ³n:** forzar `axis=` y revisar shape.
- **Error:** broadcasting accidental â†’ **CorrecciÃ³n:** usar `keepdims=True` + asserts.
- **Error:** normalizar sin `eps` â†’ **CorrecciÃ³n:** sumar `eps` a `std`.
- **Error:** no validar resultado â†’ **CorrecciÃ³n:** `assert np.allclose(X_norm.mean(axis=0), 0, atol=...)`.

#### 12) Apoyos cognitivos y retenciÃ³n
- **RecuperaciÃ³n (dÃ­a 2):** define `axis` con un ejemplo de `(2,3)`.
- **RecuperaciÃ³n (dÃ­a 7):** Â¿cuÃ¡ndo usarÃ­as `keepdims=True`?
- **Checklist:** â€œshape â†’ axis â†’ keepdims â†’ assertâ€.

#### 13) DiferenciaciÃ³n
- **Principiante:** practicar solo sum/mean con shapes pequeÃ±as (2x3, 3x2).
- **Avanzado:** derivar por quÃ© `keepdims` evita bugs en pipelines vectorizados.

#### 14) Recursos y materiales
- DocumentaciÃ³n NumPy sobre reducciones (`sum`, `mean`, `std`) y `axis`.

#### 15) Notas para el docente / facilitador
- Pide predicciÃ³n de shape antes de ejecutar; usa 5 casos rÃ¡pidos en pizarra.
</details>

### 6. Operaciones Matriciales

```python
import numpy as np

A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# Operaciones elemento a elemento
print(A + B)   # Suma
print(A * B)   # MultiplicaciÃ³n elemento a elemento (Hadamard)
print(A / B)   # DivisiÃ³n elemento a elemento

# Producto matricial (lo que usarÃ¡s en ML)
print(A @ B)           # Operador @ (Python 3.5+)
print(np.matmul(A, B)) # FunciÃ³n matmul
print(np.dot(A, B))    # FunciÃ³n dot

# Resultado:
# [[19, 22],
#  [43, 50]]

# Transpuesta
print(A.T)
# [[1, 3],
#  [2, 4]]
```

<details>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 6: Operaciones Matriciales</strong></summary>

#### 1) Metadatos (1â€“2 lÃ­neas)
- **TÃ­tulo:** Operaciones elemento a elemento vs producto matricial (`@`)
- **ID (opcional):** `M01-T06`
- **DuraciÃ³n estimada:** 60â€“90 min
- **Nivel:** Intermedio
- **Dependencias:** Tema 2 (shape), Tema 4 (broadcasting), Tema 5 (axis)

#### 2) Objetivos de aprendizaje (medibles)
- Al terminar, el estudiante podrÃ¡ **distinguir** `A * B` (Hadamard) de `A @ B` (matmul) y **predecir** el shape de salida **en â‰¥ 8 de 10 casos**.
- Al terminar, el estudiante podrÃ¡ **implementar** una predicciÃ³n lineal `y_hat = X @ w + b` y **verificar** shapes correctos con `assert`.

#### 3) Relevancia y contexto
- **Â¿Por quÃ© importa?** El corazÃ³n del ML clÃ¡sico es Ã¡lgebra lineal: regresiÃ³n, logÃ­stica y redes neuronales dependen de productos matriciales.
- **DÃ³nde encaja:** puente directo a MÃ³dulo 02 (Ãlgebra lineal) y MÃ³dulo 05 (regresiÃ³n/logÃ­stica).

#### 4) Mapa conceptual / lista de conceptos clave
- `A * B` (element-wise) â†’ requiere mismo shape o broadcasting
- `A @ B` (matmul) â†’ requiere compatibilidad de dimensiones internas
- transpuesta `A.T` â†’ cambia ejes
- caso ML: `(n,d) @ (d,) -> (n,)`

#### 5) Definiciones, notaciÃ³n y fÃ³rmulas esenciales
- **Hadamard:** `(A * B)_{ij} = A_{ij}Â·B_{ij}`
- **Matmul:** si `AâˆˆR^{nÃ—d}` y `BâˆˆR^{dÃ—k}`, entonces `A@B âˆˆ R^{nÃ—k}`.
- **Transpuesta:** si `AâˆˆR^{nÃ—d}`, entonces `A^T âˆˆ R^{dÃ—n}`.

#### 6) ExplicaciÃ³n didÃ¡ctica (2 niveles)
- **IntuiciÃ³n:** `@` combina features (columnas) para producir nuevas representaciones; `*` solo â€œescalaâ€ elemento por elemento.
- **Paso a paso:**
  - 1. escribe shapes
  - 2. para `@`, verifica que las dimensiones internas coinciden
  - 3. predice shape de salida
  - 4. valida con asserts
- **Visual recomendado:** diagrama de dimensiones: `(n,d)@(d,k)->(n,k)`.

#### 7) Ejemplos modelados (worked examples)
- **Ejemplo 1 â€” sencillo:** `A (2,2) @ B (2,2)` y contraste con `A * B`.
- **Ejemplo 2 â€” realista:** `y_hat = X @ w + b` (regresiÃ³n lineal) con `X (n,d)`.
- **Contraejemplo:** intentar `A @ B` con shapes incompatibles y â€œarreglarâ€ a ciegas sin revisar dimensiones.

#### 8) PrÃ¡ctica guiada (scaffolded)
- **Ejercicio 1:** predice shapes de 6 productos (incluye 1D vs 2D).
- **Ejercicio 2:** implementa `y_hat = X @ w + b` y valida `y_hat.shape == (n,)`.
- **Ejercicio 3:** crea `W (d,k)` y calcula `Z = X @ W` y valida `Z.shape == (n,k)`.

#### 9) PrÃ¡ctica independiente / transferencia
- **Mini-proyecto:** implementa `linear_layer(X, W, b)` con asserts de shapes.
- **Entrega:** funciÃ³n + tests mÃ­nimos (3 asserts) + explicaciÃ³n breve.

#### 10) EvaluaciÃ³n formativa
- Â¿QuÃ© condiciÃ³n de shapes exige `A @ B`?
- Â¿QuÃ© operaciÃ³n usarÃ­as para aplicar pesos por feature a una matriz: `*` o `@`?
- Â¿QuÃ© devuelve `A.T`?

#### 11) Errores comunes y estrategias correctivas
- **Error:** confundir `*` con `@` â†’ **CorrecciÃ³n:** â€œ`@` mezcla columnas; `*` noâ€.
- **Error:** `X @ w` devuelve `(n,)` pero esperabas `(n,1)` â†’ **CorrecciÃ³n:** decide convenciÃ³n y usa `reshape(-1,1)` si hace falta.
- **Error:** `np.dot` con 1D cambia semÃ¡ntica â†’ **CorrecciÃ³n:** preferir `@` y controlar dims.
- **Error:** shapes incompatibles â†’ **CorrecciÃ³n:** escribir `(n,d)@(d,k)` en papel antes.
- **Error:** broadcasting accidental con `b` â†’ **CorrecciÃ³n:** asegurar `b.shape == (k,)` o `(1,k)`.

#### 12) Apoyos cognitivos y retenciÃ³n
- (dÃ­a 2) predice shape de `(50,3)@(3,)` y `(50,3)@(3,2)`.
- (dÃ­a 7) explica diferencia `Hadamard` vs `matmul` en 3 lÃ­neas.
- checklist: â€œinternas coinciden â†’ salida externaâ€.

#### 13) DiferenciaciÃ³n
- **Principiante:** solo 2DÃ—2D con ejemplos pequeÃ±os.
- **Avanzado:** discutir cÃ³mo BLAS acelera `matmul`.

#### 14) Recursos y materiales
- NumPy `matmul` / operador `@` (docs) + introducciÃ³n a Ã¡lgebra lineal (M02).

#### 15) Notas para el docente / facilitador
- Hacer que el estudiante diga en voz alta: â€œ(n,d)@(d,k)->(n,k)â€ antes de ejecutar.
</details>

### 7. VectorizaciÃ³n: Eliminar Loops

```python
import numpy as np

# âŒ CON LOOP (lento)
def normalize_loop(data: list) -> list:
    """Normalizar datos con loop."""
    mean = sum(data) / len(data)
    std = (sum((x - mean)**2 for x in data) / len(data)) ** 0.5
    return [(x - mean) / std for x in data]

# âœ… VECTORIZADO (rÃ¡pido)
def normalize_vectorized(data: np.ndarray) -> np.ndarray:
    """Normalizar datos vectorizado."""
    return (data - np.mean(data)) / np.std(data)

# Ejemplo
data = np.random.randn(1000000)

# La versiÃ³n vectorizada es ~100x mÃ¡s rÃ¡pida
normalized = normalize_vectorized(data)
```

<details>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 7: VectorizaciÃ³n (Eliminar Loops)</strong></summary>

#### 1) Metadatos (1â€“2 lÃ­neas)
- **TÃ­tulo:** VectorizaciÃ³n en NumPy: pensar en batches, no en iteraciones
- **ID (opcional):** `M01-T07`
- **DuraciÃ³n estimada:** 60â€“90 min
- **Nivel:** Intermedio
- **Dependencias:** Tema 1 (ndarray), Tema 5 (axis)

#### 2) Objetivos de aprendizaje (medibles)
- Al terminar, el estudiante podrÃ¡ **reescribir** una operaciÃ³n con loops en una versiÃ³n vectorizada y **verificar** igualdad numÃ©rica con `np.allclose`.
- Al terminar, el estudiante podrÃ¡ **explicar** por quÃ© la versiÃ³n vectorizada suele ser mÃ¡s rÃ¡pida (loops en C/BLAS) **en â‰¤ 2 min**.

#### 3) Relevancia y contexto
- **Â¿Por quÃ© importa?** Todos los algoritmos del Pathway se vuelven impracticables si operas en Python puro elemento a elemento.
- **DÃ³nde encaja:** base para backprop, gradiente, mÃ©tricas y entrenamiento batch.

#### 4) Mapa conceptual / conceptos clave
- loop Python â†’ overhead
- ufuncs/BLAS â†’ loops internos en C
- vectorizaciÃ³n â†’ operar por bloques
- validaciÃ³n â†’ `allclose` + asserts de shape

#### 5) Definiciones, notaciÃ³n y fÃ³rmulas esenciales
- **VectorizaciÃ³n:** expresar el cÃ¡lculo como operaciones sobre arrays completos.
- **NormalizaciÃ³n (z-score):** `x_norm = (x - mean) / std`.

#### 6) ExplicaciÃ³n didÃ¡ctica (2 niveles)
- **Resumen:** â€œpiensa en el batchâ€ (`(n,d)`), no en `for i in range(n)`.
- **Paso a paso:**
  - 1. define entrada como `np.ndarray`
  - 2. escribe el cÃ¡lculo con operaciones sobre arrays
  - 3. valida con asserts y un test de equivalencia
- **Visual recomendado:** grÃ¡fica de tiempo vs tamaÃ±o (log) comparando loop vs vectorizado.

#### 7) Ejemplos modelados
- **Ejemplo 1 â€” sencillo:** normalizaciÃ³n 1D con `np.mean/np.std`.
- **Ejemplo 2 â€” realista:** normalizaciÃ³n por columnas de `X (n,d)` con `axis=0`.
- **Contraejemplo:** vectorizar sin controlar `axis`, obteniendo estadÃ­sticas globales por accidente.

#### 8) PrÃ¡ctica guiada
- **Ejercicio 1:** implementa suma de dos vectores (loop vs vectorizado) y compara.
- **Ejercicio 2:** implementa distancia L2 (loop vs vectorizado) y compara.
- **Ejercicio 3:** normaliza por columna una matriz y valida `meanâ‰ˆ0`, `stdâ‰ˆ1`.
- **Pista:** usa `axis=0` y `keepdims=True` si vas a restar/dividir a `X`.

#### 9) Transferencia
- **Actividad:** implementa `minibatch_mean(X, batch_size)` sin loops sobre features (solo batch slicing permitido).
- **Entrega:** funciÃ³n + asserts de shape + breve explicaciÃ³n.

#### 10) EvaluaciÃ³n formativa
- Â¿QuÃ© parte del cÃ³mputo se ejecuta en C cuando usas NumPy?
- Â¿QuÃ© diferencia hay entre `np.mean(X)` y `np.mean(X, axis=0)`?
- Â¿QuÃ© valida `np.allclose`?

#### 11) Errores comunes
- **Error:** â€œvectoricÃ© pero estÃ¡ malâ€ â†’ **CorrecciÃ³n:** tests de equivalencia y asserts de shape.
- **Error:** olvido de `axis` â†’ **CorrecciÃ³n:** imprimir shapes de estadÃ­sticas.
- **Error:** overflow/underflow en operaciones â†’ **CorrecciÃ³n:** tÃ©cnicas de estabilidad (p.ej. softmax estable en Tema 8).
- **Error:** mezclar listas con arrays â†’ **CorrecciÃ³n:** normalizar inputs a `np.asarray`.
- **Error:** medir tiempos con ruido â†’ **CorrecciÃ³n:** repetir y promediar.

#### 12) RetenciÃ³n
- (dÃ­a 2) explica â€œbatch vs iteraciÃ³nâ€ en 3 lÃ­neas.
- (dÃ­a 7) reescribe un loop tÃ­pico a vectorizado.
- checklist: â€œshape â†’ axis â†’ vectorizar â†’ validarâ€.

#### 13) DiferenciaciÃ³n
- **Principiante:** solo 1D (vectores).
- **Avanzado:** vectorizar parte de un algoritmo (p.ej. predicciÃ³n lineal sobre batch).

#### 14) Recursos
- NumPy ufuncs y broadcasting (docs).

#### 15) Nota docente
- Insistir en: â€œprimero correctness, luego performanceâ€.
</details>

### 8. Funciones Universales (ufuncs)

```python
import numpy as np

x = np.array([1, 2, 3, 4, 5])

# Funciones matemÃ¡ticas (aplicadas elemento a elemento)
print(np.exp(x))      # e^x
print(np.log(x))      # ln(x)
print(np.sqrt(x))     # âˆšx
print(np.sin(x))      # sin(x)

# Importante para ML:
# Sigmoid: Ïƒ(x) = 1 / (1 + e^(-x))
def sigmoid(x: np.ndarray) -> np.ndarray:
    return 1 / (1 + np.exp(-x))  # Sigmoid: mapea R -> (0,1) elemento a elemento

# ReLU: max(0, x)
def relu(x: np.ndarray) -> np.ndarray:
    return np.maximum(0, x)  # ReLU: max(0,x) elemento a elemento; anula negativos

print(sigmoid(np.array([-2, -1, 0, 1, 2])))
# [0.119, 0.269, 0.5, 0.731, 0.881]
```

<details>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 8: Funciones Universales (ufuncs)</strong></summary>

#### 1) Metadatos (1â€“2 lÃ­neas)
- **TÃ­tulo:** ufuncs: matemÃ¡ticas elemento a elemento (base de activaciones y pÃ©rdidas)
- **ID (opcional):** `M01-T08`
- **DuraciÃ³n estimada:** 60â€“90 min
- **Nivel:** Intermedio
- **Dependencias:** Tema 7 (vectorizaciÃ³n), nociÃ³n de `dtype`

#### 2) Objetivos de aprendizaje (medibles)
- Al terminar, el estudiante podrÃ¡ **identificar** cuÃ¡ndo una funciÃ³n es â€œelement-wiseâ€ y **predecir** que preserva el `shape`.
- Al terminar, el estudiante podrÃ¡ **implementar** `sigmoid` y `relu` vectorizadas y **verificar** propiedades bÃ¡sicas con `assert`.

#### 3) Relevancia y contexto
- **Â¿Por quÃ© importa?** Activaciones, transformaciones y parte de las pÃ©rdidas son ufuncs: sin esto, redes neuronales no se entienden.
- **DÃ³nde encaja:** M07 (Deep Learning) y M05 (logÃ­stica usa sigmoid).

#### 4) Mapa conceptual / conceptos clave
- ufunc (`exp/log/sqrt/max`) â†’ opera por elemento
- estabilidad numÃ©rica â†’ evitar overflow
- activaciones â†’ `sigmoid`, `relu`, `softmax`

#### 5) Definiciones, notaciÃ³n y fÃ³rmulas esenciales
- **Sigmoid:** `Ïƒ(x)=1/(1+e^{-x})`.
- **ReLU:** `max(0,x)`.
- **Softmax (idea):** `exp(x)/sum(exp(x))` (con shift por estabilidad).

#### 6) ExplicaciÃ³n didÃ¡ctica (2 niveles)
- **Resumen:** una ufunc toma un array y devuelve otro array del mismo shape (salvo agregaciones).
- **Paso a paso:**
  - 1. convierte input con `np.asarray`
  - 2. aplica ufuncs
  - 3. valida rango/propiedades (p.ej. sigmoid en (0,1))
- **Visual recomendado:** curva sigmoid y ReLU (grÃ¡fica 2D) y un histograma antes/despuÃ©s.

#### 7) Ejemplos modelados
- **Ejemplo 1:** aplicar `np.exp` a un vector y ver saturaciÃ³n.
- **Ejemplo 2:** softmax estable con `x - x.max()`.
- **Contraejemplo:** softmax sin estabilizaciÃ³n con valores grandes.

#### 8) PrÃ¡ctica guiada
- **Ejercicio 1:** implementa `softplus = log(1+exp(x))` con estabilidad.
- **Ejercicio 2:** aplica ReLU a un batch `(n,d)` y valida que no cambia shape.
- **Ejercicio 3:** verifica que `sigmoid(0)=0.5`.

#### 9) Transferencia
- **Actividad:** implementa â€œcapaâ€ `y = relu(X @ W + b)` con asserts de shapes.

#### 10) EvaluaciÃ³n formativa
- Â¿QuÃ© parte del cÃ³mputo se ejecuta en C cuando usas NumPy?
- Â¿QuÃ© propiedad del sigmoid ayuda en clasificaciÃ³n?
- Â¿QuÃ© pasa con `dtype=int` si haces operaciones in-place?

#### 11) Errores comunes
- overflow en `exp`
- asumir que ufuncs cambian shape
- usar `sum` sin `axis`

#### 12) RetenciÃ³n
- (dÃ­a 2) define sigmoid y su rango.
- (dÃ­a 7) explica softmax estable.

#### 13) DiferenciaciÃ³n
- **Principiante:** graficar ReLU y sigmoid.
- **Avanzado:** derivar quÃ© pasa con gradientes en saturaciÃ³n (intuiciÃ³n).

#### 14) Recursos
- NumPy ufuncs + notas de estabilidad numÃ©rica.

#### 15) Nota docente
- Enfatiza â€œestabilidad primeroâ€: shift en softmax, eps en divisiones.
</details>

### 9. Reshape y ManipulaciÃ³n de Forma

```python
import numpy as np  # Importa NumPy para crear arrays y cambiar su forma (reshape/flatten)
# Crear array 1D
a = np.arange(12)  # Crea un vector 1D con 12 enteros consecutivos (0..11)

# Reshape a 2D
matrix = a.reshape(3, 4)  # Reinterpreta el vector como matriz 2D de shape (3,4); 3*4 debe igualar 12
print(matrix.shape)  # Imprime el shape para verificar que ahora es (3, 4)
# [[ 0,  1,  2,  3],
#  [ 4,  5,  6,  7],
#  [ 8,  9, 10, 11]]

# Reshape a 3D
tensor = a.reshape(2, 2, 3)  # Cambia la forma a 3D (2,2,3); 2*2*3=12 conserva el total de elementos
print(tensor.shape)  # Verifica por pantalla la forma del tensor (2, 2, 3)

# Flatten: volver a 1D
flat = matrix.flatten()  # Aplana la matriz 2D y devuelve una copia 1D con todos los elementos
print(flat.shape)  # Comprueba que vuelve a tener 12 elementos en 1D: shape (12,)

# -1 para inferir dimensiÃ³n automÃ¡ticamente
auto = a.reshape(4, -1)  # Usa -1 para que NumPy infiera la dimensiÃ³n faltante: (4, 3)
auto = a.reshape(-1, 6)  # Infiera la primera dimensiÃ³n para que el total sea 12: (2, 6)

<details>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 9: Reshape y ManipulaciÃ³n de Forma</strong></summary>

#### 1) Metadatos (1â€“2 lÃ­neas)
- **TÃ­tulo:** Control de `shape`: `reshape`, `flatten`, `transpose` y contratos
- **ID (opcional):** `M01-T09`
- **DuraciÃ³n estimada:** 60â€“120 min
- **Nivel:** Intermedio
- **Dependencias:** Temas 2, 4, 6

#### 2) Objetivo(s) de aprendizaje (medibles)
- Al terminar, el estudiante podrÃ¡ **transformar** shapes entre 1D/2D/3D con `reshape` sin cambiar `size` y **verificarlo** con `assert`.
- Al terminar, el estudiante podrÃ¡ **corregir** `(n,)` vs `(n,1)` usando `reshape` o `np.newaxis` en un caso dado.

#### 3) Relevancia y contexto
- **Â¿Por quÃ© importa?** La mayorÃ­a de bugs en ML desde cero son bugs de `shape`. Si controlas shapes, controlas el pipeline.
- **DÃ³nde encaja:** preparaciÃ³n para capas lineales, batches y tensores multi-eje.

#### 4) Mapa conceptual / lista de conceptos clave
- `size` (invariante)
- `reshape` (misma cantidad de elementos)
- `-1` (inferencia)
- `flatten` (copia)
- `transpose` (reordena ejes)

#### 5) Definiciones, notaciÃ³n y fÃ³rmulas esenciales
- **Invariante:** `a.size` se conserva al hacer `reshape`.
- **NotaciÃ³n:** batches tÃ­picos en ML: `X.shape=(n,d)`.

#### 6) ExplicaciÃ³n didÃ¡ctica (2 niveles)
- **Resumen corto:** `reshape` cambia la interpretaciÃ³n del buffer, no el contenido.
- **Paso a paso:** `size â†’ shape objetivo â†’ reshape â†’ assert`.
- **Visual recomendado:** diagrama â€œvector â†’ matriz â†’ tensorâ€.

#### 7) Ejemplos modelados
- **Ejemplo 1:** `(12,) â†’ (3,4)`.
- **Ejemplo 2:** `X_flat (n,784) â†” X_img (n,28,28)`.
- **Contraejemplo:** â€œaplanarâ€ para evitar un error sin entender el contrato.

#### 8) PrÃ¡ctica guiada
- 3 ejercicios de predicciÃ³n de shape + 3 ejercicios de correcciÃ³n con `assert`.

#### 9) Transferencia
- Implementa `ensure_2d(X)` con casos `(n,)` y `(n,d)`.

#### 10) EvaluaciÃ³n
- Â¿QuÃ© garantiza el invariante `size`?
- Â¿QuÃ© hace `-1`?

#### 11) Errores comunes
- producto de dimensiones incorrecto
- confundir vector columna vs fila

#### 12) RetenciÃ³n
- (dÃ­a 2) predice shapes de 5 reshapes.
- (dÃ­a 7) explica el invariante `size`.

#### 13) DiferenciaciÃ³n
- Principiante: 1Dâ†”2D.
- Avanzado: tensores tipo imagen.

#### 14) Recursos
- NumPy docs: `reshape/transpose/flatten/ravel`.

#### 15) Nota docente
- â€œEntrada shape â†’ salida shapeâ€ antes de ejecutar.
</details>

### 9.1 OOP para ML (v5.1): mini-framework `Tensor`

**Objetivo prÃ¡ctico:** antes de llegar a redes neuronales (donde vas a tener que manejar `self`, estado y operaciones), crea una mini-abstracciÃ³n que se comporte como un â€œtensorâ€ simple.

#### QuÃ© debes dominar (sin teorÃ­a vacÃ­a)

- **Clase vs instancia:** la clase define el â€œmoldeâ€; la instancia es el objeto real en memoria.
- **`self`:** referencia a la instancia actual; ahÃ­ vive el estado.
- **Estado:** variables guardadas en el objeto (`self.data`, `self.shape`).
- **Operadores:** `+` llama a `__add__`, `@` llama a `__matmul__`.

#### Entregable (taller)

- Implementar una clase `Tensor` que:
  - acepte lista o `np.ndarray` en `__init__`
  - mantenga un estado interno `self.shape`
  - implemente `__add__` y `__matmul__` usando NumPy por dentro

#### ImplementaciÃ³n (referencia)

```python
import numpy as np  # NumPy para convertir entrada a ndarray y reutilizar operaciones vectorizadas
from typing import Union  # Union para aceptar mÃºltiples tipos de entrada en el constructor

ArrayLike = Union[list, np.ndarray]  # Tipo de entrada soportado: lista de Python o ndarray de NumPy

class Tensor:  # Contenedor mÃ­nimo para entender OOP aplicado a ML (estado + operadores)
    def __init__(self, data: ArrayLike):  # Constructor: recibe datos y construye el estado interno
        self.data = np.array(data, dtype=float)  # Normaliza a ndarray float para operar consistentemente
        self.shape = self.data.shape  # Guarda shape como parte del estado para inspecciÃ³n y debugging

    def __add__(self, other: "Tensor") -> "Tensor":  # Define el operador + (suma elemento a elemento)
        if not isinstance(other, Tensor):  # Si no es Tensor, delega a Python (permite otros tipos)
            return NotImplemented  # SeÃ±al estÃ¡ndar: operaciÃ³n no implementada para ese tipo
        return Tensor(self.data + other.data)  # Suma NumPy y devuelve un nuevo Tensor (no muta self)

    def __matmul__(self, other: "Tensor") -> "Tensor":  # Define el operador @ (producto matricial)
        if not isinstance(other, Tensor):  # Valida tipo para evitar errores silenciosos
            return NotImplemented  # Permite que Python intente la operaciÃ³n reflejada si existe
        return Tensor(self.data @ other.data)  # Usa @ de NumPy (matmul) y envuelve el resultado

    def __repr__(self) -> str:  # RepresentaciÃ³n Ãºtil para ver shape y datos rÃ¡pido al imprimir
        return f"Tensor(shape={self.shape}, data={self.data})"  # String con informaciÃ³n mÃ­nima de debugging

```

#### Ejercicios (con `assert`) â€” tu mÃ­nimo aceptable

```python
import numpy as np  # NumPy para comparar arrays con allclose y construir datos de prueba

# 1) Estado: shape debe reflejar el ndarray interno
t = Tensor([1, 2, 3])  # Crea Tensor desde lista (se convierte internamente a ndarray)
assert t.shape == (3,)  # Verifica que el shape se guardÃ³ correctamente

# 2) Suma: + llama a __add__
a = Tensor([1, 2, 3])  # Tensor A
b = Tensor([10, 20, 30])  # Tensor B
c = a + b  # Ejecuta __add__ y debe devolver un Tensor nuevo
assert isinstance(c, Tensor)  # Debe devolver Tensor
assert np.allclose(c.data, np.array([11.0, 22.0, 33.0]))  # Verifica el resultado numÃ©rico
assert c.shape == (3,)  # El shape debe permanecer (3,)

# 3) Producto matricial: @ llama a __matmul__
A = Tensor([[1, 2], [3, 4]])  # Matriz 2x2
x = Tensor([1, 1])  # Vector de entrada con shape (2,)
y = A @ x  # Producto matriz-vector -> shape (2,)
assert np.allclose(y.data, np.array([3.0, 7.0]))  # [1,2]Â·[1,1]=3 y [3,4]Â·[1,1]=7
assert y.shape == (2,)  # Verifica el shape de salida

# 4) Error de shape: debe fallar si dimensiones no son compatibles
try:  # Captura excepciÃ³n esperada de NumPy cuando shapes no son multiplicables
    _ = Tensor([[1, 2, 3], [4, 5, 6]]) @ Tensor([1, 2])  # (2,3) @ (2,) no es vÃ¡lido
    assert False  # Si no fallÃ³, el test debe fallar
except ValueError:  # NumPy lanza ValueError ante incompatibilidad de shapes
    pass  # Ã‰xito: esperÃ¡bamos el error

```

<details>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 9.1: OOP para ML (mini-framework `Tensor`)</strong></summary>

#### 1) Metadatos (1â€“2 lÃ­neas)
- **TÃ­tulo:** OOP aplicada a tensores: estado, operadores y contratos
- **ID (opcional):** `M01-T09_1`
- **DuraciÃ³n estimada:** 3â€“6 horas
- **Nivel:** Intermedio
- **Dependencias:** Temas 2, 6 y 9

#### 2) Objetivo(s) de aprendizaje (medibles)
- Al terminar, el estudiante podrÃ¡ **implementar** una clase `Tensor` con estado mÃ­nimo (`data`, `shape`) y **verificar** invariantes con `assert`.
- Al terminar, el estudiante podrÃ¡ **explicar** cÃ³mo Python resuelve `a + b` y `A @ x` mediante `__add__` y `__matmul__`.

#### 3) Relevancia y contexto
- **Â¿Por quÃ© importa?** Es el puente entre NumPy â€œsueloâ€ y los patrones reales de frameworks (mÃ³dulos con estado + operadores + debugging).
- **DÃ³nde encaja:** prepara para capas (`Linear`), composiciÃ³n y validaciÃ³n temprana de shapes.

#### 4) Mapa conceptual / lista de conceptos clave
- clase vs instancia
- `self` y estado (`self.data`, `self.shape`)
- dunder methods: `__init__`, `__add__`, `__matmul__`, `__repr__`
- `NotImplemented` (protocolo de operadores)
- contrato de shapes (fallar temprano)

#### 5) Definiciones, notaciÃ³n y fÃ³rmulas esenciales
- **Estado:** datos guardados en el objeto (persisten entre llamadas).
- **Operador `@`:** producto matricial (matmul). Para `A.shape=(m,n)` y `x.shape=(n,)`, salida `y.shape=(m,)`.

#### 6) ExplicaciÃ³n didÃ¡ctica (2 niveles)
- **Resumen corto:** un â€œtensorâ€ mÃ­nimo es un contenedor con *datos + shape* y mÃ©todos que implementan operaciones seguras.
- **Paso a paso:** normaliza entrada â†’ guarda `shape` â†’ implementa operadores â†’ valida con `assert` â†’ mejora `__repr__`.

#### 7) Ejemplos modelados
- `Tensor([1,2,3]) + Tensor([10,20,30])` (element-wise)
- `Tensor([[1,2],[3,4]]) @ Tensor([1,1])` (matmul)
- **Contraejemplo:** permitir `@` sin validar dimensiones y â€œdebuggearâ€ el error tarde.

#### 8) PrÃ¡ctica guiada
- Agrega `assert self.data.ndim in {1,2}` y prueba casos vÃ¡lidos/invalidos.
- Extiende `__repr__` para mostrar `dtype` y `ndim`.

#### 9) PrÃ¡ctica independiente / transferencia
- Implementa `__mul__` (element-wise) y prueba broadcasting controlado.
- Implementa `sum(axis=None)` para conectar con agregaciones del Tema 5.

#### 10) EvaluaciÃ³n (formativa y sumativa)
- **Formativa:** dado un snippet, predice quÃ© mÃ©todo dunder se ejecuta.
- **Sumativa:** implementa `Tensor` + tests con 8â€“10 `assert` incluyendo 2 casos de error.

#### 11) Errores comunes
- confundir â€œmatmulâ€ con multiplicaciÃ³n elemento a elemento
- no devolver `NotImplemented` y romper compatibilidad con otros tipos
- mutar `self.data` sin intenciÃ³n (efectos colaterales)

#### 12) Apoyos cognitivos / retenciÃ³n
- regla mental: **estado = lo que necesito para debuggear** (shape, dtype).
- checklist: *entrada â†’ shape â†’ operaciÃ³n â†’ salida*.

#### 13) DiferenciaciÃ³n
- **Principiante:** solo `__init__` + `__add__`.
- **Avanzado:** mensajes de error con shapes; soporte para `other` array-like.

#### 14) Recursos
- Data model de Python (dunder methods) y referencia de `operator`.
- NumPy: guÃ­a de `matmul` y broadcasting.

#### 15) Nota para el facilitador
- Pide que el alumno escriba primero el â€œcontratoâ€ (shapes esperados) antes de tocar el cÃ³digo.
</details>

### 9.2 OOP para CientÃ­ficos (ML-first): `__call__`, `__repr__`, `dataclasses`, y debugging

La meta aquÃ­ NO es â€œaprender clases por aprender clasesâ€, sino entrenar los mismos reflejos que usarÃ¡s en PyTorch/Keras:

- **Objetos invocables:** un objeto que se comporta como funciÃ³n (`layer(x)`), vÃ­a `__call__`.
- **Debugging rÃ¡pido:** `__repr__` te dice estado clave (shapes, dtypes) sin abrir el objeto.
- **Configs reproducibles:** `dataclasses` te obliga a declarar explÃ­citamente hiperparÃ¡metros.
- **Shape discipline:** si un shape estÃ¡ mal, que reviente temprano con `assert`.

#### 9.2.1 `dataclass` para configs (hiperparÃ¡metros explÃ­citos)

```python
from dataclasses import dataclass  # dataclass: clase â€œcontenedorâ€ para configs, con menos boilerplate
import numpy as np

@dataclass(frozen=True)  # frozen=True: hace la config inmutable (evita cambiar hiperparÃ¡metros por accidente)
class LinearConfig:  # Config de una capa lineal
    in_features: int  # D: nÃºmero de features de entrada
    out_features: int  # K: nÃºmero de unidades de salida
    seed: int = 0  # Semilla para inicializaciÃ³n reproducible


class Linear:  # â€œCapaâ€ lineal: implementa y = xW + b
    def __init__(self, cfg: LinearConfig):  # Constructor: recibe config explÃ­cita
        self.cfg = cfg  # Guardamos la config como estado: permite debugging y reproducibilidad

        rng = np.random.default_rng(cfg.seed)  # RNG moderno de NumPy; controlado por semilla

        # Pesos W con shape (D, K): D entradas -> K salidas
        self.W = rng.standard_normal((cfg.in_features, cfg.out_features))  # InicializaciÃ³n normal estÃ¡ndar
        self.b = np.zeros((cfg.out_features,), dtype=float)  # Bias con shape (K,), inicializado en 0

    def __call__(self, x: np.ndarray) -> np.ndarray:  # Permite invocar: y = layer(x)
        # Assert 1: x debe ser 2D: (N, D) porque en ML trabajamos con batches (N muestras)
        assert x.ndim == 2, f"x debe ser 2D (N,D). Recibido ndim={x.ndim}, shape={x.shape}"  # Falla temprano

        # Assert 2: la dimensiÃ³n D de x debe coincidir con in_features
        assert x.shape[1] == self.cfg.in_features, (
            f"D mismatch: x.shape[1]={x.shape[1]} pero cfg.in_features={self.cfg.in_features}"
        )  # Evita bugs silenciosos en matmul

        # Forward: (N, D) @ (D, K) = (N, K)
        y = x @ self.W  # Producto matricial: combina features con pesos

        # Broadcasting controlado: (N, K) + (K,) => (N, K) (b se suma a cada fila)
        y = y + self.b  # Bias por neurona

        # Assert 3: y debe tener shape (N, K)
        assert y.shape == (x.shape[0], self.cfg.out_features), (
            f"Salida mal shapeada: y.shape={y.shape} pero esperado {(x.shape[0], self.cfg.out_features)}"
        )  # Garantiza contrato de la capa

        return y  # Retorna activaciones lineales (logits si fuera clasificaciÃ³n)

    def __repr__(self) -> str:  # Debugging: quÃ© eres y quÃ© shapes manejas
        return (
            "Linear("  # Prefijo con nombre de clase
            f"in={self.cfg.in_features}, out={self.cfg.out_features}, "  # HiperparÃ¡metros clave
            f"W.shape={self.W.shape}, b.shape={self.b.shape}, "  # Estado interno crÃ­tico
            f"W.dtype={self.W.dtype}, b.dtype={self.b.dtype}"  # Dtypes para detectar casts raros
            ")"
        )  # String de diagnÃ³stico


# Demo mÃ­nimo (con disciplina de shapes)
cfg = LinearConfig(in_features=4, out_features=3, seed=42)  # Definimos una capa 4->3
layer = Linear(cfg)  # Creamos el objeto capa
print(layer)  # __repr__: imprime â€œcontratoâ€ + estado

X = np.zeros((5, 4), dtype=float)  # Batch: N=5 muestras, D=4 features
assert X.shape == (5, 4)  # Regla de oro: afirmar shapes cuando construyes tensores
Y = layer(X)  # Llamamos como funciÃ³n (gracias a __call__)
assert Y.shape == (5, 3)  # Verificamos el contrato de salida (N,K)
```

<details>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 9.2: OOP ML-first</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** `__call__`, `__repr__`, configs y debugging por invariantes
- **ID (opcional):** `M01-T09_2`
- **DuraciÃ³n estimada:** 3â€“6 horas
- **Nivel:** Intermedio
- **Dependencias:** 9.1, 6

#### 2) Objetivos
- construir mÃ³dulos invocables y validar shapes/dtypes.
- usar `dataclass(frozen=True)` para reproducibilidad.

#### 3) Relevancia
- replica el patrÃ³n de frameworks (PyTorch/Keras).

#### 4) Conceptos clave
- config â†’ estado â†’ contrato (`__call__`) â†’ cÃ³mputo (`forward`).

#### 5) Definiciones
- objeto invocable.

#### 6) DidÃ¡ctica
- separar responsabilidades y fallar temprano.

#### 7) Ejemplos
- `Linear` + `Sequential`.

#### 8) PrÃ¡ctica guiada
- forzar error de shape y mejorar mensajes.

#### 9) Transferencia
- MLP mÃ­nimo forward-only.

#### 10) EvaluaciÃ³n
- Â¿QuÃ© valida `__call__`?

#### 11) Errores comunes
- no validar invariantes.

#### 12) RetenciÃ³n
- (dÃ­a 2) lista 3 invariantes.

#### 13) DiferenciaciÃ³n
- avanzado: hooks/logs.

#### 14) Recursos
- dataclasses + lectura conceptual `nn.Module`.

#### 15) Nota docente
- contrato de shapes antes de correr.
</details>

### 10. GeneraciÃ³n de Datos Aleatorios

```python
import numpy as np  # Importa NumPy para generar nÃºmeros aleatorios y manipular arrays

# Fijar semilla para reproducibilidad
np.random.seed(42)  # Fija la semilla: hace reproducibles los resultados aleatorios

# DistribuciÃ³n uniforme [0, 1)
uniform = np.random.rand(3, 3)  # Genera una matriz 3x3 con valores uniformes en [0,1)

# DistribuciÃ³n normal (media=0, std=1)
normal = np.random.randn(3, 3)  # Genera una matriz 3x3 con valores ~ N(0,1)

# DistribuciÃ³n normal personalizada
custom_normal = np.random.normal(loc=5, scale=2, size=(100,))  # 100 muestras de N(5,2^2): media 5, std 2

# Enteros aleatorios
integers = np.random.randint(0, 10, size=(3, 3))  # Enteros aleatorios en [0,10) con shape (3,3)

# Shuffle (mezclar)
data = np.arange(10)  # Crea un array 1D [0,1,2,...,9]
np.random.shuffle(data)  # Mezcla el array *in-place* (modifica data directamente)

# Muestreo sin reemplazo
sample = np.random.choice(data, size=5, replace=False)  # Elige 5 elementos distintos de data (sin repetir)
```

<details>
<summary><strong>ğŸ“Œ Complemento pedagÃ³gico â€” Tema 10: GeneraciÃ³n de Datos Aleatorios</strong></summary>

#### 1) Metadatos
- **TÃ­tulo:** RNG reproducible: semillas, distribuciones, muestreo
- **ID (opcional):** `M01-T10`
- **DuraciÃ³n estimada:** 60â€“90 min
- **Nivel:** Intermedio
- **Dependencias:** Tema 7 (vectorizaciÃ³n), Tema 5 (agregaciones)

#### 2) Objetivos (medibles)
- Al terminar, el estudiante podrÃ¡ **generar** datos sintÃ©ticos reproducibles (seed) y **explicar** por quÃ© importa.
- Al terminar, el estudiante podrÃ¡ **seleccionar** una distribuciÃ³n adecuada (`rand`, `randn`, `normal`, `randint`) para un caso.

#### 3) Relevancia
- **Â¿Por quÃ© importa?** Reproducibilidad y experimentos controlados: inicializaciÃ³n, splits y pruebas dependen del RNG.

#### 4) Conceptos clave
- seed â†’ reproducibilidad
- distribuciÃ³n â†’ forma de datos
- `shuffle` in-place â†’ efectos colaterales
- `choice` â†’ muestreo

#### 5) Definiciones
- **In-place:** modifica el objeto original.

#### 6) DidÃ¡ctica
- fija seed â†’ genera â†’ valida shape â†’ documenta distribuciÃ³n.

#### 7) Ejemplos
- `randn(n,d)` para features gaussianas; `choice` para Ã­ndices.
- **Contraejemplo:** usar `shuffle` y luego asumir que el array original no cambiÃ³.

#### 8) PrÃ¡ctica guiada
- generar `X (200,2)` y `y = X@w + noise`.
- split reproducible usando `choice`.

#### 9) Transferencia
- construir dataset sintÃ©tico linealmente separable.

#### 10) EvaluaciÃ³n
- diferencia `rand` vs `randn`.
- quÃ© resuelve la seed.

#### 11) Errores comunes
- no fijar seed
- mezclar RNG global sin control

#### 12) RetenciÃ³n
- (dÃ­a 2) por quÃ© reproducibilidad.
- (dÃ­a 7) cÃ³mo hacer split reproducible.

#### 13) DiferenciaciÃ³n
- **Principiante:** solo shapes.
- **Avanzado:** `default_rng` (conceptual).

#### 14) Recursos
- NumPy Random (docs).

#### 15) Nota docente
- pedir reporte de seed + shape + distribuciÃ³n.
</details>

 ---

## ğŸ“Š Type Hints con NumPy

```python
import numpy as np  # Importa NumPy para operaciones numÃ©ricas
from numpy.typing import NDArray  # Tipado estÃ¡tico: NDArray permite anotar arrays de NumPy con mypy

# Type hints para arrays
def normalize(data: NDArray[np.float64]) -> NDArray[np.float64]:
    """Normaliza un array de floats."""
    return (data - np.mean(data)) / np.std(data)  # Estandariza: resta la media y divide por la desviaciÃ³n estÃ¡ndar

# Type hints genÃ©ricos
def dot_product(a: np.ndarray, b: np.ndarray) -> float:
    """Calcula el producto punto de dos vectores."""
    return float(np.dot(a, b))  # np.dot devuelve un escalar NumPy; float() lo convierte a float de Python

# Con mypy
# pip install numpy-stubs
```

---

## âš¡ Benchmark: Lista vs NumPy

```python
import numpy as np  # NumPy para operaciones vectorizadas y producto punto rÃ¡pido (np.dot)
import time  # time.time() para medir tiempos de ejecuciÃ³n (benchmark simple)
from typing import List  # Tipado: lista de floats para la implementaciÃ³n â€œcon Python puroâ€

def benchmark_dot_product():
    """Compara rendimiento de lista vs NumPy."""
    size = 1_000_000  # TamaÃ±o del vector: suficientemente grande para notar diferencias de rendimiento

    # Crear datos
    list_a: List[float] = [float(i) for i in range(size)]  # Lista de floats: implementaciÃ³n base (no vectorizada)
    list_b: List[float] = [float(i) for i in range(size)]  # Segunda lista de floats
    array_a = np.array(list_a)  # Convierte lista a ndarray: permite operaciones vectorizadas (en C)
    array_b = np.array(list_b)  # Convierte la segunda lista a ndarray

    # Benchmark lista
    start = time.time()  # Marca tiempo inicial
    result_list = sum(a * b for a, b in zip(list_a, list_b))  # Producto punto con generador + zip (Python puro)
    time_list = time.time() - start  # Tiempo total transcurrido para la versiÃ³n con listas

    # Benchmark NumPy
    start = time.time()  # Marca tiempo inicial para NumPy
    result_numpy = np.dot(array_a, array_b)  # Producto punto vectorizado: usa implementaciÃ³n optimizada (BLAS)
    time_numpy = time.time() - start  # Tiempo total transcurrido para NumPy

    print(f"Lista:  {time_list:.4f}s")  # Reporta tiempo de la implementaciÃ³n con listas
    print(f"NumPy:  {time_numpy:.4f}s")  # Reporta tiempo de la implementaciÃ³n con NumPy
    print(f"Speedup: {time_list/time_numpy:.1f}x")  # Factor de aceleraciÃ³n: cuÃ¡ntas veces NumPy es mÃ¡s rÃ¡pido

    # Verificar resultados iguales
    assert abs(result_list - result_numpy) < 1e-6  # Confirma que ambos mÃ©todos producen el mismo resultado

if __name__ == "__main__":
    benchmark_dot_product()  # Ejecuta el benchmark solo cuando el archivo se corre como script

# Output tÃ­pico:
# Lista:  0.1523s
# NumPy:  0.0015s
# Speedup: 101.5x
```

---

## ğŸ¯ Ejercicios por tema (progresivos) + Soluciones

Reglas de uso:

- **Primero intenta** sin ver soluciones.
- **Tiempo lÃ­mite sugerido:** 10â€“15 min por ejercicio antes de mirar la soluciÃ³n.
- **Ã‰xito mÃ­nimo:** que tu soluciÃ³n pase los `assert` de cada ejercicio.

 ---

  ### Ejercicio 1.1: Pandas - DataFrame y Series

  #### Enunciado

  1) **BÃ¡sico**

  - Crea un `DataFrame` llamado `df` con columnas `edad`, `salario`, `ciudad` (5 filas).
  - Extrae la columna `salario` como `Series` y calcula su media.

  2) **Intermedio**

  - Crea una nueva columna `salario_k` con `salario / 1000`.
  - Ordena el `DataFrame` por `salario` de mayor a menor.

  3) **Avanzado**

  - Calcula, por ciudad, la media de `salario` y el conteo de filas (en una sola tabla).

  #### SoluciÃ³n

```python
import pandas as pd

df = pd.DataFrame(
    {
        "edad": [25, 30, 30, 45, 50],
        "salario": [50000, 60000, 61000, 80000, 90000],
        "ciudad": ["Madrid", "Barcelona", "Madrid", "Sevilla", "Madrid"],
    }
)

salario = df["salario"]
media_salario = float(salario.mean())

df["salario_k"] = df["salario"] / 1000
df_sorted = df.sort_values("salario", ascending=False)

resumen = (
    df.groupby("ciudad", as_index=False)
    .agg(
        salario_mean=("salario", "mean"),
        n=("salario", "size"),
    )
    .sort_values("salario_mean", ascending=False)
)

assert isinstance(media_salario, float)
assert "salario_k" in df.columns
assert df_sorted.iloc[0]["salario"] == df["salario"].max()
assert set(resumen.columns) == {"ciudad", "salario_mean", "n"}
```

  ---

  ### Ejercicio 1.2: Pandas - Limpieza (missing values, dtypes, duplicados)

  #### Enunciado

  1) **BÃ¡sico**

 - Crea un `DataFrame` con valores faltantes en `edad` y `salario`.
 - Cuenta cuÃ¡ntos nulos hay por columna.

 2) **Intermedio**

 - Rellena `edad` con la media.
 - Rellena `salario` con la mediana.

 3) **Avanzado**

 - Agrega una fila duplicada a propÃ³sito.
 - Elimina duplicados.
 - Convierte `edad` a `int` **despuÃ©s** de imputar.

 #### SoluciÃ³n

```python
import pandas as pd  # Pandas para limpieza: nulos, duplicados, casting de tipos
import numpy as np  # NumPy para utilidades numÃ©ricas y verificaciÃ³n robusta de dtype

df = pd.DataFrame(  # Crea un DataFrame con missing values (None) para simular datos reales â€œsuciosâ€
    {  # Diccionario: columnas -> listas
        "edad": [25, None, 30, 45, None],  # 'None' se interpretarÃ¡ como NaN (faltante) en una columna numÃ©rica
        "salario": [50000, 60000, None, 80000, 90000],  # Otro faltante en 'salario'
        "ciudad": ["Madrid", "Barcelona", "Madrid", "Sevilla", "Madrid"],  # Columna categÃ³rica sin nulos
    }  # Cierra el diccionario
)  # Cierra el constructor del DataFrame

nulls = df.isnull().sum()  # isnull() marca NaN/None; sum() por columna cuenta True => nÃºmero de nulos

df2 = df.copy()  # Copia explÃ­cita: evita mutar df (importante si df se reutiliza en otros pasos)
df2["edad"] = df2["edad"].fillna(df2["edad"].mean())  # Imputa edad con media (supone distribuciÃ³n â€œrazonableâ€)
df2["salario"] = df2["salario"].fillna(df2["salario"].median())  # Imputa salario con mediana (robusta a outliers)

df3 = pd.concat([df2, df2.iloc[[0]]], ignore_index=True)  # AÃ±ade una fila duplicada (la primera) para probar drop_duplicates
df3 = df3.drop_duplicates()  # Elimina filas duplicadas exactas (misma combinaciÃ³n de valores)
df3["edad"] = df3["edad"].round().astype(int)  # Convierte a int al final: redondea y castea (sin NaN ya)

assert nulls["edad"] == 2  # Debe haber 2 nulos originales en edad
assert nulls["salario"] == 1  # Debe haber 1 nulo original en salario
assert df2.isnull().sum().sum() == 0  # Tras imputaciÃ³n, no deben quedar nulos
assert len(df3) == len(df2)  # Agregar un duplicado y luego quitarlo deja el mismo tamaÃ±o
assert df3["edad"].dtype == np.int64 or str(df3["edad"].dtype).startswith("int")  # Verifica tipo entero
```

 ---

### Ejercicio 1.3: Pandas - SelecciÃ³n y filtrado (`loc`, `iloc`, boolean masks)

#### Enunciado

 Usa este `DataFrame`:

```python
import pandas as pd  # Importa Pandas para construir el DataFrame de ejemplo

df = pd.DataFrame(  # DataFrame pequeÃ±o (similar a Iris) para practicar selecciÃ³n/filtrado
    {  # Diccionario columna -> valores
        "sepal_length": [5.1, 4.9, 5.8, 6.0, 5.4],  # Feature numÃ©rica: longitud del sÃ©palo
        "sepal_width": [3.5, 3.0, 2.7, 2.2, 3.9],  # Feature numÃ©rica: ancho del sÃ©palo
        "species": ["setosa", "setosa", "versicolor", "virginica", "setosa"],  # Variable categÃ³rica: especie
    }  # Cierra diccionario
)  # Cierra DataFrame
```

1) **BÃ¡sico**

- Extrae las columnas `sepal_length` y `species`.

2) **Intermedio**

- Filtra solo las filas donde `species == "setosa"` y `sepal_length > 5.0`.

3) **Avanzado**

- Calcula el promedio de `sepal_length` por `species`.
- Devuelve el resultado ordenado de mayor a menor.

#### SoluciÃ³n

```python
import pandas as pd  # Pandas para DataFrames, mÃ¡scaras booleanas y groupby

df = pd.DataFrame(  # Re-crea el DataFrame del enunciado (datos en memoria)
    {  # Columnas definidas con listas de igual longitud
        "sepal_length": [5.1, 4.9, 5.8, 6.0, 5.4],  # Longitud del sÃ©palo
        "sepal_width": [3.5, 3.0, 2.7, 2.2, 3.9],  # Ancho del sÃ©palo
        "species": ["setosa", "setosa", "versicolor", "virginica", "setosa"],  # Clase (string)
    }  # Cierra diccionario
)  # Cierra DataFrame

subset = df[["sepal_length", "species"]]  # SelecciÃ³n de mÃºltiples columnas: devuelve DataFrame con 2 columnas

filtered = df[(df["species"] == "setosa") & (df["sepal_length"] > 5.0)]  # MÃ¡scara booleana: combina condiciones con &

means = (  # AgregaciÃ³n por especie para obtener promedios
    df.groupby("species", as_index=False)  # Agrupa por 'species' y conserva 'species' como columna
    .agg(sepal_length_mean=("sepal_length", "mean"))  # Media por grupo: una fila por especie
    .sort_values("sepal_length_mean", ascending=False)  # Ordena para tener ranking de especies por media
)  # Cierra pipeline

assert list(subset.columns) == ["sepal_length", "species"]  # Confirma columnas seleccionadas
assert (filtered["species"] == "setosa").all()  # Todas las filas filtradas deben ser setosa
assert (filtered["sepal_length"] > 5.0).all()  # Todas las filas filtradas deben cumplir sepal_length > 5
assert means.iloc[0]["sepal_length_mean"] >= means.iloc[-1]["sepal_length_mean"]  # Verifica el orden descendente
```

---

### Ejercicio 1.4: NumPy - Arrays y `dtype`

#### Enunciado

1) **BÃ¡sico**

- Crea:
  - un vector de 10 ceros
  - una matriz `3x3` de unos
  - una identidad `4x4`

2) **Intermedio**

- Crea un vector `v = np.array([1, 2, 3])`.
- Convierte `v` a `float64`.
- Verifica que `v / 2` produce floats.

3) **Avanzado**

- Reproduce el caso tÃ­pico de bug por `dtype` usando divisiÃ³n in-place:
  - crea `a = np.array([1, 2, 3])`
  - aplica `a /= 2`
  - explica el resultado con un `assert` esperado

#### SoluciÃ³n

```python
import numpy as np  # NumPy: base del cÃ³mputo numÃ©rico y estructuras tipo array

z = np.zeros(10)  # Crea un vector 1D de longitud 10 con ceros (dtype float por defecto)
ones = np.ones((3, 3))  # Crea una matriz 3x3 llena de unos (shape: (3, 3))
I = np.eye(4)  # Crea una matriz identidad 4x4 (1 en diagonal, 0 fuera)

v = np.array([1, 2, 3])  # Crea un array a partir de enteros (dtype tÃ­pico: int)
v_f = v.astype(np.float64)  # Convierte a float64: evita problemas de divisiÃ³n/overflow y habilita decimales

half = v_f / 2  # DivisiÃ³n â€œnormalâ€: al ser float, el resultado preserva decimales

a = np.array([1, 2, 3])  # Array entero: aquÃ­ preparamos el caso de bug
a /= 2  # DivisiÃ³n IN-PLACE: si el dtype es int, NumPy trunca/convierte (pierde decimales) para mantener dtype

assert z.shape == (10,)  # Confirma forma del vector
assert ones.shape == (3, 3)  # Confirma forma de la matriz
assert I.shape == (4, 4)  # Confirma forma de la identidad
assert v_f.dtype == np.float64  # Confirma que la conversiÃ³n a float64 ocurriÃ³
assert half.dtype == np.float64  # Confirma que la divisiÃ³n produce floats
assert np.array_equal(a, np.array([0, 1, 1]))  # 1/2->0, 2/2->1, 3/2->1 (truncado por dtype entero)
```

---

### Ejercicio 1.5: NumPy - Indexing y Slicing

#### Enunciado

Dada la matriz:

```python
import numpy as np
X = np.arange(20).reshape(4, 5)
```

1) **BÃ¡sico**

- Extrae el elemento en fila 2, columna 3.

2) **Intermedio**

- Extrae:
  - toda la fila 1
  - toda la columna 4
  - la submatriz filas 1â€“2, columnas 2â€“4

3) **Avanzado**

- Usa indexing booleano para extraer elementos mayores que 10.
- Verifica que todos los elementos del resultado cumplan `> 10`.

#### SoluciÃ³n

```python
import numpy as np  # Importa NumPy: base para trabajar con arrays y hacer slicing/indexing sin bucles

X = np.arange(20).reshape(4, 5)  # Crea 0..19 y lo reorganiza como matriz de 4 filas y 5 columnas

e = X[2, 3]  # Indexado 2D: elemento en fila=2 y columna=3 (Ã­ndices empiezan en 0)

row1 = X[1, :]  # Slicing: fila 1 completa; ':' significa â€œtodas las columnasâ€
col4 = X[:, 4]  # Slicing: columna 4 completa; ':' significa â€œtodas las filasâ€
sub = X[1:3, 2:5]  # Submatriz: filas 1â€“2 y columnas 2â€“4 (el extremo final del slice se excluye)

gt10 = X[X > 10]  # Indexado booleano: filtra elementos > 10; el resultado es un vector 1D

assert e == 13  # Verifica el valor esperado en la posiciÃ³n (2,3)
assert row1.shape == (5,)  # Una fila completa de una matriz (4,5) tiene 5 elementos
assert col4.shape == (4,)  # Una columna completa de una matriz (4,5) tiene 4 elementos
assert sub.shape == (2, 3)  # La submatriz seleccionada tiene 2 filas y 3 columnas
assert (gt10 > 10).all()  # Confirma que todos los elementos filtrados cumplen la condiciÃ³n
```

---

### Ejercicio 1.6: NumPy - Broadcasting

#### Enunciado

1) **BÃ¡sico**

- Sin loops, suma 100 a cada elemento de una matriz `3x3`.

2) **Intermedio**

- Dada una matriz `A` de shape `(4, 3)` y un vector `v` de shape `(3,)`, suma `v` a cada fila.

3) **Avanzado**

- Dado `X` de shape `(n, d)`, normaliza por columna: `X_norm = (X - mean) / (std + eps)`.
- **Importante:** el resultado debe conservar shape `(n, d)`.

#### SoluciÃ³n

```python
import numpy as np  # NumPy: permite operaciones vectorizadas y broadcasting sin bucles

M = np.arange(9).reshape(3, 3)  # Crea una matriz 3x3 con valores 0..8
M2 = M + 100  # Broadcasting con escalar: suma 100 a cada elemento (la forma no cambia)

A = np.arange(12).reshape(4, 3)  # Matriz (4,3) con valores 0..11
v = np.array([10, 20, 30])  # Vector (3,) alineado con las columnas: se sumarÃ¡ a cada fila
B = A + v  # Broadcasting: v se â€œexpandeâ€ a (4,3) virtualmente para sumar por filas

X = np.random.randn(100, 5)  # Datos sintÃ©ticos: 100 muestras (filas) y 5 features (columnas)
eps = 1e-8  # Epsilon: evita divisiÃ³n por cero o nÃºmeros extremadamente pequeÃ±os
mean = X.mean(axis=0)  # Media por columna (por feature) => shape (5,)
std = X.std(axis=0)  # DesviaciÃ³n estÃ¡ndar por columna => shape (5,)
X_norm = (X - mean) / (std + eps)  # Normaliza por feature usando broadcasting; conserva shape (100,5)

assert M2.shape == (3, 3)  # Sumar un escalar no cambia la forma
assert B.shape == (4, 3)  # Sumar un vector alineado a columnas no cambia la forma
assert X_norm.shape == (100, 5)  # La normalizaciÃ³n por columnas debe conservar (n,d)
```

---

### Ejercicio 1.7: NumPy - Producto matricial (`@`, `np.dot`, `np.matmul`)

#### Enunciado

1) **BÃ¡sico**

- Calcula `A @ B` con:
  - `A` de shape `(2, 3)`
  - `B` de shape `(3, 2)`

2) **Intermedio**

- Demuestra la diferencia entre:
  - multiplicaciÃ³n elemento a elemento `A * B`
  - producto matricial `A @ B`
  usando matrices cuadradas `2x2`.

3) **Avanzado**

- Implementa una predicciÃ³n lineal `y_hat = X @ w + b` con:
  - `X` shape `(n, d)`
  - `w` shape `(d,)`
  - `b` escalar
- Verifica el shape de `y_hat`.

#### SoluciÃ³n

```python
import numpy as np  # NumPy: operaciones vectorizadas y Ã¡lgebra lineal (producto matricial con @)

A = np.array([[1, 2, 3], [4, 5, 6]])  # Matriz A de shape (2,3)
B = np.array([[1, 0], [0, 1], [1, 1]])  # Matriz B de shape (3,2)
C = A @ B  # Producto matricial: (2,3)@(3,2) -> (2,2)

U = np.array([[1, 2], [3, 4]])  # Matriz 2x2 para contrastar Hadamard vs matmul
V = np.array([[10, 20], [30, 40]])  # Matriz 2x2
hadamard = U * V  # MultiplicaciÃ³n elemento a elemento (Hadamard)
matmul = U @ V  # Producto matricial (fila-columna)

X = np.random.randn(50, 3)  # Datos: 50 muestras (n) y 3 features (d)
w = np.array([0.1, -0.2, 0.3])  # Vector de pesos: shape (d,)
b = 0.5  # Bias escalar: se suma a cada predicciÃ³n por broadcasting
y_hat = X @ w + b  # PredicciÃ³n lineal: (n,d)@(d,) -> (n,)

assert C.shape == (2, 2)  # Verifica shape del producto matricial A@B
assert hadamard.shape == (2, 2)  # Hadamard mantiene shape
assert matmul.shape == (2, 2)  # Matmul entre 2x2 produce 2x2
assert y_hat.shape == (50,)  # Una predicciÃ³n por muestra
```

---

### Ejercicio 1.8: NumPy - `reshape`, `flatten`, `transpose`

#### Enunciado

1) **BÃ¡sico**

- Crea `a = np.arange(12)` y conviÃ©rtelo a una matriz `(3, 4)`.

2) **Intermedio**

- Transpone la matriz anterior y verifica el shape.

3) **Avanzado**

- Convierte la matriz `(3, 4)` a un tensor `(2, 2, 3)`.
- Vuelve a 1D y verifica que recuperas 12 elementos.

#### SoluciÃ³n

```python
import numpy as np  # NumPy para manipulaciÃ³n de shape y operaciones de reshape/transpose

a = np.arange(12)  # Vector 1D con 12 elementos (0..11)
M = a.reshape(3, 4)  # Reinterpreta como matriz (3,4); 3*4=12 debe coincidir
MT = M.T  # Transpuesta: intercambia ejes (3,4) -> (4,3)

T = a.reshape(2, 2, 3)  # Reinterpreta como tensor 3D (2,2,3); 2*2*3=12
flat = T.reshape(-1)  # Aplana a 1D; -1 indica â€œinfiera el tamaÃ±oâ€

assert M.shape == (3, 4)  # Verifica forma de la matriz
assert MT.shape == (4, 3)  # Verifica forma de la transpuesta
assert T.shape == (2, 2, 3)  # Verifica forma del tensor
assert flat.shape == (12,)  # Verifica que el aplanado recupera 12 elementos
assert np.array_equal(flat, a)  # Verifica que el contenido (y el orden) se conserva
```

---

### Ejercicio 1.9: NumPy - Agregaciones y `axis`

#### Enunciado

Sea:

```python
import numpy as np
X = np.array([[1, 2, 3], [4, 5, 6]])
```

1) **BÃ¡sico**

- Calcula `X.sum()` y verifica el resultado.

2) **Intermedio**

- Calcula `X.sum(axis=0)` y `X.sum(axis=1)`.
- Predice los shapes antes de ejecutar.

3) **Avanzado**

- Calcula `mean` por columna con `keepdims=True`.
- Resta esa media a `X` y verifica el shape del resultado.

#### SoluciÃ³n

```python
import numpy as np  # NumPy: agregaciones (sum/mean) y control de ejes con axis

X = np.array([[1, 2, 3], [4, 5, 6]])  # Matriz (2,3): 2 filas, 3 columnas

s_all = X.sum()  # Suma total de TODOS los elementos => escalar (sin axis)
s0 = X.sum(axis=0)  # axis=0: reduce filas -> suma por columna => shape (3,)
s1 = X.sum(axis=1)  # axis=1: reduce columnas -> suma por fila => shape (2,)

mu = X.mean(axis=0, keepdims=True)  # Media por columna; keepdims=True deja shape (1,3) para broadcasting explÃ­cito
X_centered = X - mu  # Centrado: resta la media de cada columna a cada fila (broadcasting)

assert s_all == 21  # 1+2+3+4+5+6 = 21
assert s0.shape == (3,)  # Una suma por columna
assert s1.shape == (2,)  # Una suma por fila
assert mu.shape == (1, 3)  # Con keepdims, la media conserva el eje reducido como dimensiÃ³n 1
assert X_centered.shape == (2, 3)  # Restar mu no debe cambiar la forma
assert np.allclose(X_centered.mean(axis=0), 0.0)  # Tras centrar, la media por columna debe ser ~0
```

---

### Ejercicio 1.10: NumPy - `random` y datos sintÃ©ticos

#### Enunciado

1) **BÃ¡sico**

- Fija una semilla y genera 5 nÃºmeros con `np.random.randn`.

2) **Intermedio**

- Genera un dataset sintÃ©tico para regresiÃ³n:
  - `X` de shape `(200, 2)`
  - `w_true` de shape `(2,)`
  - `y = X @ w_true + noise`

3) **Avanzado**

- Estandariza `X` por columna (`mean=0`, `std=1` aproximadamente).
- Verifica con `np.allclose` (tolerancia razonable).

#### SoluciÃ³n

```python
import numpy as np  # NumPy para aleatoriedad reproducible, datos sintÃ©ticos y estandarizaciÃ³n

np.random.seed(42)  # Semilla fija: garantiza reproducibilidad (mismos nÃºmeros aleatorios)
z = np.random.randn(5)  # Genera 5 valores ~ N(0,1) -> vector (5,)

n = 200  # NÃºmero de muestras
X = np.random.randn(n, 2)  # Features: matriz (200,2)
w_true = np.array([1.5, -0.7])  # Pesos verdaderos (ground truth) de la relaciÃ³n lineal
noise = 0.1 * np.random.randn(n)  # Ruido gaussiano pequeÃ±o para simular variaciÃ³n
y = X @ w_true + noise  # Targets: combinaciÃ³n lineal (X@w) + ruido -> vector (200,)

eps = 1e-8  # Epsilon: evita divisiÃ³n por cero (estabilidad numÃ©rica)
X_mean = X.mean(axis=0)  # Media por columna (por feature) -> (2,)
X_std = X.std(axis=0)  # DesviaciÃ³n estÃ¡ndar por columna -> (2,)
Xz = (X - X_mean) / (X_std + eps)  # Estandariza por columnas usando broadcasting -> (200,2)

assert z.shape == (5,)  # Confirma 5 valores
assert X.shape == (200, 2)  # Confirma shape del dataset
assert w_true.shape == (2,)  # Confirma shape de pesos
assert y.shape == (200,)  # Confirma un target por muestra
assert np.allclose(Xz.mean(axis=0), np.zeros(2), atol=1e-7)  # Media ~0 por feature
assert np.allclose(Xz.std(axis=0), np.ones(2), atol=1e-6)  # Std ~1 por feature
```

---

### (Bonus) Ejercicio 1.11: VectorizaciÃ³n + funciones de activaciÃ³n (dominio)

#### Enunciado

1) **VectorizaciÃ³n**

- Implementa distancia euclidiana sin loops.

2) **Activaciones**

- Implementa:
  - `sigmoid`
  - `relu`
  - `softmax` (estable numÃ©ricamente)

#### SoluciÃ³n

```python
import numpy as np  # NumPy: operaciones vectorizadas y funciones matemÃ¡ticas (exp, sqrt, sum)

def euclidean_distance_vectorized(a: np.ndarray, b: np.ndarray) -> float:
    diff = a - b  # Resta vectorizada: diferencia componente a componente
    return float(np.sqrt(np.sum(diff * diff)))  # Distancia L2: sqrt(sum((a-b)^2)); float() devuelve un escalar nativo

def sigmoid(x: np.ndarray) -> np.ndarray:
    return 1 / (1 + np.exp(-x))  # Sigmoid: 1/(1+exp(-x)), mapea R -> (0,1) elemento a elemento

def relu(x: np.ndarray) -> np.ndarray:
    return np.maximum(0, x)  # ReLU: max(0,x), anula valores negativos y deja positivos

def softmax(x: np.ndarray) -> np.ndarray:
    x = np.asarray(x)  # Asegura np.ndarray (por si llega lista) para operaciones vectorizadas
    x_shift = x - np.max(x)  # Estabilidad numÃ©rica: resta el mÃ¡ximo para evitar overflow en exp
    exps = np.exp(x_shift)  # Exponencial elemento a elemento (estable tras el shift)
    return exps / np.sum(exps)  # Softmax: normaliza a probabilidades (la suma debe ser 1)

a = np.array([1.0, 2.0, 3.0])  # Vector de prueba
b = np.array([1.0, 1.0, 1.0])  # Segundo vector de prueba
d = euclidean_distance_vectorized(a, b)  # Distancia euclidiana sin loops

assert np.isclose(d, np.sqrt(0**2 + 1**2 + 2**2))  # Chequeo: sqrt((0)^2+(1)^2+(2)^2)
assert np.isclose(sigmoid(np.array([0.0]))[0], 0.5)  # Propiedad clave: sigmoid(0)=0.5
assert relu(np.array([-5.0, 5.0])).tolist() == [0.0, 5.0]  # ReLU anula negativos y deja positivos
assert np.isclose(softmax(np.array([1.0, 2.0, 3.0])).sum(), 1.0)  # Softmax debe sumar 1
```

---

## ğŸ“¦ Entregable del MÃ³dulo

### Script: `benchmark_vectorization.py`

```python
"""
Benchmark: Operaciones vectoriales Lista vs NumPy

Este script compara el rendimiento de operaciones comunes
usando listas de Python puras vs arrays de NumPy.

Operaciones comparadas:
1. Producto punto
2. NormalizaciÃ³n
3. Distancia euclidiana
4. Suma de matrices

Autor: [Tu nombre]
Fecha: [Fecha]
"""

import numpy as np  # NumPy: operaciones vectorizadas (arrays), Ã¡lgebra lineal y cÃ³mputo eficiente
import time  # time.time(): mediciÃ³n simple de tiempo (en segundos) para benchmarks
from typing import List, Tuple, Callable  # Tipos: anotar listas, tuplas de argumentos y funciones â€œcallablesâ€
from dataclasses import dataclass  # dataclass: genera automÃ¡ticamente __init__ y facilita structs de resultados


@dataclass  # Marca la clase como dataclass: simplifica almacenamiento de resultados
class BenchmarkResult:  # Estructura para guardar un resultado de benchmark de forma consistente
    """Resultado de un benchmark."""
    operation: str  # Nombre de la operaciÃ³n evaluada (p.ej. "Producto Punto")
    time_list: float  # Tiempo promedio por iteraciÃ³n usando listas (segundos)
    time_numpy: float  # Tiempo promedio por iteraciÃ³n usando NumPy (segundos)
    speedup: float  # AceleraciÃ³n: time_list / time_numpy


def benchmark(
    func_list: Callable,  # ImplementaciÃ³n â€œcon listasâ€ (mÃ¡s cercana a Python puro)
    func_numpy: Callable,  # ImplementaciÃ³n â€œcon NumPyâ€ (vectorizada/optimizada)
    args_list: Tuple,  # Argumentos posicionales para func_list (se expanden con *)
    args_numpy: Tuple,  # Argumentos posicionales para func_numpy
    operation_name: str,  # Nombre legible para imprimir/reportar
    iterations: int = 100  # CuÃ¡ntas repeticiones para promediar (reduce ruido)
) -> BenchmarkResult:
    """Ejecuta benchmark comparativo."""

    # Benchmark lista
    start = time.time()  # Tiempo inicial (lista)
    for _ in range(iterations):  # Repite para promediar y obtener una medida mÃ¡s estable
        func_list(*args_list)  # Llama la funciÃ³n de listas expandiendo la tupla de argumentos
    time_list = (time.time() - start) / iterations  # Tiempo promedio por iteraciÃ³n (lista)

    # Benchmark NumPy
    start = time.time()  # Tiempo inicial (NumPy)
    for _ in range(iterations):  # Misma cantidad de iteraciones para comparar â€œjustoâ€
        func_numpy(*args_numpy)  # Llama la funciÃ³n NumPy expandiendo sus argumentos
    time_numpy = (time.time() - start) / iterations  # Tiempo promedio por iteraciÃ³n (NumPy)

    return BenchmarkResult(  # Empaqueta resultados en un objeto con campos con nombre
        operation=operation_name,  # Nombre de la operaciÃ³n
        time_list=time_list,  # Tiempo promedio con listas
        time_numpy=time_numpy,  # Tiempo promedio con NumPy
        speedup=time_list / time_numpy  # Speedup: cuÃ¡ntas veces NumPy es mÃ¡s rÃ¡pido que listas
    )


# === IMPLEMENTAR TUS FUNCIONES AQUÃ ===

def dot_product_list(a: List[float], b: List[float]) -> float:
    """Producto punto con listas."""
    # TODO: Implementar el producto punto sum(a_i * b_i) recorriendo ambas listas
    pass


def dot_product_numpy(a: np.ndarray, b: np.ndarray) -> float:
    """Producto punto con NumPy."""
    # TODO: Implementar usando np.dot(a, b) (o a @ b si son 1D)
    pass


def normalize_list(data: List[float]) -> List[float]:
    """Normalizar con listas."""
    # TODO: Implementar (x - mean) / std calculando mean y std manualmente (Python puro)
    pass


def normalize_numpy(data: np.ndarray) -> np.ndarray:
    """Normalizar con NumPy."""
    # TODO: Implementar (data - data.mean()) / data.std() de forma vectorizada
    pass


def euclidean_distance_list(a: List[float], b: List[float]) -> float:
    """Distancia euclidiana con listas."""
    # TODO: Implementar sqrt(sum((a_i - b_i)^2)) recorriendo ambas listas
    pass


def euclidean_distance_numpy(a: np.ndarray, b: np.ndarray) -> float:
    """Distancia euclidiana con NumPy."""
    # TODO: Implementar usando vectorizaciÃ³n: np.sqrt(np.sum((a-b)**2)) o np.linalg.norm(a-b)
    pass


def matrix_sum_list(A: List[List[float]], B: List[List[float]]) -> List[List[float]]:
    """Suma de matrices con listas."""
    # TODO: Implementar suma elemento a elemento usando loops (filas/columnas)
    pass


def matrix_sum_numpy(A: np.ndarray, B: np.ndarray) -> np.ndarray:
    """Suma de matrices con NumPy."""
    # TODO: Implementar A + B (broadcasting/operaciÃ³n vectorizada)
    pass


def main():
    """Ejecutar todos los benchmarks."""
    size = 10000  # TamaÃ±o de los vectores para las pruebas (no tan grande para que corra rÃ¡pido)

    # Crear datos de prueba
    list_a = [float(i) for i in range(size)]  # Vector (lista) de floats: 0..size-1
    list_b = [float(i) for i in range(size)]  # Segundo vector (lista) del mismo tamaÃ±o
    array_a = np.array(list_a)  # VersiÃ³n NumPy del vector (ndarray)
    array_b = np.array(list_b)  # VersiÃ³n NumPy del segundo vector

    matrix_size = 100  # TamaÃ±o de matrices cuadradas (100x100) para prueba de suma de matrices
    list_matrix_a = [[float(i*j) for j in range(matrix_size)]  # Construye matriz A con listas (filas)
                     for i in range(matrix_size)]  # Cada fila i contiene productos i*j
    list_matrix_b = [[float(i+j) for j in range(matrix_size)]  # Construye matriz B con listas
                     for i in range(matrix_size)]  # Cada fila i contiene sumas i+j
    array_matrix_a = np.array(list_matrix_a)  # Convierte matriz A a ndarray (vectorizado)
    array_matrix_b = np.array(list_matrix_b)  # Convierte matriz B a ndarray

    # Ejecutar benchmarks
    results = []  # Acumulador de BenchmarkResult (uno por operaciÃ³n)

    results.append(benchmark(  # Ejecuta y guarda benchmark del producto punto
        dot_product_list, dot_product_numpy,
        (list_a, list_b), (array_a, array_b),
        "Producto Punto"
    ))

    results.append(benchmark(  # Ejecuta y guarda benchmark de normalizaciÃ³n
        normalize_list, normalize_numpy,
        (list_a,), (array_a,),
        "NormalizaciÃ³n"
    ))

    results.append(benchmark(  # Ejecuta y guarda benchmark de distancia euclidiana
        euclidean_distance_list, euclidean_distance_numpy,
        (list_a, list_b), (array_a, array_b),
        "Distancia Euclidiana"
    ))

    results.append(benchmark(  # Ejecuta y guarda benchmark de suma de matrices
        matrix_sum_list, matrix_sum_numpy,
        (list_matrix_a, list_matrix_b), (array_matrix_a, array_matrix_b),
        "Suma de Matrices"
    ))

    # Mostrar resultados
    print("\n" + "="*60)  # Separador visual
    print("BENCHMARK: Lista vs NumPy")  # TÃ­tulo del reporte
    print("="*60)  # Separador visual
    print(f"{'OperaciÃ³n':<25} {'Lista (ms)':<12} {'NumPy (ms)':<12} {'Speedup':<10}")  # Encabezado de tabla
    print("-"*60)  # Separador para la tabla

    for r in results:  # Itera sobre resultados de cada operaciÃ³n
        print(f"{r.operation:<25} {r.time_list*1000:<12.4f} {r.time_numpy*1000:<12.4f} {r.speedup:<10.1f}x")  # Convierte s->ms

    print("="*60)  # Cierre de la tabla
    print(f"\nSpeedup promedio: {sum(r.speedup for r in results)/len(results):.1f}x")  # Promedio de speedups


if __name__ == "__main__":
    main()  # Punto de entrada: ejecuta benchmarks al correr el script
```

---

## ğŸ› Debugging NumPy: Errores que te HarÃ¡n Perder el Tiempo (v3.2)

> âš ï¸ **CRÃTICO:** Estos 5 errores son los mÃ¡s frecuentes en las Fases 1 y 2. Resolverlos ahora previene horas de frustraciÃ³n.

### Error 1: Shape Mismatch - `(5,)` vs `(5,1)`

```python
import numpy as np  # Importa NumPy para crear arrays y analizar shapes (dimensiones)

# PROBLEMA: Vector 1D vs Vector Columna
v1 = np.array([1, 2, 3, 4, 5])      # Shape: (5,) - Vector 1D (una sola dimensiÃ³n)
v2 = np.array([[1], [2], [3], [4], [5]])  # Shape: (5, 1) - Vector columna (matriz de 5 filas y 1 columna)

print(f"v1.shape: {v1.shape}")  # Imprime el shape real de v1 para confirmar que es (5,)
print(f"v2.shape: {v2.shape}")  # Imprime el shape real de v2 para confirmar que es (5, 1)

# ESTO FALLA en RegresiÃ³n Lineal:
# Si X tiene shape (100, 5) y theta tiene shape (5,), el resultado es (100,)
# Si theta tiene shape (5, 1), el resultado es (100, 1)

# SOLUCIÃ“N: Usar reshape o keepdims
v1_columna = v1.reshape(-1, 1)  # Convierte (5,) â†’ (5,1); -1 infiere automÃ¡ticamente el nÃºmero de filas
v1_columna_alt = v1[:, np.newaxis]  # Alternativa: inserta un eje nuevo para obtener un vector columna

# REGLA: Para ML, los vectores de features deben ser (n, 1), no (n,)
```

### Error 2: Broadcasting Silencioso Incorrecto

```python
import numpy as np  # Importa NumPy para generar datos y demostrar cÃ³mo axis afecta agregaciones/broadcasting

# PROBLEMA: Broadcasting no falla, pero da resultados incorrectos
X = np.random.randn(100, 5)  # 100 samples, 5 features
mean_wrong = np.mean(X)      # Â¡INCORRECTO! Media global: colapsa todos los ejes y devuelve un escalar
mean_correct = np.mean(X, axis=0)  # Correcto: media por feature (columna) => shape (5,)

print(f"mean_wrong shape: {np.array(mean_wrong).shape}")  # () - escalar (sin dimensiones)
print(f"mean_correct shape: {mean_correct.shape}")  # (5,) - un valor por columna

# REGLA: Siempre especifica axis= en agregaciones
# axis=0: opera sobre filas (resultado por columna)
# axis=1: opera sobre columnas (resultado por fila)
```

### Error 3: ModificaciÃ³n In-Place Inesperada

```python
import numpy as np  # Importa NumPy para mostrar la diferencia entre vistas (views) y copias (.copy())

# PROBLEMA: Los slices de NumPy son VISTAS, no copias
original = np.array([1, 2, 3, 4, 5])  # Array original
slice_view = original[1:4]  # Slice: por defecto es una vista al mismo buffer de memoria
slice_view[0] = 999  # Modifica la vista; por ser vista, tambiÃ©n modifica el array original

print(original)  # [1, 999, 3, 4, 5] - Â¡ORIGINAL MODIFICADO! porque slice_view comparte memoria

# SOLUCIÃ“N: Usar .copy() explÃ­citamente
original = np.array([1, 2, 3, 4, 5])  # Reinicia el array original
slice_copy = original[1:4].copy()  # copy(): crea un nuevo buffer independiente
slice_copy[0] = 999  # Modifica la copia; NO afecta el original

print(original)  # [1, 2, 3, 4, 5] - Original intacto porque slice_copy no comparte memoria
```

### Error 4: DivisiÃ³n por Cero en NormalizaciÃ³n

```python
import numpy as np  # Importa NumPy para ejemplificar el caso std=0 y cÃ³mo estabilizar divisiones con epsilon

# PROBLEMA: DivisiÃ³n por cero cuando std = 0
data = np.array([5, 5, 5, 5, 5])
std = np.std(data)  # 0.0 porque todos los valores son idÃ©nticos (varianza cero)
normalized = (data - np.mean(data)) / std  # RuntimeWarning: divide by zero (divisiÃ³n por 0)

# SOLUCIÃ“N: AÃ±adir epsilon
epsilon = 1e-8
normalized_safe = (data - np.mean(data)) / (std + epsilon)  # Evita divisiÃ³n por cero y estabiliza el cÃ¡lculo

# REGLA: Siempre usar epsilon en divisiones (especialmente en softmax, normalizaciones)
```

### Error 5: Tipos de Datos Incorrectos

```python
import numpy as np  # Importa NumPy para demostrar problemas de dtype (int vs float) en operaciones in-place

# PROBLEMA: Operaciones con int cuando necesitas float
a = np.array([1, 2, 3])  # dtype: int64 (enteros)
b = a / 2  # dtype: float64 (OK): en Python 3 la divisiÃ³n / produce float

# PERO en operaciones in-place:
a = np.array([1, 2, 3])
a /= 2  # In-place: intenta guardar floats en int64 => trunca (pierde decimales)
print(a)  # [0, 1, 1] - Â¡TRUNCADO! por conversiÃ³n implÃ­cita a entero

# SOLUCIÃ“N: Especificar dtype al crear
a = np.array([1, 2, 3], dtype=np.float64)
a /= 2  # Ahora sÃ­: al ser float64, conserva decimales en la operaciÃ³n in-place
print(a)  # [0.5, 1.0, 1.5] - Correcto (sin truncamiento)

# REGLA: Para ML, siempre usar dtype=np.float64 o np.float32
```

---

## ğŸ› ï¸ EstÃ¡ndares de CÃ³digo Profesional (v3.2)

> ğŸ’ **FilosofÃ­a v3.2:** El cÃ³digo no se considera terminado hasta que pase `mypy`, `ruff` y `pytest`.

### ConfiguraciÃ³n del Entorno Profesional

```bash
# Crear entorno virtual
python -m venv .venv  # Crea un entorno virtual local (aislado) en la carpeta .venv
source .venv/bin/activate  # Activa el entorno virtual en Linux/Mac (usa el Python y pip de .venv)
# .venv\Scripts\activate   # Alternativa en Windows para activar el entorno virtual

# Instalar herramientas de calidad
pip install numpy pandas matplotlib  # Instala dependencias principales de ciencia de datos
pip install mypy ruff pytest  # Instala herramientas de calidad: tipos, lint/format y tests

# Archivo pyproject.toml (crear en la raÃ­z del proyecto)
```

```toml
# pyproject.toml
[tool.mypy]
python_version = "3.11"  # VersiÃ³n de Python objetivo para el anÃ¡lisis de tipos
warn_return_any = true  # Advierte cuando una funciÃ³n retorna Any (pÃ©rdida de precisiÃ³n de tipos)
warn_unused_ignores = true  # Advierte si hay "# type: ignore" que no son necesarios
disallow_untyped_defs = true  # Exige anotaciones de tipo en funciones (evita defs sin typing)

[tool.ruff]
line-length = 100  # Longitud mÃ¡xima de lÃ­nea para lint/format
select = ["E", "F", "W", "I", "UP"]  # Conjunto de reglas: estilo, errores, imports, modernizaciÃ³n

[tool.pytest.ini_options]
testpaths = ["tests"]  # Carpeta donde pytest buscarÃ¡ tests por defecto
python_files = "test_*.py"  # PatrÃ³n de archivos que pytest considera como tests
```

### Ejemplo: CÃ³digo con Type Hints

```python
# src/linear_algebra.py
"""Operaciones de Ã¡lgebra lineal desde cero."""
import numpy as np  # NumPy para operaciones vectorizadas (sum, sqrt) sobre arrays
from numpy.typing import NDArray  # Tipado: NDArray permite anotar ndarrays con dtype para mypy


def dot_product(a: NDArray[np.float64], b: NDArray[np.float64]) -> float:
    """
    Calcula el producto punto de dos vectores.

    Args:
        a: Primer vector (n,)
        b: Segundo vector (n,)

    Returns:
        El producto punto (escalar)

    Raises:
        ValueError: Si los vectores tienen shapes diferentes
    """
    if a.shape != b.shape:  # ValidaciÃ³n: el producto punto requiere vectores del mismo tamaÃ±o
        raise ValueError(f"Shapes incompatibles: {a.shape} vs {b.shape}")  # Falla explÃ­citamente con mensaje Ãºtil
    return float(np.sum(a * b))  # Multiplica elemento a elemento y suma; float() convierte escalar NumPy a float nativo


def norm_l2(v: NDArray[np.float64]) -> float:
    """Calcula la norma L2 (euclidiana) de un vector."""
    return float(np.sqrt(np.sum(v ** 2)))  # sqrt(sum(v^2)): definiciÃ³n de norma L2
```

### Ejemplo: Tests con pytest

```python
# tests/test_linear_algebra.py
"""Tests unitarios para linear_algebra.py"""
import numpy as np  # NumPy para construir vectores de prueba
import pytest  # pytest para asserts avanzados y verificaciÃ³n de excepciones
from src.linear_algebra import dot_product, norm_l2  # Funciones bajo prueba


class TestDotProduct:
    """Tests para la funciÃ³n dot_product."""

    def test_dot_product_basic(self) -> None:
        """Test bÃ¡sico: [1,2,3] Â· [4,5,6] = 32"""
        a = np.array([1.0, 2.0, 3.0])  # Primer vector
        b = np.array([4.0, 5.0, 6.0])  # Segundo vector
        assert dot_product(a, b) == 32.0  # Verifica 1*4 + 2*5 + 3*6

    def test_dot_product_orthogonal(self) -> None:
        """Vectores ortogonales tienen producto punto = 0"""
        a = np.array([1.0, 0.0])  # Vector unitario en x
        b = np.array([0.0, 1.0])  # Vector unitario en y
        assert dot_product(a, b) == 0.0  # Ortogonales => producto punto 0

    def test_dot_product_shape_mismatch(self) -> None:
        """Debe lanzar ValueError si shapes no coinciden"""
        a = np.array([1.0, 2.0])  # Shape (2,)
        b = np.array([1.0, 2.0, 3.0])  # Shape (3,)
        with pytest.raises(ValueError):  # Espera una excepciÃ³n por shapes incompatibles
            dot_product(a, b)  # Debe fallar (validaciÃ³n de shapes)


class TestNormL2:
    """Tests para la funciÃ³n norm_l2."""

    def test_norm_unit_vector(self) -> None:
        """Vector unitario tiene norma 1"""
        v = np.array([1.0, 0.0, 0.0])  # Vector unitario en 3D
        assert norm_l2(v) == 1.0  # Norma de un vector unitario es 1

    def test_norm_345(self) -> None:
        """TriÃ¡ngulo 3-4-5: norma de [3,4] = 5"""
        v = np.array([3.0, 4.0])  # Vector (3,4)
        assert norm_l2(v) == 5.0  # sqrt(3^2 + 4^2) = 5
```

### Comandos de VerificaciÃ³n

```bash
# Ejecutar en la raÃ­z del proyecto:

# 1. Verificar tipos (mypy)
mypy src/  # Revisa anotaciones de tipo y detecta inconsistencias en src/

# 2. Verificar estilo (ruff)
ruff check src/  # Lint: encuentra errores comunes (imports, variables no usadas, estilo)
ruff format src/  # Auto-formatea el cÃ³digo segÃºn reglas de estilo

# 3. Ejecutar tests (pytest)
pytest tests/ -v  # Ejecuta los tests en modo verboso

# 4. Todo junto (antes de cada commit)
mypy src/ && ruff check src/ && pytest tests/ -v  # Pipeline mÃ­nimo de calidad antes de commitear
```

---

## ğŸ¯ El Reto del Tablero Blanco (MetodologÃ­a Feynman)

> ğŸ“ **InstrucciÃ³n:** DespuÃ©s de implementar cÃ³digo, debes poder explicar el algoritmo en **mÃ¡ximo 5 lÃ­neas** sin usar jerga tÃ©cnica. Si no puedes, vuelve a la teorÃ­a.

### Ejemplo: Broadcasting

**âŒ ExplicaciÃ³n tÃ©cnica (mala):**
"Broadcasting es la capacidad de NumPy de realizar operaciones elemento a elemento entre arrays de diferentes shapes mediante la expansiÃ³n implÃ­cita de dimensiones segÃºn reglas de compatibilidad."

**âœ… ExplicaciÃ³n Feynman (buena):**
"Cuando sumas un nÃºmero a una lista, NumPy automÃ¡ticamente suma ese nÃºmero a CADA elemento. Es como si el nÃºmero se 'copiara' para que tenga el mismo tamaÃ±o que la lista. Lo mismo pasa entre listas de diferentes tamaÃ±os, siempre que una de ellas tenga tamaÃ±o 1 en alguna dimensiÃ³n."

### Tu Reto para el MÃ³dulo 01:

Explica en 5 lÃ­neas o menos:
1. Â¿Por quÃ© NumPy es mÃ¡s rÃ¡pido que listas de Python?
2. Â¿QuÃ© significa `axis=0` vs `axis=1`?
3. Â¿Por quÃ© `.copy()` es importante?

---

## âœ… Checklist de FinalizaciÃ³n (v3.2)

### Conocimiento
- [ ] Puedo crear arrays 1D, 2D y 3D con NumPy
- [ ] Entiendo indexing y slicing de arrays
- [ ] Puedo explicar broadcasting y usarlo
- [ ] SÃ© calcular agregaciones por eje (axis)
- [ ] Puedo reescribir loops como operaciones vectorizadas
- [ ] Conozco las diferencias entre `@`, `np.dot`, `np.matmul`
- [ ] Conozco los 5 errores comunes de NumPy y sus soluciones

### Entregables de CÃ³digo
- [ ] `benchmark_vectorization.py` implementado
- [ ] El speedup de NumPy vs lista es >50x en mis pruebas
- [ ] `mypy src/` pasa sin errores
- [ ] `ruff check src/` pasa sin errores
- [ ] Al menos 3 tests con `pytest` pasando

### MetodologÃ­a Feynman
- [ ] Puedo explicar broadcasting en 5 lÃ­neas sin jerga
- [ ] Puedo explicar axis=0 vs axis=1 en 5 lÃ­neas sin jerga
- [ ] Puedo explicar por quÃ© .copy() es importante

---

## ğŸ”— NavegaciÃ³n

| Anterior | Ãndice | Siguiente |
|----------|--------|-----------|
| - | [00_INDICE](00_INDICE.md) | [02_ALGEBRA_LINEAL_ML](02_ALGEBRA_LINEAL_ML.md) |
