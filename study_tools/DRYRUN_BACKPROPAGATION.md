# âœï¸ Dry Run en Papel: Backpropagation

> **Semana 18 - Obligatorio**: Hacer un paso de backprop completo a mano ANTES de codificar.
> El cÃ³digo saldrÃ¡ bien a la primera si la matemÃ¡tica en papel estÃ¡ clara.

---

## ğŸ¯ Objetivo

Antes de escribir una sola lÃ­nea de cÃ³digo de backpropagation:
1. Dibujar el grafo computacional
2. Calcular el forward pass con nÃºmeros simples
3. Calcular el backward pass a mano
4. Verificar que las dimensiones son correctas

---

## ğŸ“ Ejercicio 1: Red Neuronal MÃ­nima (2 neuronas)

### Arquitectura
```
Input â†’ [Neurona 1] â†’ [Neurona 2] â†’ Output â†’ Loss
  x         zâ‚â†’aâ‚         zâ‚‚â†’Å·        L
```

### ParÃ¡metros
- **Input**: x = 2
- **Pesos**: wâ‚ = 0.5, wâ‚‚ = 0.3
- **Bias**: bâ‚ = 0.1, bâ‚‚ = 0.2
- **ActivaciÃ³n**: ReLU
- **Target**: y = 1
- **Loss**: MSE = (Å· - y)Â²

---

### Paso 1: Dibujar Grafo Computacional

```
    x=2
     â”‚
     â–¼
    [Ã—]â—„â”€â”€ wâ‚=0.5
     â”‚
     â–¼
    [+]â—„â”€â”€ bâ‚=0.1
     â”‚
     â–¼
   zâ‚=1.1
     â”‚
     â–¼
  [ReLU]
     â”‚
     â–¼
   aâ‚=1.1
     â”‚
     â–¼
    [Ã—]â—„â”€â”€ wâ‚‚=0.3
     â”‚
     â–¼
    [+]â—„â”€â”€ bâ‚‚=0.2
     â”‚
     â–¼
   zâ‚‚=0.53
     â”‚
     â–¼
   Å·=0.53
     â”‚
     â–¼
    [-]â—„â”€â”€ y=1
     â”‚
     â–¼
    [Â²]
     â”‚
     â–¼
  L=0.2209
```

---

### Paso 2: Forward Pass (Calcular cada nodo)

| Paso | OperaciÃ³n | CÃ¡lculo | Resultado |
|------|-----------|---------|-----------|
| 1 | zâ‚ = xÂ·wâ‚ + bâ‚ | 2Â·0.5 + 0.1 | **zâ‚ = 1.1** |
| 2 | aâ‚ = ReLU(zâ‚) | max(0, 1.1) | **aâ‚ = 1.1** |
| 3 | zâ‚‚ = aâ‚Â·wâ‚‚ + bâ‚‚ | 1.1Â·0.3 + 0.2 | **zâ‚‚ = 0.53** |
| 4 | Å· = zâ‚‚ | (sin activaciÃ³n final) | **Å· = 0.53** |
| 5 | L = (Å· - y)Â² | (0.53 - 1)Â² | **L = 0.2209** |

---

### Paso 3: Backward Pass (Regla de la Cadena)

**Objetivo**: Calcular âˆ‚L/âˆ‚wâ‚, âˆ‚L/âˆ‚wâ‚‚, âˆ‚L/âˆ‚bâ‚, âˆ‚L/âˆ‚bâ‚‚

#### Derivadas de cada operaciÃ³n:

| OperaciÃ³n | Derivada local |
|-----------|----------------|
| L = (Å· - y)Â² | âˆ‚L/âˆ‚Å· = 2(Å· - y) |
| Å· = zâ‚‚ | âˆ‚Å·/âˆ‚zâ‚‚ = 1 |
| zâ‚‚ = aâ‚Â·wâ‚‚ + bâ‚‚ | âˆ‚zâ‚‚/âˆ‚wâ‚‚ = aâ‚, âˆ‚zâ‚‚/âˆ‚bâ‚‚ = 1, âˆ‚zâ‚‚/âˆ‚aâ‚ = wâ‚‚ |
| aâ‚ = ReLU(zâ‚) | âˆ‚aâ‚/âˆ‚zâ‚ = 1 si zâ‚ > 0, sino 0 |
| zâ‚ = xÂ·wâ‚ + bâ‚ | âˆ‚zâ‚/âˆ‚wâ‚ = x, âˆ‚zâ‚/âˆ‚bâ‚ = 1 |

#### CÃ¡lculo paso a paso:

```
PASO ATRÃS 1: âˆ‚L/âˆ‚Å·
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âˆ‚L/âˆ‚Å· = 2(Å· - y) = 2(0.53 - 1) = 2(-0.47) = -0.94

PASO ATRÃS 2: âˆ‚L/âˆ‚zâ‚‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âˆ‚L/âˆ‚zâ‚‚ = âˆ‚L/âˆ‚Å· Â· âˆ‚Å·/âˆ‚zâ‚‚ = -0.94 Â· 1 = -0.94

PASO ATRÃS 3: âˆ‚L/âˆ‚wâ‚‚ y âˆ‚L/âˆ‚bâ‚‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âˆ‚L/âˆ‚wâ‚‚ = âˆ‚L/âˆ‚zâ‚‚ Â· âˆ‚zâ‚‚/âˆ‚wâ‚‚ = -0.94 Â· aâ‚ = -0.94 Â· 1.1 = -1.034
âˆ‚L/âˆ‚bâ‚‚ = âˆ‚L/âˆ‚zâ‚‚ Â· âˆ‚zâ‚‚/âˆ‚bâ‚‚ = -0.94 Â· 1 = -0.94

PASO ATRÃS 4: âˆ‚L/âˆ‚aâ‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âˆ‚L/âˆ‚aâ‚ = âˆ‚L/âˆ‚zâ‚‚ Â· âˆ‚zâ‚‚/âˆ‚aâ‚ = -0.94 Â· wâ‚‚ = -0.94 Â· 0.3 = -0.282

PASO ATRÃS 5: âˆ‚L/âˆ‚zâ‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âˆ‚L/âˆ‚zâ‚ = âˆ‚L/âˆ‚aâ‚ Â· âˆ‚aâ‚/âˆ‚zâ‚
Como zâ‚ = 1.1 > 0, âˆ‚aâ‚/âˆ‚zâ‚ = 1
âˆ‚L/âˆ‚zâ‚ = -0.282 Â· 1 = -0.282

PASO ATRÃS 6: âˆ‚L/âˆ‚wâ‚ y âˆ‚L/âˆ‚bâ‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âˆ‚L/âˆ‚wâ‚ = âˆ‚L/âˆ‚zâ‚ Â· âˆ‚zâ‚/âˆ‚wâ‚ = -0.282 Â· x = -0.282 Â· 2 = -0.564
âˆ‚L/âˆ‚bâ‚ = âˆ‚L/âˆ‚zâ‚ Â· âˆ‚zâ‚/âˆ‚bâ‚ = -0.282 Â· 1 = -0.282
```

---

### Paso 4: Resumen de Gradientes

| ParÃ¡metro | Gradiente | InterpretaciÃ³n |
|-----------|-----------|----------------|
| âˆ‚L/âˆ‚wâ‚‚ | -1.034 | Aumentar wâ‚‚ reduce el loss |
| âˆ‚L/âˆ‚bâ‚‚ | -0.94 | Aumentar bâ‚‚ reduce el loss |
| âˆ‚L/âˆ‚wâ‚ | -0.564 | Aumentar wâ‚ reduce el loss |
| âˆ‚L/âˆ‚bâ‚ | -0.282 | Aumentar bâ‚ reduce el loss |

---

### Paso 5: ActualizaciÃ³n de Pesos (SGD, lr=0.1)

```
wâ‚‚_new = wâ‚‚ - lr Â· âˆ‚L/âˆ‚wâ‚‚ = 0.3 - 0.1Â·(-1.034) = 0.3 + 0.1034 = 0.4034
bâ‚‚_new = bâ‚‚ - lr Â· âˆ‚L/âˆ‚bâ‚‚ = 0.2 - 0.1Â·(-0.94) = 0.2 + 0.094 = 0.294
wâ‚_new = wâ‚ - lr Â· âˆ‚L/âˆ‚wâ‚ = 0.5 - 0.1Â·(-0.564) = 0.5 + 0.0564 = 0.5564
bâ‚_new = bâ‚ - lr Â· âˆ‚L/âˆ‚bâ‚ = 0.1 - 0.1Â·(-0.282) = 0.1 + 0.0282 = 0.1282
```

---

## ğŸ“ Ejercicio 2: Red con 2 Inputs (Verificar Dimensiones)

### Arquitectura
```
x = [xâ‚, xâ‚‚] â†’ Capa 1 (2â†’2) â†’ ReLU â†’ Capa 2 (2â†’1) â†’ Å· â†’ Loss
```

### Dimensiones

| Variable | Shape | Notas |
|----------|-------|-------|
| X | (1, 2) | 1 ejemplo, 2 features |
| Wâ‚ | (2, 2) | 2 inputs â†’ 2 neuronas |
| bâ‚ | (1, 2) | Un bias por neurona |
| Zâ‚ = XWâ‚ + bâ‚ | (1, 2) | (1,2)@(2,2) + (1,2) |
| Aâ‚ = ReLU(Zâ‚) | (1, 2) | Misma shape que Zâ‚ |
| Wâ‚‚ | (2, 1) | 2 inputs â†’ 1 output |
| bâ‚‚ | (1, 1) | Un bias |
| Zâ‚‚ = Aâ‚Wâ‚‚ + bâ‚‚ | (1, 1) | (1,2)@(2,1) + (1,1) |
| Å· | (1, 1) | Un escalar |
| y | (1, 1) | Target |
| L | () | Escalar (sin shape) |

### Dimensiones de Gradientes (CRÃTICO)

| Gradiente | Shape | FÃ³rmula de VerificaciÃ³n |
|-----------|-------|-------------------------|
| âˆ‚L/âˆ‚Å· | (1, 1) | Misma shape que Å· |
| âˆ‚L/âˆ‚Zâ‚‚ | (1, 1) | Misma shape que Zâ‚‚ |
| âˆ‚L/âˆ‚Wâ‚‚ | (2, 1) | **Misma shape que Wâ‚‚** |
| âˆ‚L/âˆ‚bâ‚‚ | (1, 1) | Misma shape que bâ‚‚ |
| âˆ‚L/âˆ‚Aâ‚ | (1, 2) | Misma shape que Aâ‚ |
| âˆ‚L/âˆ‚Zâ‚ | (1, 2) | Misma shape que Zâ‚ |
| âˆ‚L/âˆ‚Wâ‚ | (2, 2) | **Misma shape que Wâ‚** |
| âˆ‚L/âˆ‚bâ‚ | (1, 2) | Misma shape que bâ‚ |

### Regla de Oro
> **El gradiente de un parÃ¡metro SIEMPRE tiene la misma shape que el parÃ¡metro.**

---

## ğŸ“ Plantilla en Blanco para Tus Ejercicios

### Datos del Problema
```
Input: x = ___
Pesos capa 1: Wâ‚ = ___
Bias capa 1: bâ‚ = ___
ActivaciÃ³n: ___
Pesos capa 2: Wâ‚‚ = ___
Bias capa 2: bâ‚‚ = ___
Target: y = ___
Loss function: ___
```

### Forward Pass
| Paso | OperaciÃ³n | CÃ¡lculo | Resultado |
|------|-----------|---------|-----------|
| 1 | zâ‚ = | | |
| 2 | aâ‚ = | | |
| 3 | zâ‚‚ = | | |
| 4 | Å· = | | |
| 5 | L = | | |

### Backward Pass
| Paso | Gradiente | CÃ¡lculo | Resultado |
|------|-----------|---------|-----------|
| 1 | âˆ‚L/âˆ‚Å· = | | |
| 2 | âˆ‚L/âˆ‚zâ‚‚ = | | |
| 3 | âˆ‚L/âˆ‚wâ‚‚ = | | |
| 4 | âˆ‚L/âˆ‚bâ‚‚ = | | |
| 5 | âˆ‚L/âˆ‚aâ‚ = | | |
| 6 | âˆ‚L/âˆ‚zâ‚ = | | |
| 7 | âˆ‚L/âˆ‚wâ‚ = | | |
| 8 | âˆ‚L/âˆ‚bâ‚ = | | |

### VerificaciÃ³n de Dimensiones
| ParÃ¡metro | Shape | âˆ‚L/âˆ‚param Shape | âœ“/âœ— |
|-----------|-------|-----------------|-----|
| Wâ‚ | | | |
| bâ‚ | | | |
| Wâ‚‚ | | | |
| bâ‚‚ | | | |

---

## ğŸ§ª CÃ³digo de VerificaciÃ³n

DespuÃ©s de hacer el dry run en papel, verifica con este cÃ³digo:

```python
import numpy as np

def verificar_backprop_manual():
    """Verifica los cÃ¡lculos manuales del Ejercicio 1."""
    
    # Datos
    x = 2.0
    w1, b1 = 0.5, 0.1
    w2, b2 = 0.3, 0.2
    y = 1.0
    
    # Forward
    z1 = x * w1 + b1
    a1 = max(0, z1)  # ReLU
    z2 = a1 * w2 + b2
    y_hat = z2
    L = (y_hat - y) ** 2
    
    print("=== FORWARD PASS ===")
    print(f"z1 = {z1:.4f}")
    print(f"a1 = {a1:.4f}")
    print(f"z2 = {z2:.4f}")
    print(f"Å· = {y_hat:.4f}")
    print(f"L = {L:.4f}")
    
    # Backward
    dL_dy_hat = 2 * (y_hat - y)
    dL_dz2 = dL_dy_hat * 1
    dL_dw2 = dL_dz2 * a1
    dL_db2 = dL_dz2 * 1
    dL_da1 = dL_dz2 * w2
    dL_dz1 = dL_da1 * (1 if z1 > 0 else 0)  # ReLU derivative
    dL_dw1 = dL_dz1 * x
    dL_db1 = dL_dz1 * 1
    
    print("\n=== BACKWARD PASS ===")
    print(f"âˆ‚L/âˆ‚Å· = {dL_dy_hat:.4f}")
    print(f"âˆ‚L/âˆ‚zâ‚‚ = {dL_dz2:.4f}")
    print(f"âˆ‚L/âˆ‚wâ‚‚ = {dL_dw2:.4f}")
    print(f"âˆ‚L/âˆ‚bâ‚‚ = {dL_db2:.4f}")
    print(f"âˆ‚L/âˆ‚aâ‚ = {dL_da1:.4f}")
    print(f"âˆ‚L/âˆ‚zâ‚ = {dL_dz1:.4f}")
    print(f"âˆ‚L/âˆ‚wâ‚ = {dL_dw1:.4f}")
    print(f"âˆ‚L/âˆ‚bâ‚ = {dL_db1:.4f}")
    
    # VerificaciÃ³n con numerical gradient
    eps = 1e-5
    
    def compute_loss(x, w1, b1, w2, b2, y):
        z1 = x * w1 + b1
        a1 = max(0, z1)
        z2 = a1 * w2 + b2
        return (z2 - y) ** 2
    
    numerical_dw1 = (compute_loss(x, w1+eps, b1, w2, b2, y) - 
                    compute_loss(x, w1-eps, b1, w2, b2, y)) / (2*eps)
    
    print(f"\n=== VERIFICACIÃ“N NUMÃ‰RICA ===")
    print(f"âˆ‚L/âˆ‚wâ‚ analÃ­tico: {dL_dw1:.6f}")
    print(f"âˆ‚L/âˆ‚wâ‚ numÃ©rico:  {numerical_dw1:.6f}")
    print(f"Diferencia: {abs(dL_dw1 - numerical_dw1):.2e}")

verificar_backprop_manual()
```

---

## âœ… Checklist Antes de Codificar

- [ ] DibujÃ© el grafo computacional completo
- [ ] CalculÃ© forward pass con nÃºmeros de ejemplo
- [ ] CalculÃ© backward pass paso a paso
- [ ] VerifiquÃ© que cada gradiente tiene la shape correcta
- [ ] Los gradientes de parÃ¡metros tienen la misma shape que los parÃ¡metros
- [ ] VerifiquÃ© con numerical gradients (eps = 1e-5)

---

## ğŸš« Errores Comunes a Evitar

1. **Olvidar la derivada de ReLU**: Es 0 cuando z â‰¤ 0
2. **Confundir shapes en matmul**: (m,n) @ (n,p) = (m,p)
3. **No transponer correctamente**: âˆ‚L/âˆ‚W = X.T @ âˆ‚L/âˆ‚Z
4. **Sumar gradientes en batch**: dL/db = sum(dL/dz, axis=0)
5. **Olvidar el 2 en MSE**: âˆ‚(Å·-y)Â²/âˆ‚Å· = **2**(Å·-y)
