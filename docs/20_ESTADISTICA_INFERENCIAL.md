# MÃ³dulo 05 - EstadÃ­stica Inferencial para IA

> **ğŸ¯ Objetivo:** Dominar estimaciÃ³n, pruebas de hipÃ³tesis e inferencia  
> **â­ PATHWAY LÃNEA 2:** Statistical Estimation for Data Science and AI

---

## ğŸ§  AnalogÃ­a: Inferir la PoblaciÃ³n desde la Muestra

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                             â”‚
â”‚   INFERENCIA ESTADÃSTICA                                                    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                    â”‚
â”‚                                                                             â”‚
â”‚   POBLACIÃ“N (desconocida) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚   â€¢ ParÃ¡metros verdaderos: Î¼, ÏƒÂ², Î¸                                         â”‚
â”‚   â€¢ Imposible medir todos los individuos                                    â”‚
â”‚                                                                             â”‚
â”‚               â†“ Muestreo                                                    â”‚
â”‚                                                                             â”‚
â”‚   MUESTRA (observada) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚   â€¢ EstadÃ­sticos: xÌ„, sÂ², Î¸Ì‚                                                  â”‚
â”‚   â€¢ n observaciones                                                         â”‚
â”‚                                                                             â”‚
â”‚               â†“ Inferencia                                                  â”‚
â”‚                                                                             â”‚
â”‚   ESTIMACIÃ“N â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚   â€¢ Punto: Î¸Ì‚ â‰ˆ Î¸                                                            â”‚
â”‚   â€¢ Intervalo: [Î¸Ì‚ - error, Î¸Ì‚ + error] contiene Î¸ con 95% confianza          â”‚
â”‚                                                                             â”‚
â”‚   APLICACIÃ“N EN ML:                                                         â”‚
â”‚   â€¢ Train set = muestra                                                     â”‚
â”‚   â€¢ Performance en test = estimaciÃ³n de performance real                    â”‚
â”‚   â€¢ Cross-validation = reducir varianza de la estimaciÃ³n                    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Contenido

1. [EstimaciÃ³n Puntual](#1-estimacion-puntual)
2. [Maximum Likelihood Estimation (MLE)](#2-mle)
3. [Maximum A Posteriori (MAP)](#3-map)
4. [Intervalos de Confianza](#4-intervalos)
5. [Pruebas de HipÃ³tesis](#5-hipotesis)
6. [RegresiÃ³n EstadÃ­stica](#6-regresion)

---

## 1. EstimaciÃ³n Puntual {#1-estimacion-puntual}

### 1.1 Propiedades de Buenos Estimadores

```
PROPIEDADES DESEABLES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. INSESGADO (Unbiased):
   E[Î¸Ì‚] = Î¸
   El estimador acierta "en promedio"

2. CONSISTENTE:
   Î¸Ì‚ â†’ Î¸ cuando n â†’ âˆ
   Mejora con mÃ¡s datos

3. EFICIENTE:
   MÃ­nima varianza entre estimadores insesgados
   Menor incertidumbre

SESGO-VARIANZA TRADE-OFF (crucial para ML):
â€¢ Sesgo alto â†’ underfitting (modelo muy simple)
â€¢ Varianza alta â†’ overfitting (modelo muy complejo)
â€¢ Objetivo: minimizar error total = sesgoÂ² + varianza
```

### 1.2 Estimadores Comunes

```python
from typing import List
import math

def sample_mean(data: List[float]) -> float:
    """Unbiased estimator of population mean.
    
    E[xÌ„] = Î¼
    """
    return sum(data) / len(data)


def sample_variance_unbiased(data: List[float]) -> float:
    """Unbiased estimator of population variance.
    
    Uses n-1 (Bessel's correction) for unbiasedness.
    E[sÂ²] = ÏƒÂ²
    """
    n = len(data)
    mean = sample_mean(data)
    return sum((x - mean) ** 2 for x in data) / (n - 1)


def sample_variance_mle(data: List[float]) -> float:
    """MLE estimator of variance (biased but consistent).
    
    Uses n instead of n-1.
    """
    n = len(data)
    mean = sample_mean(data)
    return sum((x - mean) ** 2 for x in data) / n


def standard_error(data: List[float]) -> float:
    """Standard error of the mean: SE = s / âˆšn.
    
    Measures uncertainty in our estimate of the mean.
    """
    return math.sqrt(sample_variance_unbiased(data) / len(data))
```

---

## 2. Maximum Likelihood Estimation (MLE) {#2-mle}

### 2.1 Concepto

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                             â”‚
â”‚   MAXIMUM LIKELIHOOD ESTIMATION                                             â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚
â”‚                                                                             â”‚
â”‚   Pregunta: Â¿QuÃ© parÃ¡metros Î¸ hacen MÃS PROBABLE los datos observados?      â”‚
â”‚                                                                             â”‚
â”‚   Likelihood: L(Î¸|data) = P(data|Î¸)                                         â”‚
â”‚                                                                             â”‚
â”‚   MLE: Î¸Ì‚_MLE = argmax L(Î¸|data)                                             â”‚
â”‚                    Î¸                                                        â”‚
â”‚                                                                             â”‚
â”‚   PrÃ¡ctica: maximizar log-likelihood (mÃ¡s estable numÃ©ricamente):           â”‚
â”‚   Î¸Ì‚_MLE = argmax log L(Î¸|data)                                              â”‚
â”‚                    Î¸                                                        â”‚
â”‚                                                                             â”‚
â”‚   EJEMPLO - Moneda sesgada:                                                 â”‚
â”‚   Datos: 7 caras en 10 lanzamientos                                         â”‚
â”‚   L(p) = C(10,7) Ã— p^7 Ã— (1-p)^3                                            â”‚
â”‚   MLE: pÌ‚ = 7/10 = 0.7                                                       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 MLE para Distribuciones Comunes

```python
def mle_bernoulli(successes: int, trials: int) -> float:
    """MLE for Bernoulli parameter p.
    
    pÌ‚_MLE = number of successes / number of trials
    
    Example:
        >>> mle_bernoulli(7, 10)
        0.7
    """
    return successes / trials


def mle_normal_mean(data: List[float]) -> float:
    """MLE for normal distribution mean.
    
    Î¼Ì‚_MLE = sample mean
    """
    return sample_mean(data)


def mle_normal_variance(data: List[float]) -> float:
    """MLE for normal distribution variance.
    
    Note: This is BIASED (uses n, not n-1).
    ÏƒÌ‚Â²_MLE = (1/n) Î£(xáµ¢ - xÌ„)Â²
    """
    return sample_variance_mle(data)


def mle_poisson(data: List[int]) -> float:
    """MLE for Poisson rate parameter Î».
    
    Î»Ì‚_MLE = sample mean
    """
    return sum(data) / len(data)
```

### 2.3 MLE con Gradient Descent (Logistic Regression)

```python
def sigmoid(z: float) -> float:
    """Logistic sigmoid function."""
    if z < -500:  # Prevent overflow
        return 0.0
    elif z > 500:
        return 1.0
    return 1.0 / (1.0 + math.exp(-z))


def log_likelihood_logistic(
    X: List[List[float]], 
    y: List[int], 
    weights: List[float]
) -> float:
    """Log-likelihood for logistic regression.
    
    â„“(w) = Î£ [yáµ¢ log(Ïƒ(wáµ€xáµ¢)) + (1-yáµ¢) log(1-Ïƒ(wáµ€xáµ¢))]
    """
    ll = 0.0
    for xi, yi in zip(X, y):
        z = sum(w * x for w, x in zip(weights, xi))
        p = sigmoid(z)
        # Avoid log(0)
        p = max(min(p, 1 - 1e-15), 1e-15)
        ll += yi * math.log(p) + (1 - yi) * math.log(1 - p)
    return ll


def gradient_log_likelihood(
    X: List[List[float]], 
    y: List[int], 
    weights: List[float]
) -> List[float]:
    """Gradient of log-likelihood for logistic regression.
    
    âˆ‚â„“/âˆ‚wâ±¼ = Î£ (yáµ¢ - Ïƒ(wáµ€xáµ¢)) Ã— xáµ¢â±¼
    """
    n_features = len(weights)
    gradient = [0.0] * n_features
    
    for xi, yi in zip(X, y):
        z = sum(w * x for w, x in zip(weights, xi))
        p = sigmoid(z)
        error = yi - p
        for j in range(n_features):
            gradient[j] += error * xi[j]
    
    return gradient
```

---

## 3. Maximum A Posteriori (MAP) {#3-map}

### 3.1 Concepto: MLE + Prior

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                             â”‚
â”‚   MAXIMUM A POSTERIORI (MAP)                                                â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚
â”‚                                                                             â”‚
â”‚   MLE: Solo usa datos                                                       â”‚
â”‚   Î¸Ì‚_MLE = argmax P(data|Î¸)                                                  â”‚
â”‚                                                                             â”‚
â”‚   MAP: Incorpora conocimiento previo (prior)                                â”‚
â”‚   Î¸Ì‚_MAP = argmax P(Î¸|data) = argmax P(data|Î¸) Ã— P(Î¸)                        â”‚
â”‚                                                                             â”‚
â”‚   Usando Bayes:                                                             â”‚
â”‚   P(Î¸|data) âˆ P(data|Î¸) Ã— P(Î¸)                                              â”‚
â”‚   posterior âˆ likelihood Ã— prior                                            â”‚
â”‚                                                                             â”‚
â”‚   RELACIÃ“N CON REGULARIZACIÃ“N:                                              â”‚
â”‚   â€¢ Prior Gaussiano â†’ L2 regularization (Ridge)                             â”‚
â”‚   â€¢ Prior Laplaciano â†’ L1 regularization (Lasso)                            â”‚
â”‚                                                                             â”‚
â”‚   Â¿CUÃNDO USAR MAP vs MLE?                                                  â”‚
â”‚   â€¢ Datos abundantes: MLE â‰ˆ MAP (prior se vuelve irrelevante)               â”‚
â”‚   â€¢ Datos escasos: MAP mÃ¡s estable (prior regulariza)                       â”‚
â”‚   â€¢ Conocimiento previo: MAP permite incorporarlo                           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Ejemplo: MAP con Prior Gaussiano

```python
def map_with_gaussian_prior(
    data: List[float], 
    prior_mean: float, 
    prior_variance: float,
    likelihood_variance: float
) -> float:
    """MAP estimate for normal mean with Gaussian prior.
    
    Conjugate prior: Normal prior + Normal likelihood = Normal posterior
    
    Î¸Ì‚_MAP = (n/ÏƒÂ² Ã— xÌ„ + 1/Ï„Â² Ã— Î¼â‚€) / (n/ÏƒÂ² + 1/Ï„Â²)
    
    where:
    - xÌ„: sample mean
    - n: sample size
    - ÏƒÂ²: likelihood variance
    - Î¼â‚€: prior mean
    - Ï„Â²: prior variance
    """
    n = len(data)
    sample_mean_val = sample_mean(data)
    
    precision_likelihood = n / likelihood_variance
    precision_prior = 1 / prior_variance
    
    numerator = precision_likelihood * sample_mean_val + precision_prior * prior_mean
    denominator = precision_likelihood + precision_prior
    
    return numerator / denominator
```

---

## 4. Intervalos de Confianza {#4-intervalos}

### 4.1 Concepto

```
INTERVALO DE CONFIANZA:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Un intervalo [a, b] tal que:
P(a â‰¤ Î¸ â‰¤ b) = 1 - Î±

Para 95% de confianza: Î± = 0.05

INTERPRETACIÃ“N CORRECTA:
Si repitiÃ©ramos el experimento muchas veces,
95% de los intervalos construidos contendrÃ­an Î¸.

INTERPRETACIÃ“N INCORRECTA:
"Hay 95% de probabilidad de que Î¸ estÃ© en [a,b]"
(Î¸ es fijo, no aleatorio en estadÃ­stica frecuentista)
```

### 4.2 Intervalo para la Media

```python
def confidence_interval_mean(
    data: List[float], 
    confidence: float = 0.95
) -> tuple[float, float]:
    """Confidence interval for population mean.
    
    Assumes large sample (n > 30) using Normal approximation.
    For small samples, use t-distribution.
    
    CI = xÌ„ Â± z* Ã— (s / âˆšn)
    
    Example:
        >>> data = [23, 25, 27, 29, 31]
        >>> confidence_interval_mean(data, 0.95)
        (23.5..., 30.5...)
    """
    n = len(data)
    mean = sample_mean(data)
    se = standard_error(data)
    
    # z* values for common confidence levels
    z_values = {
        0.90: 1.645,
        0.95: 1.96,
        0.99: 2.576
    }
    z = z_values.get(confidence, 1.96)
    
    margin = z * se
    return (mean - margin, mean + margin)


def confidence_interval_proportion(
    successes: int, 
    trials: int, 
    confidence: float = 0.95
) -> tuple[float, float]:
    """Confidence interval for population proportion.
    
    Uses normal approximation (valid when np > 5 and n(1-p) > 5).
    
    CI = pÌ‚ Â± z* Ã— âˆš(pÌ‚(1-pÌ‚)/n)
    """
    p_hat = successes / trials
    se = math.sqrt(p_hat * (1 - p_hat) / trials)
    
    z_values = {0.90: 1.645, 0.95: 1.96, 0.99: 2.576}
    z = z_values.get(confidence, 1.96)
    
    margin = z * se
    return (p_hat - margin, p_hat + margin)
```

---

## 5. Pruebas de HipÃ³tesis {#5-hipotesis}

### 5.1 Framework

```
ESTRUCTURA DE UNA PRUEBA:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. HIPÃ“TESIS NULA (Hâ‚€): Lo que asumimos es verdad
   "No hay efecto" / "No hay diferencia"

2. HIPÃ“TESIS ALTERNATIVA (Hâ‚): Lo que queremos probar
   "Hay efecto" / "Hay diferencia"

3. ESTADÃSTICO DE PRUEBA: Resume los datos

4. P-VALUE: P(observar datos tan extremos | Hâ‚€ es verdad)
   p < Î± â†’ Rechazar Hâ‚€

5. DECISIÃ“N:
   â€¢ p < 0.05 â†’ "EstadÃ­sticamente significativo"
   â€¢ p â‰¥ 0.05 â†’ "No hay evidencia suficiente"

TIPOS DE ERROR:
â€¢ Tipo I (Î±): Rechazar Hâ‚€ cuando es verdadera (falso positivo)
â€¢ Tipo II (Î²): No rechazar Hâ‚€ cuando es falsa (falso negativo)
â€¢ Power = 1 - Î²: Probabilidad de detectar efecto real
```

### 5.2 Z-Test para la Media

```python
def z_test_one_sample(
    data: List[float], 
    population_mean: float, 
    population_std: float,
    alternative: str = "two-sided"
) -> tuple[float, float]:
    """One-sample Z-test for population mean.
    
    Hâ‚€: Î¼ = Î¼â‚€
    Hâ‚: Î¼ â‰  Î¼â‚€ (two-sided) / Î¼ > Î¼â‚€ (greater) / Î¼ < Î¼â‚€ (less)
    
    Returns:
        z_statistic, p_value
    """
    n = len(data)
    x_bar = sample_mean(data)
    
    z = (x_bar - population_mean) / (population_std / math.sqrt(n))
    
    # Calculate p-value using standard normal CDF approximation
    if alternative == "two-sided":
        p_value = 2 * (1 - standard_normal_cdf_approx(abs(z)))
    elif alternative == "greater":
        p_value = 1 - standard_normal_cdf_approx(z)
    else:  # less
        p_value = standard_normal_cdf_approx(z)
    
    return z, p_value


def standard_normal_cdf_approx(x: float) -> float:
    """Approximation of standard normal CDF."""
    return 0.5 * (1 + math.erf(x / math.sqrt(2)))
```

### 5.3 T-Test (Muestras PequeÃ±as)

```python
def t_test_two_sample(
    group1: List[float], 
    group2: List[float]
) -> tuple[float, float]:
    """Two-sample t-test (Welch's t-test).
    
    Tests if two groups have different means.
    Does not assume equal variances.
    
    Hâ‚€: Î¼â‚ = Î¼â‚‚
    Hâ‚: Î¼â‚ â‰  Î¼â‚‚
    """
    n1, n2 = len(group1), len(group2)
    mean1, mean2 = sample_mean(group1), sample_mean(group2)
    var1 = sample_variance_unbiased(group1)
    var2 = sample_variance_unbiased(group2)
    
    # Welch's t-statistic
    se = math.sqrt(var1/n1 + var2/n2)
    t_stat = (mean1 - mean2) / se
    
    # Welch-Satterthwaite degrees of freedom
    num = (var1/n1 + var2/n2) ** 2
    denom = (var1/n1)**2/(n1-1) + (var2/n2)**2/(n2-1)
    df = num / denom
    
    # Approximate p-value (would need t-distribution for exact)
    # For large df, t approaches normal
    p_value = 2 * (1 - standard_normal_cdf_approx(abs(t_stat)))
    
    return t_stat, p_value
```

### 5.4 Chi-Square Test (Datos CategÃ³ricos)

```python
def chi_square_test(
    observed: List[int], 
    expected: List[float]
) -> tuple[float, int]:
    """Chi-square goodness of fit test.
    
    Tests if observed frequencies match expected.
    
    Ï‡Â² = Î£ (O - E)Â² / E
    
    Returns:
        chi_square_statistic, degrees_of_freedom
    """
    chi_sq = sum(
        (o - e) ** 2 / e 
        for o, e in zip(observed, expected)
    )
    df = len(observed) - 1
    
    return chi_sq, df
```

---

## 6. RegresiÃ³n EstadÃ­stica {#6-regresion}

### 6.1 RegresiÃ³n Lineal Simple

```python
def linear_regression_ols(
    X: List[float], 
    y: List[float]
) -> tuple[float, float]:
    """Ordinary Least Squares linear regression.
    
    y = Î²â‚€ + Î²â‚x + Îµ
    
    Minimizes Î£(yáµ¢ - Å·áµ¢)Â²
    
    Returns:
        intercept (Î²â‚€), slope (Î²â‚)
    """
    n = len(X)
    mean_x = sum(X) / n
    mean_y = sum(y) / n
    
    # Slope: Î²â‚ = Cov(x,y) / Var(x)
    numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(X, y))
    denominator = sum((xi - mean_x) ** 2 for xi in X)
    
    slope = numerator / denominator
    intercept = mean_y - slope * mean_x
    
    return intercept, slope


def r_squared(y_true: List[float], y_pred: List[float]) -> float:
    """Coefficient of determination (RÂ²).
    
    RÂ² = 1 - SS_res / SS_tot
    
    Proportion of variance explained by the model.
    0 â‰¤ RÂ² â‰¤ 1 (for linear regression with intercept)
    """
    mean_y = sum(y_true) / len(y_true)
    
    ss_tot = sum((yi - mean_y) ** 2 for yi in y_true)
    ss_res = sum((yt - yp) ** 2 for yt, yp in zip(y_true, y_pred))
    
    return 1 - ss_res / ss_tot
```

### 6.2 RegresiÃ³n Lineal MÃºltiple (Forma Matricial)

```python
def matrix_multiply(A: List[List[float]], B: List[List[float]]) -> List[List[float]]:
    """Matrix multiplication A Ã— B."""
    rows_a, cols_a = len(A), len(A[0])
    cols_b = len(B[0])
    
    result = [[0.0] * cols_b for _ in range(rows_a)]
    
    for i in range(rows_a):
        for j in range(cols_b):
            for k in range(cols_a):
                result[i][j] += A[i][k] * B[k][j]
    
    return result


def transpose(A: List[List[float]]) -> List[List[float]]:
    """Matrix transpose."""
    return [[A[j][i] for j in range(len(A))] for i in range(len(A[0]))]


# Note: Full OLS requires matrix inversion
# Î² = (Xáµ€X)â»Â¹ Xáµ€y
# In practice, use numerical libraries (numpy.linalg.lstsq)
```

---

## âš ï¸ ConexiÃ³n con Machine Learning

```
ESTADÃSTICA â†’ MACHINE LEARNING:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â€¢ MLE â†’ Training neural networks (minimize cross-entropy)
â€¢ MAP â†’ Regularization (L1/L2 penalties)
â€¢ Hypothesis testing â†’ Model comparison, A/B testing
â€¢ Confidence intervals â†’ Uncertainty quantification
â€¢ Bias-variance â†’ Model selection, regularization tuning

DIFERENCIAS DE ENFOQUE:
â€¢ EstadÃ­stica: explicar, inferir sobre parÃ¡metros
â€¢ ML: predecir, generalizar a nuevos datos

Pero los fundamentos matemÃ¡ticos son los MISMOS.
```

---

## ğŸ”§ Ejercicios PrÃ¡cticos

### Ejercicio 20.1: MLE para Datos Reales
Dado un dataset de tiempos de respuesta, estimar Î» de distribuciÃ³n exponencial.

### Ejercicio 20.2: A/B Testing
Implementar prueba de proporciones para comparar dos versiones.

### Ejercicio 20.3: RegresiÃ³n con RegularizaciÃ³n
Comparar OLS vs Ridge (MAP con prior gaussiano).

---

## ğŸ“š Recursos Externos

| Recurso | Tipo | Prioridad |
|---------|------|-----------|
| [Statistical Learning](https://www.statlearning.com/) | Libro (gratis) | ğŸ”´ Obligatorio |
| [Seeing Theory](https://seeing-theory.brown.edu/) | Interactivo | ğŸŸ¡ Recomendado |
| [StatQuest: Statistics](https://www.youtube.com/playlist?list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9) | Videos | ğŸ”´ Obligatorio |

---

## ğŸ§­ NavegaciÃ³n

| â† Anterior | Ãndice | Siguiente â†’ |
|------------|--------|-------------|
| [19_PROBABILIDAD_FUNDAMENTOS](19_PROBABILIDAD_FUNDAMENTOS.md) | [00_INDICE](00_INDICE.md) | [21_CADENAS_MARKOV_MONTECARLO](21_CADENAS_MARKOV_MONTECARLO.md) |
