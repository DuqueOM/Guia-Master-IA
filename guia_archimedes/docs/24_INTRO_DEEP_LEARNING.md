# M√≥dulo 09 - Introducci√≥n al Deep Learning

> **üéØ Objetivo:** Dominar fundamentos de redes neuronales y backpropagation  
> **‚≠ê PATHWAY L√çNEA 1:** Introduction to Deep Learning

---

## üß† Analog√≠a: El Cerebro Artificial

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ   RED NEURONAL = Funciones Compuestas que Aprenden                          ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                          ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   NEURONA BIOL√ìGICA:          NEURONA ARTIFICIAL:                           ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                          ‚îÇ
‚îÇ   Dendritas ‚Üí Soma ‚Üí Ax√≥n     Inputs ‚Üí Œ£(wx+b) ‚Üí Activaci√≥n ‚Üí Output        ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ        x‚ÇÅ ‚îÄ‚îÄw‚ÇÅ‚îÄ‚îÄ‚îê                                                           ‚îÇ
‚îÇ                 ‚îÇ                                                           ‚îÇ
‚îÇ        x‚ÇÇ ‚îÄ‚îÄw‚ÇÇ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∂ Œ£ ‚îÄ‚îÄ‚ñ∂ f(z) ‚îÄ‚îÄ‚ñ∂ y                                     ‚îÇ
‚îÇ                 ‚îÇ    (suma)  (activ)                                        ‚îÇ
‚îÇ        x‚ÇÉ ‚îÄ‚îÄw‚ÇÉ‚îÄ‚îÄ‚îò                                                           ‚îÇ
‚îÇ               +b                                                            ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   z = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + w‚ÇÉx‚ÇÉ + b                                                ‚îÇ
‚îÇ   y = f(z)                                                                  ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   ¬øPOR QU√â "PROFUNDO"?                                                      ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                    ‚îÇ
‚îÇ   M√∫ltiples capas permiten aprender representaciones jer√°rquicas:           ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Capa 1: Bordes, texturas                                                  ‚îÇ
‚îÇ   Capa 2: Formas simples                                                    ‚îÇ
‚îÇ   Capa 3: Partes de objetos                                                 ‚îÇ
‚îÇ   Capa N: Conceptos complejos                                               ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã Contenido

1. [Perceptr√≥n y Neurona](#1-perceptron)
2. [Funciones de Activaci√≥n](#2-activaciones)
3. [Redes Multicapa (MLP)](#3-mlp)
4. [Backpropagation](#4-backpropagation)
5. [Optimizaci√≥n y Regularizaci√≥n](#5-optimizacion)
6. [Arquitecturas Especiales (CNN, RNN)](#6-arquitecturas)

---

## 1. Perceptr√≥n y Neurona {#1-perceptron}

### 1.1 Perceptr√≥n Simple

```python
from typing import List, Tuple
import math
import random

class Perceptron:
    """Single layer perceptron (binary classifier).
    
    The simplest neural network: one neuron.
    Can only learn linearly separable patterns.
    
    Model: y = sign(w¬∑x + b)
    """
    
    def __init__(self, n_features: int, learning_rate: float = 0.01):
        self.lr = learning_rate
        self.weights = [random.uniform(-1, 1) for _ in range(n_features)]
        self.bias = random.uniform(-1, 1)
    
    def predict_one(self, x: List[float]) -> int:
        """Predict for single sample."""
        z = sum(w * xi for w, xi in zip(self.weights, x)) + self.bias
        return 1 if z >= 0 else 0
    
    def predict(self, X: List[List[float]]) -> List[int]:
        """Predict for multiple samples."""
        return [self.predict_one(x) for x in X]
    
    def fit(self, X: List[List[float]], y: List[int], 
            epochs: int = 100) -> 'Perceptron':
        """Train perceptron using perceptron learning rule.
        
        Update rule: w = w + lr √ó (y - ≈∑) √ó x
        Only updates when prediction is wrong.
        """
        for _ in range(epochs):
            errors = 0
            for xi, yi in zip(X, y):
                y_pred = self.predict_one(xi)
                error = yi - y_pred
                
                if error != 0:
                    errors += 1
                    for j in range(len(self.weights)):
                        self.weights[j] += self.lr * error * xi[j]
                    self.bias += self.lr * error
            
            if errors == 0:
                break  # Converged
        
        return self
```

### 1.2 Neurona con Activaci√≥n Continua

```python
class Neuron:
    """Single neuron with continuous activation.
    
    More expressive than perceptron.
    Can use different activation functions.
    """
    
    def __init__(
        self, 
        n_inputs: int, 
        activation: str = 'sigmoid'
    ):
        self.weights = [random.gauss(0, 0.1) for _ in range(n_inputs)]
        self.bias = 0.0
        self.activation = activation
        
        # For backprop
        self.last_input: List[float] = []
        self.last_z: float = 0.0
        self.last_output: float = 0.0
    
    def _activate(self, z: float) -> float:
        """Apply activation function."""
        if self.activation == 'sigmoid':
            if z < -500:
                return 0.0
            elif z > 500:
                return 1.0
            return 1.0 / (1.0 + math.exp(-z))
        elif self.activation == 'relu':
            return max(0, z)
        elif self.activation == 'tanh':
            return math.tanh(z)
        elif self.activation == 'linear':
            return z
        else:
            raise ValueError(f"Unknown activation: {self.activation}")
    
    def _activation_derivative(self, z: float) -> float:
        """Derivative of activation function."""
        if self.activation == 'sigmoid':
            s = self._activate(z)
            return s * (1 - s)
        elif self.activation == 'relu':
            return 1.0 if z > 0 else 0.0
        elif self.activation == 'tanh':
            return 1 - math.tanh(z) ** 2
        elif self.activation == 'linear':
            return 1.0
        else:
            raise ValueError(f"Unknown activation: {self.activation}")
    
    def forward(self, x: List[float]) -> float:
        """Forward pass."""
        self.last_input = x
        self.last_z = sum(w * xi for w, xi in zip(self.weights, x)) + self.bias
        self.last_output = self._activate(self.last_z)
        return self.last_output
```

---

## 2. Funciones de Activaci√≥n {#2-activaciones}

### 2.1 Comparaci√≥n

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     FUNCIONES DE ACTIVACI√ìN                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ   SIGMOID: œÉ(z) = 1 / (1 + e‚Åª·∂ª)                                             ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                             ‚îÇ
‚îÇ   Rango: (0, 1)                                                             ‚îÇ
‚îÇ   Uso: Output para probabilidad binaria                                     ‚îÇ
‚îÇ   Problema: Vanishing gradient para |z| grande                              ‚îÇ
‚îÇ        ___________                                                          ‚îÇ
‚îÇ       /                                                                     ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ/‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                           ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   TANH: tanh(z) = (e·∂ª - e‚Åª·∂ª) / (e·∂ª + e‚Åª·∂ª)                                   ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                  ‚îÇ
‚îÇ   Rango: (-1, 1)                                                            ‚îÇ
‚îÇ   Uso: Capas ocultas (centrado en 0)                                        ‚îÇ
‚îÇ          ___                                                                ‚îÇ
‚îÇ         /                                                                   ‚îÇ
‚îÇ   _____/                                                                    ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   ReLU: f(z) = max(0, z)                                                    ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                    ‚îÇ
‚îÇ   Rango: [0, ‚àû)                                                             ‚îÇ
‚îÇ   Uso: EST√ÅNDAR para capas ocultas                                          ‚îÇ
‚îÇ   Ventaja: No vanishing gradient, r√°pido                                    ‚îÇ
‚îÇ   Problema: "Dying ReLU" (neuronas muertas)                                 ‚îÇ
‚îÇ            /                                                                ‚îÇ
‚îÇ   ________/                                                                 ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Leaky ReLU: f(z) = max(Œ±z, z), Œ± ‚âà 0.01                                   ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                    ‚îÇ
‚îÇ   Soluciona dying ReLU                                                      ‚îÇ
‚îÇ            /                                                                ‚îÇ
‚îÇ   _      /                                                                  ‚îÇ
‚îÇ    \____/                                                                   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Softmax: œÉ(z)·µ¢ = e·∂ª‚Å± / Œ£e·∂ª ≤                                               ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                               ‚îÇ
‚îÇ   Rango: (0, 1), suma = 1                                                   ‚îÇ
‚îÇ   Uso: Output para clasificaci√≥n multiclase                                 ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.2 Implementaci√≥n

```python
def sigmoid(z: float) -> float:
    """Sigmoid activation."""
    if z < -500:
        return 0.0
    elif z > 500:
        return 1.0
    return 1.0 / (1.0 + math.exp(-z))


def sigmoid_derivative(z: float) -> float:
    """Derivative of sigmoid."""
    s = sigmoid(z)
    return s * (1 - s)


def relu(z: float) -> float:
    """ReLU activation."""
    return max(0, z)


def relu_derivative(z: float) -> float:
    """Derivative of ReLU."""
    return 1.0 if z > 0 else 0.0


def leaky_relu(z: float, alpha: float = 0.01) -> float:
    """Leaky ReLU activation."""
    return z if z > 0 else alpha * z


def softmax(z: List[float]) -> List[float]:
    """Softmax for vector (numerically stable)."""
    max_z = max(z)
    exp_z = [math.exp(zi - max_z) for zi in z]
    sum_exp = sum(exp_z)
    return [e / sum_exp for e in exp_z]
```

---

## 3. Redes Multicapa (MLP) {#3-mlp}

### 3.1 Arquitectura

```
MULTILAYER PERCEPTRON (MLP):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

INPUT       HIDDEN 1     HIDDEN 2     OUTPUT
LAYER       LAYER        LAYER        LAYER

  x‚ÇÅ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ≈∑‚ÇÅ
          ‚îÇ           ‚îÇ            ‚îÇ
  x‚ÇÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ≈∑‚ÇÇ
          ‚îÇ           ‚îÇ            ‚îÇ
  x‚ÇÉ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ≈∑‚ÇÉ

FORWARD PROPAGATION:
h‚ÇÅ = f(W‚ÇÅx + b‚ÇÅ)      # Primera capa oculta
h‚ÇÇ = f(W‚ÇÇh‚ÇÅ + b‚ÇÇ)     # Segunda capa oculta
≈∑ = g(W‚ÇÉh‚ÇÇ + b‚ÇÉ)      # Output (g puede ser softmax)

PAR√ÅMETROS TOTALES:
Para arquitectura [input, h1, h2, output] = [784, 128, 64, 10]:
W‚ÇÅ: 784√ó128 + 128 = 100,480
W‚ÇÇ: 128√ó64 + 64 = 8,256
W‚ÇÉ: 64√ó10 + 10 = 650
Total: ~109,000 par√°metros
```

### 3.2 Implementaci√≥n

```python
class Layer:
    """A single layer in a neural network."""
    
    def __init__(
        self, 
        n_inputs: int, 
        n_neurons: int, 
        activation: str = 'relu'
    ):
        # Xavier initialization
        limit = math.sqrt(6 / (n_inputs + n_neurons))
        self.weights = [
            [random.uniform(-limit, limit) for _ in range(n_inputs)]
            for _ in range(n_neurons)
        ]
        self.biases = [0.0] * n_neurons
        self.activation = activation
        
        # Cache for backprop
        self.inputs: List[float] = []
        self.z: List[float] = []  # Pre-activation
        self.outputs: List[float] = []
        
        # Gradients
        self.weight_gradients: List[List[float]] = []
        self.bias_gradients: List[float] = []
    
    def _activate(self, z: float) -> float:
        """Apply activation function."""
        if self.activation == 'sigmoid':
            return sigmoid(z)
        elif self.activation == 'relu':
            return relu(z)
        elif self.activation == 'tanh':
            return math.tanh(z)
        elif self.activation == 'linear':
            return z
        else:
            raise ValueError(f"Unknown activation: {self.activation}")
    
    def _activation_derivative(self, z: float) -> float:
        """Derivative of activation function."""
        if self.activation == 'sigmoid':
            return sigmoid_derivative(z)
        elif self.activation == 'relu':
            return relu_derivative(z)
        elif self.activation == 'tanh':
            return 1 - math.tanh(z) ** 2
        elif self.activation == 'linear':
            return 1.0
        else:
            raise ValueError(f"Unknown activation: {self.activation}")
    
    def forward(self, inputs: List[float]) -> List[float]:
        """Forward pass through layer."""
        self.inputs = inputs
        self.z = []
        self.outputs = []
        
        for neuron_idx in range(len(self.weights)):
            # Linear combination
            z = sum(
                w * x for w, x in zip(self.weights[neuron_idx], inputs)
            ) + self.biases[neuron_idx]
            self.z.append(z)
            
            # Activation
            self.outputs.append(self._activate(z))
        
        return self.outputs
    
    def backward(self, output_gradients: List[float]) -> List[float]:
        """Backward pass: compute gradients."""
        n_neurons = len(self.weights)
        n_inputs = len(self.weights[0])
        
        # Gradient of activation
        activation_gradients = [
            output_gradients[i] * self._activation_derivative(self.z[i])
            for i in range(n_neurons)
        ]
        
        # Weight gradients
        self.weight_gradients = [
            [activation_gradients[i] * self.inputs[j] for j in range(n_inputs)]
            for i in range(n_neurons)
        ]
        
        # Bias gradients
        self.bias_gradients = activation_gradients[:]
        
        # Input gradients (for previous layer)
        input_gradients = [0.0] * n_inputs
        for j in range(n_inputs):
            for i in range(n_neurons):
                input_gradients[j] += activation_gradients[i] * self.weights[i][j]
        
        return input_gradients
    
    def update(self, learning_rate: float) -> None:
        """Update weights using computed gradients."""
        for i in range(len(self.weights)):
            for j in range(len(self.weights[i])):
                self.weights[i][j] -= learning_rate * self.weight_gradients[i][j]
            self.biases[i] -= learning_rate * self.bias_gradients[i]


class NeuralNetwork:
    """Multilayer Perceptron neural network."""
    
    def __init__(self, layer_sizes: List[int], activations: List[str] = None):
        """
        Args:
            layer_sizes: [input_size, hidden1, hidden2, ..., output_size]
            activations: activation for each layer (default: relu + linear)
        """
        if activations is None:
            activations = ['relu'] * (len(layer_sizes) - 2) + ['linear']
        
        self.layers = []
        for i in range(len(layer_sizes) - 1):
            layer = Layer(
                layer_sizes[i], 
                layer_sizes[i + 1], 
                activations[i]
            )
            self.layers.append(layer)
    
    def forward(self, x: List[float]) -> List[float]:
        """Forward pass through all layers."""
        output = x
        for layer in self.layers:
            output = layer.forward(output)
        return output
    
    def backward(self, loss_gradient: List[float]) -> None:
        """Backward pass through all layers."""
        gradient = loss_gradient
        for layer in reversed(self.layers):
            gradient = layer.backward(gradient)
    
    def update(self, learning_rate: float) -> None:
        """Update all layers."""
        for layer in self.layers:
            layer.update(learning_rate)
    
    def predict(self, X: List[List[float]]) -> List[List[float]]:
        """Predict for batch."""
        return [self.forward(x) for x in X]
```

---

## 4. Backpropagation {#4-backpropagation}

### 4.1 La Regla de la Cadena

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ   BACKPROPAGATION = Regla de la Cadena Aplicada                             ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Objetivo: ‚àÇL/‚àÇw·µ¢‚±º (c√≥mo cambiar cada peso para reducir el loss)           ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Forward:  x ‚Üí [W‚ÇÅ] ‚Üí h‚ÇÅ ‚Üí [W‚ÇÇ] ‚Üí h‚ÇÇ ‚Üí [W‚ÇÉ] ‚Üí ≈∑ ‚Üí L                        ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Backward: x ‚Üê [‚àÇ] ‚Üê h‚ÇÅ ‚Üê [‚àÇ] ‚Üê h‚ÇÇ ‚Üê [‚àÇ] ‚Üê ≈∑ ‚Üê ‚àÇL/‚àÇ≈∑                       ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   REGLA DE LA CADENA:                                                       ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                       ‚îÇ
‚îÇ   ‚àÇL/‚àÇW‚ÇÇ = ‚àÇL/‚àÇ≈∑ √ó ‚àÇ≈∑/‚àÇh‚ÇÇ √ó ‚àÇh‚ÇÇ/‚àÇW‚ÇÇ                                         ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Para cada capa:                                                           ‚îÇ
‚îÇ   1. Recibir gradiente de la capa siguiente (‚àÇL/‚àÇoutput)                    ‚îÇ
‚îÇ   2. Multiplicar por derivada de la activaci√≥n (‚àÇoutput/‚àÇz)                 ‚îÇ
‚îÇ   3. Calcular gradientes de pesos: ‚àÇL/‚àÇW = (grad) √ó input                   ‚îÇ
‚îÇ   4. Pasar gradiente a capa anterior: ‚àÇL/‚àÇinput = W·µÄ √ó (grad)               ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 4.2 Funciones de P√©rdida

```python
def mse_loss(y_true: List[float], y_pred: List[float]) -> float:
    """Mean Squared Error loss."""
    return sum((yt - yp) ** 2 for yt, yp in zip(y_true, y_pred)) / len(y_true)


def mse_gradient(y_true: List[float], y_pred: List[float]) -> List[float]:
    """Gradient of MSE loss with respect to predictions."""
    n = len(y_true)
    return [2 * (yp - yt) / n for yt, yp in zip(y_true, y_pred)]


def binary_cross_entropy(y_true: List[float], y_pred: List[float]) -> float:
    """Binary cross-entropy loss."""
    eps = 1e-15
    loss = 0.0
    for yt, yp in zip(y_true, y_pred):
        yp = max(min(yp, 1 - eps), eps)  # Clip to avoid log(0)
        loss -= yt * math.log(yp) + (1 - yt) * math.log(1 - yp)
    return loss / len(y_true)


def bce_gradient(y_true: List[float], y_pred: List[float]) -> List[float]:
    """Gradient of binary cross-entropy."""
    eps = 1e-15
    return [
        ((yp - yt) / (yp * (1 - yp) + eps)) / len(y_true)
        for yt, yp in zip(y_true, y_pred)
    ]


def categorical_cross_entropy(y_true: List[int], y_pred: List[List[float]]) -> float:
    """Cross-entropy for multi-class classification.
    
    y_true: class indices
    y_pred: softmax probabilities
    """
    eps = 1e-15
    loss = 0.0
    for i, (true_class, pred_probs) in enumerate(zip(y_true, y_pred)):
        pred = max(pred_probs[true_class], eps)
        loss -= math.log(pred)
    return loss / len(y_true)
```

### 4.3 Training Loop Completo

```python
def train_network(
    network: NeuralNetwork,
    X_train: List[List[float]],
    y_train: List[List[float]],
    epochs: int = 100,
    learning_rate: float = 0.01,
    batch_size: int = 32,
    verbose: bool = True
) -> List[float]:
    """Train neural network with mini-batch gradient descent.
    
    Returns list of losses per epoch.
    """
    n_samples = len(X_train)
    losses = []
    
    for epoch in range(epochs):
        # Shuffle data
        indices = list(range(n_samples))
        random.shuffle(indices)
        
        epoch_loss = 0.0
        n_batches = 0
        
        for start in range(0, n_samples, batch_size):
            end = min(start + batch_size, n_samples)
            batch_indices = indices[start:end]
            
            batch_loss = 0.0
            
            for idx in batch_indices:
                x = X_train[idx]
                y = y_train[idx]
                
                # Forward
                y_pred = network.forward(x)
                
                # Loss
                loss = mse_loss(y, y_pred)
                batch_loss += loss
                
                # Backward
                gradient = mse_gradient(y, y_pred)
                network.backward(gradient)
                
                # Update
                network.update(learning_rate)
            
            epoch_loss += batch_loss
            n_batches += 1
        
        avg_loss = epoch_loss / n_samples
        losses.append(avg_loss)
        
        if verbose and (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.6f}")
    
    return losses
```

---

## 5. Optimizaci√≥n y Regularizaci√≥n {#5-optimizacion}

### 5.1 Optimizadores

```python
class SGD:
    """Stochastic Gradient Descent with momentum."""
    
    def __init__(self, learning_rate: float = 0.01, momentum: float = 0.0):
        self.lr = learning_rate
        self.momentum = momentum
        self.velocities: Dict = {}
    
    def update(self, param_id: str, param: List[float], 
               gradient: List[float]) -> List[float]:
        """Update parameters."""
        if param_id not in self.velocities:
            self.velocities[param_id] = [0.0] * len(param)
        
        v = self.velocities[param_id]
        
        for i in range(len(param)):
            v[i] = self.momentum * v[i] - self.lr * gradient[i]
            param[i] += v[i]
        
        return param


class Adam:
    """Adam optimizer (simplified)."""
    
    def __init__(
        self, 
        learning_rate: float = 0.001, 
        beta1: float = 0.9,
        beta2: float = 0.999,
        epsilon: float = 1e-8
    ):
        self.lr = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m: Dict = {}  # First moment
        self.v: Dict = {}  # Second moment
        self.t: int = 0    # Time step
    
    def update(self, param_id: str, param: List[float], 
               gradient: List[float]) -> List[float]:
        """Update parameters using Adam."""
        self.t += 1
        
        if param_id not in self.m:
            self.m[param_id] = [0.0] * len(param)
            self.v[param_id] = [0.0] * len(param)
        
        m = self.m[param_id]
        v = self.v[param_id]
        
        for i in range(len(param)):
            # Update biased first moment
            m[i] = self.beta1 * m[i] + (1 - self.beta1) * gradient[i]
            
            # Update biased second moment
            v[i] = self.beta2 * v[i] + (1 - self.beta2) * gradient[i] ** 2
            
            # Bias correction
            m_hat = m[i] / (1 - self.beta1 ** self.t)
            v_hat = v[i] / (1 - self.beta2 ** self.t)
            
            # Update parameter
            param[i] -= self.lr * m_hat / (math.sqrt(v_hat) + self.epsilon)
        
        return param
```

### 5.2 Regularizaci√≥n

```python
def l2_regularization(weights: List[List[float]], lambda_: float) -> float:
    """L2 regularization term: Œª √ó Œ£ w¬≤."""
    total = 0.0
    for layer_weights in weights:
        for row in layer_weights:
            total += sum(w ** 2 for w in row)
    return lambda_ * total


def l2_gradient(weight: float, lambda_: float) -> float:
    """Gradient of L2 regularization: 2Œªw."""
    return 2 * lambda_ * weight


def dropout(layer_output: List[float], keep_prob: float = 0.8, 
            training: bool = True) -> List[float]:
    """Dropout regularization.
    
    Randomly zeros out neurons during training.
    Scales outputs during inference.
    """
    if not training:
        return layer_output
    
    result = []
    for val in layer_output:
        if random.random() < keep_prob:
            result.append(val / keep_prob)  # Inverted dropout
        else:
            result.append(0.0)
    
    return result
```

### 5.3 Batch Normalization (Concepto)

```
BATCH NORMALIZATION:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Normaliza las activaciones de cada capa:

1. Calcular Œº y œÉ del batch
2. Normalizar: xÃÇ = (x - Œº) / œÉ
3. Escalar y desplazar: y = Œ≥xÃÇ + Œ≤ (par√°metros aprendidos)

BENEFICIOS:
‚Ä¢ Permite learning rates m√°s altas
‚Ä¢ Reduce dependencia de inicializaci√≥n
‚Ä¢ Act√∫a como regularizador
‚Ä¢ Acelera el entrenamiento

NOTA: Comportamiento diferente en train vs inference
‚Ä¢ Train: estad√≠sticas del batch
‚Ä¢ Inference: estad√≠sticas acumuladas (running mean/var)
```

---

## 6. Arquitecturas Especiales (CNN, RNN) {#6-arquitecturas}

### 6.1 Convolutional Neural Networks (CNN)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ   CONVOLUTIONAL NEURAL NETWORKS                                             ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Para datos con estructura espacial (im√°genes).                            ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   CONVOLUCI√ìN:                                                              ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                             ‚îÇ
‚îÇ   Filtro 3√ó3 desliz√°ndose sobre la imagen:                                  ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Input Image        Filter        Feature Map                              ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                 ‚îÇ
‚îÇ   ‚îÇ 1 2 3 4 ‚îÇ   *   ‚îÇa b‚îÇ    =    ‚îÇ . . . ‚îÇ                                 ‚îÇ
‚îÇ   ‚îÇ 5 6 7 8 ‚îÇ       ‚îÇc d‚îÇ         ‚îÇ . . . ‚îÇ                                 ‚îÇ
‚îÇ   ‚îÇ 9 . . . ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                 ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                               ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   output[i,j] = Œ£ input[i+k, j+l] √ó filter[k,l]                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   POOLING:                                                                  ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                                 ‚îÇ
‚îÇ   Reduce dimensi√≥n espacial:                                                ‚îÇ
‚îÇ   ‚Ä¢ Max Pooling: toma el m√°ximo de cada regi√≥n                              ‚îÇ
‚îÇ   ‚Ä¢ Average Pooling: promedio de cada regi√≥n                                ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   ARQUITECTURA T√çPICA:                                                      ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                      ‚îÇ
‚îÇ   [Conv ‚Üí ReLU ‚Üí Pool] √ó N ‚Üí Flatten ‚Üí [Dense] √ó M ‚Üí Output                 ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 6.2 Recurrent Neural Networks (RNN)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ   RECURRENT NEURAL NETWORKS                                                 ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                 ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Para secuencias (texto, series temporales).                               ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   ESTADO OCULTO:                                                            ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                           ‚îÇ
‚îÇ   h‚Çú = f(W‚Çì‚Çï √ó x‚Çú + W‚Çï‚Çï √ó h‚Çú‚Çã‚ÇÅ + b)                                            ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   El estado anterior influye en el actual.                                  ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ       x‚ÇÅ        x‚ÇÇ        x‚ÇÉ        x‚ÇÑ                                      ‚îÇ
‚îÇ        ‚Üì         ‚Üì         ‚Üì         ‚Üì                                      ‚îÇ
‚îÇ       [h] ‚îÄ‚îÄ‚îÄ‚ñ∂ [h] ‚îÄ‚îÄ‚îÄ‚ñ∂ [h] ‚îÄ‚îÄ‚îÄ‚ñ∂ [h]                                      ‚îÇ
‚îÇ        ‚Üì         ‚Üì         ‚Üì         ‚Üì                                      ‚îÇ
‚îÇ       y‚ÇÅ        y‚ÇÇ        y‚ÇÉ        y‚ÇÑ                                      ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   PROBLEMA: Vanishing/Exploding Gradients                                   ‚îÇ
‚îÇ   SOLUCI√ìN: LSTM, GRU (gated architectures)                                 ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   LSTM (Long Short-Term Memory):                                            ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                            ‚îÇ
‚îÇ   ‚Ä¢ Forget gate: qu√© olvidar del estado anterior                            ‚îÇ
‚îÇ   ‚Ä¢ Input gate: qu√© nueva informaci√≥n agregar                               ‚îÇ
‚îÇ   ‚Ä¢ Output gate: qu√© output producir                                        ‚îÇ
‚îÇ   ‚Ä¢ Cell state: memoria a largo plazo                                       ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 6.3 Convolution Simplificada

```python
def convolve_2d(
    image: List[List[float]], 
    kernel: List[List[float]]
) -> List[List[float]]:
    """2D convolution (no padding, stride=1).
    
    Simplified implementation for understanding.
    """
    h, w = len(image), len(image[0])
    kh, kw = len(kernel), len(kernel[0])
    
    output_h = h - kh + 1
    output_w = w - kw + 1
    
    output = [[0.0] * output_w for _ in range(output_h)]
    
    for i in range(output_h):
        for j in range(output_w):
            total = 0.0
            for ki in range(kh):
                for kj in range(kw):
                    total += image[i + ki][j + kj] * kernel[ki][kj]
            output[i][j] = total
    
    return output


def max_pool_2d(
    feature_map: List[List[float]], 
    pool_size: int = 2
) -> List[List[float]]:
    """Max pooling with given pool size."""
    h, w = len(feature_map), len(feature_map[0])
    output_h = h // pool_size
    output_w = w // pool_size
    
    output = [[0.0] * output_w for _ in range(output_h)]
    
    for i in range(output_h):
        for j in range(output_w):
            max_val = float('-inf')
            for pi in range(pool_size):
                for pj in range(pool_size):
                    val = feature_map[i * pool_size + pi][j * pool_size + pj]
                    max_val = max(max_val, val)
            output[i][j] = max_val
    
    return output
```

---

## ‚ö†Ô∏è Mejores Pr√°cticas

```
DEEP LEARNING BEST PRACTICES:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

DATOS:
‚Ä¢ M√°s datos > modelo m√°s complejo
‚Ä¢ Augmentaci√≥n para aumentar datos
‚Ä¢ Normalizaci√≥n de inputs

ARQUITECTURA:
‚Ä¢ Empezar simple, agregar complejidad si es necesario
‚Ä¢ ReLU para capas ocultas
‚Ä¢ Batch normalization despu√©s de capas densas

ENTRENAMIENTO:
‚Ä¢ Adam optimizer por defecto
‚Ä¢ Learning rate scheduling
‚Ä¢ Early stopping
‚Ä¢ Validaci√≥n para detectar overfitting

DEBUGGING:
‚Ä¢ Verificar que loss disminuye en train peque√±o
‚Ä¢ Graficar loss curves
‚Ä¢ Monitorear gradientes (no vanishing/exploding)
```

---

## üîß Ejercicios Pr√°cticos

### Ejercicio 24.1: Perceptr√≥n para AND/OR
Entrenar perceptr√≥n en funciones l√≥gicas simples.

### Ejercicio 24.2: MLP para XOR
Red de 2 capas para resolver XOR (no linealmente separable).

### Ejercicio 24.3: MNIST desde Cero
Clasificar d√≠gitos con MLP implementado manualmente.

---

## üìö Recursos Externos

| Recurso | Tipo | Prioridad |
|---------|------|-----------|
| [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) | Curso | üî¥ Obligatorio |
| [3Blue1Brown: Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) | Videos | üî¥ Obligatorio |
| [Neural Networks from Scratch](https://nnfs.io/) | Libro | üü° Recomendado |

---

## üß≠ Navegaci√≥n

| ‚Üê Anterior | √çndice | Siguiente ‚Üí |
|------------|--------|-------------|
| [23_ML_NO_SUPERVISADO](23_ML_NO_SUPERVISADO.md) | [00_INDICE](00_INDICE.md) | [12_PROYECTO_INTEGRADOR](12_PROYECTO_INTEGRADOR.md) |
