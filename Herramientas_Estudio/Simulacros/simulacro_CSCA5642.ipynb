{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a11410",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simulacro de Examen: CSCA 5642 - Deep Learning\n",
    "===============================================\n",
    "\n",
    "M√≥dulo: M07 - Deep Learning\n",
    "Tiempo Estimado: 90 minutos\n",
    "Puntuaci√≥n Total: 100 puntos\n",
    "\n",
    "Estructura:\n",
    "- Parte A: Preguntas Te√≥ricas (30 puntos)\n",
    "- Parte B: Ejercicios de C√≥digo (70 puntos)\n",
    "\n",
    "Criterio para aprobar con B: >= 80 puntos\n",
    "\n",
    "Ejecutar tests: pytest tests/test_simulacro_csca5642.py -v\n",
    "\"\"\"\n",
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47135c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0adf6d0",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "PARTE A: PREGUNTAS TE√ìRICAS (30 puntos)\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7090519",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PARTE A: PREGUNTAS TE√ìRICAS (30 puntos)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb32551",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------\n",
    "Pregunta A1 (8 puntos): Backpropagation\n",
    "-----------------------------------------------------------------------------\n",
    "Considera una red con una capa oculta: x ‚Üí h ‚Üí y\n",
    "h = œÉ(W‚ÇÅx + b‚ÇÅ), y = W‚ÇÇh + b‚ÇÇ\n",
    "Loss: L = ¬Ω(y - t)¬≤\n",
    "\n",
    "a) Escribe la expresi√≥n para ‚àÇL/‚àÇW‚ÇÇ (3 pts)\n",
    "b) Escribe la expresi√≥n para ‚àÇL/‚àÇW‚ÇÅ usando la regla de la cadena (3 pts)\n",
    "c) ¬øPor qu√© el gradiente puede \"desvanecerse\" en redes profundas con sigmoid? (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16a910",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "respuesta_A1_a: str = \"\"  # TODO: ‚àÇL/‚àÇW‚ÇÇ\n",
    "respuesta_A1_b: str = \"\"  # TODO: ‚àÇL/‚àÇW‚ÇÅ\n",
    "respuesta_A1_c: str = \"\"  # TODO: Vanishing gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71112a",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------\n",
    "Pregunta A2 (8 puntos): CNNs\n",
    "-----------------------------------------------------------------------------\n",
    "Una imagen de entrada tiene dimensiones 32√ó32√ó3.\n",
    "Se aplica una convoluci√≥n con 16 filtros de 5√ó5, stride=1, padding=0.\n",
    "\n",
    "a) ¬øCu√°les son las dimensiones de la salida? (3 pts)\n",
    "b) ¬øCu√°ntos par√°metros tiene esta capa (incluyendo bias)? (3 pts)\n",
    "c) ¬øPor qu√© las CNNs son m√°s eficientes que MLPs para im√°genes? (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff642d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "respuesta_A2_a: str = \"\"  # TODO: Dimensiones de salida\n",
    "respuesta_A2_b: int = 0  # TODO: N√∫mero de par√°metros\n",
    "respuesta_A2_c: str = \"\"  # TODO: Eficiencia de CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d714557",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------\n",
    "Pregunta A3 (7 puntos): Regularizaci√≥n\n",
    "-----------------------------------------------------------------------------\n",
    "a) Explica c√≥mo funciona Dropout durante entrenamiento vs inferencia. (3 pts)\n",
    "b) ¬øPor qu√© Batch Normalization act√∫a como regularizador? (2 pts)\n",
    "c) ¬øQu√© es Early Stopping y c√≥mo previene overfitting? (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b78445",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "respuesta_A3_a: str = \"\"  # TODO: Dropout train vs inference\n",
    "respuesta_A3_b: str = \"\"  # TODO: BatchNorm como regularizador\n",
    "respuesta_A3_c: str = \"\"  # TODO: Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e322ddf3",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------\n",
    "Pregunta A4 (7 puntos): RNNs y LSTMs\n",
    "-----------------------------------------------------------------------------\n",
    "a) ¬øCu√°l es el problema principal de RNNs vanilla para secuencias largas? (2 pts)\n",
    "b) ¬øC√≥mo resuelve LSTM este problema? Menciona las gates. (3 pts)\n",
    "c) ¬øCu√°ndo usar√≠as Bidirectional LSTM vs unidireccional? (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3681b6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "respuesta_A4_a: str = \"\"  # TODO: Problema de RNN vanilla\n",
    "respuesta_A4_b: str = \"\"  # TODO: Soluci√≥n LSTM\n",
    "respuesta_A4_c: str = \"\"  # TODO: Bidirectional vs unidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa595d12",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "PARTE B: EJERCICIOS DE C√ìDIGO (70 puntos)\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d04966",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PARTE B: EJERCICIOS DE C√ìDIGO (70 puntos)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1c8781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Ejercicio B1 (20 puntos): Forward Pass de MLP\n",
    "# -----------------------------------------------------------------------------\n",
    "def relu(z: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "    \"\"\"Funci√≥n de activaci√≥n ReLU: max(0, z).\"\"\"\n",
    "    # TODO: Implementar\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c7bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(z: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "    \"\"\"Derivada de ReLU: 1 si z > 0, else 0.\"\"\"\n",
    "    # TODO: Implementar\n",
    "    return (z > 0).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Funci√≥n softmax para clasificaci√≥n multiclase.\n",
    "\n",
    "    softmax(z)_i = exp(z_i) / Œ£ exp(z_j)\n",
    "\n",
    "    Tip: Restar max(z) para estabilidad num√©rica.\n",
    "    \"\"\"\n",
    "    # TODO: Implementar con estabilidad num√©rica\n",
    "    exp_z = np.exp(z - np.max(z, axis=-1, keepdims=True))\n",
    "    return np.asarray(exp_z / np.sum(exp_z, axis=-1, keepdims=True), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fb7d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_forward(\n",
    "    X: NDArray[np.float64],\n",
    "    weights: list[NDArray[np.float64]],\n",
    "    biases: list[NDArray[np.float64]],\n",
    ") -> tuple[list[NDArray[np.float64]], list[NDArray[np.float64]]]:\n",
    "    \"\"\"\n",
    "    Forward pass de un MLP con ReLU en capas ocultas y softmax en salida.\n",
    "\n",
    "    Par√°metros\n",
    "    ----------\n",
    "    X : NDArray[np.float64]\n",
    "        Datos de entrada (batch_size, input_dim).\n",
    "    weights : list[NDArray]\n",
    "        Lista de matrices de pesos [W1, W2, ...].\n",
    "    biases : list[NDArray]\n",
    "        Lista de vectores de bias [b1, b2, ...].\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    tuple[list[NDArray], list[NDArray]]\n",
    "        - activations: Lista de activaciones de cada capa [a0, a1, ..., output]\n",
    "        - pre_activations: Lista de valores pre-activaci√≥n [z1, z2, ...]\n",
    "    \"\"\"\n",
    "    activations = [X]\n",
    "    pre_activations = []\n",
    "\n",
    "    current_input = X\n",
    "    n_layers = len(weights)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        # TODO: Calcular z = W @ a + b\n",
    "        z = current_input @ weights[i] + biases[i]\n",
    "        pre_activations.append(z)\n",
    "\n",
    "        # TODO: Aplicar activaci√≥n (ReLU para ocultas, softmax para √∫ltima)\n",
    "        if i < n_layers - 1:\n",
    "            a = relu(z)\n",
    "        else:\n",
    "            a = softmax(z)\n",
    "\n",
    "        activations.append(a)\n",
    "        current_input = a\n",
    "\n",
    "    return activations, pre_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5295eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test B1\n",
    "print(\"\\n--- Test B1: MLP Forward Pass ---\")\n",
    "X_mlp = rng.standard_normal((5, 4))  # 5 samples, 4 features\n",
    "W1 = rng.standard_normal((4, 8)) * 0.1\n",
    "b1 = np.zeros(8)\n",
    "W2 = rng.standard_normal((8, 3)) * 0.1\n",
    "b2 = np.zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1458906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations, pre_activations = mlp_forward(X_mlp, [W1, W2], [b1, b2])\n",
    "print(f\"Input shape: {X_mlp.shape}\")\n",
    "print(f\"Hidden shape: {activations[1].shape}\")\n",
    "print(f\"Output shape: {activations[2].shape}\")\n",
    "print(f\"Output sums to 1? {np.allclose(activations[2].sum(axis=1), 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c340d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Ejercicio B2 (25 puntos): Backward Pass y Gradientes\n",
    "# -----------------------------------------------------------------------------\n",
    "def cross_entropy_loss(\n",
    "    y_pred: NDArray[np.float64],\n",
    "    y_true: NDArray[np.int64],\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calcula Cross-Entropy Loss.\n",
    "\n",
    "    L = -1/n * Œ£ log(y_pred[i, y_true[i]])\n",
    "\n",
    "    Par√°metros\n",
    "    ----------\n",
    "    y_pred : NDArray[np.float64]\n",
    "        Probabilidades predichas (batch_size, n_classes).\n",
    "    y_true : NDArray[np.int64]\n",
    "        Labels verdaderos como √≠ndices (batch_size,).\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    float\n",
    "        Cross-entropy loss promedio.\n",
    "    \"\"\"\n",
    "    n = len(y_true)\n",
    "    # TODO: Implementar (a√±adir epsilon para estabilidad)\n",
    "    eps = 1e-15\n",
    "    log_probs = np.log(np.clip(y_pred[np.arange(n), y_true], eps, 1 - eps))\n",
    "    return float(-np.mean(log_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_gradient(\n",
    "    y_pred: NDArray[np.float64],\n",
    "    y_true: NDArray[np.int64],\n",
    ") -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Gradiente de Cross-Entropy + Softmax combinado.\n",
    "\n",
    "    Para softmax + CE, el gradiente simplificado es:\n",
    "    ‚àÇL/‚àÇz = y_pred - y_onehot\n",
    "\n",
    "    Par√°metros\n",
    "    ----------\n",
    "    y_pred : NDArray[np.float64]\n",
    "        Probabilidades predichas (batch_size, n_classes).\n",
    "    y_true : NDArray[np.int64]\n",
    "        Labels verdaderos (batch_size,).\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    NDArray[np.float64]\n",
    "        Gradiente respecto a z (batch_size, n_classes).\n",
    "    \"\"\"\n",
    "    n = len(y_true)\n",
    "    n_classes = y_pred.shape[1]\n",
    "\n",
    "    # TODO: Crear one-hot encoding de y_true\n",
    "    y_onehot = np.zeros((n, n_classes), dtype=np.float64)\n",
    "    y_onehot[np.arange(n), y_true] = 1\n",
    "\n",
    "    # TODO: Calcular gradiente\n",
    "    grad = (y_pred - y_onehot) / n\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c5ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_backward(\n",
    "    activations: list[NDArray[np.float64]],\n",
    "    pre_activations: list[NDArray[np.float64]],\n",
    "    weights: list[NDArray[np.float64]],\n",
    "    y_true: NDArray[np.int64],\n",
    ") -> tuple[list[NDArray[np.float64]], list[NDArray[np.float64]]]:\n",
    "    \"\"\"\n",
    "    Backward pass de un MLP.\n",
    "\n",
    "    Par√°metros\n",
    "    ----------\n",
    "    activations : list[NDArray]\n",
    "        Activaciones de cada capa [a0, a1, ..., output].\n",
    "    pre_activations : list[NDArray]\n",
    "        Valores pre-activaci√≥n [z1, z2, ...].\n",
    "    weights : list[NDArray]\n",
    "        Matrices de pesos.\n",
    "    y_true : NDArray[np.int64]\n",
    "        Labels verdaderos.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    tuple[list[NDArray], list[NDArray]]\n",
    "        - grad_weights: Gradientes de pesos [‚àÇL/‚àÇW1, ‚àÇL/‚àÇW2, ...]\n",
    "        - grad_biases: Gradientes de biases [‚àÇL/‚àÇb1, ‚àÇL/‚àÇb2, ...]\n",
    "    \"\"\"\n",
    "    n_layers = len(weights)\n",
    "    grad_weights: list[NDArray[np.float64]] = []\n",
    "    grad_biases: list[NDArray[np.float64]] = []\n",
    "\n",
    "    # TODO: Gradiente de la capa de salida (softmax + CE)\n",
    "    delta = cross_entropy_gradient(activations[-1], y_true)\n",
    "\n",
    "    # Backpropagate\n",
    "    for i in range(n_layers - 1, -1, -1):\n",
    "        # TODO: Gradiente de pesos: ‚àÇL/‚àÇW = a^T @ delta\n",
    "        grad_w = activations[i].T @ delta\n",
    "        grad_weights.insert(0, grad_w)\n",
    "\n",
    "        # TODO: Gradiente de bias: ‚àÇL/‚àÇb = sum(delta, axis=0)\n",
    "        grad_b = np.sum(delta, axis=0)\n",
    "        grad_biases.insert(0, grad_b)\n",
    "\n",
    "        if i > 0:\n",
    "            # TODO: Propagar delta a capa anterior\n",
    "            delta = (delta @ weights[i].T) * relu_derivative(pre_activations[i - 1])\n",
    "\n",
    "    return grad_weights, grad_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a5086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test B2\n",
    "print(\"\\n--- Test B2: MLP Backward Pass ---\")\n",
    "y_true_mlp = np.array([0, 1, 2, 0, 1])  # 5 samples, 3 classes\n",
    "loss = cross_entropy_loss(activations[2], y_true_mlp)\n",
    "print(f\"Cross-Entropy Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e242c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_w, grad_b = mlp_backward(activations, pre_activations, [W1, W2], y_true_mlp)\n",
    "print(f\"Gradiente W1 shape: {grad_w[0].shape}\")\n",
    "print(f\"Gradiente W2 shape: {grad_w[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aabb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Ejercicio B3 (25 puntos): Implementar una Capa Convolucional\n",
    "# -----------------------------------------------------------------------------\n",
    "def conv2d_forward(\n",
    "    X: NDArray[np.float64],\n",
    "    W: NDArray[np.float64],\n",
    "    b: NDArray[np.float64],\n",
    "    stride: int = 1,\n",
    "    padding: int = 0,\n",
    ") -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Forward pass de convoluci√≥n 2D.\n",
    "\n",
    "    Par√°metros\n",
    "    ----------\n",
    "    X : NDArray[np.float64]\n",
    "        Input (batch_size, height, width, in_channels).\n",
    "    W : NDArray[np.float64]\n",
    "        Filtros (filter_h, filter_w, in_channels, out_channels).\n",
    "    b : NDArray[np.float64]\n",
    "        Bias (out_channels,).\n",
    "    stride : int\n",
    "        Stride de la convoluci√≥n.\n",
    "    padding : int\n",
    "        Zero-padding.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    NDArray[np.float64]\n",
    "        Output (batch_size, out_height, out_width, out_channels).\n",
    "\n",
    "    F√≥rmula de dimensiones:\n",
    "    -----------------------\n",
    "    out_height = (height + 2*padding - filter_h) // stride + 1\n",
    "    out_width = (width + 2*padding - filter_w) // stride + 1\n",
    "    \"\"\"\n",
    "    batch_size, height, width, in_channels = X.shape\n",
    "    filter_h, filter_w, _, out_channels = W.shape\n",
    "\n",
    "    # Calcular dimensiones de salida\n",
    "    out_height = (height + 2 * padding - filter_h) // stride + 1\n",
    "    out_width = (width + 2 * padding - filter_w) // stride + 1\n",
    "\n",
    "    # TODO: Aplicar padding si es necesario\n",
    "    if padding > 0:\n",
    "        X_padded = np.pad(\n",
    "            X,\n",
    "            ((0, 0), (padding, padding), (padding, padding), (0, 0)),\n",
    "            mode=\"constant\",\n",
    "        )\n",
    "    else:\n",
    "        X_padded = X\n",
    "\n",
    "    # TODO: Inicializar output\n",
    "    output = np.zeros((batch_size, out_height, out_width, out_channels))\n",
    "\n",
    "    # TODO: Realizar convoluci√≥n\n",
    "    for i in range(out_height):\n",
    "        for j in range(out_width):\n",
    "            h_start = i * stride\n",
    "            h_end = h_start + filter_h\n",
    "            w_start = j * stride\n",
    "            w_end = w_start + filter_w\n",
    "\n",
    "            # Extraer regi√≥n\n",
    "            region = X_padded[:, h_start:h_end, w_start:w_end, :]\n",
    "\n",
    "            # Convoluci√≥n: sum over (filter_h, filter_w, in_channels)\n",
    "            for k in range(out_channels):\n",
    "                output[:, i, j, k] = (\n",
    "                    np.sum(region * W[:, :, :, k], axis=(1, 2, 3)) + b[k]\n",
    "                )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1256e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool2d(\n",
    "    X: NDArray[np.float64],\n",
    "    pool_size: int = 2,\n",
    "    stride: int = 2,\n",
    ") -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Max Pooling 2D.\n",
    "\n",
    "    Par√°metros\n",
    "    ----------\n",
    "    X : NDArray[np.float64]\n",
    "        Input (batch_size, height, width, channels).\n",
    "    pool_size : int\n",
    "        Tama√±o de la ventana de pooling.\n",
    "    stride : int\n",
    "        Stride del pooling.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    NDArray[np.float64]\n",
    "        Output con dimensiones reducidas.\n",
    "    \"\"\"\n",
    "    batch_size, height, width, channels = X.shape\n",
    "\n",
    "    out_height = (height - pool_size) // stride + 1\n",
    "    out_width = (width - pool_size) // stride + 1\n",
    "\n",
    "    output = np.zeros((batch_size, out_height, out_width, channels))\n",
    "\n",
    "    # TODO: Aplicar max pooling\n",
    "    for i in range(out_height):\n",
    "        for j in range(out_width):\n",
    "            h_start = i * stride\n",
    "            h_end = h_start + pool_size\n",
    "            w_start = j * stride\n",
    "            w_end = w_start + pool_size\n",
    "\n",
    "            region = X[:, h_start:h_end, w_start:w_end, :]\n",
    "            output[:, i, j, :] = np.max(region, axis=(1, 2))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44831918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test B3\n",
    "print(\"\\n--- Test B3: Conv2D Forward ---\")\n",
    "X_conv = rng.standard_normal((2, 8, 8, 3))  # 2 images, 8x8, 3 channels\n",
    "W_conv = rng.standard_normal((3, 3, 3, 16)) * 0.1  # 16 filters, 3x3\n",
    "b_conv = np.zeros(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2637fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_conv = conv2d_forward(X_conv, W_conv, b_conv, stride=1, padding=0)\n",
    "print(f\"Input shape: {X_conv.shape}\")\n",
    "print(f\"Output shape: {out_conv.shape}\")\n",
    "print(\"Expected output shape: (2, 6, 6, 16)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d010e324",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "out_pool = max_pool2d(out_conv, pool_size=2, stride=2)\n",
    "print(f\"After MaxPool: {out_pool.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff4aebd",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "VALIDACI√ìN FINAL\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0eeb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VALIDACI√ìN FINAL\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f98846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validar_simulacro() -> dict[str, bool]:\n",
    "    \"\"\"Valida todas las respuestas del simulacro.\"\"\"\n",
    "    resultados: dict[str, bool] = {}\n",
    "\n",
    "    # Validar A2 (CNN par√°metros)\n",
    "    # (5*5*3 + 1) * 16 = 1216\n",
    "    resultados[\"A2_params\"] = respuesta_A2_b == 1216\n",
    "\n",
    "    # Validar B1 (Forward pass)\n",
    "    X_test = rng.standard_normal((3, 4))\n",
    "    W1_test = rng.standard_normal((4, 5)) * 0.1\n",
    "    b1_test = np.zeros(5)\n",
    "    W2_test = rng.standard_normal((5, 2)) * 0.1\n",
    "    b2_test = np.zeros(2)\n",
    "\n",
    "    acts, _ = mlp_forward(X_test, [W1_test, W2_test], [b1_test, b2_test])\n",
    "    resultados[\"B1_softmax_sum\"] = np.allclose(acts[-1].sum(axis=1), 1)\n",
    "    resultados[\"B1_shapes\"] = acts[1].shape == (3, 5) and acts[2].shape == (3, 2)\n",
    "\n",
    "    # Validar B2 (Cross-entropy)\n",
    "    y_pred_test = np.array([[0.9, 0.1], [0.2, 0.8], [0.5, 0.5]])\n",
    "    y_true_test = np.array([0, 1, 0])\n",
    "    loss_test = cross_entropy_loss(y_pred_test, y_true_test)\n",
    "    resultados[\"B2_ce_loss\"] = 0 < loss_test < 1  # Reasonable range\n",
    "\n",
    "    # Validar B3 (Conv2D)\n",
    "    X_c = rng.standard_normal((1, 6, 6, 1))\n",
    "    W_c = rng.standard_normal((3, 3, 1, 4)) * 0.1\n",
    "    b_c = np.zeros(4)\n",
    "    out_c = conv2d_forward(X_c, W_c, b_c, stride=1, padding=0)\n",
    "    resultados[\"B3_conv_shape\"] = out_c.shape == (1, 4, 4, 4)\n",
    "\n",
    "    out_p = max_pool2d(out_c, pool_size=2, stride=2)\n",
    "    resultados[\"B3_pool_shape\"] = out_p.shape == (1, 2, 2, 4)\n",
    "\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar validaci√≥n\n",
    "print(\"\\nüîç Validando respuestas...\")\n",
    "resultados = validar_simulacro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa3895",
   "metadata": {},
   "outputs": [],
   "source": [
    "puntos = 0\n",
    "for test, passed in resultados.items():\n",
    "    status = \"‚úÖ\" if passed else \"‚ùå\"\n",
    "    pts = 12 if passed else 0\n",
    "    puntos += pts\n",
    "    print(f\"  {status} {test}: {pts} pts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìä PUNTUACI√ìN ESTIMADA: {puntos}/70 (solo c√≥digo)\")\n",
    "print(\"   + Parte Te√≥rica: /30 (requiere revisi√≥n manual)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "if puntos >= 56:\n",
    "    print(\"\\nüéâ ¬°Vas bien! El c√≥digo cumple el criterio para B.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Necesitas revisar las implementaciones antes del examen real.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FIN DEL SIMULACRO\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
