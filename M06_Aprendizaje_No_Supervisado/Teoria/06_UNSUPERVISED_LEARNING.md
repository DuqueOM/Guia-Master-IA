# M√≥dulo 06 - Unsupervised Learning

> **üéØ Objetivo:** Dominar K-Means clustering y PCA para reducci√≥n dimensional
> **Fase:** 2 - N√∫cleo de ML | **Semanas 13-16**
> **Curso del Pathway:** Unsupervised Algorithms in Machine Learning

---

<a id="m06-0"></a>

## üß≠ C√≥mo usar este m√≥dulo (modo 0‚Üí100)

**Prop√≥sito:** que puedas:

- encontrar estructura sin etiquetas (clustering)
- reducir dimensionalidad con rigor (PCA)
- decidir cu√°ndo NO usar estos m√©todos

### Objetivos de aprendizaje (medibles)

Al terminar este m√≥dulo podr√°s:

- **Implementar** K-Means (Lloyd + K-Means++).
- **Evaluar** clustering con inercia/codo y silhouette (entendiendo limitaciones).
- **Implementar** PCA con SVD y usar varianza explicada para elegir `n_components`.
- **Diagnosticar** cu√°ndo K-Means/PCA fallan y proponer alternativas.

### üß™ Ver para Entender (Laboratorios Interactivos)

- Gu√≠a central: [INTERACTIVE_LABS.md](../../Recursos_Adicionales/INTERACTIVE_LABS.md)
- PCA: rotaci√≥n manual 3D ‚Üí proyecci√≥n 2D (intuici√≥n de varianza m√°xima) + referencia SVD:
  - `streamlit run M06_unsupervised/pca_rotation_plotly_app.py`

Enlaces r√°pidos:

- [GLOSARIO.md](GLOSARIO.md)
- [RECURSOS.md](RECURSOS.md)
- [PLAN_V4_ESTRATEGICO.md](../../Recursos_Adicionales/Planes_Estrategicos/PLAN_V4_ESTRATEGICO.md)
- [PLAN_V5_ESTRATEGICO.md](../../Recursos_Adicionales/Planes_Estrategicos/PLAN_V5_ESTRATEGICO.md)
- Evaluaci√≥n (r√∫brica): [Herramientas_Estudio/RUBRICA_v1.md](../Herramientas_Estudio/RUBRICA_v1.md) (scope `M06` en `rubrica.csv`; incluye PB-16)

### Recursos (cu√°ndo usarlos)

| Prioridad | Recurso | Cu√°ndo usarlo en este m√≥dulo | Para qu√© |
|----------|---------|------------------------------|----------|
| **Obligatorio** | `Herramientas_Estudio/DRILL_DIMENSIONES_NUMPY.md` | Semana 13‚Äì16, cada vez que implementes distancias/proyecciones y se rompan shapes | Evitar errores silenciosos en broadcasting/`axis` |
| **Obligatorio** | `Herramientas_Estudio/DIARIO_ERRORES.md` | Cuando K-Means produzca clusters vac√≠os, `NaN` o PCA devuelva resultados inestables | Registrar el caso y dejarlo ‚Äúdebuggeable‚Äù |
| **Complementario** | [3Blue1Brown: Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) | Semana 15 (PCA), al ver varianza/proyecciones/autovectores | Intuici√≥n visual para PCA |
| **Complementario** | [VisuAlgo](https://visualgo.net/en) | Semana 13‚Äì14, al estudiar el comportamiento iterativo de K-Means y su sensibilidad a inicializaci√≥n | Visualizar algoritmos paso a paso para construir intuici√≥n |
| **Complementario** | [Mathematics for ML (book)](https://mml-book.github.io/) | Semana 15‚Äì16, al formalizar covarianza, eigen/SVD | Notaci√≥n y derivaciones m√°s rigurosas |
| **Opcional** | [RECURSOS.md](RECURSOS.md) | Al terminar el m√≥dulo (para profundizar en clustering/reducci√≥n dimensional) | Elegir material extra sin dispersarte |

---

## üß† ¬øQu√© es Unsupervised Learning?

```text
APRENDIZAJE NO SUPERVISADO

Tenemos:
- Datos de entrada X (features)
- NO tenemos etiquetas Y

Objetivo: Encontrar estructura oculta en los datos

Tipos principales:
‚îú‚îÄ‚îÄ CLUSTERING: Agrupar puntos similares
‚îÇ   ‚îî‚îÄ‚îÄ K-Means, DBSCAN, Hierarchical
‚îú‚îÄ‚îÄ REDUCCI√ìN DIMENSIONAL: Comprimir features
‚îÇ   ‚îî‚îÄ‚îÄ PCA, t-SNE, UMAP
‚îî‚îÄ‚îÄ DETECCI√ìN DE ANOMAL√çAS: Encontrar outliers
    ‚îî‚îÄ‚îÄ Isolation Forest, GMM
```

---

## üìö Contenido del M√≥dulo

| Semana | Tema | Entregable |
|--------|------|------------|
| 13 | K-Means Clustering | `kmeans.py` |
| 14 | Evaluaci√≥n de Clusters | M√©tricas de clustering |
| 15 | PCA | `pca.py` |
| 16 | PCA Aplicado + GMM | Compresi√≥n de im√°genes |

---

## üß© Micro-Cap√≠tulo Maestro: PCA (Principal Component Analysis) ‚Äî Nivel: Avanzado

### 1) Intuici√≥n: la mejor foto

Imagina que tienes un objeto 3D (tus datos en alta dimensi√≥n) y solo puedes tomar una ‚Äúfoto‚Äù en 2D.

- Si tomas la foto desde un √°ngulo malo, la sombra se ve ‚Äúaplastada‚Äù y pierdes estructura.
- Si tomas la foto desde el √°ngulo correcto, la sombra conserva la mayor cantidad de informaci√≥n posible.

PCA busca matem√°ticamente ese √°ngulo: la proyecci√≥n donde la **varianza proyectada** es m√°xima.

### 2) Derivaci√≥n l√≥gica (covarianza ‚Üí eigen)

1) **Centrar**

Mueves el origen para que el promedio sea 0:

`X_c = X - Œº`

2) **Covarianza**

La matriz de covarianza captura c√≥mo ‚Äúse estiran‚Äù los datos:

`Œ£ = (1/(n-1)) X_c^T X_c`

Si (en 2D) `Œ£ = [[10, 0],[0, 1]]`, significa: hay mucha m√°s varianza en X que en Y.

3) **Eigenvectors y eigenvalues**

- Los **eigenvectors** de `Œ£` apuntan en direcciones principales de estiramiento.
- Los **eigenvalues** dicen cu√°nta varianza hay en esas direcciones.

PCA elige los eigenvectors con eigenvalues m√°s grandes y proyecta ah√≠.

### 3) Por qu√© SVD suele ser mejor que eigen en c√≥digo

Si calculas `X_c^T X_c` puedes amplificar problemas num√©ricos (est√°s ‚Äúcuadrando‚Äù escalas).

En cambio, con SVD:

`X_c = U S V^T`

se obtienen las componentes principales directamente desde `V` sin formar expl√≠citamente `Œ£`.

Regla pr√°ctica:

- **En teor√≠a:** PCA = eigen de la covarianza.
- **En pr√°ctica:** PCA = SVD de `X_c` (m√°s estable; es lo que usan implementaciones modernas).

---

## üíª Parte 1: K-Means Clustering

### 1.0 K-Means ‚Äî Nivel: intermedio (core del Pathway)

**Prop√≥sito:** pasar de ‚Äús√© que K-Means agrupa puntos‚Äù a **poder implementarlo desde cero, elegir `k` con criterio y detectar cu√°ndo NO usarlo**.

#### Objetivos de aprendizaje (medibles)

Al terminar esta parte podr√°s:

- **Explicar** la funci√≥n objetivo de K-Means (inercia) y por qu√© usa distancia euclidiana.
- **Aplicar** el algoritmo de Lloyd (asignar ‚Üí actualizar ‚Üí repetir) y reconocer convergencia.
- **Implementar** K-Means++ y justificar por qu√© mejora la inicializaci√≥n.
- **Analizar** fallas t√≠picas: clusters vac√≠os, sensibilidad a escala, m√≠nimos locales.
- **Evaluar** resultados usando inercia y silhouette (y entender limitaciones de ambas).

#### Prerrequisitos

- De `M√≥dulo 01`: NumPy (broadcasting, `axis`, shapes).
- De `M√≥dulo 02`: norma L2 / distancia euclidiana.

Enlaces r√°pidos:

- [GLOSARIO: K-Means](GLOSARIO.md#k-means)
- [GLOSARIO: K-Means++](GLOSARIO.md#k-means-1)
- [GLOSARIO: Inertia](GLOSARIO.md#inertia)
- [GLOSARIO: Clustering](GLOSARIO.md#clustering)

#### Resumen ejecutivo (big idea)

K-Means alterna dos pasos que **siempre reducen (o no aumentan)** la inercia:

- **Asignaci√≥n:** cada punto va al centroide m√°s cercano.
- **Actualizaci√≥n:** cada centroide se mueve al promedio de sus puntos.

Esto garantiza que el algoritmo converge (en iteraciones finitas), pero **no garantiza el m√≠nimo global**: por eso la inicializaci√≥n (K-Means++) importa.

#### Intuici√≥n ‚Üí formalizaci√≥n

##### a) Intuici√≥n

K-Means intenta poner `k` ‚Äúimanes‚Äù (centroides) y moverlos hasta que cada im√°n represente bien a los puntos que atrajo.

##### a.1 Intuici√≥n geom√©trica: Voronoi tessellation (territorios)

Una forma visual de entender K-Means:

- pones `k` centroides como ‚Äúsemillas‚Äù en el plano
- cada semilla **reclama el territorio** de los puntos m√°s cercanos

Eso induce un particionado del espacio en **celdas de Voronoi**: regiones poligonales donde todos los puntos est√°n m√°s cerca de un centroide que de cualquier otro.

En cada iteraci√≥n de Lloyd:

- **Asignaci√≥n:** recalculas las celdas (qui√©n pertenece a qui√©n)
- **Actualizaci√≥n:** cada semilla se mueve al centro de masa de su celda

##### b) Formalizaci√≥n

Funci√≥n objetivo:

`J = Œ£·µ¢ Œ£_{x‚ààC·µ¢} ||x - Œº·µ¢||¬≤`

Donde:

- `Œº·µ¢` es el centroide del cluster `i`.
- `C·µ¢` es el conjunto de puntos asignados al cluster `i`.

##### c) Condiciones donde K-Means funciona bien

- clusters ‚Äúredondos‚Äù / aproximadamente esf√©ricos
- tama√±os similares
- distancia euclidiana tiene sentido (features en la misma escala)

##### d) Casos donde falla (y c√≥mo detectarlo)

- clusters alargados/no convexos (ej.: ‚Äúdos lunas‚Äù)
- escalas distintas sin normalizar (una feature domina)
- outliers fuertes arrastran centroides

#### Actividades activas (aprendizaje activo)

- **Retrieval practice (3‚Äì5 min):** escribe sin mirar:
  - los dos pasos del algoritmo de Lloyd
  - la funci√≥n objetivo `J`
- **Ejercicio de diagn√≥stico:** crea 2 features con escalas distintas y observa c√≥mo cambia el clustering si normalizas.

#### Debugging / validaci√≥n (v5)

- Si obtienes resultados raros, revisa primero:
  - shapes (`X: (n_samples, n_features)`, `centroids: (k, n_features)`, `labels: (n_samples,)`)
  - `NaN` por clusters vac√≠os (centroide sin puntos)
- Registra hallazgos en `Herramientas_Estudio/DIARIO_ERRORES.md`.
- Antes de usar un dataset real ‚Äúsucio‚Äù, aplica `Herramientas_Estudio/DIRTY_DATA_CHECK.md`.
- Para integrar el protocolo completo:
  - [PLAN_V4_ESTRATEGICO.md](../../Recursos_Adicionales/Planes_Estrategicos/PLAN_V4_ESTRATEGICO.md)
  - [PLAN_V5_ESTRATEGICO.md](../../Recursos_Adicionales/Planes_Estrategicos/PLAN_V5_ESTRATEGICO.md)

#### Cheat sheet (repaso r√°pido)

- **Paso 1:** `labels = argmin(||x - Œº·µ¢||¬≤)`
- **Paso 2:** `Œº·µ¢ = mean(points_in_cluster_i)`
- **Convergencia:** `||Œº_new - Œº_old||¬≤ < tol`
- **Riesgo:** m√≠nimos locales ‚Üí usar K-Means++ y/o m√∫ltiples inicializaciones

### 1.1 Algoritmo de Lloyd

```python
import numpy as np  # Importa NumPy: se usa para RNG, arrays, distancias cuadr√°ticas y muestreo probabil√≠stico en K-Means++

"""
K-MEANS CLUSTERING (Algoritmo de Lloyd)

Objetivo: Particionar n puntos en k clusters, minimizando la
varianza intra-cluster (inercia).

Algoritmo:
1. Inicializar k centroides (aleatorio o k-means++)
2. Repetir hasta convergencia:
   a. ASIGNAR: cada punto al centroide m√°s cercano
   b. ACTUALIZAR: mover cada centroide al promedio de sus puntos
3. Retornar centroides y asignaciones

Funci√≥n objetivo (minimizar):
    J = Œ£·µ¢ Œ£‚±º ||x‚±º - Œº·µ¢||¬≤

Donde x‚±º pertenece al cluster i con centroide Œº·µ¢
"""

def euclidean_distance(a: np.ndarray, b: np.ndarray) -> float:  # Distancia euclidiana entre dos puntos.
    """Distancia euclidiana entre dos puntos."""
    return np.sqrt(np.sum((a - b) ** 2))  # Calcula ||a-b||‚ÇÇ: resta vectorial, eleva al cuadrado, suma por dimensi√≥n y aplica ra√≠z

def assign_clusters(X: np.ndarray, centroids: np.ndarray) -> np.ndarray:  # Asigna cada punto al centroide m√°s cercano.
    """
    Asigna cada punto al centroide m√°s cercano.

    Args:
        X: datos (n_samples, n_features)
        centroids: centroides actuales (k, n_features)

    Returns:
        labels: √≠ndice del cluster para cada punto (n_samples,)
    """
    n_samples = X.shape[0]  # N√∫mero de puntos: define cu√°ntas filas tendr√° la matriz de distancias y la longitud de labels
    k = centroids.shape[0]  # N√∫mero de centroides/clusters: define cu√°ntas columnas tendr√° la matriz de distancias

    # Calcular distancia de cada punto a cada centroide
    distances = np.zeros((n_samples, k))  # Reserva distancias (n,k): cada entrada [j,i] ser√° la distancia de X[j] al centroide i
    for i in range(k):  # Recorre centroides: calcula distancias de TODOS los puntos a un centroide a la vez (vectorizado por filas)
        distances[:, i] = np.sqrt(np.sum((X - centroids[i]) ** 2, axis=1))  # Distancia Eucl√≠dea por punto: sum over features y sqrt para cada fila

    # Asignar al m√°s cercano
    return np.argmin(distances, axis=1)  # Label por punto: √≠ndice i del centroide con distancia m√≠nima (argmin sobre columnas)

def update_centroids(X: np.ndarray, labels: np.ndarray, k: int) -> np.ndarray:  # Actualiza centroides como el promedio de los puntos asignados.
    """
    Actualiza centroides como el promedio de los puntos asignados.

    Args:
        X: datos
        labels: asignaciones actuales
        k: n√∫mero de clusters

    Returns:
        nuevos centroides
    """
    n_features = X.shape[1]  # Dimensionalidad d: n√∫mero de features por punto, define el ancho del array de centroides
    centroids = np.zeros((k, n_features))  # Inicializa centroides nuevos (k,d): se llenan con medias por cluster

    for i in range(k):  # Recorre cada cluster i: recalcula su centroide como promedio de sus puntos asignados
        points_in_cluster = X[labels == i]  # Selecciona puntos asignados al cluster i: indexaci√≥n booleana
        if len(points_in_cluster) > 0:  # Evita cluster vac√≠o: sin puntos no se puede calcular media (mean sobre vac√≠o -> warning/NaN)
            centroids[i] = np.mean(points_in_cluster, axis=0)  # Media por feature: define el nuevo centroide como el "centro" de su nube

    return centroids  # Devuelve centroides actualizados (k,d): promedios por cluster para el siguiente paso de asignaci√≥n
```

### 1.2 K-Means++ Initialization

```python
import numpy as np  # Importa NumPy: se usa para RNG, distancias cuadr√°ticas y muestreo ponderado en la inicializaci√≥n K-Means++

def kmeans_plus_plus_init(X: np.ndarray, k: int, random_state: int = None) -> np.ndarray:  # Inicializa centroides con K-Means++ (mejora convergencia vs random)
    """
    Inicializaci√≥n K-Means++.

    Mejor que inicializaci√≥n aleatoria porque:
    - Elige centroides que est√°n lejos entre s√≠
    - Reduce la probabilidad de mala convergencia
    - Garantiza O(log k) de la soluci√≥n √≥ptima

    Algoritmo:
    1. Elegir primer centroide aleatoriamente
    2. Para cada centroide restante:
       a. Calcular distancia de cada punto al centroide m√°s cercano
       b. Elegir nuevo centroide con probabilidad proporcional a d¬≤
    """
    if random_state is not None:  # Si se provee semilla, fijamos el RNG para reproducibilidad del muestreo de centroides
        np.random.seed(random_state)  # Setea semilla global: controla randint/choice usados abajo

    n_samples, n_features = X.shape  # Extrae shapes: n puntos y d features para dimensionar estructuras y muestrear √≠ndices
    centroids = np.zeros((k, n_features))  # Reserva matriz de centroides: (k,d) para ir llen√°ndola iterativamente

    # Primer centroide aleatorio
    first_idx = np.random.randint(n_samples)  # Elige √≠ndice inicial uniforme: primer centroide se toma al azar (paso 1 de K-Means++)
    centroids[0] = X[first_idx]  # Copia el primer centroide desde X: garantiza que el centroide es un punto real del dataset

    # Centroides restantes
    for c in range(1, k):  # Para cada centroide restante: selecciona un nuevo centroide sesgado hacia puntos lejanos
        # Calcular distancia al centroide m√°s cercano para cada punto
        distances = np.zeros(n_samples)  # Vector d¬≤ m√≠nimo por punto: almacenar√° la distancia^2 al centroide m√°s cercano
        for i in range(n_samples):  # Recorre cada muestra i: calcula su distancia al centroide m√°s cercano (entre los ya elegidos)
            min_dist = float('inf')  # Inicializa m√≠nimo: se actualizar√° comparando con cada centroide existente
            for j in range(c):  # Recorre centroides ya elegidos (0..c-1): busca el m√°s cercano al punto i
                dist = np.sum((X[i] - centroids[j]) ** 2)  # Distancia^2 a centroide j: evita sqrt y preserva orden para argmin
                min_dist = min(min_dist, dist)  # Actualiza m√≠nimo: mantiene la menor distancia^2 encontrada hasta ahora
            distances[i] = min_dist  # Guarda d¬≤ m√≠nimo para el punto i: define su probabilidad de ser elegido

        # Probabilidad proporcional a d¬≤
        probabilities = distances / np.sum(distances)  # Normaliza d¬≤ a distribuci√≥n: suma 1 y prioriza puntos lejanos a centroides actuales

        # Elegir nuevo centroide
        new_idx = np.random.choice(n_samples, p=probabilities)  # Samplea √≠ndice seg√∫n probs: implementa el sesgo K-Means++ (paso 2)
        centroids[c] = X[new_idx]  # Asigna el nuevo centroide: toma un punto real de X para evitar centroides fuera del soporte

    return centroids  # Devuelve centroides iniciales (k,d): se pasan a K-Means/Lloyd para empezar iteraciones desde una buena semilla
```

### 1.3 Implementaci√≥n Completa

```python
import numpy as np  # Importa NumPy: se usa para RNG, √°lgebra vectorizada y operaciones de distancia/centroides
from typing import Tuple  # Importa typing: documenta tipos de retorno/entradas (no afecta runtime)

class KMeans:  # Implementa K-Means desde cero: alterna asignaci√≥n de clusters y actualizaci√≥n de centroides hasta convergencia
    """K-Means Clustering implementado desde cero."""  # Docstring: describe la clase; es un literal string y no cambia el comportamiento

    def __init__(  # Inicializa hiperpar√°metros y atributos del modelo
        self,  # Referencia a la instancia: permite setear atributos persistentes del estimador
        n_clusters: int = 3,  # k: cantidad de clusters/centroides a aprender
        max_iter: int = 300,  # M√°ximo de iteraciones: tope de seguridad si no converge
        tol: float = 1e-4,  # Tolerancia de convergencia: umbral para detener cuando los centroides cambian muy poco
        init: str = 'kmeans++',  # Estrategia de inicializaci√≥n: 'kmeans++' o 'random'
        random_state: int = None  # Semilla opcional: hace reproducibles tanto init como sampling aleatorio
    ):  # Cierra firma: se ejecuta al definir el m√©todo
        """
        Args:
            n_clusters: n√∫mero de clusters (k)
            max_iter: m√°ximo de iteraciones
            tol: tolerancia para convergencia
            init: 'kmeans++' o 'random'
            random_state: semilla para reproducibilidad
        """
        self.n_clusters = n_clusters  # Guarda k: se reutiliza en fit/predict y en loops internos
        self.max_iter = max_iter  # Guarda l√≠mite de iteraciones: controla el ciclo de entrenamiento
        self.tol = tol  # Guarda tolerancia: define criterio de parada por desplazamiento de centroides
        self.init = init  # Guarda modo de init: decide c√≥mo se eligen centroides iniciales
        self.random_state = random_state  # Guarda semilla: permite reproducibilidad de resultados

        self.centroids = None  # Placeholder: centroides aprendidos (k, n_features); se setea en fit
        self.labels_ = None  # Placeholder: asignaci√≥n por muestra (n_samples,); se setea en fit
        self.inertia_ = None  # Placeholder: SSE final dentro de clusters; se computa al final del fit
        self.n_iter_ = 0  # Contador de iteraciones ejecutadas: √∫til para diagn√≥stico de convergencia

    def _init_centroids(self, X: np.ndarray) -> np.ndarray:  # Inicializa centroides de acuerdo al modo elegido
        """Inicializa centroides."""  # Docstring: describe el helper; no altera la l√≥gica
        if self.random_state is not None:  # Si hay semilla, fijamos el RNG global para reproducibilidad del muestreo de centroides
            np.random.seed(self.random_state)  # Setea semilla: controla np.random.choice/np.random.randint usados abajo

        if self.init == 'kmeans++':  # Rama 1: init informado por distancia (mejor que random para evitar m√≠nimos malos)
            return kmeans_plus_plus_init(X, self.n_clusters, self.random_state)  # Devuelve centroides iniciales con K-Means++
        else:  # Rama 2: init aleatorio (baseline) para comparaci√≥n/rapidez
            # Inicializaci√≥n aleatoria
            indices = np.random.choice(len(X), self.n_clusters, replace=False)  # Samplea k √≠ndices distintos: evita centroides duplicados
            return X[indices].copy()  # Copia centroides iniciales: evita aliasing con X al modificarlos durante el training

    def _compute_inertia(self, X: np.ndarray) -> float:  # Calcula inercia/SSE dentro de clusters para el estado actual
        """
        Calcula inercia (within-cluster sum of squares).

        Inercia = Œ£·µ¢ Œ£‚±º ||x‚±º - Œº·µ¢||¬≤
        """
        inertia = 0  # Acumulador SSE: suma distancias cuadradas de cada punto a su centroide asignado
        for i in range(self.n_clusters):  # Itera clusters: computa contribuci√≥n por centroide i
            cluster_points = X[self.labels_ == i]  # Selecciona puntos asignados al cluster i: indexaci√≥n booleana
            if len(cluster_points) > 0:  # Evita cluster vac√≠o: si no hay puntos, su SSE contribuye 0
                inertia += np.sum((cluster_points - self.centroids[i]) ** 2)  # Suma distancias^2 al centroide i
        return inertia  # Devuelve SSE total: se usa en elbow method y diagn√≥stico

    def fit(self, X: np.ndarray) -> 'KMeans':  # Entrena el modelo: aprende centroides y labels para X
        """Entrena el modelo."""  # Docstring: describe m√©todo; no modifica el entrenamiento
        # Inicializar centroides
        self.centroids = self._init_centroids(X)  # Setea centroides iniciales: punto de partida del loop iterativo

        for iteration in range(self.max_iter):  # Loop principal: alterna asignaci√≥n (E-step) y actualizaci√≥n (M-step)
            # Guardar centroides anteriores
            old_centroids = self.centroids.copy()  # Copia para medir desplazamiento: criterio de convergencia

            # Paso 1: Asignar puntos a clusters
            self.labels_ = assign_clusters(X, self.centroids)  # Asigna cada punto al centroide m√°s cercano (por distancia^2)

            # Paso 2: Actualizar centroides
            self.centroids = update_centroids(X, self.labels_, self.n_clusters)  # Recalcula centroides como media de puntos asignados

            # Verificar convergencia
            centroid_shift = np.sum((self.centroids - old_centroids) ** 2)  # Shift global: suma de desplazamientos^2 entre iteraciones
            if centroid_shift < self.tol:  # Si el cambio total es peque√±o, asumimos convergencia (ya no mejora materialmente)
                break  # Sale temprano: ahorra c√≥mputo manteniendo la soluci√≥n estable

        self.n_iter_ = iteration + 1  # Guarda iteraciones efectivas (iteration es 0-index)
        self.inertia_ = self._compute_inertia(X)  # Calcula inercia final: m√©trica interna del ajuste

        return self  # Permite chaining (kmeans.fit(X).predict(X)) y acceso a atributos entrenados

    def predict(self, X: np.ndarray) -> np.ndarray:  # Predice cluster para datos nuevos usando centroides ya aprendidos
        """Predice clusters para nuevos datos."""  # Docstring: describe uso; no cambia la predicci√≥n
        return assign_clusters(X, self.centroids)  # Reutiliza misma asignaci√≥n: calcula distancias a centroides y retorna argmin

    def fit_predict(self, X: np.ndarray) -> np.ndarray:  # Atajo: fit + retorna labels en una sola llamada
        """Entrena y predice."""  # Docstring: describe atajo
        self.fit(X)  # Entrena primero: actualiza centroides/labels/inercia
        return self.labels_  # Devuelve labels aprendidas: resultado principal del clustering


# Demo
np.random.seed(42)  # Fija semilla: hace reproducible el dataset sint√©tico del demo

# Generar datos sint√©ticos (3 clusters)
cluster1 = np.random.randn(100, 2) + [0, 0]  # Cluster 1: nube gaussiana centrada en (0,0)
cluster2 = np.random.randn(100, 2) + [5, 5]  # Cluster 2: nube gaussiana centrada en (5,5)
cluster3 = np.random.randn(100, 2) + [10, 0]  # Cluster 3: nube gaussiana centrada en (10,0)
X = np.vstack([cluster1, cluster2, cluster3])  # Dataset final: concatena clusters => shape (300,2)

# Entrenar
kmeans = KMeans(n_clusters=3, random_state=42)  # Crea estimador con k=3: coincide con generaci√≥n sint√©tica
labels = kmeans.fit_predict(X)  # Ajusta el modelo y obtiene labels: deber√≠a separar bien los 3 grupos

print(f"Iteraciones: {kmeans.n_iter_}")  # Muestra iteraciones: indica rapidez de convergencia
print(f"Inercia: {kmeans.inertia_:.2f}")  # Muestra SSE final: menor suele implicar clusters m√°s compactos (pero depende de k)
print(f"Centroides:\n{kmeans.centroids}")  # Muestra centroides aprendidos: aproximan los centros de las nubes
```

---

## üíª Parte 2: Evaluaci√≥n de Clusters

### 2.0 Evaluaci√≥n ‚Äî c√≥mo decidir si el clustering ‚Äútiene sentido‚Äù

**Prop√≥sito:** evitar el error com√∫n de ‚ÄúK-Means siempre devuelve clusters, entonces siempre sirve‚Äù. Aqu√≠ aprendes a **medir calidad** y a entender por qu√© esas m√©tricas pueden enga√±ar.

#### Objetivos de aprendizaje (medibles)

- **Explicar** qu√© mide la inercia y por qu√© siempre baja al subir `k`.
- **Aplicar** el m√©todo del codo como heur√≠stica (no como regla matem√°tica).
- **Interpretar** silhouette score (qu√© significa cerca de 1, 0 y valores negativos).
- **Analizar** cu√°ndo no puedes validar bien (porque no hay ground truth).

Enlaces r√°pidos:

- [GLOSARIO: Inertia](GLOSARIO.md#inertia)
- [GLOSARIO: Silhouette Score](GLOSARIO.md#silhouette-score)

#### Resumen ejecutivo

- **Inercia:** mide compactaci√≥n interna; √∫til para comparar `k`, pero sesgada (siempre favorece `k` grande).
- **Silhouette:** mezcla cohesi√≥n y separaci√≥n; √∫til para comparar modelos, pero costosa de calcular de forma exacta.

#### Actividades activas

- Ejecuta elbow + silhouette sobre el mismo dataset y escribe una conclusi√≥n:
  - ¬øcoinciden en el `k`?
  - si no coinciden, ¬øpor qu√© podr√≠a pasar?

### 2.1 Inercia (Within-Cluster Sum of Squares)

```python
def compute_inertia(X: np.ndarray, labels: np.ndarray, centroids: np.ndarray) -> float:  # Calcula inercia/SSE: suma de distancias^2 de puntos a su centroide (m√©trica interna de compactaci√≥n)
    """
    Inercia: suma de distancias cuadradas al centroide.

    Menor inercia = clusters m√°s compactos.

    Problema: siempre disminuye al aumentar k.
    Soluci√≥n: usar m√©todo del codo.
    """
    inertia = 0  # Acumulador SSE: suma de distancias cuadr√°ticas intra-cluster (cuanto menor, m√°s compactos los clusters)
    for i, centroid in enumerate(centroids):  # Recorre centroides: agrega contribuci√≥n de cada cluster i a la inercia total
        cluster_points = X[labels == i]  # Selecciona puntos asignados al cluster i: subconjunto sobre el que se mide compactaci√≥n
        inertia += np.sum((cluster_points - centroid) ** 2)  # Suma ||x-Œº_i||^2 sobre puntos del cluster i: define la SSE intra-cluster
    return inertia  # Devuelve inercia total: se usa para comparar k en el elbow method (aunque siempre decrece al aumentar k)
```

### 2.2 M√©todo del Codo (Elbow Method)

```python
import numpy as np  # Importa NumPy: se usa para almacenar/operar con listas de inercia y manejar rangos de k
import matplotlib.pyplot as plt  # Importa Matplotlib: se usa para graficar la curva de inercia vs k ("codo")

def elbow_method(X: np.ndarray, k_range: range) -> list:  # Ejecuta KMeans para m√∫ltiples k y devuelve la lista de inercias para detectar el ‚Äúcodo‚Äù
    """
    M√©todo del codo para elegir k √≥ptimo.

    Busca el punto donde a√±adir m√°s clusters
    no reduce significativamente la inercia.
    """
    inertias = []  # Lista de inercia por k: se llena en el loop y luego se grafica para buscar el ‚Äúcodo‚Äù

    for k in k_range:  # Itera candidatos k: prueba distintos n√∫meros de clusters para ver c√≥mo cae la inercia
        kmeans = KMeans(n_clusters=k, random_state=42)  # Instancia KMeans: fija random_state para que comparaciones entre k sean reproducibles
        kmeans.fit(X)  # Ajusta el modelo: ejecuta Lloyd y aprende centroides/labels; calcula inercia final
        inertias.append(kmeans.inertia_)  # Guarda la inercia del modelo: suma de distancias cuadr√°ticas intra-cluster (menor es mejor pero sesga a k alto)

    return inertias  # Devuelve lista alineada con k_range: se usa para graficar y detectar visualmente el punto de codo

def plot_elbow(k_range: range, inertias: list):  # Grafica la curva k vs inercia para elegir k por criterio visual/heur√≠stico
    """Visualiza el m√©todo del codo."""
    plt.figure(figsize=(8, 5))  # Crea figura: define tama√±o para una lectura clara de la curva
    plt.plot(list(k_range), inertias, 'bo-')  # Curva k vs inercia: puntos azules con l√≠nea (visualiza tendencia y posible ‚Äúcodo‚Äù)
    plt.xlabel('N√∫mero de clusters (k)')  # Etiqueta eje x: variable controlada (cantidad de clusters)
    plt.ylabel('Inercia')  # Etiqueta eje y: m√©trica interna que siempre baja con k (no debe optimizarse ‚Äúa ciegas‚Äù)
    plt.title('M√©todo del Codo')  # T√≠tulo: contextualiza la gr√°fica
    plt.grid(True)  # Activa grilla: facilita comparar ca√≠das relativas entre k consecutivos
    plt.show()  # Renderiza la figura: muestra el plot al usuario

# Demo
# inertias = elbow_method(X, range(1, 11))
# plot_elbow(range(1, 11), inertias)
```

### 2.3 Silhouette Score

```python
import numpy as np  # Importa NumPy: se usa para sqrt/sum/mean, comparaciones, uniques y manejo de arrays

def silhouette_sample(X: np.ndarray, labels: np.ndarray, idx: int) -> float:  # Silhouette puntual: calcula s(i) para una muestra i
    """
    Calcula silhouette para un solo punto.

    s(i) = (b(i) - a(i)) / max(a(i), b(i))

    Donde:
    - a(i): distancia promedio a puntos del mismo cluster
    - b(i): distancia promedio m√≠nima a puntos de otro cluster

    Rango: [-1, 1]
    - 1: punto bien asignado
    - 0: punto en frontera entre clusters
    - -1: punto mal asignado
    """
    point = X[idx]  # Punto i: vector de features para el que se calcula silhouette
    label = labels[idx]  # Label del punto i: define su cluster para calcular cohesi√≥n (a) y separaci√≥n (b)

    # a(i): distancia promedio intra-cluster
    same_cluster = X[labels == label]  # Puntos del mismo cluster: se usan para promedio intra-cluster a(i)
    if len(same_cluster) > 1:  # Si el cluster tiene m√°s de 1 punto, a(i) se define como promedio de distancias a los dem√°s
        a = np.mean([np.sqrt(np.sum((point - p) ** 2))  # Distancia Eucl√≠dea a cada punto del mismo cluster
                     for p in same_cluster if not np.array_equal(p, point)])  # Excluye el propio punto: evita incluir distancia 0
    else:  # Si el cluster es unitario (solo i), no hay vecinos intra-cluster para promediar: usamos convenci√≥n a=0
        a = 0  # Edge case: cluster unitario (solo el punto) => cohesi√≥n se define como 0 por convenci√≥n

    # b(i): distancia promedio al cluster m√°s cercano
    unique_labels = np.unique(labels)  # Clusters presentes: se iteran para buscar el cluster alternativo m√°s cercano
    b = float('inf')  # Inicializa b(i): se busca el m√≠nimo promedio a cualquier cluster distinto
    for other_label in unique_labels:  # Itera clusters alternativos: busca el cluster ‚Äúvecino‚Äù con menor distancia media (b(i))
        if other_label != label:  # Omite el propio cluster: b(i) se define respecto a otros clusters
            other_cluster = X[labels == other_label]  # Puntos del cluster candidato: se usan para distancia media inter-cluster
            if len(other_cluster) > 0:  # Evita mean sobre vac√≠o: aunque np.unique suele garantizar que el cluster existe, es defensa adicional
                avg_dist = np.mean([np.sqrt(np.sum((point - p) ** 2))  # Distancia Eucl√≠dea a cada punto del cluster candidato
                                   for p in other_cluster])  # Promedio: distancia media de i al cluster other_label
                b = min(b, avg_dist)  # Actualiza mejor b(i): elige el cluster con menor distancia media

    if b == float('inf'):  # Edge case: no se encontr√≥ cluster alternativo (labels degenerados) => b(i) no est√° definido
        return 0  # Convenci√≥n: score neutral cuando no hay comparaci√≥n posible

    return (b - a) / max(a, b)  # F√≥rmula silhouette puntual: normaliza para acotar en [-1,1] y comparar cohesi√≥n vs separaci√≥n

def silhouette_score(X: np.ndarray, labels: np.ndarray) -> float:  # Silhouette global: promedio de s(i) sobre todas las muestras
    """
    Silhouette Score promedio para todos los puntos.

    Mayor es mejor (max = 1).
    """
    scores = [silhouette_sample(X, labels, i) for i in range(len(X))]  # Calcula s(i) para cada punto: comprensi√≥n lista para promediar
    return np.mean(scores)  # Promedia scores por punto: devuelve silhouette global del clustering


# Demo
# score = silhouette_score(X, labels)
# print(f"Silhouette Score: {score:.4f}")
```

---

## üíª Parte 3: PCA (Principal Component Analysis)

### 3.0 PCA ‚Äî Nivel: intermedio (reducci√≥n dimensional con rigor)

**Prop√≥sito:** pasar de ‚ÄúPCA reduce dimensiones‚Äù a **poder derivar su l√≥gica, implementarlo con SVD y usar varianza explicada para tomar decisiones**.

#### Objetivos de aprendizaje (medibles)

Al terminar esta parte podr√°s:

- **Explicar** por qu√© PCA encuentra direcciones de m√°xima varianza (y qu√© NO significa eso).
- **Aplicar** el pipeline correcto: centrar ‚Üí descomponer (SVD) ‚Üí proyectar ‚Üí reconstruir.
- **Implementar** PCA con SVD y calcular `explained_variance_ratio_`.
- **Elegir** `n_components` por varianza acumulada y justificar el trade-off.
- **Diagnosticar** errores t√≠picos: no centrar datos, confundir componentes con scores, reconstrucci√≥n incorrecta.

#### Motivaci√≥n / por qu√© importa

En la mayor√≠a de los problemas reales, la intuici√≥n visual se pierde en espacios de alta dimensi√≥n (ej.: cientos o miles de features). PCA te permite:

- **Visualizar** en 2D/3D sin tirar informaci√≥n ‚Äúa ojo‚Äù.
- **Eliminar ruido** (qued√°ndote con las direcciones dominantes de variaci√≥n).
- **Comprimir** (reconstruir aproximaciones controlando el error).

Regla pr√°ctica: PCA no ‚Äúencuentra lo que separa clases‚Äù; encuentra lo que **m√°s var√≠a**.

#### Prerrequisitos

- De `M√≥dulo 02`: SVD (intuici√≥n) y producto matricial.
- De `M√≥dulo 02`: matriz de covarianza, eigenvalues y eigenvectors.
- De `M√≥dulo 02`: proyecci√≥n (producto punto) y norma.
- De `M√≥dulo 01`: manipulaci√≥n de shapes y `axis`.

Enlaces r√°pidos:

- [GLOSARIO: PCA](GLOSARIO.md#pca-principal-component-analysis)
- [GLOSARIO: SVD](GLOSARIO.md#svd-singular-value-decomposition)
- [RECURSOS.md](RECURSOS.md)

#### Resumen ejecutivo (big idea)

PCA crea un nuevo sistema de coordenadas donde:

- el eje 1 (PC1) captura la mayor varianza,
- el eje 2 (PC2) captura la mayor varianza restante, y as√≠ sucesivamente,

y luego te permite quedarte con los primeros `k` ejes para comprimir.

#### Intuici√≥n ‚Üí formalizaci√≥n

##### a) Intuici√≥n

Si tus datos viven cerca de un plano dentro de un espacio 100D, PCA intenta encontrar ese plano (o subespacio) para representar los datos con menos n√∫meros.

Analog√≠a: ‚Äúbuscar el mejor √°ngulo para tomar una foto‚Äù

- Tienes un objeto 3D (tus datos en alta dimensi√≥n).
- Una foto 2D pierde informaci√≥n.
- PCA elige el **√°ngulo de c√°mara** que preserva la mayor ‚Äúinformaci√≥n‚Äù medible como **varianza**.

Met√°fora complementaria (baguette): imagina una nube de puntos alargada como una baguette flotando en 3D. Si tomas la foto desde la punta, parece un c√≠rculo (pierdes estructura). Si la tomas de lado, ves su longitud real. PCA busca ese ‚Äúlado‚Äù matem√°ticamente.

Ojo: ‚Äúm√°s varianza‚Äù no significa ‚Äúm√°s √∫til para clasificar‚Äù; solo significa ‚Äúm√°s dispersi√≥n‚Äù.

##### b) Conceptos clave (glosario m√≠nimo)

- **Varianza:** dispersi√≥n de los datos; PCA busca maximizarla *despu√©s* de proyectar.
- **Matriz de covarianza (`Œ£`):** matriz sim√©trica que describe c√≥mo var√≠an las variables y c√≥mo co-var√≠an entre s√≠.
- **Eigenvector (vector propio):** direcci√≥n que no cambia (salvo escala) al aplicar `Œ£`; en PCA, son los ejes principales.
- **Eigenvalue (valor propio):** varianza capturada en la direcci√≥n de su eigenvector.
- **Componente principal:** eje (eigenvector) ordenado por eigenvalue descendente.

##### c) Formalizaci√≥n m√≠nima

- Centrar: `X_c = X - mean(X)`
- SVD: `X_c = U S V·µÄ`
- Componentes principales: columnas de `V` (o filas de `V·µÄ`)
- Proyecci√≥n a `k` componentes: `Z = X_c @ V_k`
- Reconstrucci√≥n: `X_hat = Z @ V_k·µÄ + mean`

##### c.1 Maximizando la varianza (derivaci√≥n l√≥gica ‚Üí ecuaci√≥n de eigenvalores)

Idea: buscas un vector unitario `u` (direcci√≥n) tal que la varianza de la proyecci√≥n `u·µÄx` sea m√°xima.

Si `x` est√° centrado, la varianza proyectada es:

`Var(u·µÄx) = u·µÄ Œ£ u`

Planteas el problema:

`max_u  u·µÄ Œ£ u   s.a.  ||u||‚ÇÇ = 1`

Con multiplicadores de Lagrange, la condici√≥n de √≥ptimo lleva a:

`Œ£u = Œªu`

Interpretaci√≥n directa:

- `u` es un componente principal.
- `Œª` es la varianza capturada por ese componente.

##### c.2 Relaci√≥n SVD ‚Üî eigenvalues (por qu√© SVD es el m√©todo preferido)

Si `X_c` son los datos centrados y haces:

```
X_c = U S V·µÄ
```

Entonces la covarianza muestral es:

```
Œ£ = (1/(n-1)) X_c·µÄ X_c
  = (1/(n-1)) (V S U·µÄ)(U S V·µÄ)
  = V (S¬≤/(n-1)) V·µÄ
```

Conclusi√≥n:

- **Los eigenvectors de `Œ£`** son las columnas de `V`.
- **Los eigenvalues de `Œ£`** son `S¬≤/(n-1)`.

Esto conecta directamente con `M√≥dulo 02` (eigenvalues/eigenvectors) y explica por qu√© PCA ‚Äúv√≠a SVD‚Äù suele ser m√°s estable.

##### c.3 Worked example: PCA manual en 2D (rotaci√≥n de ejes)

Sup√≥n datos 2D que ‚Äúviven‚Äù casi sobre la diagonal `y = x`.

1) Centrar los datos:

```
X_c = X - mean(X)
```

2) Imagina que la covarianza queda (caso idealizado):

```
Œ£ = [[1, 1],
     [1, 1]]
```

3) Sus eigenvectors (direcciones principales) son:

- `v1 = (1, 1)/‚àö2`  (direcci√≥n diagonal)
- `v2 = (1, -1)/‚àö2` (direcci√≥n anti-diagonal)

Y sus eigenvalues:

- `Œª1 = 2` (mucha varianza en la diagonal)
- `Œª2 = 0` (casi nada en la anti-diagonal)

4) Proyecci√≥n a 1D:

```
z = X_c @ v1
```

Interpretaci√≥n: rotaste ejes y te quedaste solo con el eje donde ‚Äúvive‚Äù casi toda la variaci√≥n.

##### c.4 Worked example (num√©rico): covarianza y primer componente a mano

Datos centrados (6 puntos):

```text
X = [(-1,-1), (-2,-1), (-3,-2), (1,1), (2,1), (3,2)]
```

1) Construye `X` como matriz `(n_samples, 2)` y calcula:

`Œ£ = (1/(n-1)) X·µÄ X`

Aqu√≠:

```text
X·µÄX = [[28, 18],
       [18, 12]]
n-1 = 5
```

Por tanto:

```text
Œ£ = [[5.6, 3.6],
     [3.6, 2.4]]
```

2) Eigenvalues/eigenvectors (aprox.)

- `Œª1 ‚âà 7.94`, `Œª2 ‚âà 0.06`
- primer eigenvector (normalizado) `u1 ‚âà (0.84, 0.55)`

3) Varianza explicada del primer componente:

`Œª1/(Œª1+Œª2) ‚âà 7.94/8.00 ‚âà 99.3%`

Lectura: la nube est√° casi en una l√≠nea; proyectar a 1D conserva casi toda la estructura.

#### Algoritmo (paso a paso)

1) Centrar (y t√≠picamente escalar si tus features tienen unidades distintas).
2) SVD de `X_c` (recomendado) o eigen de `Œ£`.
3) Elegir `k` por varianza acumulada (y/o error de reconstrucci√≥n).
4) Proyectar `Z = X_c @ V_k`.
5) (Opcional) Reconstruir `X_hat = Z @ V_k·µÄ + mean`.

#### Implementaci√≥n pr√°ctica (c√≥digo)

En esta gu√≠a ya tienes:

- `pca_eigen(...)` en **3.2** (√∫til para entender la teor√≠a).
- `pca_svd(...)` y la clase `PCA` en **3.3‚Äì3.4** (recomendado para pr√°ctica).

#### Evaluaci√≥n formativa (r√°pida)

Pregunta: si `Œª1 = 9` y `Œª2 = 1`, ¬øqu√© proporci√≥n de varianza captura PC1?

Respuesta: `9/(9+1) = 90%`.

#### Actividades activas

- **Retrieval practice:** escribe las 4 ecuaciones (centrar, SVD, proyectar, reconstruir).
- **Experimento m√≠nimo:** genera datos 3D correlacionados, reduce a 1D y reporta:
  - varianza explicada
  - error de reconstrucci√≥n

#### Errores comunes

- **No centrar**: PCA se sesga hacia la media (resultado incorrecto).
- **Confundir `components` vs `X_pca`**: componentes son ejes; `X_pca` son coordenadas en esos ejes.
- **Elegir `n_components` ‚Äúa ojo‚Äù**: usar varianza acumulada.

### 3.1 Concepto

```python
"""
PCA - AN√ÅLISIS DE COMPONENTES PRINCIPALES

Objetivo: Reducir dimensionalidad preservando la m√°xima varianza.

Idea:
1. Centrar los datos (restar media)
2. Encontrar direcciones de m√°xima varianza (eigenvectors)
3. Proyectar datos en las top-k direcciones

Matem√°ticamente:
- Las componentes principales son los eigenvectors de la matriz de covarianza
- Los eigenvalues indican cu√°nta varianza captura cada componente

Aplicaciones:
- Visualizaci√≥n (reducir a 2D/3D)
- Preprocesamiento (eliminar ruido, reducir features)
- Compresi√≥n de datos/im√°genes
"""
```

### 3.2 PCA via Eigendecomposition

```python
import numpy as np  # Importa NumPy: se usa para medias, covarianza, eigen, ordenamientos y proyecciones en PCA (v√≠a eig)

def pca_eigen(X: np.ndarray, n_components: int) -> tuple:  # PCA por eigendecomposition: forma covarianza y extrae eigenvectors/eigenvalues
    """
    PCA usando eigendecomposition de la matriz de covarianza.

    Pasos:
    1. Centrar datos: X_centered = X - mean(X)
    2. Calcular matriz de covarianza: Œ£ = (1/(n-1)) X^T X
    3. Eigendecomposition: Œ£v = Œªv
    4. Ordenar eigenvectors por eigenvalue descendente
    5. Proyectar: X_pca = X_centered @ V[:, :k]

    Returns:
        X_pca: datos transformados
        components: eigenvectors (componentes principales)
        explained_variance_ratio: proporci√≥n de varianza por componente
    """
    # 1. Centrar
    mean = np.mean(X, axis=0)  # Media por feature: se resta para que la covarianza represente variaci√≥n alrededor de 0
    X_centered = X - mean  # Centra datos: elimina offset por feature para que PCA encuentre direcciones de varianza

    # 2. Matriz de covarianza
    n_samples = X.shape[0]  # N√∫mero de muestras n: se usa para normalizar covarianza con factor (n-1)
    cov_matrix = (X_centered.T @ X_centered) / (n_samples - 1)  # Covarianza (d,d): Xc^T Xc /(n-1) (asumiendo centrado)

    # 3. Eigendecomposition
    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)  # Eig: encuentra Œª y v tales que cov v = Œª v (direcciones principales)

    # Convertir a reales (puede haber componentes imaginarias peque√±as)
    eigenvalues = eigenvalues.real  # Descarta parte imaginaria peque√±a: puede aparecer por errores num√©ricos en eig
    eigenvectors = eigenvectors.real  # Mantiene eigenvectors reales: PCA real espera ejes en ‚Ñù^d

    # 4. Ordenar por eigenvalue descendente
    idx = np.argsort(eigenvalues)[::-1]  # √çndices ordenados desc: prioriza componentes que explican m√°s varianza
    eigenvalues = eigenvalues[idx]  # Reordena eigenvalues: queda Œª1 ‚â• Œª2 ‚â• ... para seleccionar top-k
    eigenvectors = eigenvectors[:, idx]  # Reordena columnas de eigenvectors: alinea v_j con eigenvalue Œª_j ordenado

    # 5. Seleccionar top k componentes
    components = eigenvectors[:, :n_components]  # Toma primeras k columnas: matriz (d,k) de componentes principales

    # 6. Proyectar
    X_pca = X_centered @ components  # Proyecci√≥n lineal: (n,d)@(d,k)->(n,k) da coordenadas en el subespacio de m√°xima varianza

    # 7. Varianza explicada
    total_variance = np.sum(eigenvalues)  # Varianza total: suma de eigenvalues equivale al trace(cov) (varianza total en d dims)
    explained_variance_ratio = eigenvalues[:n_components] / total_variance  # Ratio por componente: fracci√≥n de varianza explicada por cada Œª_j

    return X_pca, components, explained_variance_ratio, mean  # Devuelve proyecci√≥n, ejes, ratios y media para poder reconstruir/inferir
```

### 3.3 PCA via SVD (M√°s Estable)

```python
import numpy as np  # Importa NumPy: se usa para SVD, medias, proyecciones y varianza explicada en PCA (v√≠a SVD)

def pca_svd(X: np.ndarray, n_components: int) -> tuple:  # PCA por SVD: alternativa m√°s estable que eig para obtener componentes principales
    """
    PCA usando SVD (Singular Value Decomposition).

    M√°s estable num√©ricamente que eigendecomposition.

    Si X = UŒ£V^T, entonces:
    - V contiene las componentes principales
    - Œ£¬≤/(n-1) son los eigenvalues (varianzas)
    """
    # 1. Centrar
    mean = np.mean(X, axis=0)  # Media por feature: se resta para centrar y alinear PCA con covarianza (no con offsets)
    X_centered = X - mean  # Centra X: elimina offset para que SVD capture direcciones de varianza

    # 2. SVD
    U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)  # SVD compacta: Xc=U diag(S) Vt; estable num√©ricamente

    # 3. Componentes principales (filas de Vt, o columnas de V)
    components = Vt[:n_components].T  # Componentes (d,k): toma primeras k filas de Vt y transpone para usar como matriz de proyecci√≥n

    # 4. Proyectar
    X_pca = X_centered @ components  # Proyecta a k dims: coordenadas en el subespacio principal (scores)

    # 5. Varianza explicada
    n_samples = X.shape[0]  # n: se usa en el factor (n-1) para convertir S^2 en varianzas (eigenvalues)
    variance = (S ** 2) / (n_samples - 1)  # Varianza por componente: S^2/(n-1) corresponde a eigenvalues de cov(X)
    explained_variance_ratio = variance[:n_components] / np.sum(variance)  # Ratio truncado: var explicada por cada una de las k componentes

    return X_pca, components, explained_variance_ratio, mean  # Devuelve proyecci√≥n, ejes (d,k), ratios y media para reconstrucci√≥n
```

### 3.4 Implementaci√≥n Completa

```python
import numpy as np  # Importa NumPy: se usa para SVD, medias, proyecciones y generaci√≥n de datos sint√©ticos en el demo

class PCA:  # Implementa PCA desde cero: aprende ejes principales (componentes) y permite proyectar/reconstruir
    """Principal Component Analysis implementado desde cero."""  # Docstring: describe la clase; no afecta c√°lculos

    def __init__(self, n_components: int = 2):  # Inicializa PCA con k componentes a retener (dimensi√≥n reducida)
        """
        Args:
            n_components: n√∫mero de componentes a retener
        """
        self.n_components = n_components  # Guarda k: se usa para truncar Vt y para shapes de proyecci√≥n
        self.components_ = None  # (n_features, n_components)  # Placeholder: ejes principales aprendidos (columnas)
        self.explained_variance_ratio_ = None  # Placeholder: fracci√≥n de varianza explicada por cada componente
        self.mean_ = None  # Placeholder: media por feature para centrar y descentrar (inverse_transform)

    def fit(self, X: np.ndarray) -> 'PCA':  # Ajusta PCA: estima media, componentes y varianza explicada a partir de X
        """Calcula componentes principales."""  # Docstring: describe fit; no altera el resultado
        # Centrar
        self.mean_ = np.mean(X, axis=0)  # Media por columna: PCA est√°ndar requiere centrar para capturar covarianza
        X_centered = X - self.mean_  # Centra X: elimina offset para que SVD capture direcciones de varianza

        # SVD
        U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)  # SVD: X=U S V^T; V contiene direcciones principales en espacio de features

        # Componentes principales
        self.components_ = Vt[:self.n_components].T  # Componentes (d,k): toma primeras k filas de Vt y transpone para usar como matriz de proyecci√≥n

        # Varianza explicada
        n_samples = X.shape[0]  # n: se usa en el factor (n-1) para convertir S^2 en varianzas (eigenvalues)
        variance = (S ** 2) / (n_samples - 1)  # Varianza por componente: S^2/(n-1) corresponde a eigenvalues de cov(X)
        self.explained_variance_ratio_ = variance[:self.n_components] / np.sum(variance)  # Normaliza por varianza total: proporci√≥n explicada
        self.singular_values_ = S[:self.n_components]  # Guarda valores singulares truncados: √∫tiles para covarianza aproximada

        return self  # Permite chaining (pca.fit(X).transform(X)) y acceso a atributos entrenados

    def transform(self, X: np.ndarray) -> np.ndarray:  # Proyecta X al subespacio PCA (coordenadas en base de componentes)
        """Proyecta datos a espacio de componentes principales."""  # Docstring: describe proyecci√≥n
        X_centered = X - self.mean_  # Centra con la media aprendida: garantiza consistencia entre train y test
        return X_centered @ self.components_  # Proyecci√≥n lineal: (n_samples,n_features)@(n_features,k) -> (n_samples,k)

    def fit_transform(self, X: np.ndarray) -> np.ndarray:  # Atajo: ajusta PCA y devuelve la proyecci√≥n en una llamada
        """Fit y transform en un paso."""  # Docstring: describe atajo
        self.fit(X)  # Aprende media/componentes: actualiza estado interno
        return self.transform(X)  # Proyecta X usando el estado reci√©n aprendido

    def inverse_transform(self, X_pca: np.ndarray) -> np.ndarray:  # Reconstruye aproximaci√≥n en espacio original desde coordenadas PCA
        """
        Reconstruye datos desde el espacio PCA.

        X_reconstructed = X_pca @ components.T + mean

        Nota: hay p√©rdida de informaci√≥n si n_components < n_features
        """
        return X_pca @ self.components_.T + self.mean_  # Re-proyecta a features y suma media: reconstrucci√≥n es aproximada si k<n_features

    def get_covariance(self) -> np.ndarray:  # Aproxima cov(X) usando componentes y valores singulares (relaci√≥n con Œ£^2)
        """Retorna matriz de covarianza aproximada."""  # Docstring: describe la salida (matriz n_features x n_features)
        return self.components_ @ np.diag(self.singular_values_ ** 2) @ self.components_.T  # Reconstruye cov aprox en base PCA


# Demo
np.random.seed(42)  # Fija semilla: hace reproducible el demo (mismos datos => mismas m√©tricas/resultados)

# Datos correlacionados en 3D
n_samples = 200  # Cantidad de muestras sint√©ticas: controla tama√±o del dataset para el ejemplo
X = np.random.randn(n_samples, 3)  # Genera datos base iid: luego se induce correlaci√≥n entre columnas
X[:, 1] = X[:, 0] * 2 + np.random.randn(n_samples) * 0.1  # y correlacionado con x
X[:, 2] = X[:, 0] + X[:, 1] + np.random.randn(n_samples) * 0.1  # z correlacionado con x e y: crea estructura subespacial

# PCA
pca = PCA(n_components=2)  # Instancia PCA para reducir de 3D a 2D: deber√≠a capturar casi toda la varianza
X_pca = pca.fit_transform(X)  # Ajusta PCA y proyecta: obtiene coordenadas (n_samples,2)

print(f"Shape original: {X.shape}")  # Reporta shape original: (n,3) para verificar dimensiones del dataset
print(f"Shape reducido: {X_pca.shape}")  # Reporta shape reducido: debe ser (n,2) por n_components=2
print(f"Varianza explicada: {pca.explained_variance_ratio_}")  # Muestra varianza por componente: deber√≠a ser alta en datos correlacionados
print(f"Varianza total: {np.sum(pca.explained_variance_ratio_):.2%}")  # Suma varianza explicada: indica cu√°nto conserva la reducci√≥n
```

### 3.5 Reconstrucci√≥n y Error

```python
import numpy as np  # Importa NumPy: se usa para sum/mean/cumsum/argmax y c√°lculo de errores en utilidades de PCA

def reconstruction_error(X: np.ndarray, pca: PCA) -> float:  # Error de reconstrucci√≥n relativo: cuantifica p√©rdida al proyectar y reconstruir
    """
    Calcula el error de reconstrucci√≥n.

    Error = ||X - X_reconstructed||¬≤ / ||X||¬≤
    """
    X_pca = pca.transform(X)  # Proyecta X al subespacio PCA: obtiene coordenadas de dimensi√≥n reducida
    X_reconstructed = pca.inverse_transform(X_pca)  # Reconstruye a espacio original: aproxima X usando solo n_components

    error = np.sum((X - X_reconstructed) ** 2)  # SSE de reconstrucci√≥n: energ√≠a del residuo (cu√°nto ‚Äúse perdi√≥‚Äù al comprimir)
    total = np.sum((X - np.mean(X, axis=0)) ** 2)  # SSE total alrededor de la media: normaliza para obtener un error relativo comparable

    return error / total  # Retorna fracci√≥n de varianza no explicada (aprox): m√°s bajo implica mejor reconstrucci√≥n

def choose_n_components(X: np.ndarray, variance_threshold: float = 0.95) -> int:  # Elige k m√≠nimo tal que la varianza acumulada supere un umbral
    """
    Elige n√∫mero de componentes para retener cierta varianza.

    Args:
        variance_threshold: proporci√≥n de varianza a retener (ej: 0.95 = 95%)
    """
    # PCA con todos los componentes
    pca = PCA(n_components=min(X.shape))  # Ajusta PCA con el m√°ximo posible: k=min(n_samples,n_features) para capturar toda la varianza
    pca.fit(X)  # Entrena PCA completo: llena explained_variance_ratio_ para luego acumular y decidir k

    # Varianza acumulada
    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)  # Suma acumulada: varianza explicada por las primeras j componentes

    # Encontrar n_components
    n_components = np.argmax(cumulative_variance >= variance_threshold) + 1  # Primer √≠ndice donde se supera el umbral (+1 por 0-index)

    return n_components, cumulative_variance  # Devuelve k elegido y la curva acumulada: permite auditar visualmente la decisi√≥n
```

---

## üß© Consolidaci√≥n (PCA)

### Errores comunes

- **No centrar:** si no restas la media, el primer componente puede capturar ‚Äúoffset‚Äù en vez de estructura.
- **Confundir `components` con `X_pca`:**
  - `components` = ejes
  - `X_pca` = coordenadas en esos ejes
- **Elegir `n_components` sin criterio:** usa varianza acumulada + error de reconstrucci√≥n.

### Debugging / validaci√≥n (v5)

- Verifica:
  - `X_centered.mean(axis=0)` cerca de 0
  - shapes: `components: (n_features, k)`, `X_pca: (n_samples, k)`
- Si tu reconstrucci√≥n explota, revisa `X_hat = Z @ V_k·µÄ + mean`.
- Registra hallazgos en `Herramientas_Estudio/DIARIO_ERRORES.md`.
- Protocolos completos:
  - [PLAN_V4_ESTRATEGICO.md](../../Recursos_Adicionales/Planes_Estrategicos/PLAN_V4_ESTRATEGICO.md)
  - [PLAN_V5_ESTRATEGICO.md](../../Recursos_Adicionales/Planes_Estrategicos/PLAN_V5_ESTRATEGICO.md)

### Reto Feynman (tablero blanco)

Explica en 5 l√≠neas o menos:

1) ¬øPor qu√© PCA ‚Äúelige un √°ngulo‚Äù y qu√© significa ‚Äúm√°xima varianza‚Äù?
2) ¬øPor qu√© `S¬≤/(n-1)` son eigenvalues de la covarianza?
3) ¬øQu√© pierde la reconstrucci√≥n cuando `k < n_features`?

---

## üíª Parte 5: Gaussian Mixture Models (GMM)

### 5.0 GMM ‚Äî Nivel: intermedio/avanzado (clustering probabil√≠stico)

**Prop√≥sito:** pasar de ‚ÄúK-Means agrupa‚Äù a **entender cu√°ndo K-Means es geom√©tricamente incorrecto** y usar un modelo que capture **clusters el√≠pticos** y asignaci√≥n ‚Äúsuave‚Äù (*soft clustering*).

#### Objetivos de aprendizaje (medibles)

Al terminar este bloque podr√°s:

- **Explicar** por qu√© K-Means asume clusters aproximadamente esf√©ricos (misma varianza en todas direcciones).
- **Describir** un GMM como ‚Äúmezcla de Gaussianas‚Äù con una variable latente de componente.
- **Derivar** la idea operacional del algoritmo EM (E-step y M-step) a nivel implementable.
- **Interpretar** *responsibilities* `Œ≥(z_k)` como probabilidad de pertenencia.
- **Diagnosticar** fallas t√≠picas: colapso de covarianzas, sensibilidad a inicializaci√≥n, singularidad.

#### Intuici√≥n geom√©trica: clusters el√≠pticos y pertenencia suave

Imagina que tus datos forman ‚Äúnubes‚Äù alargadas:

- K-Means solo puede poner centroides y partir el espacio por regiones de Voronoi con distancia euclidiana.
- GMM asume que cada cluster es una **Gaussiana** con su propia forma:
  - media `Œº_k` (centro)
  - covarianza `Œ£_k` (orientaci√≥n y elongaci√≥n)

La diferencia clave es que GMM no dice ‚Äúeste punto es del cluster 2‚Äù. Dice:

> ‚ÄúEste punto es 70% del componente 2 y 30% del componente 1‚Äù.

Eso es extremadamente √∫til cuando los clusters se solapan.

#### Conceptos clave (glosario m√≠nimo)

- **Mezcla:** combinaci√≥n ponderada de distribuciones.
- **Pesos `œÄ_k`:** probabilidades a priori de cada componente (suman 1).
- **Variable latente `z`:** indica qu√© componente ‚Äúgener√≥‚Äù el punto.
- **Responsibilities `Œ≥_{ik}`:** `P(z=k | x_i)`.
- **EM (Expectation-Maximization):** alterna ‚Äúasignar probabilidades‚Äù y ‚Äúre-estimar par√°metros‚Äù.

#### Formalizaci√≥n m√≠nima

Modelo:

`p(x) = Œ£_{k=1..K} œÄ_k  N(x | Œº_k, Œ£_k)`

Log-likelihood de datos `X = {x_i}`:

`‚Ñì = Œ£_i log( Œ£_k œÄ_k N(x_i | Œº_k, Œ£_k) )`

No puedes maximizar esto de forma cerrada por el `log(Œ£ ...)`. EM lo hace iterativamente.

#### EM (idea implementable)

**E-step:** calcula responsibilities

`Œ≥_{ik} = P(z=k | x_i) = (œÄ_k N(x_i|Œº_k,Œ£_k)) / (Œ£_j œÄ_j N(x_i|Œº_j,Œ£_j))`

**M-step:** actualiza par√°metros usando promedios ponderados

- `N_k = Œ£_i Œ≥_{ik}`
- `œÄ_k = N_k / n`
- `Œº_k = (1/N_k) Œ£_i Œ≥_{ik} x_i`
- `Œ£_k = (1/N_k) Œ£_i Œ≥_{ik} (x_i-Œº_k)(x_i-Œº_k)·µÄ`

#### Worked example (m√≠nimo, 1D para ver EM sin √°lgebra pesada)

Sup√≥n puntos 1D `x = [-2, -1, 0, 2, 3]` y `K=2`.

Idea:

1) Inicializas dos Gaussianas (medias distintas).
2) En E-step, los puntos negativos tienen `Œ≥` alto para el componente ‚Äúizquierdo‚Äù y bajo para el derecho.
3) En M-step, la media izquierda se va hacia el promedio ponderado de los negativos, la derecha hacia los positivos.
4) Repites hasta que el log-likelihood deja de mejorar.

La intuici√≥n: es como K-Means, pero en vez de asignar ‚Äúduro‚Äù, asignas *responsabilities* y actualizas con pesos.

#### Cu√°ndo usar GMM vs K-Means (regla pr√°ctica)

- **Usa K-Means** si esperas clusters aproximadamente esf√©ricos, bien separados y quieres simplicidad/velocidad.
- **Usa GMM** si:
  - esperas **clusters el√≠pticos** o con varianzas distintas por direcci√≥n
  - hay **solapamiento** y necesitas pertenencia probabil√≠stica
  - quieres un modelo generativo simple para densidad

#### Errores comunes / debugging

- **No estandarizar features:** si una dimensi√≥n domina, la covarianza se distorsiona.
- **Singularidad/collapse:** una `Œ£_k` puede volverse casi singular si un componente ‚Äúse queda‚Äù con muy pocos puntos.
- **Inicializaci√≥n pobre:** EM converge a √≥ptimos locales; iniciar con K-Means suele ayudar.

---

## üö´ Cu√°ndo NO usar K-Means / PCA (y qu√© hacer en su lugar)

### Diagn√≥stico r√°pido (regla pr√°ctica)

Si no puedes justificar ‚Äúpor qu√© este m√©todo tiene sentido para este dataset‚Äù, asume que est√°s en zona de riesgo.

#### K-Means: se√±ales de que NO es buena idea

- **Geometr√≠a incorrecta:** clusters no convexos (formas tipo ‚Äúdos lunas‚Äù) o estructuras alargadas.
- **Densidades muy distintas:** un cluster muy denso y otro muy disperso.
- **Outliers fuertes:** centroides se mueven para ‚Äúperseguir‚Äù outliers.
- **Escalas distintas:** una feature domina la distancia euclidiana.

**S√≠ntomas medibles t√≠picos:**

- El **m√©todo del codo** no muestra un ‚Äúcodo‚Äù claro.
- **Silhouette score** bajo (cerca de 0) o negativo.
- Resultados muy distintos entre distintas inicializaciones.

**Qu√© hacer en su lugar (seg√∫n el problema):**

- **Clusters el√≠pticos (varianza diferente por direcci√≥n):** GMM (Gaussian Mixture Models).
- **Clusters con formas arbitrarias y ruido:** DBSCAN / HDBSCAN (no implementados aqu√≠, pero recomendados).
- **Estructura jer√°rquica:** Hierarchical clustering.

#### PCA: se√±ales de que NO es buena idea

- **Se√±al no alineada con varianza:** la direcci√≥n con mayor varianza no es la que separa clases (com√∫n en tareas supervisadas).
- **Relaci√≥n no lineal:** datos sobre un manifold curvo (PCA lineal pierde estructura).
- **Interpretaci√≥n equivocada:** usar PCA como ‚Äúselector de features importantes‚Äù sin analizar varianza explicada y reconstrucci√≥n.

**S√≠ntomas medibles t√≠picos:**

- Necesitas muchos componentes para llegar a 95% de varianza (PCA no est√° comprimiendo bien).
- La visualizaci√≥n en 2D parece ‚Äúmezclar‚Äù todo sin estructura (ojo: esto no prueba que no haya estructura, pero es una se√±al).

**Qu√© hacer en su lugar (seg√∫n el objetivo):**

- **Visualizaci√≥n no lineal:** t-SNE / UMAP (√∫tiles para explorar, no para entrenar modelos lineales directamente).
- **Compresi√≥n aprendida:** autoencoders (M√≥dulo 07, enfoque DL).
- **Si solo quieres acelerar:** reducir features por ingenier√≠a o seleccionar por dominio.

### Checklist de decisi√≥n (antes de usar el m√©todo)

- **Datos escalados:** ¬øfeatures comparables? (si no, normaliza).
- **Outliers:** ¬øhay outliers? (si s√≠, documenta su impacto).
- **Objetivo real:** ¬øquieres compresi√≥n, visualizaci√≥n, o clustering interpretable?

Integraci√≥n con ejecuci√≥n y validaci√≥n:

- [PLAN_V4_ESTRATEGICO.md](../../Recursos_Adicionales/Planes_Estrategicos/PLAN_V4_ESTRATEGICO.md)
- [PLAN_V5_ESTRATEGICO.md](../../Recursos_Adicionales/Planes_Estrategicos/PLAN_V5_ESTRATEGICO.md)
- Diario: `Herramientas_Estudio/DIARIO_ERRORES.md`

## üíª Parte 4: Aplicaciones de PCA

### 4.1 Compresi√≥n de Im√°genes

```python
import numpy as np  # Importa NumPy: representa la imagen como array y soporta operaciones vectorizadas usadas por PCA

def compress_image_pca(image: np.ndarray, n_components: int) -> tuple:  # Compresi√≥n PCA: reduce dimensionalidad de filas y reconstruye una aproximaci√≥n
    """
    Comprime una imagen usando PCA.

    Args:
        image: imagen grayscale (height, width)
        n_components: n√∫mero de componentes a retener

    Returns:
        imagen comprimida, pca model
    """
    # Tratar filas como muestras
    pca = PCA(n_components=n_components)  # Instancia PCA: retiene k componentes para comprimir cada fila (ancho) de la imagen
    image_pca = pca.fit_transform(image)  # Proyecta filas: (height,width)->(height,k) reduce dimensi√≥n horizontal conservando varianza

    # Reconstruir
    image_reconstructed = pca.inverse_transform(image_pca)  # Reconstruye a width original: aproxima la imagen usando solo k componentes

    return image_reconstructed, pca  # Devuelve imagen reconstruida y el modelo PCA: permite inspeccionar varianza/errores

def compression_ratio_pca(original_shape: tuple, n_components: int) -> float:  # Estima ratio de compresi√≥n: compara n√∫meros a guardar vs tama√±o original (heur√≠stico)
    """Calcula ratio de compresi√≥n."""
    height, width = original_shape  # Desempaqueta shape: alto y ancho para estimar tama√±os de almacenamiento
    original_size = height * width  # Tama√±o original: n√∫mero de p√≠xeles (asumiendo 1 valor por pixel)
    # Almacenamos: componentes + proyecciones + media
    compressed_size = n_components * width + height * n_components + width  # Estima par√°metros a guardar: componentes + scores + media (aprox)
    return compressed_size / original_size  # Ratio: <1 implica compresi√≥n (menos n√∫meros que almacenar que la imagen original)
```

### 4.2 Visualizaci√≥n en 2D

```python
import numpy as np  # Importa NumPy: se usa para obtener labels √∫nicos y crear m√°scaras booleanas de selecci√≥n
import matplotlib.pyplot as plt  # Importa Matplotlib: se usa para dibujar el scatter 2D y la leyenda/ejes

def visualize_pca_2d(X: np.ndarray, labels: np.ndarray = None, title: str = "PCA"):  # Reduce a 2D con PCA y grafica (coloreando por label si existe)
    """Reduce a 2D y visualiza."""
    pca = PCA(n_components=2)  # Instancia PCA 2D: elige 2 componentes para poder graficar en un plano
    X_2d = pca.fit_transform(X)  # Ajusta y proyecta: transforma X (n,d) a coordenadas (n,2)

    plt.figure(figsize=(10, 6))  # Crea figura: define tama√±o para legibilidad (ancho/alto en pulgadas)

    if labels is not None:  # Si hay etiquetas, colorea por clase/cluster para interpretar separaci√≥n en el plano PCA
        for label in np.unique(labels):  # Itera clases √∫nicas: crea una nube por label para la leyenda
            mask = labels == label  # M√°scara booleana: selecciona puntos que pertenecen a la clase actual
            plt.scatter(X_2d[mask, 0], X_2d[mask, 1],  # Scatter por clase: x=PC1, y=PC2 para los puntos filtrados
                       label=f'Clase {label}', alpha=0.7)  # Etiqueta/alpha: identifica clase y hace puntos semi-transl√∫cidos
        plt.legend()  # Muestra leyenda: permite mapear colores a clases
    else:  # Caso sin labels: se grafica todo en un solo color para ver estructura global sin segmentaci√≥n
        plt.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.7)  # Scatter sin labels: muestra la estructura global sin separar por clase

    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} var)')  # Etiqueta eje x: incluye % varianza explicada por PC1
    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} var)')  # Etiqueta eje y: incluye % varianza explicada por PC2
    plt.title(title)  # T√≠tulo del gr√°fico: permite contextualizar dataset/experimento
    plt.grid(True, alpha=0.3)  # Grilla suave: mejora lectura de densidades/posiciones
    plt.show()  # Renderiza la figura: despliega el plot en pantalla/notebook
```

---

## üéØ Ejercicios por tema (progresivos) + Soluciones

Reglas:

- **Intenta primero** sin mirar la soluci√≥n.
- **Timebox sugerido:** 25‚Äì60 min por ejercicio.
- **√âxito m√≠nimo:** tu soluci√≥n debe pasar los `assert`.

---

### Ejercicio 6.1: Distancias vectorizadas (K-Means) - shapes y argmin

#### Enunciado

1) **B√°sico**

- Dado `X` con shape `(n,d)` y centroides `C` con shape `(k,d)`, construye una matriz `D2` con shape `(n,k)` donde `D2[i,j] = ||X_i - C_j||^2`.

2) **Intermedio**

- Obt√©n asignaciones `labels = argmin_j D2[i,j]`.

3) **Avanzado**

- Verifica por `assert` que el resultado coincide con un c√°lculo manual en un punto.

#### Soluci√≥n

```python
import numpy as np  # Importa NumPy: se usa para construir arrays demo, broadcasting y verificaciones num√©ricas con asserts

X = np.array([[0.0, 0.0], [1.0, 0.0], [0.0, 2.0], [3.0, 3.0]])  # Dataset peque√±o (n=4,d=2): puntos 2D para validar distancias y argmin
C = np.array([[0.0, 0.0], [2.0, 2.0]])  # Centroides (k=2,d=2): dos centros candidatos para asignaci√≥n por distancia^2

# (n,1,d) - (1,k,d) -> (n,k,d)
diff = X[:, None, :] - C[None, :, :]  # Broadcasting: resta cada centroide a cada punto para obtener tensor (n,k,d) de diferencias
D2 = np.sum(diff ** 2, axis=2)  # Distancias^2 (n,k): suma sobre d para obtener ||X_i - C_j||^2 sin sqrt (m√°s eficiente)

assert D2.shape == (X.shape[0], C.shape[0])  # Verifica shape: debe ser (n,k) para poder hacer argmin por punto

labels = np.argmin(D2, axis=1)  # Asignaci√≥n: elige el centroide m√°s cercano por punto (argmin sobre k)
assert labels.shape == (X.shape[0],)  # Verifica shape de labels: un label por muestra
assert labels.min() >= 0 and labels.max() < C.shape[0]  # Verifica rango: labels debe ser un √≠ndice v√°lido en [0, k-1]

i = 2  # X[i] = [0,2]
manual0 = np.sum((X[i] - C[0]) ** 2)  # Distancia^2 manual a C0: sirve para comprobar que el c√°lculo vectorizado es correcto
manual1 = np.sum((X[i] - C[1]) ** 2)  # Distancia^2 manual a C1: segunda comparaci√≥n para el mismo punto i
assert np.isclose(D2[i, 0], manual0)  # Verifica D2 vectorizado vs manual: entrada (i,0) coincide num√©ricamente
assert np.isclose(D2[i, 1], manual1)  # Verifica D2 vectorizado vs manual: entrada (i,1) coincide num√©ricamente
assert labels[i] == int(np.argmin([manual0, manual1]))  # Verifica argmin: el label debe coincidir con el m√≠nimo de las distancias manuales
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Ejercicio 6.1: Distancias vectorizadas (shapes + broadcasting + argmin)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** De `||x-c||¬≤` a una matriz `(n,k)` sin loops
- **ID (opcional):** `M06-E06_1`
- **Duraci√≥n estimada:** 30‚Äì60 min
- **Nivel:** Intermedio
- **Dependencias:** Broadcasting + `axis` (M01), norma L2 (M02)

#### 2) Objetivos
- Construir `D2:(n,k)` sin bucles sobre `n` ni `k`.
- Elegir el `axis` correcto en `sum` y `argmin`.
- Debuggear shapes con un ejemplo peque√±o y verificable.

#### 3) Errores comunes
- Reducir el eje equivocado en `sum` (debe ser el eje de features `d`).
- Calcular `sqrt` sin necesidad (para `argmin`, dist y dist¬≤ ordenan igual).
- Usar `argmin(axis=0)` (contesta otra pregunta).

#### 4) Nota docente
- Pide que el alumno explique qu√© representa cada eje de `D2`.
</details>

---

### Ejercicio 6.2: Paso de actualizaci√≥n (centroides como promedio)

#### Enunciado

1) **B√°sico**

- Dado `X` y `labels`, recalcula `C_new[j] = mean(X[labels==j])`.

2) **Intermedio**

- Verifica shapes y que no aparecen `NaN`.

3) **Avanzado**

- Maneja el caso de cluster vac√≠o: si no hay puntos para un `j`, conserva el centroide anterior.

#### Soluci√≥n

```python
import numpy as np  # Importa NumPy: se usa para arrays, broadcasting, argmin y validaci√≥n con np.isfinite en el ejemplo

X = np.array([[0.0, 0.0], [1.0, 0.0], [10.0, 10.0], [11.0, 10.0]])  # Dataset 2D: dos grupos alrededor de (0,0) y (10,10) para probar actualizaci√≥n de centroides
C = np.array([[0.0, 0.0], [10.0, 10.0]])  # Centroides iniciales: uno por cada grupo para asignaci√≥n por distancia^2

diff = X[:, None, :] - C[None, :, :]  # Tensor (n,k,d): diferencias punto-centroide por broadcasting para calcular distancias en lote
labels = np.argmin(np.sum(diff ** 2, axis=2), axis=1)  # Asignaci√≥n por distancia^2: elige el centroide m√°s cercano para cada punto

C_new = C.copy()  # Inicializa centroides nuevos: copia para poder conservar centroides si un cluster queda vac√≠o
for j in range(C.shape[0]):  # Recorre clusters: recalcula centroide j como promedio de los puntos asignados
    mask = labels == j  # M√°scara booleana: selecciona puntos del cluster j
    if np.any(mask):  # Evita cluster vac√≠o: sin puntos, mean producir√≠a NaN y romper√≠a el algoritmo
        C_new[j] = np.mean(X[mask], axis=0)  # Actualiza centroide: media por feature minimiza SSE con labels fijos

assert C_new.shape == C.shape  # Verifica shape: la actualizaci√≥n no debe cambiar dimensionalidad ni n√∫mero de centroides
assert np.isfinite(C_new).all()  # Verifica finitud: asegura que no se generaron NaN/inf por clusters vac√≠os u operaciones inv√°lidas
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Ejercicio 6.2: Actualizaci√≥n de centroides (promedios + clusters vac√≠os)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Por qu√© el centroide es la media (y qu√© hacer si un cluster queda vac√≠o)
- **ID (opcional):** `M06-E06_2`
- **Duraci√≥n estimada:** 30‚Äì60 min
- **Nivel:** Intermedio

#### 2) Ideas clave
- Con `labels` fijos, la media minimiza `Œ£ ||x-Œº||¬≤`.
- Si `labels==j` no selecciona puntos, `mean` sobre slice vac√≠o produce `NaN`.

#### 3) Estrategias para cluster vac√≠o
- Conservar el centroide anterior (simple y estable).
- Reinicializar en un punto aleatorio de `X`.
- Reinicializar en el punto con mayor error (m√°s avanzado).

#### 4) Errores comunes
- Promediar con `axis=1` (debe ser `axis=0` para obtener un vector `(d,)`).
- No validar con `np.isfinite` y propagar `NaN`.

#### 5) Nota docente
- Pide que el alumno cree a prop√≥sito un cluster vac√≠o y explique el fallo.
</details>

---

### Ejercicio 6.3: Inercia (funci√≥n objetivo) + monoton√≠a de Lloyd

#### Enunciado

1) **B√°sico**

- Implementa `inertia(X, C, labels) = sum_i ||X_i - C_{labels_i}||^2`.

2) **Intermedio**

- Ejecuta 1 iteraci√≥n de Lloyd (asignaci√≥n ‚Üí actualizaci√≥n) y compara inercia.

3) **Avanzado**

- Verifica que la inercia **no aumenta** tras la iteraci√≥n (debe bajar o quedar igual).

#### Soluci√≥n

```python
import numpy as np  # Importa NumPy: se usa para broadcasting, sumas, argmin y generaci√≥n de datos sint√©ticos

def assign_labels(X: np.ndarray, C: np.ndarray) -> np.ndarray:  # Asigna cada punto al centroide m√°s cercano (Lloyd step: asignaci√≥n)
    D2 = np.sum((X[:, None, :] - C[None, :, :]) ** 2, axis=2)  # Distancias^2 a cada centroide: (n,1,d)-(1,k,d)->(n,k,d) y suma en d
    return np.argmin(D2, axis=1)  # Label por punto: √≠ndice del centroide con menor distancia^2


def update_centroids(X: np.ndarray, labels: np.ndarray, C: np.ndarray) -> np.ndarray:  # Recalcula centroides como media de puntos asignados (Lloyd step: actualizaci√≥n)
    C_new = C.copy()  # Copia centroides: evita modificar C in-place (mantiene comparaciones/diagn√≥stico coherentes)
    for j in range(C.shape[0]):  # Itera cada cluster j: actualiza su centro si tiene puntos asignados
        mask = labels == j  # M√°scara booleana: selecciona los puntos cuya etiqueta es j
        if np.any(mask):  # Solo actualiza si hay puntos: evita mean sobre vac√≠o y conserva centroide si cluster qued√≥ vac√≠o
            C_new[j] = np.mean(X[mask], axis=0)  # Nuevo centroide: media minimiza SSE intra-cluster para labels fijos
    return C_new  # Devuelve centroides actualizados: se usan en la siguiente asignaci√≥n


def inertia(X: np.ndarray, C: np.ndarray, labels: np.ndarray) -> float:  # Inercia/SSE: suma distancias^2 de puntos a su centroide asignado
    diffs = X - C[labels]  # Residuales por punto: resta el centroide correspondiente a cada label (indexaci√≥n avanzada)
    return float(np.sum(diffs ** 2))  # SSE total: escalar float √∫til para asserts/prints (no depende de dtype)


np.random.seed(0)  # Fija semilla: hace reproducible el experimento/validaci√≥n
X = np.vstack([  # Construye dataset: concatena dos nubes gaussianas (dos clusters) para probar Lloyd
    np.random.randn(50, 2) + np.array([0.0, 0.0]),  # Cluster 0: 50 puntos alrededor de (0,0)
    np.random.randn(50, 2) + np.array([5.0, 5.0]),  # Cluster 1: 50 puntos alrededor de (5,5)
])  # Cierra vstack: X queda con shape (100,2)

C0 = np.array([[0.0, 5.0], [5.0, 0.0]])  # Centroides iniciales ‚Äúcruzados‚Äù: no coinciden con centros reales a prop√≥sito
labels0 = assign_labels(X, C0)  # Asignaci√≥n inicial (E-step): etiqueta por punto seg√∫n C0
J0 = inertia(X, C0, labels0)  # Inercia inicial: SSE antes de la actualizaci√≥n de centroides

C1 = update_centroids(X, labels0, C0)  # Actualiza centroides (M-step): recomputa C con labels0 fijos
labels1 = assign_labels(X, C1)  # Re-asigna con centroides nuevos: completa 1 iteraci√≥n de Lloyd
J1 = inertia(X, C1, labels1)  # Inercia tras 1 iteraci√≥n: debe no aumentar (monoton√≠a)

assert J1 <= J0 + 1e-12  # Monoton√≠a de Lloyd: la inercia baja o se mantiene (tolerancia por flotantes)
assert J0 >= 0.0 and J1 >= 0.0  # Inercia no-negativa: suma de cuadrados nunca debe ser negativa
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Ejercicio 6.3: Inercia y monoton√≠a de Lloyd (convergencia ‚â† √≥ptimo global)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Qu√© mide la inercia y por qu√© Lloyd la baja
- **ID (opcional):** `M06-E06_3`
- **Duraci√≥n estimada:** 30‚Äì75 min
- **Nivel:** Intermedio

#### 2) Idea central
- Asignaci√≥n: con `C` fijo, elegir el centro m√°s cercano minimiza `J` respecto a `labels`.
- Actualizaci√≥n: con `labels` fijos, poner cada centro en la media minimiza `J` respecto a `C`.
- Alternar ambos pasos ‚áí `J` baja o queda igual.

#### 3) Convergencia ‚â† √≥ptimo global
- Lloyd converge, pero depende de la inicializaci√≥n y puede caer en m√≠nimos locales.
- Por eso K-Means++ y reinicios m√∫ltiples son est√°ndar.

#### 4) Debugging
- Si `J` aumenta, casi siempre es un error de `axis`, indexado (`C[labels]`) o `NaN`.

#### 5) Nota docente
- Pide que el alumno explique en 2 l√≠neas: ‚Äúconverge‚Äù vs ‚Äúmejor clustering posible‚Äù.
</details>

---

### Ejercicio 6.4: K-Means++ (probabilidades correctas)

#### Enunciado

1) **B√°sico**

- Implementa K-Means++ para elegir `k` centroides desde `X`.

2) **Intermedio**

- Verifica que los centroides pertenecen a `X`.

3) **Avanzado**

- Verifica que las probabilidades de muestreo suman 1 (en cada paso).

#### Soluci√≥n

```python
import numpy as np  # Importa NumPy: se usa para RNG moderno, operaciones vectorizadas y aserciones num√©ricas en K-Means++

def kmeans_plus_plus(X: np.ndarray, k: int, seed: int = 0) -> np.ndarray:  # Inicializaci√≥n K-Means++: elige centroides separados para mejorar el arranque de Lloyd
    rng = np.random.default_rng(seed)  # Crea generador RNG local: evita depender del estado global de np.random y hace reproducible la selecci√≥n
    n = X.shape[0]  # N√∫mero de muestras: define el rango v√°lido de √≠ndices para muestrear puntos de X
    centroids = [X[rng.integers(n)]]  # Elige primer centroide uniforme: toma un punto real de X para iniciar la lista de centroides

    for _ in range(1, k):  # Itera para seleccionar los k-1 centroides restantes: en cada paso recalcula distancias al centroide m√°s cercano
        C = np.array(centroids)  # Apila centroides actuales a array (c,d): facilita broadcasting contra X para calcular distancias en lote
        d2 = np.min(np.sum((X[:, None, :] - C[None, :, :]) ** 2, axis=2), axis=1)  # d¬≤ m√≠nimo por punto: distancia^2 al centroide m√°s cercano (para probabilidad K-Means++)
        assert np.all(d2 >= 0)  # Chequeo: distancias cuadradas no deben ser negativas (sirve para detectar NaNs/errores num√©ricos)
        probs = d2 / np.sum(d2)  # Normaliza a distribuci√≥n: cada punto se elige con probabilidad proporcional a su d¬≤ (m√°s lejos => m√°s probable)
        assert np.isclose(np.sum(probs), 1.0)  # Valida normalizaci√≥n: la suma de probabilidades debe ser 1 (tolerancia de float)
        centroids.append(X[rng.choice(n, p=probs)])  # Samplea nuevo centroide seg√∫n probs: implementa la regla de muestreo de K-Means++

    return np.array(centroids)  # Devuelve centroides iniciales: shape (k, n_features) para arrancar el loop de K-Means


np.random.seed(1)  # Fija semilla global: hace reproducible la generaci√≥n de datos sint√©ticos del ejemplo
X = np.random.randn(30, 2)  # Genera dataset demo (30,2): puntos 2D para verificar que los centroides elegidos pertenecen a X
C = kmeans_plus_plus(X, k=3, seed=123)  # Inicializa 3 centroides con K-Means++: se valida output y pertenencia a X

assert C.shape == (3, 2)  # Verifica shape: debe haber k centroides y cada uno con d=2 features
for j in range(C.shape[0]):  # Recorre centroides devueltos: valida que cada centroide sea exactamente uno de los puntos del dataset
    assert np.any(np.all(np.isclose(X, C[j]), axis=1))  # Chequea pertenencia: existe una fila en X (casi igual) a cada centroide C[j]
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Ejercicio 6.4: K-Means++ (probabilidades correctas)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Inicializaci√≥n que reduce m√≠nimos locales malos
- **ID (opcional):** `M06-E06_4`
- **Duraci√≥n estimada:** 30‚Äì60 min
- **Nivel:** Intermedio

#### 2) Idea clave
- K-Means++ elige nuevos centroides con probabilidad proporcional a la distancia¬≤ al centroide m√°s cercano.
- Intuici√≥n: fuerza a que los centroides iniciales queden separados, cubriendo mejor el espacio.

#### 3) Chequeos importantes
- `d2 >= 0` siempre (son distancias cuadradas).
- `probs` debe sumar 1.
- Los centroides seleccionados deben ser puntos existentes de `X`.

#### 4) Caso borde
- Si todos los puntos ya est√°n a distancia 0 de alg√∫n centroide (`sum(d2)=0`), no hay se√±al para muestrear: en pr√°ctica puedes romper el loop o elegir aleatorio.

#### 5) Nota docente
- Pide que el alumno compare K-Means con init aleatoria vs K-Means++ en un dataset con dos clusters separados.
</details>

---

### Ejercicio 6.5: Sensibilidad a escala (por qu√© normalizar importa)

#### Enunciado

1) **B√°sico**

- Construye un ejemplo donde escalar una feature cambie la asignaci√≥n al centroide m√°s cercano.

2) **Intermedio**

- Calcula labels con una escala `s=0.1` y con `s=10`.

3) **Avanzado**

- Verifica que hay al menos un punto cuyo label cambia.

#### Soluci√≥n

```python
import numpy as np  # Importa NumPy: se usa para broadcasting, argmin y scaling por feature para ilustrar sensibilidad a escala

def assign_labels(X: np.ndarray, C: np.ndarray) -> np.ndarray:  # Asigna labels por distancia^2: mismo criterio que K-Means (sin actualizar centroides)
    D2 = np.sum((X[:, None, :] - C[None, :, :]) ** 2, axis=2)  # Matriz (n,k) de distancias^2: usa broadcasting para evitar loops
    return np.argmin(D2, axis=1)  # Devuelve label por punto: √≠ndice del centroide m√°s cercano (m√≠nimo sobre k)


# Punto cerca en x pero lejos en y (y domina si la escalas)
X = np.array([  # Define punto de prueba: se elige para que el segundo eje pueda dominar al escalarlo
    [2.0, 0.0],  # Punto (x=2,y=0): su distancia depende de cu√°nto pese el eje y en la m√©trica eucl√≠dea
], dtype=float)  # Fuerza dtype float: evita enteros y hace expl√≠citas operaciones de scaling y distancias
C = np.array([  # Define dos centroides: uno en origen y otro en (2,2) para que el punto cambie de asignaci√≥n con scaling
    [0.0, 0.0],  # Centroide 0: origen, cercano en y cuando y est√° poco escalado
    [2.0, 2.0],  # Centroide 1: comparte x con el punto pero difiere en y, clave para provocar cambio al escalar y
], dtype=float)  # dtype float: mantiene coherencia num√©rica con X para comparaciones de distancia

labels_s_small = assign_labels(X * np.array([1.0, 0.1]), C * np.array([1.0, 0.1]))  # Escala y por 0.1: reduce su contribuci√≥n a la distancia
labels_s_big = assign_labels(X * np.array([1.0, 10.0]), C * np.array([1.0, 10.0]))  # Escala y por 10: amplifica su contribuci√≥n y puede cambiar el argmin

assert labels_s_small.shape == (1,)  # Verifica shape: un label para el √∫nico punto en X
assert labels_s_big.shape == (1,)  # Verifica shape: se preserva el formato tras el re-escalado
assert labels_s_small[0] != labels_s_big[0]  # Verifica sensibilidad: el label debe cambiar al modificar escalas (distancia eucl√≠dea cambia)
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Ejercicio 6.5: Sensibilidad a escala (normalizaci√≥n)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Por qu√© K-Means necesita features comparables
- **ID (opcional):** `M06-E06_5`
- **Duraci√≥n estimada:** 20‚Äì45 min
- **Nivel:** Intermedio

#### 2) Idea clave
- K-Means optimiza distancias euclidianas: si una feature est√° en escala 100√ó mayor, domina la distancia.

#### 3) Regla pr√°ctica
- Antes de K-Means/PCA, suele ser obligatorio:
  - estandarizar (media 0, var 1) o
  - normalizar por rango, seg√∫n el dominio.

#### 4) Nota docente
- Pide que el alumno explique por qu√© ‚Äúnormalizar cambia el significado de ‚Äòcerca‚Äô‚Äù.
</details>

---

### Ejercicio 6.6: PCA v√≠a SVD (shapes + varianza explicada ordenada)

#### Enunciado

1) **B√°sico**

- Centra `X` y calcula `U,S,Vt = svd(Xc)`.

2) **Intermedio**

- Proyecta a `k=2` componentes y verifica shapes.

3) **Avanzado**

- Calcula varianza explicada y verifica que est√° ordenada de mayor a menor.

#### Soluci√≥n

```python
import numpy as np  # Importa NumPy: se usa para SVD, generaci√≥n de datos sint√©ticos y validaci√≥n de shapes/propiedades

def pca_svd(X: np.ndarray, k: int):  # PCA por SVD: aprende componentes principales sin formar la covarianza expl√≠cita
    Xc = X - X.mean(axis=0)  # Centra datos: PCA requiere media cero por feature para que SVD capture direcciones de m√°xima varianza
    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)  # SVD compacta: Xc=U diag(S) Vt; Vt contiene direcciones principales en espacio de features
    comps = Vt[:k].T  # Selecciona k componentes: toma primeras k filas de Vt y transpone => matriz (d,k) para proyectar
    Xk = Xc @ comps  # Proyecci√≥n a k dims: (n,d)@(d,k)->(n,k) da coordenadas en el subespacio PCA
    var = (S ** 2) / (Xc.shape[0] - 1)  # Varianza por componente: S^2/(n-1) equivale a eigenvalues de covarianza (cuando X est√° centrado)
    ratio = var / np.sum(var)  # Ratio de varianza explicada: normaliza para obtener proporciones que suman 1 sobre todas las componentes
    return Xk, comps, ratio[:k]  # Devuelve proyecci√≥n, componentes (d,k) y ratios truncados (k,) para inspecci√≥n/validaci√≥n


np.random.seed(0)  # Fija semilla global: hace reproducible el dataset sint√©tico usado para validar PCA
n = 300  # N√∫mero de muestras: controla tama√±o del dataset (m√°s n => estimaciones de varianza m√°s estables)
z = np.random.randn(n)  # Latente 1D: variable base que induce correlaciones lineales entre columnas
X = np.stack([z, 2.0 * z + 0.1 * np.random.randn(n), -z + 0.1 * np.random.randn(n)], axis=1)  # Construye X (n,3): features correlacionadas para que PCA tenga estructura

X2, comps, r = pca_svd(X, k=2)  # Aplica PCA a 2 componentes: obtiene proyecci√≥n 2D, matriz de componentes y ratios de varianza

assert X2.shape == (n, 2)  # Verifica proyecci√≥n: n filas (muestras) y k=2 columnas (componentes)
assert comps.shape == (3, 2)  # Verifica componentes: d=3 features originales y k=2 ejes principales retenidos
assert r.shape == (2,)  # Verifica ratios: debe haber exactamente k proporciones de varianza explicada
assert r[0] >= r[1]  # Verifica orden: la primera componente debe explicar >= varianza que la segunda
assert 0.0 <= r.sum() <= 1.0  # Verifica rango: suma parcial de ratios debe estar entre 0 y 1 (al truncar, suele ser < 1)
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Ejercicio 6.6: PCA v√≠a SVD (shapes + varianza explicada)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** PCA estable en c√≥digo (SVD) sin construir covarianza
- **ID (opcional):** `M06-E06_6`
- **Duraci√≥n estimada:** 45‚Äì90 min
- **Nivel:** Intermedio/Avanzado

#### 2) Shapes que debes poder justificar
- `X:(n,d)` ‚Üí `Xc:(n,d)` (centrado)
- `Vt:(d,d)` (o `(d,rank)` si `full_matrices=False` y `n<d`)
- `comps = Vt[:k].T ‚Üí (d,k)`
- `Xk = Xc @ comps ‚Üí (n,k)`

#### 3) Varianza explicada
- Con SVD, los valores singulares `S` te dan varianzas: `var = S^2/(n-1)`.
- El ratio `var/sum(var)` indica qu√© porcentaje explica cada componente.

#### 4) Nota docente
- Pide que el alumno explique por qu√© centrar `X` es obligatorio para PCA.
</details>

---

### Ejercicio 6.7: Reconstrucci√≥n PCA (error decrece al aumentar componentes)

#### Enunciado

1) **B√°sico**

- Reconstruye `X` desde `k` componentes: `X_rec = Xc @ V_k @ V_k^T + mean`.

2) **Intermedio**

- Compara el error de reconstrucci√≥n con `k=1` vs `k=2`.

3) **Avanzado**

- Verifica que el error con `k=2` es menor o igual.

#### Soluci√≥n

```python
import numpy as np  # Importa NumPy: se usa para SVD, medias, normas y generaci√≥n de datos sint√©ticos del ejercicio

def pca_reconstruct(X: np.ndarray, k: int) -> np.ndarray:  # Reconstrucci√≥n PCA: proyecta a k dims y vuelve al espacio original (aprox)
    mu = X.mean(axis=0)  # Calcula media por feature: se usa para centrar y luego descentrar (reconstrucci√≥n en el sistema original)
    Xc = X - mu  # Centra X: PCA trabaja sobre datos de media cero para que los ejes representen covarianza
    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)  # SVD: obtiene base ortonormal de componentes; Vt contiene vectores principales
    Vk = Vt[:k].T  # Toma subespacio de dimensi√≥n k: matriz (d,k) con los k ejes principales (columnas)
    Xk = Xc @ Vk  # Proyecci√≥n a subespacio: coordenadas (n,k) en la base PCA truncada
    X_rec = Xk @ Vk.T + mu  # Reconstruye: vuelve a (n,d) aplicando el proyector Vk Vk^T y re-suma la media
    return X_rec  # Devuelve reconstrucci√≥n aproximada: error debe no aumentar al incrementar k


np.random.seed(1)  # Fija semilla global: hace reproducible el dataset sint√©tico para comparar errores de reconstrucci√≥n
n = 200  # N√∫mero de muestras: tama√±o del dataset para el test de monoton√≠a del error
z = np.random.randn(n)  # Latente 1D: induce correlaci√≥n lineal entre columnas para que PCA capture estructura en pocas componentes
X = np.stack([z, 2.0 * z + 0.2 * np.random.randn(n), -z + 0.2 * np.random.randn(n)], axis=1)  # Construye X (n,3): features correlacionadas + ruido

X1 = pca_reconstruct(X, k=1)  # Reconstrucci√≥n con 1 componente: mayor compresi√≥n => t√≠picamente mayor error
X2 = pca_reconstruct(X, k=2)  # Reconstrucci√≥n con 2 componentes: subespacio m√°s grande => error no debe aumentar

err1 = np.linalg.norm(X - X1)  # Error de reconstrucci√≥n k=1: norma Frobenius (por defecto) del residuo total
err2 = np.linalg.norm(X - X2)  # Error de reconstrucci√≥n k=2: deber√≠a ser <= err1 por propiedad de proyecciones ortogonales

assert err2 <= err1 + 1e-12  # Verifica monoton√≠a: permitir epsilon num√©rico por redondeo en SVD/multiplicaciones
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Ejercicio 6.7: Reconstrucci√≥n PCA (sesgo vs compresi√≥n)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** M√°s componentes ‚áí menos error (pero menos compresi√≥n)
- **ID (opcional):** `M06-E06_7`
- **Duraci√≥n estimada:** 30‚Äì60 min
- **Nivel:** Intermedio

#### 2) Idea clave
- `Vk Vk^T` es el proyector al subespacio de dimensi√≥n `k`.
- Si aumentas `k`, el subespacio crece y la proyecci√≥n puede ‚Äúexplicar‚Äù m√°s energ√≠a ‚áí el error no aumenta.

#### 3) Nota docente
- Pide que el alumno conecte ‚Äúerror de reconstrucci√≥n‚Äù con ‚Äúvarianza explicada acumulada‚Äù.
</details>

---

### (Bonus) Ejercicio 6.8: Silhouette (implementaci√≥n m√≠nima para dataset peque√±o)

#### Enunciado

- Implementa silhouette para un dataset peque√±o.
- Verifica que el score promedio est√° en `[-1, 1]`.

#### Soluci√≥n

```python
import numpy as np  # Importa NumPy: se usa para √°lgebra vectorizada, broadcasting y funciones de agregaci√≥n

def pairwise_dist(X: np.ndarray) -> np.ndarray:  # Distancias pairwise: construye matriz (n,n) de distancias Eucl√≠deas
    D2 = np.sum((X[:, None, :] - X[None, :, :]) ** 2, axis=2)  # Distancias^2 por broadcasting: (n,1,d)-(1,n,d)->(n,n,d) y suma en d
    return np.sqrt(np.maximum(D2, 0.0))  # Ra√≠z para Eucl√≠dea y clamp num√©rico: evita sqrt de valores negativos por redondeo


def silhouette_score(X: np.ndarray, labels: np.ndarray) -> float:  # Silhouette promedio: s(i)=(b(i)-a(i))/max(a(i),b(i)) en [-1,1]
    X = np.asarray(X, dtype=float)  # Normaliza entrada a float: garantiza operaciones de distancia y medias en tipo num√©rico estable
    labels = np.asarray(labels, dtype=int)  # Normaliza labels a int: facilita comparaciones e indexaci√≥n booleana por cluster
    D = pairwise_dist(X)  # Precalcula distancias pairwise (n,n): se reutiliza para a(i) y b(i) sin recomputar distancias
    n = X.shape[0]  # N√∫mero de puntos: controla el loop que calcula s(i) por cada muestra
    uniq = np.unique(labels)  # Clusters √∫nicos presentes: define el conjunto de clusters a evaluar para b(i)
    s = np.zeros(n, dtype=float)  # Vector de silhouettes por punto: se promedia al final
    for i in range(n):  # Recorre cada punto i: silhouette se define punto a punto
        same = labels == labels[i]  # M√°scara del cluster de i: selecciona puntos del mismo cluster
        same[i] = False  # Excluye el propio punto: evita distancia 0 consigo mismo en el promedio intra-cluster
        a = np.mean(D[i, same]) if np.any(same) else 0.0  # a(i): distancia media intra-cluster; 0 si i est√° solo en su cluster

        b = np.inf  # Inicializa b(i): buscamos el m√≠nimo promedio a cualquier cluster distinto (si no hay, queda inf)
        for c in uniq:  # Itera clusters candidatos: calcula distancia media de i a cada cluster distinto
            if c == labels[i]:  # Omite el cluster propio: b(i) se define sobre otros clusters
                continue  # Salta a siguiente cluster candidato
            mask = labels == c  # M√°scara del cluster candidato c: selecciona sus puntos
            if np.any(mask):  # Asegura que hay puntos en el cluster: evita mean sobre array vac√≠o
                b = min(b, float(np.mean(D[i, mask])))  # Actualiza mejor b(i): toma el cluster con menor distancia media

        if b == np.inf:  # Edge case: no existe otro cluster v√°lido (o labels degenerados), entonces b(i) no se define
            s[i] = 0.0  # Convenci√≥n: score neutral si no hay comparaci√≥n posible
        else:  # Caso normal: existe otro cluster para comparar, as√≠ que podemos calcular s(i) con a(i) y b(i)
            denom = max(a, b)  # Denominador est√°ndar: normaliza para acotar en [-1,1] y evita dividir por valores peque√±os
            s[i] = 0.0 if denom == 0.0 else (b - a) / denom  # Calcula s(i): si denom=0 (distancias 0), fuerza 0 para evitar NaN
    return float(np.mean(s))  # Promedio final: silhouette global del clustering


X = np.array([[0.0, 0.0], [0.2, 0.1], [5.0, 5.0], [5.1, 4.9]])  # Dataset mini 2D: dos clusters bien separados (cerca de (0,0) y (5,5))
labels = np.array([0, 0, 1, 1])  # Etiquetas de cluster: agrupa los dos primeros y los dos √∫ltimos
score = silhouette_score(X, labels)  # Calcula silhouette: deber√≠a ser positivo si clusters est√°n bien definidos
assert -1.0 <= score <= 1.0  # Invariante del score: silhouette siempre cae en el rango [-1,1]
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Ejercicio 6.8: Silhouette (intuici√≥n y l√≠mites)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** M√©trica de clustering ‚Äúinterna‚Äù (sin etiquetas)
- **ID (opcional):** `M06-E06_8`
- **Duraci√≥n estimada:** 30‚Äì75 min
- **Nivel:** Avanzado

#### 2) Intuici√≥n
- Para cada punto:
  - `a` = distancia media a su propio cluster
  - `b` = mejor (m√≠nima) distancia media a otro cluster
- `s = (b-a)/max(a,b)` ‚àà [-1, 1]

#### 3) Limitaciones
- Requiere distancias pairwise: costo O(n¬≤) (por eso lo hacemos ‚Äúmini‚Äù).
- Depende de la m√©trica de distancia (igual que K-Means).

#### 4) Nota docente
- Pide que el alumno interprete 3 casos: `s‚âà1`, `s‚âà0`, `s<0`.
</details>

---

## üì¶ Entregable del M√≥dulo

### `unsupervised_learning.py`

```python
"""
Unsupervised Learning Module

Implementaci√≥n desde cero de:
- K-Means Clustering (con K-Means++ initialization)
- PCA (Principal Component Analysis)
- M√©tricas de evaluaci√≥n de clusters

Autor: [Tu nombre]
M√≥dulo: 06 - Unsupervised Learning
"""

import numpy as np  # Importa NumPy: base para √°lgebra lineal, RNG, broadcasting y operaciones vectorizadas en clustering/PCA
from typing import Tuple, List  # Importa typing: documenta retornos/colecciones (no afecta runtime)


# ============================================================
# K-MEANS CLUSTERING
# ============================================================

def kmeans_plus_plus(X: np.ndarray, k: int, seed: int = None) -> np.ndarray:  # Inicializaci√≥n K-Means++: elige centroides separados para mejorar convergencia
    """Inicializaci√≥n K-Means++."""  # Docstring 1-l√≠nea: describe prop√≥sito; se ejecuta como literal string y no cambia el algoritmo
    if seed: np.random.seed(seed)  # Fija semilla si es truthy: hace reproducible la inicializaci√≥n (nota: seed=0 no entra por este if)
    n = len(X)  # N√∫mero de muestras: se usa para muestrear √≠ndices v√°lidos al escoger centroides
    centroids = [X[np.random.randint(n)]]  # Elige primer centroide al azar: punto inicial para el esquema de selecci√≥n probabil√≠stica

    for _ in range(1, k):  # Selecciona los k-1 centroides restantes: cada paso agrega un centroide nuevo
        distances = np.array([min(np.sum((x - c)**2) for c in centroids) for x in X])  # Distancia^2 al centroide m√°s cercano: define qu√© tan ‚Äúmal cubierto‚Äù est√° cada punto
        probs = distances / distances.sum()  # Normaliza a distribuci√≥n: puntos m√°s lejanos tienen mayor probabilidad de ser elegidos
        centroids.append(X[np.random.choice(n, p=probs)])  # Samplea nuevo centroide seg√∫n probs: mejora separaci√≥n inicial de clusters

    return np.array(centroids)  # Devuelve centroides iniciales (k,d): salida que se usa como init en K-Means/Lloyd


class KMeans:  # Implementaci√≥n simple de K-Means: alterna asignaci√≥n de clusters y actualizaci√≥n de centroides hasta converger
    def __init__(self, n_clusters=3, max_iter=300, tol=1e-4, seed=None):  # Configura hiperpar√°metros (k, iteraciones, tolerancia, semilla)
        self.n_clusters = n_clusters  # k: n√∫mero de clusters/centroides a aprender
        self.max_iter = max_iter  # L√≠mite de iteraciones: evita loops infinitos si no converge
        self.tol = tol  # Tolerancia: umbral para decidir convergencia (en este c√≥digo, compara shift cuadr√°tico)
        self.seed = seed  # Semilla opcional: se pasa a K-Means++ para reproducibilidad de inicializaci√≥n
        self.centroids = None  # Centroides aprendidos: se setea en fit y luego se usa en predict
        self.labels_ = None  # Etiquetas por muestra (cluster asignado): output principal del clustering
        self.inertia_ = None  # Inercia final: suma de distancias cuadradas intra-cluster (m√©trica interna)
        self.n_iter_ = 0  # Iteraciones ejecutadas: √∫til para diagn√≥stico (convergi√≥ r√°pido vs lento)

    def fit(self, X: np.ndarray) -> 'KMeans':  # Entrena K-Means sobre X: aprende centroides y asignaciones
        self.centroids = kmeans_plus_plus(X, self.n_clusters, self.seed)  # Inicializa centroides: un buen init reduce iteraciones y malos m√≠nimos

        for i in range(self.max_iter):  # Loop EM-like: alterna asignaci√≥n (E-step) y actualizaci√≥n (M-step)
            old_centroids = self.centroids.copy()  # Guarda centroides previos: permite medir desplazamiento para criterio de parada

            # Asignar
            distances = np.array([[np.sum((x - c)**2) for c in self.centroids] for x in X])  # Matriz (n_samples,k) de distancias^2 a cada centroide
            self.labels_ = np.argmin(distances, axis=1)  # Asigna cada punto al centroide m√°s cercano: minimiza SSE localmente

            # Actualizar
            for j in range(self.n_clusters):  # Recalcula cada centroide j usando los puntos asignados
                points = X[self.labels_ == j]  # Subconjunto del cluster j: todas las muestras cuyo label es j
                if len(points) > 0:  # Evita cluster vac√≠o: si no hay puntos, se conserva el centroide anterior
                    self.centroids[j] = points.mean(axis=0)  # Nuevo centroide: promedio (minimiza SSE para ese cluster)

            if np.sum((self.centroids - old_centroids)**2) < self.tol:  # Criterio de convergencia: shift total cuadr√°tico bajo tolerancia
                break  # Detiene iteraciones: ya no cambia significativamente la soluci√≥n

        self.n_iter_ = i + 1  # Guarda iteraciones realmente ejecutadas (i es 0-index)
        self.inertia_ = sum(np.sum((X[self.labels_ == j] - self.centroids[j])**2)  # SSE por cluster: suma distancias^2 de puntos al centroide asignado
                          for j in range(self.n_clusters))  # Suma sobre todos los clusters: m√©trica interna usada en elbow method
        return self  # Permite chaining (kmeans.fit(X).labels_)

    def predict(self, X: np.ndarray) -> np.ndarray:  # Predice labels para nuevos datos usando centroides ya aprendidos
        distances = np.array([[np.sum((x - c)**2) for c in self.centroids] for x in X])  # Distancias^2 a centroides aprendidos: (n,k)
        return np.argmin(distances, axis=1)  # Retorna √≠ndice del centroide m√°s cercano para cada muestra

    def fit_predict(self, X: np.ndarray) -> np.ndarray:  # Convenience: entrena y devuelve labels en una sola llamada
        self.fit(X)  # Ejecuta entrenamiento: produce centroides y labels_
        return self.labels_  # Retorna labels aprendidas: evita llamar fit() y luego acceder a labels_


# ============================================================
# PCA
# ============================================================

class PCA:  # PCA v√≠a SVD: aprende ejes principales y proyecta datos a un subespacio de menor dimensi√≥n
    def __init__(self, n_components: int = 2):  # Configura cu√°ntas componentes (dimensi√≥n reducida) se desea retener
        self.n_components = n_components  # k: n√∫mero de componentes principales a conservar
        self.components_ = None  # Matriz de componentes: se setea en fit (n_features, k)
        self.explained_variance_ratio_ = None  # Fracci√≥n de varianza explicada por cada componente: √∫til para decidir k
        self.mean_ = None  # Media por feature: necesaria para centrar en fit y para transformar/invertir consistentemente

    def fit(self, X: np.ndarray) -> 'PCA':  # Ajusta PCA: calcula media, componentes y varianza explicada a partir de X
        self.mean_ = X.mean(axis=0)  # Media por columna: centrar es obligatorio para PCA est√°ndar (captura covarianza, no offset)
        X_centered = X - self.mean_  # Centra datos: elimina el sesgo de traslaci√≥n para que SVD capture direcciones de m√°xima varianza

        U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)  # SVD compacta: descompone X centrado para extraer componentes principales (Vt)

        self.components_ = Vt[:self.n_components].T  # Toma las k filas principales de Vt y transpone: (n_features,k) para proyecci√≥n X@components_
        variance = (S**2) / (len(X) - 1)  # Eigenvalues de covarianza: Œ£^2/(n-1) corresponde a varianza por componente
        self.explained_variance_ratio_ = variance[:self.n_components] / variance.sum()  # Proporci√≥n explicada: normaliza por varianza total

        return self  # Devuelve instancia entrenada: permite chaining y acceso a componentes/ratios aprendidos

    def transform(self, X: np.ndarray) -> np.ndarray:  # Proyecta datos al subespacio PCA (dimensi√≥n k)
        return (X - self.mean_) @ self.components_  # Centra con la misma media aprendida y proyecta: (n,k)

    def fit_transform(self, X: np.ndarray) -> np.ndarray:  # Atajo: fit + transform en una sola llamada
        self.fit(X)  # Aprende componentes y media
        return self.transform(X)  # Devuelve proyecci√≥n PCA sin requerir llamada extra

    def inverse_transform(self, X_pca: np.ndarray) -> np.ndarray:  # Reconstruye aproximaci√≥n en espacio original desde coordenadas PCA
        return X_pca @ self.components_.T + self.mean_  # Re-proyecta a features y re-agrega la media: reconstrucci√≥n pierde info si k<n_features


# ============================================================
# M√âTRICAS
# ============================================================

def inertia(X: np.ndarray, labels: np.ndarray, centroids: np.ndarray) -> float:  # Inercia/SSE: suma de distancias^2 intra-cluster (m√©trica interna)
    """Within-cluster sum of squares."""  # Docstring 1-l√≠nea: define la m√©trica; cuenta como literal ejecutado
    return sum(np.sum((X[labels == i] - centroids[i])**2)  # SSE por cluster i: distancias^2 de sus puntos al centroide i
               for i in range(len(centroids)))  # Suma sobre todos los centroides: se usa en elbow method y diagn√≥stico

def silhouette_score(X: np.ndarray, labels: np.ndarray) -> float:  # Silhouette promedio: combina cohesi√≥n (a) y separaci√≥n (b) sin ground truth
    """Silhouette score promedio."""  # Docstring 1-l√≠nea: explica salida; se ejecuta como string literal
    n = len(X)  # N√∫mero de puntos: controla el loop externo del c√°lculo por muestra
    scores = []  # Acumula s_i por punto: luego se promedia para el score global

    for i in range(n):  # Recorre cada punto i: silhouette requiere evaluar su cohesi√≥n/separaci√≥n relativa
        # a: distancia promedio intra-cluster
        same = X[labels == labels[i]]  # Puntos del mismo cluster que i (incluye a i): base para cohesi√≥n intra-cluster
        a = np.mean([np.sqrt(np.sum((X[i] - x)**2)) for x in same if not np.array_equal(x, X[i])])  # Distancia media a otros del mismo cluster (excluye i)

        # b: distancia promedio al cluster m√°s cercano
        b = float('inf')  # Inicializa b con infinito: buscamos el m√≠nimo promedio a cualquier cluster alternativo
        for label in np.unique(labels):  # Recorre clusters existentes: eval√∫a el ‚Äúcluster vecino‚Äù m√°s cercano en distancia promedio
            if label != labels[i]:  # Excluye el cluster propio: b se define como mejor cluster distinto
                other = X[labels == label]  # Puntos del cluster candidato: se usa para distancia media inter-cluster
                if len(other) > 0:  # Evita clusters vac√≠os: no aportan un promedio definido
                    b = min(b, np.mean([np.sqrt(np.sum((X[i] - x)**2)) for x in other]))  # Actualiza m√≠nimo: elige cluster alternativo m√°s cercano

        if b == float('inf'):  # Si no existi√≥ cluster alternativo v√°lido (edge case), no se puede definir b correctamente
            scores.append(0)  # Convenci√≥n simple: retorna 0 para ese punto (neutral)
        else:  # Caso normal: existe un cluster alternativo; se calcula s(i) comparando cohesi√≥n (a) vs separaci√≥n (b)
            scores.append((b - a) / max(a, b))  # F√≥rmula silhouette: s=(b-a)/max(a,b) en [-1,1]; >0 indica buena asignaci√≥n

    return np.mean(scores)  # Promedia s_i: define el silhouette global del clustering (m√°s alto => mejor separaci√≥n/cohesi√≥n)


# ============================================================
# TESTS
# ============================================================

if __name__ == "__main__":  # Entry point: permite ejecutar este m√≥dulo como script para correr pruebas r√°pidas
    np.random.seed(42)  # Fija semilla global: hace reproducible el dataset sint√©tico y por tanto los resultados del test

    # Test K-Means
    c1 = np.random.randn(50, 2) + [0, 0]  # Cluster 1: 50 puntos alrededor de (0,0)
    c2 = np.random.randn(50, 2) + [5, 5]  # Cluster 2: 50 puntos alrededor de (5,5)
    c3 = np.random.randn(50, 2) + [10, 0]  # Cluster 3: 50 puntos alrededor de (10,0)
    X = np.vstack([c1, c2, c3])  # Dataset final: concatena clusters (150,2) para probar K-Means y PCA

    kmeans = KMeans(n_clusters=3, seed=42)  # Instancia K-Means con k=3: coincide con la generaci√≥n sint√©tica
    labels = kmeans.fit_predict(X)  # Entrena y obtiene labels: debe separar aproximadamente los 3 grupos

    print(f"K-Means Inertia: {kmeans.inertia_:.2f}")  # Reporta inercia final: √∫til para comparar con otros k/datasets
    print(f"Silhouette Score: {silhouette_score(X, labels):.4f}")  # Reporta silhouette: idealmente cercano a 1 si clusters bien separados

    # Test PCA
    pca = PCA(n_components=2)  # Instancia PCA para reducir 2D (aqu√≠ X ya es 2D, sirve para validar pipeline)
    X_pca = pca.fit_transform(X)  # Ajusta y transforma: obtiene proyecci√≥n en el subespacio (n,2)
    X_reconstructed = pca.inverse_transform(X_pca)  # Reconstruye desde PCA: √∫til para medir error de reconstrucci√≥n

    print(f"\nPCA Varianza explicada: {pca.explained_variance_ratio_}")  # Muestra proporci√≥n de varianza por componente: sanity check
    print(f"Error reconstrucci√≥n: {np.mean((X - X_reconstructed)**2):.6f}")  # Mide MSE de reconstrucci√≥n: debe ser peque√±o si k es suficiente

    print("\n‚úì Todos los tests pasaron!")  # Mensaje final: indica ejecuci√≥n completa del bloque de pruebas sin excepciones
```

---

## üîç Shadow Mode: Validaci√≥n con sklearn (v3.3)

> ‚ö†Ô∏è **Regla:** sklearn est√° **prohibido para aprender**, pero es **√∫til para validar**. Si tus resultados difieren de forma grande y consistente, primero asume bug.

### Protocolo m√≠nimo

- **K-Means:** comparar inercia y silhouette para el mismo `k`.
- **PCA:** comparar `explained_variance_ratio_` y reconstrucci√≥n aproximada.

```python
"""
Shadow Mode - Unsupervised Learning
Comparaci√≥n: implementaciones desde cero vs sklearn.
"""

import numpy as np  # Importa NumPy: se usa para type hints, posibles conversiones y operaciones auxiliares
from sklearn.cluster import KMeans as SklearnKMeans  # Importa KMeans de sklearn: baseline para validar resultados (no para aprender)
from sklearn.decomposition import PCA as SklearnPCA  # Importa PCA de sklearn: baseline para comparar varianza explicada


def shadow_mode_kmeans(X: np.ndarray, k: int = 3, seed: int = 42) -> None:  # Compara tu K-Means (from scratch) contra sklearn (referencia)
    """Compara inercia de tu K-Means vs sklearn."""
    # Tu implementaci√≥n
    # my = KMeans(n_clusters=k, random_state=seed)
    # my_labels = my.fit_predict(X)
    # my_inertia = my.inertia_

    # Placeholder (reemplazar con tu c√≥digo)
    my_inertia = 0.0  # Placeholder: aqu√≠ debe ir la inercia de TU implementaci√≥n (se deja en 0 para que el ejemplo sea ejecutable)

    # sklearn
    sk = SklearnKMeans(n_clusters=k, init="k-means++", n_init=10, random_state=seed)  # Instancia sklearn KMeans: usa K-Means++ y reinicios para estabilidad
    sk.fit(X)  # Ajusta sklearn KMeans: aprende centroides y calcula inercia interna en sk.inertia_

    print("=" * 60)  # Separador visual: hace m√°s legible la salida en consola
    print("SHADOW MODE: K-Means")  # Encabezado: indica que esta secci√≥n corresponde a la comparaci√≥n de K-Means
    print("=" * 60)  # Repite separador: encuadra el bloque de resultados
    print(f"Tu inercia:      {my_inertia:.4f}")  # Reporta la inercia de tu implementaci√≥n (placeholder hasta reemplazar)
    print(f"sklearn inertia: {sk.inertia_:.4f}")  # Reporta inercia de sklearn: referencia para detectar discrepancias grandes


def shadow_mode_pca(X: np.ndarray, n_components: int = 2) -> None:  # Compara PCA from scratch vs sklearn en varianza explicada
    """Compara varianza explicada de tu PCA vs sklearn."""
    # Tu implementaci√≥n
    # my = PCA(n_components=n_components)
    # X_my = my.fit_transform(X)

    # sklearn
    sk = SklearnPCA(n_components=n_components)  # Instancia sklearn PCA: calcula componentes principales por SVD internamente
    sk.fit(X)  # Ajusta PCA: estima explained_variance_ratio_ para comparar con tu implementaci√≥n

    print("=" * 60)  # Separador visual: delimita salida del bloque PCA
    print("SHADOW MODE: PCA")  # Encabezado: indica comparaci√≥n de PCA
    print("=" * 60)  # Repite separador: mantiene consistencia con el bloque anterior
    print(f"sklearn explained_variance_ratio_: {sk.explained_variance_ratio_}")  # Varianza explicada de sklearn: baseline para tu PCA
```

---

## üß≠ Puente al M√≥dulo 08 (MNIST Analyst)

En la Semana 21 del proyecto:

- **PCA:** lo usas para reducir MNIST y visualizar estructura en 2D (y para acelerar m√©todos posteriores).
- **K-Means:** lo usas para agrupar d√≠gitos sin etiquetas y visualizar centroides como ‚Äúprototipos‚Äù.

Checklist de integraci√≥n:

- **Entrada:** MNIST normalizado a `[0, 1]`.
- **PCA 2D:** gr√°fico con clusters/colores.
- **K-Means:** elegir `k=10` y analizar si los clusters se alinean con d√≠gitos.
- **Salida:** guarda figuras y conclusiones para el informe.

---

## ‚úÖ Checklist de Finalizaci√≥n

- [ ] Implement√© K-Means con inicializaci√≥n K-Means++
- [ ] Entiendo el algoritmo de Lloyd (asignar-actualizar)
- [ ] Puedo calcular inercia y usarla para el m√©todo del codo
- [ ] Implement√© silhouette score
- [ ] Implement√© PCA usando SVD
- [ ] Entiendo varianza explicada y puedo elegir n_components
- [ ] Puedo reconstruir datos desde PCA
- [ ] Apliqu√© PCA para visualizaci√≥n 2D
- [ ] Todos los tests del m√≥dulo pasan

---

# üìò Extensi√≥n Acad√©mica: Nivel MS-AI (University of Colorado Boulder Pathway)

> Rigor matem√°tico formal, contexto hist√≥rico y conexiones te√≥ricas profundas.

---

## A.1 Contexto Hist√≥rico

- **1901:** Karl Pearson ‚Äî PCA
- **1957:** Lloyd ‚Äî K-means (publicado 1982)
- **1977:** Dempster, Laird, Rubin ‚Äî Algoritmo EM
- **2008:** van der Maaten & Hinton ‚Äî t-SNE

### El Paradigma No Supervisado

A diferencia del supervisado, **no hay etiquetas**. El objetivo es descubrir **estructura latente**.

---

## A.2 Analog√≠a: El Arque√≥logo

Un arque√≥logo encuentra fragmentos de cer√°mica:

- **Sin etiquetas:** No sabe la civilizaci√≥n
- **Objetivo:** Agrupar por similitud
- **Descubrimiento:** Identifica culturas desconocidas

> **Aprendizaje no supervisado = encontrar patrones ocultos sin gu√≠a externa.**

---

## A.3 PCA: Derivaci√≥n Formal

### Objetivo de Optimizaci√≥n

Maximizar varianza de la proyecci√≥n:

$$\max_{\mathbf{w}} \mathbf{w}^T \Sigma \mathbf{w} \quad \text{s.t.} \quad \|\mathbf{w}\|_2 = 1$$

### Soluci√≥n

$\mathbf{w}$ es el **eigenvector** de $\Sigma$ con mayor eigenvalue.

### Conexi√≥n con SVD

Para $X$ centrado: $X = U\Sigma V^T$

- Componentes principales = columnas de $V$
- Scores = $XV = U\Sigma$

---

## A.4 K-Means: An√°lisis Te√≥rico

### Objetivo

$$J = \sum_{k=1}^{K} \sum_{x_i \in C_k} \|x_i - \mu_k\|^2$$

### Propiedades

- **Convergencia garantizada** (a m√≠nimo local)
- **Complejidad:** $O(nKdI)$
- **NP-hard** encontrar √≥ptimo global

### K-Means++

Inicializaci√≥n que garantiza:

$$\mathbb{E}[J] \leq 8(\ln k + 2) \cdot J_{\text{OPT}}$$

---

## A.5 GMM y Algoritmo EM

### Modelo

$$p(x) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x | \mu_k, \Sigma_k)$$

### EM: Garant√≠as

- **Monoton√≠a:** $\mathcal{L}(\theta^{(t+1)}) \geq \mathcal{L}(\theta^{(t)})$
- **Convergencia:** A punto estacionario
- **Limitaci√≥n:** Puede converger a m√°ximo local

---

## A.6 t-SNE y UMAP

### t-SNE: Objetivo

Minimizar KL divergence entre distribuciones de similitud:

$$D_{KL}(P \| Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}$$

### UMAP: Ventajas

- M√°s r√°pido: $O(n^{1.14})$ vs $O(n^2)$
- Mejor preservaci√≥n global
- Puede proyectar nuevos datos

---

## A.7 Conexiones con MS-AI Pathway

| Concepto | Curso | Aplicaci√≥n |
|----------|-------|------------|
| PCA | DTSA 5510 | Reducci√≥n dimensional |
| K-Means | DTSA 5510 | Clustering |
| GMM | DTSA 5510 | Modelos generativos |
| t-SNE/UMAP | DTSA 5510 | Visualizaci√≥n |

---

## A.8 Referencias Acad√©micas

1. **Bishop, C.M. (2006).** *PRML*, Chapters 9, 12.
2. **van der Maaten, L., & Hinton, G. (2008).** "Visualizing Data using t-SNE." JMLR.
3. **McInnes, L., et al. (2018).** "UMAP: Uniform Manifold Approximation." arXiv.

---

*Extensi√≥n acad√©mica del MS-AI Pathway de la University of Colorado Boulder.*

---

## üîó Navegaci√≥n

| Anterior | √çndice | Siguiente |
|----------|--------|-----------|
| [M05_Aprendizaje_Supervisado](../../M05_Aprendizaje_Supervisado/) | [README](../../README.md) | [M07_Deep_Learning](../../M07_Deep_Learning/) |
