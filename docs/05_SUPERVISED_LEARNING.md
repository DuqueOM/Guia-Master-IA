# MÃ³dulo 05 - Supervised Learning

> **ğŸ¯ Objetivo:** Dominar regresiÃ³n lineal, logÃ­stica y mÃ©tricas de evaluaciÃ³n
> **Fase:** 2 - NÃºcleo de ML | **Semanas 9-12**
> **Curso del Pathway:** Introduction to Machine Learning: Supervised Learning

---

<a id="m05-0"></a>

## ğŸ§­ CÃ³mo usar este mÃ³dulo (modo 0â†’100)

**PropÃ³sito:** que puedas construir un pipeline supervisado â€œde examenâ€:

- entrenar (regresiÃ³n lineal/logÃ­stica)
- evaluar (mÃ©tricas)
- validar (train/test, K-fold)
- controlar overfitting (regularizaciÃ³n)

### Objetivos de aprendizaje (medibles)

Al terminar este mÃ³dulo podrÃ¡s:

- **Implementar** regresiÃ³n lineal y regresiÃ³n logÃ­stica desde cero.
- **Derivar** el gradiente de MSE y de cross-entropy (con la forma `Xáµ€(Å· - y)`).
- **Elegir** mÃ©tricas correctas segÃºn el costo de FP/FN.
- **Aplicar** validaciÃ³n (split y K-fold) evitando leakage.
- **Validar** tu implementaciÃ³n con Shadow Mode (sklearn) como ground truth.
- **Explicar** EntropÃ­a/Gini, Information Gain y el contraste **Bagging vs Boosting** (Random Forest vs Gradient Boosting) a nivel conceptual.

Enlaces rÃ¡pidos:

- [04_PROBABILIDAD_ML.md](04_PROBABILIDAD_ML.md) (MLE â†’ cross-entropy)
- [GLOSARIO.md](GLOSARIO.md)
- [RECURSOS.md](RECURSOS.md)
- [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)
- [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md)
- EvaluaciÃ³n (rÃºbrica): [study_tools/RUBRICA_v1.md](../study_tools/RUBRICA_v1.md) (scope `M05` en `rubrica.csv`)

### Recursos (cuÃ¡ndo usarlos)

| Prioridad | Recurso | CuÃ¡ndo usarlo en este mÃ³dulo | Para quÃ© |
|----------|---------|------------------------------|----------|
| **Obligatorio** | [04_PROBABILIDAD_ML.md](04_PROBABILIDAD_ML.md) | Antes de implementar `log-loss`/cross-entropy y el gradiente de logÃ­stica | Conectar MLE â†’ cross-entropy y evitar derivaciones â€œde memoriaâ€ |
| **Obligatorio** | `study_tools/DIRTY_DATA_CHECK.md` | Antes del primer entrenamiento real (Semana 9â€“10), al preparar datasets | Evitar que el modelo â€œaprenda basuraâ€ por fallas de datos |
| **Obligatorio** | `study_tools/DIARIO_ERRORES.md` | Cada vez que veas mÃ©tricas incoherentes, accuracy â€œmÃ¡gicoâ€ o divergencia | Registrar bugs, causas y fixes reproducibles |
| **Complementario** | [StatQuest ML (playlist)](https://www.youtube.com/playlist?list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF) | Semana 10â€“12 (logÃ­stica, mÃ©tricas, regularizaciÃ³n) | Refuerzo conceptual rÃ¡pido + ejemplos |
| **Complementario** | [Stanford CS229](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) | DespuÃ©s de implementar regresiÃ³n lineal/logÃ­stica (para profundizar) | Profundizar en teorÃ­a y derivaciones estÃ¡ndar |
| **Opcional** | [RECURSOS.md](RECURSOS.md) | Al finalizar el mÃ³dulo (para escoger prÃ¡ctica extra) | Expandir sin perder el foco del Pathway |

---

## ğŸ§  Â¿QuÃ© es Supervised Learning?

```text
APRENDIZAJE SUPERVISADO

Tenemos:
- Datos de entrada X (features)
- Etiquetas Y (targets/labels)

Objetivo: Aprender una funciÃ³n f tal que f(X) â‰ˆ Y

Tipos principales:
â”œâ”€â”€ REGRESIÃ“N: Y es continuo (precio, temperatura)
â”‚   â””â”€â”€ Output: nÃºmero real
â””â”€â”€ CLASIFICACIÃ“N: Y es discreto (spam/no spam, dÃ­gito 0-9)
    â””â”€â”€ Output: clase o probabilidad
```

---

## ğŸ“š Contenido del MÃ³dulo

| Semana | Tema | Entregable |
|--------|------|------------|
| 9 | RegresiÃ³n Lineal | `linear_regression.py` |
| 10 | RegresiÃ³n LogÃ­stica | `logistic_regression.py` |
| 11 | MÃ©tricas de EvaluaciÃ³n | `metrics.py` |
| 12 | ValidaciÃ³n + RegularizaciÃ³n + Ãrboles | Cross-validation, L1/L2 + Tree-Based Models |

---

## ğŸ’» Parte 1: RegresiÃ³n Lineal

### 1.1 Modelo

```python
import numpy as np

"""
REGRESIÃ“N LINEAL

HipÃ³tesis: h(x) = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚™xâ‚™
         = Î¸áµ€x (forma matricial)

Donde:
- Î¸ (theta): parÃ¡metros/pesos del modelo
- x: vector de features (con xâ‚€ = 1 para el bias)

En forma matricial para mÃºltiples muestras:
    Å· = XÎ¸

Donde:
- X: matriz (m Ã— n+1) con m muestras y n features + columna de 1s
- Î¸: vector (n+1 Ã— 1) de parÃ¡metros
- Å·: vector (m Ã— 1) de predicciones
"""

def add_bias_term(X: np.ndarray) -> np.ndarray:
    """AÃ±ade columna de 1s para el tÃ©rmino de bias."""
    m = X.shape[0]
    return np.column_stack([np.ones(m), X])

def predict_linear(X: np.ndarray, theta: np.ndarray) -> np.ndarray:
    """PredicciÃ³n lineal: Å· = XÎ¸"""
    return X @ theta
```

### 1.2 FunciÃ³n de Costo (MSE)

```python
import numpy as np

def mse_cost(X: np.ndarray, y: np.ndarray, theta: np.ndarray) -> float:
    """
    Mean Squared Error Cost Function.

    J(Î¸) = (1/2m) Î£áµ¢ (h(xáµ¢) - yáµ¢)Â²
         = (1/2m) ||XÎ¸ - y||Â²

    El factor 1/2 es por conveniencia (cancela con la derivada).
    """
    m = len(y)
    predictions = X @ theta
    errors = predictions - y
    return (1 / (2 * m)) * np.sum(errors ** 2)

def mse_gradient(X: np.ndarray, y: np.ndarray, theta: np.ndarray) -> np.ndarray:
    """
    Gradiente del MSE respecto a Î¸.

    âˆ‚J/âˆ‚Î¸ = (1/m) Xáµ€(XÎ¸ - y)
    """
    m = len(y)
    predictions = X @ theta
    errors = predictions - y
    return (1 / m) * X.T @ errors
```

### 1.3 SoluciÃ³n Cerrada (Normal Equation)

```python
import numpy as np

def normal_equation(X: np.ndarray, y: np.ndarray) -> np.ndarray:
    """
    SoluciÃ³n cerrada para regresiÃ³n lineal.

    Î¸ = (Xáµ€X)â»Â¹ Xáµ€y

    Ventajas:
    - No requiere iteraciones
    - No hay hiperparÃ¡metros (learning rate)

    Desventajas:
    - O(nÂ³) por la inversiÃ³n de matriz
    - No funciona si Xáµ€X es singular
    - No escala bien para n grande (>10,000 features)
    """
    XtX = X.T @ X
    Xty = X.T @ y

    # Usar solve en lugar de inv para estabilidad numÃ©rica
    theta = np.linalg.solve(XtX, Xty)
    return theta
```

### 1.4 Gradient Descent para RegresiÃ³n

```python
import numpy as np
from typing import List, Tuple

class LinearRegression:
    """RegresiÃ³n Lineal implementada desde cero."""

    def __init__(self):
        self.theta = None
        self.cost_history = []

    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        method: str = 'gradient_descent',
        learning_rate: float = 0.01,
        n_iterations: int = 1000
    ) -> 'LinearRegression':
        """
        Entrena el modelo.

        Args:
            X: features (m, n)
            y: targets (m,)
            method: 'gradient_descent' o 'normal_equation'
            learning_rate: tasa de aprendizaje (solo para GD)
            n_iterations: nÃºmero de iteraciones (solo para GD)
        """
        # AÃ±adir bias
        X_b = add_bias_term(X)
        m, n = X_b.shape

        if method == 'normal_equation':
            self.theta = normal_equation(X_b, y)
        else:
            # Inicializar theta con ceros o valores pequeÃ±os
            self.theta = np.zeros(n)

            for i in range(n_iterations):
                # Calcular gradiente
                gradient = mse_gradient(X_b, y, self.theta)

                # Actualizar theta
                self.theta = self.theta - learning_rate * gradient

                # Guardar costo para monitoreo
                cost = mse_cost(X_b, y, self.theta)
                self.cost_history.append(cost)

        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        """Predice valores."""
        X_b = add_bias_term(X)
        return X_b @ self.theta

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        """RÂ² score."""
        y_pred = self.predict(X)
        ss_res = np.sum((y - y_pred) ** 2)
        ss_tot = np.sum((y - np.mean(y)) ** 2)
        return 1 - (ss_res / ss_tot)


# Demo
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X.flatten() + np.random.randn(100) * 0.5  # y = 4 + 3x + ruido

model = LinearRegression()
model.fit(X, y, method='gradient_descent', learning_rate=0.1, n_iterations=1000)

print(f"ParÃ¡metros aprendidos: {model.theta}")
print(f"Esperados: [4, 3]")
print(f"RÂ² score: {model.score(X, y):.4f}")
```

---

## ğŸ’» Parte 2: RegresiÃ³n LogÃ­stica

### 2.0 RegresiÃ³n LogÃ­stica â€” Nivel: intermedio (core del Pathway)

**PropÃ³sito:** pasar de â€œsÃ© aplicar sigmoidâ€ a **poder entrenar, derivar y validar** un clasificador binario (y extenderlo a multiclase con One-vs-All).

#### Objetivos de aprendizaje (medibles)

Al terminar esta parte podrÃ¡s:

- **Explicar** por quÃ© regresiÃ³n logÃ­stica es un modelo lineal *sobre el log-odds* (aunque la salida sea una probabilidad).
- **Derivar** (a mano) el gradiente de la pÃ©rdida *Binary Cross-Entropy* y reconocer la forma `Xáµ€(Å· - y)`.
- **Implementar** `fit()` con gradient descent estable (con `clip`/`eps`) y verificar convergencia.
- **Diagnosticar** errores tÃ­picos: shapes, overflow en `exp`, signos invertidos, saturaciÃ³n de sigmoid.
- **Validar** tu implementaciÃ³n con **Shadow Mode** (comparaciÃ³n con sklearn) y con un *overfit test* en dataset pequeÃ±o.

#### Prerrequisitos

- De `MÃ³dulo 03`: Chain Rule y gradiente.
- De `MÃ³dulo 04`: interpretaciÃ³n de MLE (conexiÃ³n con cross-entropy).

Enlaces rÃ¡pidos:

- [GLOSARIO: Logistic Regression](GLOSARIO.md#logistic-regression)
- [GLOSARIO: Sigmoid](GLOSARIO.md#sigmoid)
- [GLOSARIO: Binary Cross-Entropy](GLOSARIO.md#binary-cross-entropy)
- [GLOSARIO: Gradient Descent](GLOSARIO.md#gradient-descent)
- [RECURSOS.md](RECURSOS.md)

#### ExplicaciÃ³n progresiva (intuiciÃ³n â†’ formalizaciÃ³n â†’ implementaciÃ³n)

##### a) IntuiciÃ³n

Quieres un modelo que devuelva:

- un **score lineal** `z = Î¸áµ€x` (como en regresiÃ³n lineal), y
- lo convierta en una **probabilidad** en `(0, 1)`.

Eso lo hace `Ïƒ(z)`.

##### a.1 Odds, log-odds y por quÃ© esto â€œsigue siendo linealâ€

Si el modelo produce `p = P(y=1|x)`, define:

```
odds = p / (1 - p)
logit(p) = log(odds)
```

La regresiÃ³n logÃ­stica asume que **el log-odds es lineal**:

```
logit(p) = Î¸áµ€x
```

Y la sigmoide es simplemente la funciÃ³n que vuelve de logit a probabilidad:

```
p = Ïƒ(Î¸áµ€x) = 1 / (1 + exp(-Î¸áµ€x))
```

Esto importa porque te permite interpretar el modelo:

- subir `Î¸áµ€x` en +1 incrementa el **log-odds** en +1 (cambio multiplicativo en odds).

##### a.2 Por quÃ© NO usar MSE para clasificaciÃ³n

PodrÃ­as intentar usar MSE con `Å· = Ïƒ(z)`, pero en prÃ¡ctica es mala idea:

- **La geometrÃ­a del entrenamiento empeora:** el gradiente se vuelve poco informativo cuando `Ïƒ(z)` se satura (cerca de 0 o 1).
- **La funciÃ³n objetivo deja de ser convexa** (puede tener mÃ­nimos locales / mesetas), haciendo el descenso de gradiente menos confiable.
- **No penaliza bien el caso â€œseguro y equivocadoâ€:** si `y=1` pero `Å·â‰ˆ0`, quieres un castigo enorme; eso lo da `-log(Å·)`.

Por eso usamos **Log-Loss / Binary Cross-Entropy**, que viene de MLE y es convexa para este modelo.

##### a.3 Visual: frontera de decisiÃ³n

La frontera de decisiÃ³n es el conjunto de puntos donde `p = 0.5`:

```
Ïƒ(Î¸áµ€x) = 0.5  â‡”  Î¸áµ€x = 0
```

##### a.3.1 IntuiciÃ³n geomÃ©trica: el â€œplano de corteâ€

Piensa en tus datos como puntos en un espacio.

- En 2D, `Î¸áµ€x + b = 0` es una **lÃ­nea**.
- En 3D, es un **plano**.
- En `n` dimensiones, es un **hiperplano**.

La cantidad `z = Î¸áµ€x + b` es un **score con signo**:

- `z > 0` â†’ estÃ¡s del lado â€œpositivoâ€ del plano
- `z < 0` â†’ estÃ¡s del lado â€œnegativoâ€

La sigmoide `Ïƒ(z)` convierte ese score (relacionado con la distancia al plano) en probabilidad:

- puntos muy lejos del plano (|z| grande) â†’ probabilidad cerca de 0 o 1
- puntos cerca del plano (`z â‰ˆ 0`) â†’ probabilidad cerca de 0.5

VisualizaciÃ³n sugerida (dibÃºjalo): una nube roja/azul y una lÃ­nea que la corta; marca puntos a distinta distancia y escribe su `z` y `Ïƒ(z)`.

##### a.3.2 ConexiÃ³n conceptual: SVM y la idea de â€œmargenâ€ (sin implementar)

Aunque no implementes SVM aquÃ­, su intuiciÃ³n te mejora la comprensiÃ³n de regularizaciÃ³n.

Idea:

- En clasificaciÃ³n lineal, hay muchas lÃ­neas/planos que separan (si los datos lo permiten).
- SVM busca el separador que deja la â€œcarreteraâ€ mÃ¡s ancha entre clases: **mÃ¡ximo margen**.

ConexiÃ³n con lo que sÃ­ implementas:

- La **regularizaciÃ³n** (L2/L1) controla complejidad efectiva.
- En problemas separables o casi separables, regularizar suele empujar a soluciones mÃ¡s estables, con fronteras menos extremas.

VisualizaciÃ³n sugerida: dos lÃ­neas separadoras posibles y dibujar cuÃ¡l deja mÃ¡s espacio mÃ­nimo a los puntos mÃ¡s cercanos (support vectors).

En 2D, `Î¸áµ€x = 0` es una **lÃ­nea**.

```
clase 1:   o o o o o
           o o o o o

frontera:  ---------

clase 0:   x x x x x
           x x x x x
```

##### a.4 Worked example (numÃ©rico) de BCE

Datos: `x=2`, `y=1`.

- `w=0.5`, `b=0`
- `z = wx + b = 1`
- `Å· = Ïƒ(1) â‰ˆ 0.731`

Como `y=1`, la loss por muestra es:

```
L = -log(Å·) â‰ˆ -log(0.731) â‰ˆ 0.313
```

InterpretaciÃ³n: la predicciÃ³n es â€œbastanteâ€ correcta, por eso la loss es pequeÃ±a. Si `Å·` fuera 0.01, la loss serÃ­a enorme.

##### a.5 CÃ³digo generador de intuiciÃ³n (Protocolo D): frontera de decisiÃ³n en 2D

Objetivo: ver que la **frontera de decisiÃ³n** (`p=0.5`) es lineal, aunque la salida `Ïƒ(z)` sea curva (curva en *probabilidad*, no en geometrÃ­a de la frontera).

```python
import numpy as np
import matplotlib.pyplot as plt


def make_blobs_2d(n=200, seed=42):
    rng = np.random.default_rng(seed)
    c0 = rng.normal(loc=(-2.0, -1.5), scale=0.8, size=(n // 2, 2))
    c1 = rng.normal(loc=(2.0, 1.5), scale=0.8, size=(n // 2, 2))
    X = np.vstack([c0, c1])
    y = np.array([0] * (n // 2) + [1] * (n // 2))
    return X, y


def sigmoid(z):
    z = np.clip(z, -500, 500)
    return 1 / (1 + np.exp(-z))


def add_bias(X):
    return np.column_stack([np.ones(len(X)), X])


def plot_decision_boundary(model, X, y, title="Decision boundary"):
    x_min, x_max = X[:, 0].min() - 1.0, X[:, 0].max() + 1.0
    y_min, y_max = X[:, 1].min() - 1.0, X[:, 1].max() + 1.0

    xx, yy = np.meshgrid(
        np.linspace(x_min, x_max, 250),
        np.linspace(y_min, y_max, 250),
    )

    grid = np.column_stack([xx.ravel(), yy.ravel()])
    proba = model.predict_proba(grid).reshape(xx.shape)

    plt.figure(figsize=(7, 6))
    plt.contourf(xx, yy, proba, levels=20, cmap="RdBu", alpha=0.35)
    plt.contour(xx, yy, proba, levels=[0.5], colors="black", linewidths=2)

    plt.scatter(X[y == 0, 0], X[y == 0, 1], s=18, label="Clase 0")
    plt.scatter(X[y == 1, 0], X[y == 1, 1], s=18, label="Clase 1")

    plt.title(title)
    plt.legend()
    plt.grid(True, alpha=0.2)
    plt.show()


# Usa TU LogisticRegression del mÃ³dulo (la clase ya existe mÃ¡s abajo)
# X, y = make_blobs_2d(n=300)
# model = LogisticRegression()
# model.fit(X, y, learning_rate=0.1, n_iterations=2000)
# plot_decision_boundary(model, X, y)
```

Reto visual (opcional, si usas sklearn solo para generar datos):

- genera `make_moons` y grafica la frontera
- verÃ¡s por quÃ© logÃ­stica falla (frontera lineal)
- luego entrena tu MLP (M07) y observa cÃ³mo la frontera se curva

##### b) FormalizaciÃ³n mÃ­nima

- **Modelo:** `Å· = Ïƒ(XÎ¸)`
- **DecisiÃ³n:** `Å· â‰¥ 0.5 â†’ clase 1` (umbral configurable)
- **Loss (BCE):** penaliza fuerte cuando estÃ¡s â€œseguro y equivocadoâ€ (ej. `Å·â‰ˆ0` pero `y=1`).

##### c) Regla de oro de shapes

Evita bugs silenciosos usando una convenciÃ³n consistente:

- `X`: `(m, n)`
- `Î¸`: `(n,)` (o `(n, 1)` si prefieres columnas)
- `y`: `(m,)`

Y verifica que `X @ Î¸` te da `(m,)`.

#### Actividades activas (para convertir teorÃ­a en habilidad)

- **Retrieval practice (5 min):** escribe sin mirar:
  - la ecuaciÃ³n de BCE,
  - el gradiente `âˆ‡Î¸`.
- **Ejercicio de calibraciÃ³n:** cambia el `threshold` de 0.5 a 0.3 y explica quÃ© pasa con precision/recall.
- **Sanity check obligatorio:** entrena con 20 ejemplos hasta obtener accuracy ~100% (si no, hay bug).

#### EvaluaciÃ³n (criterios de â€œdominioâ€)

- **Dominio matemÃ¡tico:** puedes explicar por quÃ© aparece `(Å· - y)` en el gradiente.
- **Dominio de implementaciÃ³n:** tu `fit()` reduce BCE de forma monotÃ³nica (o casi) en un dataset simple.
- **Dominio de validaciÃ³n:** tu accuracy difiere <5% de sklearn en Shadow Mode.

#### Errores comunes (los que mÃ¡s queman tiempo)

- **Overflow/NaN:** `exp(500)` revienta. SoluciÃ³n: `clip(z)` y `eps` en logs.
- **SaturaciÃ³n:** si `|z|` crece, `Ïƒ(z)` se pega a 0/1 y el gradiente se hace pequeÃ±o.
- **Signo invertido:** si actualizas en la direcciÃ³n equivocada, la loss sube.
- **Sin normalizaciÃ³n:** features en escalas muy distintas hacen que GD sea inestable.

#### IntegraciÃ³n con Plan v4/v5

- **v4.0:** usa `study_tools/SIMULACRO_EXAMEN_TEORICO.md` para preguntas tipo examen (sigmoid vs softmax, BCE vs MSE).
- **v5.0:** ejecuta **Shadow Mode** como verificaciÃ³n externa antes de dar por terminado el mÃ³dulo.

### 2.1 FunciÃ³n Sigmoid

```python
import numpy as np

def sigmoid(z: np.ndarray) -> np.ndarray:
    """
    FunciÃ³n sigmoid/logÃ­stica.

    Ïƒ(z) = 1 / (1 + e^(-z))

    Propiedades:
    - Rango: (0, 1) - perfecto para probabilidades
    - Ïƒ(0) = 0.5
    - Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z))
    """
    # Clip para evitar overflow
    z = np.clip(z, -500, 500)
    return 1 / (1 + np.exp(-z))

# Visualizar
import matplotlib.pyplot as plt

z = np.linspace(-10, 10, 100)
plt.figure(figsize=(8, 4))
plt.plot(z, sigmoid(z))
plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5)
plt.axvline(x=0, color='r', linestyle='--', alpha=0.5)
plt.xlabel('z')
plt.ylabel('Ïƒ(z)')
plt.title('FunciÃ³n Sigmoid')
plt.grid(True)
# plt.show()
```

### 2.2 HipÃ³tesis LogÃ­stica

```python
"""
REGRESIÃ“N LOGÃSTICA

No predice un valor continuo, sino la PROBABILIDAD de pertenecer a la clase 1.

h(x) = P(y=1|x; Î¸) = Ïƒ(Î¸áµ€x)

DecisiÃ³n:
- Si h(x) â‰¥ 0.5 â†’ predicir clase 1
- Si h(x) < 0.5 â†’ predicir clase 0

Equivalente a:
- Si Î¸áµ€x â‰¥ 0 â†’ clase 1
- Si Î¸áµ€x < 0 â†’ clase 0

El "decision boundary" estÃ¡ en Î¸áµ€x = 0
"""

def predict_proba(X: np.ndarray, theta: np.ndarray) -> np.ndarray:
    """Predice probabilidad de clase 1."""
    return sigmoid(X @ theta)

def predict_class(X: np.ndarray, theta: np.ndarray, threshold: float = 0.5) -> np.ndarray:
    """Predice clase (0 o 1)."""
    return (predict_proba(X, theta) >= threshold).astype(int)
```

### 2.3 Binary Cross-Entropy Loss

```python
import numpy as np

def binary_cross_entropy(
    X: np.ndarray,
    y: np.ndarray,
    theta: np.ndarray,
    eps: float = 1e-15
) -> float:
    """
    Binary Cross-Entropy (Log Loss).

    J(Î¸) = -(1/m) Î£áµ¢ [yáµ¢ log(háµ¢) + (1-yáµ¢) log(1-háµ¢)]

    Donde háµ¢ = Ïƒ(Î¸áµ€xáµ¢)

    Por quÃ© esta funciÃ³n de costo:
    - Es convexa (tiene un Ãºnico mÃ­nimo global)
    - Penaliza mucho las predicciones muy incorrectas
    - Es la derivaciÃ³n de Maximum Likelihood Estimation
    """
    m = len(y)
    h = sigmoid(X @ theta)

    # Clip para evitar log(0)
    h = np.clip(h, eps, 1 - eps)

    cost = -(1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
    return cost

def bce_gradient(X: np.ndarray, y: np.ndarray, theta: np.ndarray) -> np.ndarray:
    """
    Gradiente de Binary Cross-Entropy.

    âˆ‚J/âˆ‚Î¸ = (1/m) Xáµ€(h - y)

    Â¡Tiene la misma forma que el gradiente del MSE!
    Esto es porque derivamos Ïƒ(z) y la derivada Ïƒ'(z) = Ïƒ(z)(1-Ïƒ(z))
    cancela parte de la expresiÃ³n.
    """
    m = len(y)
    h = sigmoid(X @ theta)
    return (1/m) * X.T @ (h - y)
```

### 2.4 ImplementaciÃ³n Completa

```python
import numpy as np
from typing import List

class LogisticRegression:
    """RegresiÃ³n LogÃ­stica implementada desde cero."""

    def __init__(self):
        self.theta = None
        self.cost_history = []

    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        learning_rate: float = 0.1,
        n_iterations: int = 1000
    ) -> 'LogisticRegression':
        """Entrena con gradient descent."""
        # AÃ±adir bias
        X_b = np.column_stack([np.ones(len(X)), X])
        m, n = X_b.shape

        # Inicializar
        self.theta = np.zeros(n)

        for i in range(n_iterations):
            # Gradiente
            gradient = bce_gradient(X_b, y, self.theta)

            # Actualizar
            self.theta = self.theta - learning_rate * gradient

            # Guardar costo
            cost = binary_cross_entropy(X_b, y, self.theta)
            self.cost_history.append(cost)

        return self

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """Predice probabilidades."""
        X_b = np.column_stack([np.ones(len(X)), X])
        return sigmoid(X_b @ self.theta)

    def predict(self, X: np.ndarray, threshold: float = 0.5) -> np.ndarray:
        """Predice clases."""
        return (self.predict_proba(X) >= threshold).astype(int)

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        """Accuracy."""
        return np.mean(self.predict(X) == y)


# Demo con datos sintÃ©ticos
np.random.seed(42)

# Generar datos de dos clases
n_samples = 200
X_class0 = np.random.randn(n_samples // 2, 2) + np.array([-2, -2])
X_class1 = np.random.randn(n_samples // 2, 2) + np.array([2, 2])
X = np.vstack([X_class0, X_class1])
y = np.array([0] * (n_samples // 2) + [1] * (n_samples // 2))

# Entrenar
model = LogisticRegression()
model.fit(X, y, learning_rate=0.1, n_iterations=1000)

print(f"Accuracy: {model.score(X, y):.2%}")
print(f"ParÃ¡metros: {model.theta}")
```

---

## ğŸ§© ConsolidaciÃ³n (RegresiÃ³n LogÃ­stica)

### Entregable conceptual (v3.3): InterpretaciÃ³n de pesos (LogReg)

Objetivo: conectar el vector de pesos con â€œquÃ© estÃ¡ mirandoâ€ el modelo.

- Dataset recomendado: MNIST (28x28) en binario (p. ej. 0 vs 1) usando `sklearn.datasets.fetch_openml("mnist_784", as_frame=False)`.
- Entrena tu regresiÃ³n logÃ­stica sobre imÃ¡genes aplanadas (`784` features).
- Visualiza:
  - toma `theta[1:]` (sin bias), reshapea a `(28, 28)` y grafica con `imshow`.
  - usa un mapa de color divergente (p. ej. centrado en 0) y guarda una imagen.
- Interpreta en 5â€“10 lÃ­neas:
  - Â¿quÃ© regiones tienen peso positivo/negativo?
  - Â¿por quÃ© eso tiene sentido para el dÃ­gito?

### Errores comunes

- **Etiquetas incorrectas:** BCE asume `y âˆˆ {0,1}` (no `{-1,1}`) si usas la fÃ³rmula estÃ¡ndar.
- **Olvidar el bias:** si no agregas columna de 1s, la frontera se forza a pasar por el origen.
- **`exp` overflow:** si `z` crece, `exp(-z)` puede overflow/underflow â†’ usa `clip`.
- **`log(0)`:** si `h` llega a 0 o 1 exactos, `log` revienta â†’ usa `eps`.
- **Sin escalado:** features con escalas distintas hacen el GD inestable.

### Debugging / validaciÃ³n (v5)

- **Overfit test:** entrena con 20 ejemplos hasta casi 100% accuracy. Si no, asume bug.
- **Shadow Mode:** compara con sklearn para la misma semilla/dataset.
- Registra hallazgos en `study_tools/DIARIO_ERRORES.md`.
- Protocolos completos:
  - [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)
  - [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md)

### Reto Feynman (tablero blanco)

Explica en 5 lÃ­neas o menos:

1) Â¿QuÃ© es el logit y por quÃ© logÃ­stica es lineal â€œen el espacio de log-oddsâ€?
2) Â¿Por quÃ© `-log(Å·)` explota cuando estÃ¡s seguro y equivocado?
3) Â¿QuÃ© significa `Xáµ€(Å· - y)` y por quÃ© aparece en el gradiente?

---

## ğŸ’» Parte 3: MÃ©tricas de EvaluaciÃ³n

### 3.0 MÃ©tricas â€” Nivel: intermedio (de â€œcalcularâ€ a â€œtomar decisionesâ€)

**PropÃ³sito:** que no te quedes en â€œsÃ© calcular accuracyâ€, sino que puedas **elegir la mÃ©trica correcta segÃºn el riesgo** (FP vs FN), detectar desbalance de clases y justificar tus decisiones como en un informe.

#### Objetivos de aprendizaje (medibles)

Al terminar esta parte podrÃ¡s:

- **Explicar** la matriz de confusiÃ³n y derivar TP/TN/FP/FN sin mirar apuntes.
- **Aplicar** accuracy/precision/recall/F1/specificity y explicar cuÃ¡ndo cada una es adecuada.
- **Analizar** el impacto del umbral (`threshold`) en precision/recall.
- **Diagnosticar** trampas comunes: accuracy alta con clases desbalanceadas, leakage, evaluar sobre train.

#### Prerrequisitos y conexiones

- ConexiÃ³n directa con probabilidad/loss:
  - [04_PROBABILIDAD_ML.md](04_PROBABILIDAD_ML.md) (MLE â†’ cross-entropy)
- Glosario:
  - [GLOSARIO: Confusion Matrix](GLOSARIO.md#confusion-matrix)
  - [GLOSARIO: Accuracy](GLOSARIO.md#accuracy)
  - [GLOSARIO: Precision](GLOSARIO.md#precision)
  - [GLOSARIO: Recall](GLOSARIO.md#recall)
  - [GLOSARIO: F1 Score](GLOSARIO.md#f1-score)

#### Resumen ejecutivo (big idea)

La mÃ©trica es una traducciÃ³n explÃ­cita de â€œquÃ© error es mÃ¡s caroâ€:

- Si te preocupa **no perder positivos reales** â†’ prioriza **recall**.
- Si te preocupa **no disparar falsas alarmas** â†’ prioriza **precision**.
- Si necesitas balance â†’ **F1**.
- Si tu dataset estÃ¡ balanceado y el costo es simÃ©trico â†’ **accuracy** puede servir.

#### Actividades activas (obligatorias)

- **Retrieval practice (5 min):** escribe la matriz 2x2 y define TP/TN/FP/FN.
- **Experimento de umbral:** evalÃºa con `threshold = 0.3, 0.5, 0.7` y anota cÃ³mo cambian precision/recall.
- **Caso desbalanceado:** crea un dataset donde 95% sea clase 0 y muestra por quÃ© accuracy engaÃ±a.

#### Errores comunes (los que mÃ¡s daÃ±an resultados)

- **Evaluar en training:** te da una â€œmÃ©trica falsaâ€ por overfitting.
- **Leakage:** normalizar/seleccionar features usando todo el dataset antes del split.
- **No fijar semilla:** resultados no reproducibles.

IntegraciÃ³n con Plan v4/v5:

- [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md) (rutina + simulacros)
- [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md) (validaciÃ³n externa / rigor)
- Diario: `study_tools/DIARIO_ERRORES.md`

### 3.1 Matriz de ConfusiÃ³n

```python
import numpy as np

def confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
    """
    Calcula la matriz de confusiÃ³n.

    Para clasificaciÃ³n binaria:

                    Predicho
                    0       1
    Real    0      TN      FP
            1      FN      TP

    - TP (True Positive): Predijo 1, era 1
    - TN (True Negative): Predijo 0, era 0
    - FP (False Positive): Predijo 1, era 0 (Error Tipo I)
    - FN (False Negative): Predijo 0, era 1 (Error Tipo II)
    """
    classes = np.unique(np.concatenate([y_true, y_pred]))
    n_classes = len(classes)
    cm = np.zeros((n_classes, n_classes), dtype=int)

    for i, true_class in enumerate(classes):
        for j, pred_class in enumerate(classes):
            cm[i, j] = np.sum((y_true == true_class) & (y_pred == pred_class))

    return cm

def extract_tp_tn_fp_fn(y_true: np.ndarray, y_pred: np.ndarray):
    """Extrae TP, TN, FP, FN para clasificaciÃ³n binaria."""
    tp = np.sum((y_true == 1) & (y_pred == 1))
    tn = np.sum((y_true == 0) & (y_pred == 0))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))
    return tp, tn, fp, fn
```

### 3.2 Accuracy, Precision, Recall, F1

```python
import numpy as np

def accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Accuracy = (TP + TN) / (TP + TN + FP + FN)

    ProporciÃ³n de predicciones correctas.

    Problema: Puede ser engaÃ±oso con clases desbalanceadas.
    Si 99% son clase 0, predecir siempre 0 da 99% accuracy.
    """
    return np.mean(y_true == y_pred)

def precision(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Precision = TP / (TP + FP)

    De todos los que predije como positivos, Â¿cuÃ¡ntos realmente lo son?

    Alta precisiÃ³n = pocos falsos positivos.
    Importante cuando el costo de FP es alto (ej: spam â†’ inbox).
    """
    tp, tn, fp, fn = extract_tp_tn_fp_fn(y_true, y_pred)
    if tp + fp == 0:
        return 0.0
    return tp / (tp + fp)

def recall(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Recall (Sensitivity, True Positive Rate) = TP / (TP + FN)

    De todos los positivos reales, Â¿cuÃ¡ntos capturÃ©?

    Alto recall = pocos falsos negativos.
    Importante cuando el costo de FN es alto (ej: detecciÃ³n de cÃ¡ncer).
    """
    tp, tn, fp, fn = extract_tp_tn_fp_fn(y_true, y_pred)
    if tp + fn == 0:
        return 0.0
    return tp / (tp + fn)

def f1_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    F1 = 2 * (precision * recall) / (precision + recall)

    Media armÃ³nica de precision y recall.

    Ãštil cuando quieres un balance entre ambas mÃ©tricas.
    F1 alto solo si AMBAS precision y recall son altas.
    """
    p = precision(y_true, y_pred)
    r = recall(y_true, y_pred)
    if p + r == 0:
        return 0.0
    return 2 * (p * r) / (p + r)

def specificity(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Specificity (True Negative Rate) = TN / (TN + FP)

    De todos los negativos reales, Â¿cuÃ¡ntos identifiquÃ©?
    """
    tp, tn, fp, fn = extract_tp_tn_fp_fn(y_true, y_pred)
    if tn + fp == 0:
        return 0.0
    return tn / (tn + fp)
```

### 3.3 Clase Metrics Completa

```python
import numpy as np
from dataclasses import dataclass

@dataclass
class ClassificationReport:
    """Reporte de mÃ©tricas de clasificaciÃ³n."""
    accuracy: float
    precision: float
    recall: float
    f1: float
    specificity: float
    confusion_matrix: np.ndarray

    def __str__(self) -> str:
        cm = self.confusion_matrix
        return f"""
Classification Report
=====================
Accuracy:    {self.accuracy:.4f}
Precision:   {self.precision:.4f}
Recall:      {self.recall:.4f}
F1 Score:    {self.f1:.4f}
Specificity: {self.specificity:.4f}

Confusion Matrix:
           Pred 0  Pred 1
Actual 0   {cm[0,0]:5d}   {cm[0,1]:5d}
Actual 1   {cm[1,0]:5d}   {cm[1,1]:5d}
"""

def classification_report(y_true: np.ndarray, y_pred: np.ndarray) -> ClassificationReport:
    """Genera reporte completo de mÃ©tricas."""
    return ClassificationReport(
        accuracy=accuracy(y_true, y_pred),
        precision=precision(y_true, y_pred),
        recall=recall(y_true, y_pred),
        f1=f1_score(y_true, y_pred),
        specificity=specificity(y_true, y_pred),
        confusion_matrix=confusion_matrix(y_true, y_pred)
    )

# Demo
y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])
y_pred = np.array([0, 0, 1, 0, 1, 1, 0, 1, 1, 1])

report = classification_report(y_true, y_pred)
print(report)
```

---

## ğŸ’» Parte 4: ValidaciÃ³n y RegularizaciÃ³n

### 4.0 ValidaciÃ³n y regularizaciÃ³n â€” Nivel: intermedio/avanzado

**PropÃ³sito:** aprender el â€œworkflow realâ€ que evita autoengaÃ±o:

- dividir datos correctamente
- validar de forma robusta
- controlar overfitting (regularizaciÃ³n)

#### Objetivos de aprendizaje (medibles)

Al terminar esta parte podrÃ¡s:

- **Explicar** la diferencia entre train/val/test y por quÃ© el test no se toca.
- **Aplicar** K-fold cross validation y reportar media Â± desviaciÃ³n.
- **Diagnosticar** sesgo-varianza en tÃ©rminos prÃ¡cticos (quÃ© cambia si aumentas `Î»` o si cambias el tamaÃ±o del modelo).
- **Implementar** regularizaciÃ³n L2 y justificar por quÃ© se excluye el bias.

#### Resumen ejecutivo (big idea)

- **ValidaciÃ³n** te dice si generalizas.
- **RegularizaciÃ³n** controla complejidad efectiva.

Conectar esto con el Pathway:

- En el curso, se evalÃºa tanto la *matemÃ¡tica* como tu capacidad de **evitar leakage** y reportar resultados correctamente.

#### Actividades activas (obligatorias)

- Ejecuta `train_test_split` con al menos 2 semillas distintas y compara varianza en accuracy.
- Haz K-fold (k=5) y reporta `mean Â± std`.
- Prueba `lambda_` en `{0, 0.01, 0.1, 1.0}` y describe el efecto.

#### Errores comunes

- **Data leakage** por normalizar antes del split.
- **Elegir hiperparÃ¡metros mirando el test** (invalidas el test).
- **Regularizar el bias** sin querer.

#### IntegraciÃ³n con Plan v4/v5

- v4.0: usa simulacros para preguntas tipo examen (`study_tools/SIMULACRO_EXAMEN_TEORICO.md`).
- v5.0: valida tu implementaciÃ³n con Shadow Mode (sklearn) antes de cerrar el mÃ³dulo.

### 4.1 Train/Test Split

```python
import numpy as np

def train_test_split(
    X: np.ndarray,
    y: np.ndarray,
    test_size: float = 0.2,
    random_state: int = None
) -> tuple:
    """
    Divide datos en conjuntos de entrenamiento y prueba.

    Args:
        X: features
        y: targets
        test_size: proporciÃ³n para test (0-1)
        random_state: semilla para reproducibilidad
    """
    if random_state is not None:
        np.random.seed(random_state)

    n = len(y)
    indices = np.random.permutation(n)

    test_size_n = int(n * test_size)
    test_indices = indices[:test_size_n]
    train_indices = indices[test_size_n:]

    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]
```

### 4.2 K-Fold Cross Validation

```python
import numpy as np
from typing import List, Tuple

def k_fold_split(n: int, k: int) -> List[Tuple[np.ndarray, np.ndarray]]:
    """
    Genera Ã­ndices para K-Fold Cross Validation.

    Returns:
        Lista de (train_indices, val_indices) para cada fold
    """
    indices = np.arange(n)
    np.random.shuffle(indices)

    fold_size = n // k
    folds = []

    for i in range(k):
        start = i * fold_size
        end = start + fold_size if i < k - 1 else n

        val_indices = indices[start:end]
        train_indices = np.concatenate([indices[:start], indices[end:]])

        folds.append((train_indices, val_indices))

    return folds

def cross_validate(
    model_class,
    X: np.ndarray,
    y: np.ndarray,
    k: int = 5,
    **model_params
) -> dict:
    """
    Realiza K-Fold Cross Validation.

    Returns:
        Dict con scores de cada fold y promedio
    """
    folds = k_fold_split(len(y), k)
    scores = []

    for i, (train_idx, val_idx) in enumerate(folds):
        # Split
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        # Train
        model = model_class()
        model.fit(X_train, y_train, **model_params)

        # Evaluate
        score = model.score(X_val, y_val)
        scores.append(score)

    return {
        'scores': scores,
        'mean': np.mean(scores),
        'std': np.std(scores)
    }

# Demo
# cv_results = cross_validate(LogisticRegression, X, y, k=5, learning_rate=0.1, n_iterations=500)
# print(f"CV Accuracy: {cv_results['mean']:.4f} Â± {cv_results['std']:.4f}")
```

### 4.3 RegularizaciÃ³n

```python
import numpy as np

class LogisticRegressionRegularized:
    """Logistic Regression con regularizaciÃ³n L1/L2."""

    def __init__(self, regularization: str = 'l2', lambda_: float = 0.01):
        """
        Args:
            regularization: 'l1', 'l2', o None
            lambda_: fuerza de regularizaciÃ³n
        """
        self.regularization = regularization
        self.lambda_ = lambda_
        self.theta = None
        self.cost_history = []

    def _cost(self, X: np.ndarray, y: np.ndarray) -> float:
        """Costo con regularizaciÃ³n."""
        m = len(y)
        h = sigmoid(X @ self.theta)
        h = np.clip(h, 1e-15, 1 - 1e-15)

        # Cross-entropy base
        bce = -(1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))

        # RegularizaciÃ³n (excluir bias theta[0])
        if self.regularization == 'l2':
            # Ridge: Î»/2m * Î£Î¸â±¼Â²
            reg = (self.lambda_ / (2 * m)) * np.sum(self.theta[1:] ** 2)
        elif self.regularization == 'l1':
            # Lasso: Î»/m * Î£|Î¸â±¼|
            reg = (self.lambda_ / m) * np.sum(np.abs(self.theta[1:]))
        else:
            reg = 0

        return bce + reg

    def _gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:
        """Gradiente con regularizaciÃ³n."""
        m = len(y)
        h = sigmoid(X @ self.theta)

        # Gradiente base
        grad = (1/m) * X.T @ (h - y)

        # RegularizaciÃ³n (excluir bias)
        if self.regularization == 'l2':
            reg_grad = np.concatenate([[0], (self.lambda_ / m) * self.theta[1:]])
        elif self.regularization == 'l1':
            reg_grad = np.concatenate([[0], (self.lambda_ / m) * np.sign(self.theta[1:])])
        else:
            reg_grad = 0

        return grad + reg_grad

    def fit(self, X: np.ndarray, y: np.ndarray,
            learning_rate: float = 0.1, n_iterations: int = 1000):
        X_b = np.column_stack([np.ones(len(X)), X])
        self.theta = np.zeros(X_b.shape[1])

        for _ in range(n_iterations):
            gradient = self._gradient(X_b, y)
            self.theta -= learning_rate * gradient
            self.cost_history.append(self._cost(X_b, y))

        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        X_b = np.column_stack([np.ones(len(X)), X])
        return (sigmoid(X_b @ self.theta) >= 0.5).astype(int)

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        return np.mean(self.predict(X) == y)
```

---

### âš ï¸ Aviso crÃ­tico antes de Ãrboles: Recursividad (Semana 12)

La implementaciÃ³n de Ã¡rboles se basa en **recursiÃ³n**. Si no defines y pruebas condiciones de parada, vas a generar Ã¡rboles infinitos o muy profundos.

- Condiciones de parada mÃ­nimas: `max_depth`, pureza (todas las etiquetas iguales), `min_samples_split`, â€œno split improvesâ€.
- Recurso recomendado: https://realpython.com/python-recursion/
- Debug mÃ­nimo: imprime `depth`, `n_samples` y el criterio elegido por nodo durante desarrollo.

## ğŸŒ³ Parte 5: Tree-Based Models (Semana 12)

Esta semana cubre modelos supervisados **no diferenciables** (no entrenan con Gradient Descent). La lÃ³gica de entrenamiento es:

- elegir un *split* (feature + threshold)
- medir quÃ© tan â€œpuroâ€ queda cada lado (EntropÃ­a o Gini)
- repetir recursivamente

### 5.1 EntropÃ­a, Gini e Information Gain

Definiciones base (para clasificaciÃ³n):

- **EntropÃ­a:** `H(y) = - Î£ p(c) log2 p(c)`
- **Gini:** `G(y) = 1 - Î£ p(c)^2`

Un split `(j, t)` divide el dataset en:

- izquierda: `x_j â‰¤ t`
- derecha: `x_j > t`

La idea es maximizar la mejora en pureza:

- **Information Gain:** `IG = impurity(parent) - weighted_impurity(children)`

### 5.2 Entrenable desde cero (entregable)

Entregable runnable:

- `scripts/decision_tree_from_scratch.py`

Ejecuta:

```bash
python3 scripts/decision_tree_from_scratch.py --criterion gini --max-depth 5
```

Objetivo mÃ­nimo:

- que el script entrene un Ã¡rbol y reporte accuracy train/test en un dataset toy
- que puedas explicar (en 5 lÃ­neas) cÃ³mo el Ã¡rbol decide el mejor split

### 5.3 Ensembles (intro): Bagging vs Boosting

Conceptos clave:

- **Bagging (Random Forest):** muchos Ã¡rboles entrenados en *bootstrap samples*; reduce varianza.
- **Boosting (Gradient Boosting/XGBoost):** Ã¡rboles entrenados secuencialmente corrigiendo errores; reduce bias (pero puede sobreajustar).

---

## ğŸ“¦ Entregable del MÃ³dulo

- `supervised_learning.py` (regresiÃ³n lineal + logÃ­stica + mÃ©tricas + validaciÃ³n).
- `scripts/decision_tree_from_scratch.py` (Ã¡rbol de decisiÃ³n simple desde cero, sin gradientes).

### `supervised_learning.py`

```python
"""
Supervised Learning Module

ImplementaciÃ³n desde cero de:
- Linear Regression (con Normal Equation y Gradient Descent)
- Logistic Regression (con regularizaciÃ³n L1/L2)
- MÃ©tricas de evaluaciÃ³n
- Cross Validation

Autor: [Tu nombre]
MÃ³dulo: 05 - Supervised Learning
"""

import numpy as np
from typing import Tuple, List, Optional
from dataclasses import dataclass


# ============================================================
# FUNCIONES AUXILIARES
# ============================================================

def sigmoid(z: np.ndarray) -> np.ndarray:
    z = np.clip(z, -500, 500)
    return 1 / (1 + np.exp(-z))

def add_bias(X: np.ndarray) -> np.ndarray:
    return np.column_stack([np.ones(len(X)), X])


# ============================================================
# REGRESIÃ“N LINEAL
# ============================================================

class LinearRegression:
    def __init__(self):
        self.theta = None
        self.cost_history = []

    def fit(self, X: np.ndarray, y: np.ndarray,
            method: str = 'normal', lr: float = 0.01, n_iter: int = 1000):
        X_b = add_bias(X)

        if method == 'normal':
            self.theta = np.linalg.solve(X_b.T @ X_b, X_b.T @ y)
        else:
            m, n = X_b.shape
            self.theta = np.zeros(n)
            for _ in range(n_iter):
                grad = (1/m) * X_b.T @ (X_b @ self.theta - y)
                self.theta -= lr * grad
                self.cost_history.append(np.mean((X_b @ self.theta - y)**2))
        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        return add_bias(X) @ self.theta

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        y_pred = self.predict(X)
        ss_res = np.sum((y - y_pred)**2)
        ss_tot = np.sum((y - np.mean(y))**2)
        return 1 - ss_res / ss_tot


# ============================================================
# REGRESIÃ“N LOGÃSTICA
# ============================================================

class LogisticRegression:
    def __init__(self, reg: str = None, lambda_: float = 0.01):
        self.reg = reg
        self.lambda_ = lambda_
        self.theta = None
        self.cost_history = []

    def fit(self, X: np.ndarray, y: np.ndarray,
            lr: float = 0.1, n_iter: int = 1000):
        X_b = add_bias(X)
        m, n = X_b.shape
        self.theta = np.zeros(n)

        for _ in range(n_iter):
            h = sigmoid(X_b @ self.theta)
            grad = (1/m) * X_b.T @ (h - y)

            if self.reg == 'l2':
                grad[1:] += (self.lambda_/m) * self.theta[1:]
            elif self.reg == 'l1':
                grad[1:] += (self.lambda_/m) * np.sign(self.theta[1:])

            self.theta -= lr * grad
        return self

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        return sigmoid(add_bias(X) @ self.theta)

    def predict(self, X: np.ndarray) -> np.ndarray:
        return (self.predict_proba(X) >= 0.5).astype(int)

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        return np.mean(self.predict(X) == y)


# ============================================================
# MÃ‰TRICAS
# ============================================================

def accuracy(y_true, y_pred):
    return np.mean(y_true == y_pred)

def precision(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    return tp / (tp + fp) if (tp + fp) > 0 else 0

def recall(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))
    return tp / (tp + fn) if (tp + fn) > 0 else 0

def f1_score(y_true, y_pred):
    p, r = precision(y_true, y_pred), recall(y_true, y_pred)
    return 2*p*r/(p+r) if (p+r) > 0 else 0

def confusion_matrix(y_true, y_pred):
    cm = np.zeros((2, 2), dtype=int)
    cm[0, 0] = np.sum((y_true == 0) & (y_pred == 0))
    cm[0, 1] = np.sum((y_true == 0) & (y_pred == 1))
    cm[1, 0] = np.sum((y_true == 1) & (y_pred == 0))
    cm[1, 1] = np.sum((y_true == 1) & (y_pred == 1))
    return cm


# ============================================================
# VALIDACIÃ“N
# ============================================================

def train_test_split(X, y, test_size=0.2, seed=None):
    if seed: np.random.seed(seed)
    n = len(y)
    idx = np.random.permutation(n)
    split = int(n * test_size)
    return X[idx[split:]], X[idx[:split]], y[idx[split:]], y[idx[:split]]

def cross_validate(model_class, X, y, k=5, **params):
    n = len(y)
    idx = np.random.permutation(n)
    fold_size = n // k
    scores = []

    for i in range(k):
        val_idx = idx[i*fold_size:(i+1)*fold_size]
        train_idx = np.concatenate([idx[:i*fold_size], idx[(i+1)*fold_size:]])

        model = model_class()
        model.fit(X[train_idx], y[train_idx], **params)
        scores.append(model.score(X[val_idx], y[val_idx]))

    return {'scores': scores, 'mean': np.mean(scores), 'std': np.std(scores)}


# ============================================================
# TESTS
# ============================================================

if __name__ == "__main__":
    np.random.seed(42)

    # Test Linear Regression
    X = 2 * np.random.rand(100, 1)
    y = 4 + 3 * X.flatten() + np.random.randn(100) * 0.5

    lr = LinearRegression()
    lr.fit(X, y)
    print(f"Linear Regression RÂ²: {lr.score(X, y):.4f}")

    # Test Logistic Regression
    X_c0 = np.random.randn(50, 2) + [-2, -2]
    X_c1 = np.random.randn(50, 2) + [2, 2]
    X_clf = np.vstack([X_c0, X_c1])
    y_clf = np.array([0]*50 + [1]*50)

    log_reg = LogisticRegression()
    log_reg.fit(X_clf, y_clf)
    print(f"Logistic Regression Accuracy: {log_reg.score(X_clf, y_clf):.4f}")

    # Test metrics
    y_true = np.array([0,0,0,1,1,1,1,1])
    y_pred = np.array([0,0,1,1,1,0,1,1])
    print(f"Precision: {precision(y_true, y_pred):.4f}")
    print(f"Recall: {recall(y_true, y_pred):.4f}")
    print(f"F1: {f1_score(y_true, y_pred):.4f}")

    # Test CV
    cv = cross_validate(LogisticRegression, X_clf, y_clf, k=5, lr=0.1, n_iter=500)
    print(f"CV Score: {cv['mean']:.4f} Â± {cv['std']:.4f}")

    print("\nâœ“ Todos los tests pasaron!")
```

---

## ğŸ“ DerivaciÃ³n AnalÃ­tica: El Entregable de LÃ¡piz y Papel (v3.3)

> ğŸ“ **SimulaciÃ³n de Examen:** En la maestrÃ­a te pedirÃ¡n: *"Derive la regla de actualizaciÃ³n de pesos para Logistic Regression"*. Debes poder hacerlo a mano.

### DerivaciÃ³n del Gradiente de Logistic Regression

**Objetivo:** Derivar `âˆ‚L/âˆ‚w` para la funciÃ³n de costo Cross-Entropy.

#### Paso 1: Definir la FunciÃ³n de Costo

```
L(w) = -(1/n) Î£_{i=1..n} [ y_i log(Å·_i) + (1 - y_i) log(1 - Å·_i) ]
```

Donde:
- `Å·_i = Ïƒ(wáµ€ x_i) = 1 / (1 + e^{-wáµ€ x_i})`
- `Ïƒ(z)` es la funciÃ³n sigmoid

#### Paso 2: Derivar la Sigmoid

```
dÏƒ/dz = Ïƒ(z)(1 - Ïƒ(z))
```

**DemostraciÃ³n:**
```
Ïƒ(z) = 1 / (1 + e^{-z})

dÏƒ/dz = e^{-z} / (1 + e^{-z})^2
      = (1 / (1 + e^{-z})) Â· (e^{-z} / (1 + e^{-z}))
      = Ïƒ(z)(1 - Ïƒ(z))
```

#### Paso 3: Aplicar la Regla de la Cadena

Para un solo ejemplo `(x_i, y_i)`:

```
âˆ‚L_i/âˆ‚w = (âˆ‚L_i/âˆ‚Å·_i) Â· (âˆ‚Å·_i/âˆ‚z_i) Â· (âˆ‚z_i/âˆ‚w)
```

Donde `z_i = wáµ€ x_i`

**Calcular cada tÃ©rmino:**

1. `âˆ‚L_i/âˆ‚Å·_i = -y_i/Å·_i + (1 - y_i)/(1 - Å·_i)`

2. `âˆ‚Å·_i/âˆ‚z_i = Å·_i(1 - Å·_i)`

3. `âˆ‚z_i/âˆ‚w = x_i`

#### Paso 4: Simplificar

```
âˆ‚L_i/âˆ‚w = ( -y_i/Å·_i + (1 - y_i)/(1 - Å·_i) ) Â· Å·_i(1 - Å·_i) Â· x_i
```

Simplificando el tÃ©rmino entre parÃ©ntesis:
```
= ( (-y_i(1 - Å·_i) + (1 - y_i)Å·_i) / (Å·_i(1 - Å·_i)) ) Â· Å·_i(1 - Å·_i) Â· x_i
= (-y_i + y_iÅ·_i + Å·_i - y_iÅ·_i) Â· x_i
= (Å·_i - y_i) Â· x_i
```

#### Resultado Final

```
âˆ‚L/âˆ‚w = (1/n) Î£_{i=1..n} (Å·_i - y_i) x_i
      = (1/n) Xáµ€ (Å· - y)
```

**Forma vectorizada (para cÃ³digo):**
```python
gradient = (1/n) * X.T @ (y_pred - y_true)
```

### Tu Entregable

Escribe en un documento (Markdown o LaTeX):
1. La derivaciÃ³n completa del gradiente de Cross-Entropy
2. La derivaciÃ³n de la regla de actualizaciÃ³n: `w <- w - Î± âˆ‡L`
3. Por quÃ© el gradiente tiene la forma `(Å· - y)` (interpretaciÃ³n geomÃ©trica)

---

## ğŸ¯ El Reto del Tablero Blanco (MetodologÃ­a Feynman)

Explica en **mÃ¡ximo 5 lÃ­neas** sin jerga tÃ©cnica:

1. **Â¿Por quÃ© usamos sigmoid en clasificaciÃ³n?**
   > Pista: Piensa en probabilidades entre 0 y 1.

2. **Â¿Por quÃ© Cross-Entropy y no MSE para clasificaciÃ³n?**
   > Pista: Piensa en quÃ© pasa cuando `Å· â‰ˆ 0` pero `y = 1`.

3. **Â¿QuÃ© significa "One-vs-All"?**
   > Pista: Piensa en cÃ³mo clasificar 10 dÃ­gitos con clasificadores binarios.

---

## ğŸ” Shadow Mode: ValidaciÃ³n con sklearn (v3.3)

> âš ï¸ **Regla:** sklearn estÃ¡ **prohibido para aprender**, pero es **necesario para validar**. Si tu implementaciÃ³n difiere significativamente de sklearn, tienes un bug.

### Protocolo de ValidaciÃ³n (Viernes de Fase 2)

```python
"""
Shadow Mode - ValidaciÃ³n de Implementaciones
Compara tu cÃ³digo desde cero vs sklearn para detectar bugs.

Regla: Si la diferencia de accuracy es >5%, revisar matemÃ¡ticas.
"""
import numpy as np
from sklearn.linear_model import LogisticRegression as SklearnLR
from sklearn.linear_model import LinearRegression as SklearnLinReg
from sklearn.metrics import accuracy_score, mean_squared_error

# Importar tu implementaciÃ³n
# from src.logistic_regression import LogisticRegression as MyLR
# from src.linear_regression import LinearRegression as MyLinReg


def shadow_mode_logistic_regression(X_train, y_train, X_test, y_test):
    """
    Compara tu Logistic Regression vs sklearn.

    Los coeficientes y accuracy deben ser casi idÃ©nticos.
    """
    print("=" * 60)
    print("SHADOW MODE: Logistic Regression")
    print("=" * 60)

    # ========== TU IMPLEMENTACIÃ“N ==========
    # my_model = MyLR()
    # my_model.fit(X_train, y_train, lr=0.1, n_iter=1000)
    # my_pred = my_model.predict(X_test)
    # my_acc = accuracy_score(y_test, my_pred)
    # my_weights = my_model.weights

    # Placeholder (reemplazar con tu cÃ³digo)
    my_acc = 0.85
    my_weights = np.zeros(X_train.shape[1])

    # ========== SKLEARN (GROUND TRUTH) ==========
    sklearn_model = SklearnLR(max_iter=1000, solver='lbfgs')
    sklearn_model.fit(X_train, y_train)
    sklearn_pred = sklearn_model.predict(X_test)
    sklearn_acc = accuracy_score(y_test, sklearn_pred)
    sklearn_weights = sklearn_model.coef_.flatten()

    # ========== COMPARACIÃ“N ==========
    acc_diff = abs(my_acc - sklearn_acc)
    weight_diff = np.linalg.norm(my_weights - sklearn_weights[:len(my_weights)])

    print(f"\nğŸ“Š RESULTADOS:")
    print(f"  Tu Accuracy:     {my_acc:.4f}")
    print(f"  sklearn Accuracy: {sklearn_acc:.4f}")
    print(f"  Diferencia:       {acc_diff:.4f}")

    print(f"\nğŸ“ PESOS:")
    print(f"  Diferencia L2 de pesos: {weight_diff:.4f}")

    # Veredicto
    print("\n" + "-" * 60)
    if acc_diff < 0.05:
        print("âœ“ PASSED: Tu implementaciÃ³n es correcta")
        return True
    else:
        print("âœ— FAILED: Diferencia significativa - revisa tu matemÃ¡tica")
        print("  Posibles causas:")
        print("  - Gradiente mal calculado")
        print("  - Learning rate muy alto/bajo")
        print("  - Falta de normalizaciÃ³n de datos")
        return False


def shadow_mode_linear_regression(X_train, y_train, X_test, y_test):
    """
    Compara tu Linear Regression vs sklearn.
    """
    print("=" * 60)
    print("SHADOW MODE: Linear Regression")
    print("=" * 60)

    # ========== TU IMPLEMENTACIÃ“N ==========
    # my_model = MyLinReg()
    # my_model.fit(X_train, y_train)
    # my_pred = my_model.predict(X_test)
    # my_mse = mean_squared_error(y_test, my_pred)

    # Placeholder
    my_mse = 0.5

    # ========== SKLEARN ==========
    sklearn_model = SklearnLinReg()
    sklearn_model.fit(X_train, y_train)
    sklearn_pred = sklearn_model.predict(X_test)
    sklearn_mse = mean_squared_error(y_test, sklearn_pred)

    # ========== COMPARACIÃ“N ==========
    mse_ratio = my_mse / sklearn_mse if sklearn_mse > 0 else float('inf')

    print(f"\nğŸ“Š RESULTADOS:")
    print(f"  Tu MSE:     {my_mse:.4f}")
    print(f"  sklearn MSE: {sklearn_mse:.4f}")
    print(f"  Ratio:       {mse_ratio:.2f}x")

    print("\n" + "-" * 60)
    if mse_ratio < 1.1:  # Dentro del 10%
        print("âœ“ PASSED: Tu implementaciÃ³n es correcta")
        return True
    else:
        print("âœ— FAILED: Tu MSE es significativamente mayor")
        return False


# ============================================================
# EJEMPLO DE USO
# ============================================================

if __name__ == "__main__":
    from sklearn.datasets import make_classification, make_regression
    from sklearn.model_selection import train_test_split

    # Dataset de clasificaciÃ³n
    X_clf, y_clf = make_classification(
        n_samples=1000, n_features=10, n_classes=2, random_state=42
    )
    X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
        X_clf, y_clf, test_size=0.2, random_state=42
    )

    # Dataset de regresiÃ³n
    X_reg, y_reg = make_regression(
        n_samples=1000, n_features=10, noise=10, random_state=42
    )
    X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(
        X_reg, y_reg, test_size=0.2, random_state=42
    )

    # Ejecutar Shadow Mode
    shadow_mode_logistic_regression(X_train_c, y_train_c, X_test_c, y_test_c)
    print("\n")
    shadow_mode_linear_regression(X_train_r, y_train_r, X_test_r, y_test_r)
```

### Checklist Shadow Mode

| DÃ­a | Algoritmo | Validar |
|-----|-----------|---------|
| Viernes Sem 10 | Linear Regression | MSE â‰ˆ sklearn |
| Viernes Sem 11 | Logistic Regression | Accuracy â‰ˆ sklearn |
| Viernes Sem 12 | MÃ©tricas | Precision/Recall = sklearn |

---

## âœ… Checklist de FinalizaciÃ³n (v3.3)

### Conocimiento
- [ ] ImplementÃ© regresiÃ³n lineal con Normal Equation y GD
- [ ] Entiendo MSE y su gradiente
- [ ] ImplementÃ© regresiÃ³n logÃ­stica desde cero
- [ ] Entiendo sigmoid y binary cross-entropy
- [ ] Puedo calcular TP, TN, FP, FN de una matriz de confusiÃ³n
- [ ] ImplementÃ© accuracy, precision, recall, F1
- [ ] ImplementÃ© train/test split
- [ ] ImplementÃ© K-fold cross validation
- [ ] Entiendo regularizaciÃ³n L1 vs L2

### Shadow Mode (v3.3 - Obligatorio)
- [ ] **Linear Regression**: Mi MSE â‰ˆ sklearn (ratio < 1.1)
- [ ] **Logistic Regression**: Mi Accuracy â‰ˆ sklearn (diff < 5%)

### Entregables de CÃ³digo
- [ ] `logistic_regression.py` con tests pasando
- [ ] `artifacts/m05_logreg_weights.png` + 5â€“10 lÃ­neas de interpretaciÃ³n (pesos 28x28)
- [ ] `mypy src/` pasa sin errores
- [ ] `pytest tests/` pasa sin errores

### DerivaciÃ³n AnalÃ­tica (Obligatorio)
- [ ] DerivÃ© el gradiente de Cross-Entropy a mano
- [ ] Documento con derivaciÃ³n completa (Markdown o LaTeX)
- [ ] Puedo explicar por quÃ© `âˆ‡L = Xáµ€(Å· - y)`

### MetodologÃ­a Feynman
- [ ] Puedo explicar sigmoid en 5 lÃ­neas sin jerga
- [ ] Puedo explicar Cross-Entropy vs MSE en 5 lÃ­neas
- [ ] Puedo explicar One-vs-All en 5 lÃ­neas

---

## ğŸ”— NavegaciÃ³n

| Anterior | Ãndice | Siguiente |
|----------|--------|-----------|
| [04_PROBABILIDAD_ML](04_PROBABILIDAD_ML.md) | [00_INDICE](00_INDICE.md) | [06_UNSUPERVISED_LEARNING](06_UNSUPERVISED_LEARNING.md) |
