{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25384bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lab 2: Monte Carlo y MCMC (Markov Chain Monte Carlo)\n",
    "=====================================================\n",
    "\n",
    "Módulo: M04 - Probabilidad y Estadística\n",
    "Tiempo Estimado: 3-4 horas\n",
    "Prerequisitos: Probabilidad básica, distribuciones\n",
    "\n",
    "Objetivos de Aprendizaje:\n",
    "-------------------------\n",
    "1. Entender el método de Monte Carlo para integración\n",
    "2. Implementar Metropolis-Hastings desde cero\n",
    "3. Implementar Gibbs Sampling para distribución bivariada\n",
    "4. Diagnosticar convergencia con trace plots y R-hat\n",
    "\n",
    "Referencias:\n",
    "------------\n",
    "- Murphy, \"ML: A Probabilistic Perspective\", Cap. 24\n",
    "- Bishop, \"Pattern Recognition and ML\", Cap. 11\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections.abc import Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from scipy import stats\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22450f36",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Lab 2: Monte Carlo y MCMC\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Los métodos de Monte Carlo nos permiten aproximar integrales y muestrear\n",
    "de distribuciones complejas. MCMC extiende esto para distribuciones donde\n",
    "no podemos muestrear directamente.\n",
    "\n",
    "### ¿Por qué es importante?\n",
    "\n",
    "| Aplicación | Uso de MCMC |\n",
    "|------------|-------------|\n",
    "| Inferencia Bayesiana | Muestrear del posterior P(θ|D) |\n",
    "| Modelos Gráficos | Inferencia en redes bayesianas |\n",
    "| Deep Learning | Dropout (aproximación Monte Carlo) |\n",
    "| Física Estadística | Simulación de sistemas complejos |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80652538",
   "metadata": {
    "title": "Imports"
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Configuración de plots\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e0219b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Parte 1: Monte Carlo Simple (30 min)\n",
    "\n",
    "### 1.1 Estimación de π\n",
    "\n",
    "Usamos el hecho de que un círculo de radio 1 inscrito en un cuadrado\n",
    "de lado 2 tiene área = π.\n",
    "\n",
    "Si lanzamos puntos aleatorios uniformemente en el cuadrado [-1,1]×[-1,1],\n",
    "la proporción que cae dentro del círculo es π/4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9811d0d6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def estimate_pi_monte_carlo(n_samples: int) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Estima π usando Monte Carlo.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Número de puntos aleatorios.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    tuple[float, float]\n",
    "        - Estimación de π\n",
    "        - Error estándar de la estimación\n",
    "    \"\"\"\n",
    "    # Generar puntos uniformes en [-1, 1] × [-1, 1]\n",
    "    x = rng.uniform(-1, 1, n_samples)\n",
    "    y = rng.uniform(-1, 1, n_samples)\n",
    "\n",
    "    # Contar puntos dentro del círculo unitario\n",
    "    inside_circle = (x**2 + y**2) <= 1\n",
    "\n",
    "    # Proporción × 4 = estimación de π\n",
    "    pi_estimate = 4 * np.mean(inside_circle)\n",
    "\n",
    "    # Error estándar: SE = σ / √n\n",
    "    std_error = 4 * np.std(inside_circle) / np.sqrt(n_samples)\n",
    "\n",
    "    return pi_estimate, std_error\n",
    "\n",
    "\n",
    "# Demostración\n",
    "print(\"=== Estimación de π con Monte Carlo ===\\n\")\n",
    "for n in [100, 1000, 10000, 100000]:\n",
    "    pi_est, se = estimate_pi_monte_carlo(n)\n",
    "    error = abs(pi_est - np.pi)\n",
    "    print(f\"n={n:>6}: π ≈ {pi_est:.6f} ± {se:.6f} (error = {error:.6f})\")\n",
    "\n",
    "print(f\"\\nValor real: π = {np.pi:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388a3ae0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### 1.2 Integración Monte Carlo\n",
    "\n",
    "Para calcular ∫f(x)dx sobre un dominio, usamos:\n",
    "\n",
    "∫f(x)dx ≈ (Volumen del dominio) × (1/n) × Σf(xᵢ)\n",
    "\n",
    "donde xᵢ son muestras uniformes del dominio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241af8a4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def monte_carlo_integration(\n",
    "    f: Callable[[NDArray[np.float64]], NDArray[np.float64]],\n",
    "    a: float,\n",
    "    b: float,\n",
    "    n_samples: int,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Integra f(x) de a hasta b usando Monte Carlo.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    f : Callable\n",
    "        Función a integrar.\n",
    "    a, b : float\n",
    "        Límites de integración.\n",
    "    n_samples : int\n",
    "        Número de muestras.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    tuple[float, float]\n",
    "        - Estimación de la integral\n",
    "        - Error estándar\n",
    "    \"\"\"\n",
    "    # Muestrear uniformemente en [a, b]\n",
    "    x = rng.uniform(a, b, n_samples)\n",
    "\n",
    "    # Evaluar función\n",
    "    fx = f(x)\n",
    "\n",
    "    # Integral = (b-a) × promedio de f(x)\n",
    "    integral = (b - a) * np.mean(fx)\n",
    "    std_error = (b - a) * np.std(fx) / np.sqrt(n_samples)\n",
    "\n",
    "    return integral, std_error\n",
    "\n",
    "\n",
    "# Ejemplo: ∫₀¹ x² dx = 1/3\n",
    "def f_cuadrado(x: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "    return x**2\n",
    "\n",
    "\n",
    "print(\"\\n=== Integración Monte Carlo ===\\n\")\n",
    "print(\"∫₀¹ x² dx = 1/3 ≈ 0.3333...\\n\")\n",
    "\n",
    "for n in [100, 1000, 10000]:\n",
    "    integral, se = monte_carlo_integration(f_cuadrado, 0, 1, n)\n",
    "    print(f\"n={n:>5}: Integral ≈ {integral:.6f} ± {se:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d907f738",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Parte 2: Metropolis-Hastings (60 min)\n",
    "\n",
    "### 2.1 El Algoritmo\n",
    "\n",
    "Metropolis-Hastings nos permite muestrear de una distribución objetivo\n",
    "π(x) cuando solo conocemos una función proporcional a ella.\n",
    "\n",
    "**Algoritmo:**\n",
    "1. Inicializar x₀\n",
    "2. Para t = 1, 2, ..., T:\n",
    "   a. Proponer x' ~ q(x'|xₜ₋₁)\n",
    "   b. Calcular ratio de aceptación: α = min(1, π(x')q(xₜ₋₁|x') / π(xₜ₋₁)q(x'|xₜ₋₁))\n",
    "   c. Aceptar x' con probabilidad α, sino mantener xₜ₋₁\n",
    "\n",
    "Para proposal simétrico (q(x'|x) = q(x|x')):\n",
    "α = min(1, π(x') / π(xₜ₋₁))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1db761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings(\n",
    "    log_target: Callable[[float], float],\n",
    "    proposal_std: float,\n",
    "    n_samples: int,\n",
    "    x_init: float = 0.0,\n",
    "    burn_in: int = 1000,\n",
    ") -> tuple[NDArray[np.float64], float]:\n",
    "    \"\"\"\n",
    "    Implementa Metropolis-Hastings con proposal Gaussiano.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    log_target : Callable\n",
    "        Log de la distribución objetivo (hasta constante).\n",
    "    proposal_std : float\n",
    "        Desviación estándar del proposal Gaussiano.\n",
    "    n_samples : int\n",
    "        Número de muestras a generar (después de burn-in).\n",
    "    x_init : float\n",
    "        Valor inicial.\n",
    "    burn_in : int\n",
    "        Muestras a descartar al inicio.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    tuple[NDArray, float]\n",
    "        - Muestras de la distribución objetivo\n",
    "        - Tasa de aceptación\n",
    "    \"\"\"\n",
    "    total_samples = n_samples + burn_in\n",
    "    samples: NDArray[np.float64] = np.zeros(total_samples, dtype=np.float64)\n",
    "    samples[0] = x_init\n",
    "\n",
    "    n_accepted = 0\n",
    "\n",
    "    for t in range(1, total_samples):\n",
    "        # Estado actual\n",
    "        x_current = samples[t - 1]\n",
    "\n",
    "        # Proponer nuevo estado (proposal Gaussiano simétrico)\n",
    "        x_proposed = x_current + rng.normal(0, proposal_std)\n",
    "\n",
    "        # Calcular log-ratio de aceptación\n",
    "        log_alpha = log_target(x_proposed) - log_target(x_current)\n",
    "\n",
    "        # Aceptar con probabilidad min(1, α)\n",
    "        if np.log(rng.uniform()) < log_alpha:\n",
    "            samples[t] = x_proposed\n",
    "            n_accepted += 1\n",
    "        else:\n",
    "            samples[t] = x_current\n",
    "\n",
    "    acceptance_rate = n_accepted / total_samples\n",
    "\n",
    "    return samples[burn_in:], acceptance_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22175e4",
   "metadata": {},
   "source": [
    "### 2.2 Ejemplo: Muestrear de una Gaussiana\n",
    "\n",
    "Objetivo: N(μ=3, σ²=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da9a73",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Definir log de la distribución objetivo\n",
    "mu_target = 3.0\n",
    "sigma_target = np.sqrt(2.0)\n",
    "\n",
    "\n",
    "def log_normal(x: float) -> float:\n",
    "    \"\"\"Log de N(3, 2).\"\"\"\n",
    "    return float(-0.5 * ((x - mu_target) / sigma_target) ** 2)\n",
    "\n",
    "\n",
    "# Ejecutar M-H\n",
    "print(\"\\n=== Metropolis-Hastings: Muestreo de N(3, 2) ===\\n\")\n",
    "\n",
    "samples_mh, acceptance = metropolis_hastings(\n",
    "    log_target=log_normal,\n",
    "    proposal_std=1.5,\n",
    "    n_samples=10000,\n",
    "    x_init=0.0,\n",
    "    burn_in=1000,\n",
    ")\n",
    "\n",
    "print(f\"Tasa de aceptación: {acceptance:.2%}\")\n",
    "print(f\"Media muestral:     {np.mean(samples_mh):.4f} (teórico: {mu_target})\")\n",
    "print(f\"Std muestral:       {np.std(samples_mh):.4f} (teórico: {sigma_target:.4f})\")\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Trace plot\n",
    "axes[0].plot(samples_mh[:1000], alpha=0.7)\n",
    "axes[0].axhline(mu_target, color=\"red\", linestyle=\"--\", label=\"μ teórico\")\n",
    "axes[0].set_xlabel(\"Iteración\")\n",
    "axes[0].set_ylabel(\"x\")\n",
    "axes[0].set_title(\"Trace Plot (primeras 1000 muestras)\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Histograma vs distribución teórica\n",
    "x_range = np.linspace(-2, 8, 200)\n",
    "axes[1].hist(samples_mh, bins=50, density=True, alpha=0.7, label=\"Muestras M-H\")\n",
    "axes[1].plot(\n",
    "    x_range,\n",
    "    stats.norm.pdf(x_range, mu_target, sigma_target),\n",
    "    \"r-\",\n",
    "    linewidth=2,\n",
    "    label=\"N(3, √2) teórica\",\n",
    ")\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"Densidad\")\n",
    "axes[1].set_title(\"Histograma vs Distribución Objetivo\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../assets/mh_gaussian_sampling.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5bfaf1",
   "metadata": {},
   "source": [
    "### 2.3 Efecto del Proposal Width\n",
    "\n",
    "El ancho del proposal afecta la eficiencia:\n",
    "- Muy pequeño: alta aceptación pero exploración lenta\n",
    "- Muy grande: baja aceptación, muchos rechazos\n",
    "- Óptimo: ~23-44% de aceptación (teoría para distribuciones unimodales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee5451",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Efecto del Proposal Width ===\\n\")\n",
    "\n",
    "proposal_widths = [0.1, 0.5, 1.5, 5.0, 20.0]\n",
    "results = []\n",
    "\n",
    "for width in proposal_widths:\n",
    "    samples, acc = metropolis_hastings(log_normal, width, n_samples=5000, burn_in=500)\n",
    "    results.append((width, acc, np.mean(samples), np.std(samples)))\n",
    "    print(\n",
    "        f\"σ_proposal = {width:>4.1f}: Aceptación = {acc:>5.1%}, \"\n",
    "        f\"μ̂ = {np.mean(samples):>5.2f}, σ̂ = {np.std(samples):>4.2f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n⚠️ Regla práctica: buscar aceptación entre 20-50%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b896d0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Parte 3: Gibbs Sampling (45 min)\n",
    "\n",
    "Gibbs Sampling es un caso especial de M-H donde siempre aceptamos.\n",
    "Funciona cuando podemos muestrear de las distribuciones condicionales.\n",
    "\n",
    "Para (x, y) ~ π(x, y):\n",
    "1. Muestrear x ~ π(x|y)\n",
    "2. Muestrear y ~ π(y|x)\n",
    "3. Repetir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49c2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sampling_bivariate_normal(\n",
    "    mu: NDArray[np.float64],\n",
    "    sigma: NDArray[np.float64],\n",
    "    n_samples: int,\n",
    "    burn_in: int = 500,\n",
    ") -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Gibbs Sampling para distribución normal bivariada.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    mu : NDArray\n",
    "        Vector de medias [μ₁, μ₂].\n",
    "    sigma : NDArray\n",
    "        Matriz de covarianza 2×2.\n",
    "    n_samples : int\n",
    "        Número de muestras.\n",
    "    burn_in : int\n",
    "        Muestras a descartar.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    NDArray\n",
    "        Muestras (n_samples, 2).\n",
    "    \"\"\"\n",
    "    total = n_samples + burn_in\n",
    "    samples = np.zeros((total, 2))\n",
    "\n",
    "    # Extraer parámetros\n",
    "    mu1, mu2 = mu\n",
    "    sigma1 = np.sqrt(sigma[0, 0])\n",
    "    sigma2 = np.sqrt(sigma[1, 1])\n",
    "    rho = sigma[0, 1] / (sigma1 * sigma2)\n",
    "\n",
    "    # Inicializar\n",
    "    x, y = 0.0, 0.0\n",
    "\n",
    "    for t in range(total):\n",
    "        # Muestrear x | y\n",
    "        mu_x_given_y = mu1 + rho * (sigma1 / sigma2) * (y - mu2)\n",
    "        std_x_given_y = sigma1 * np.sqrt(1 - rho**2)\n",
    "        x = rng.normal(mu_x_given_y, std_x_given_y)\n",
    "\n",
    "        # Muestrear y | x\n",
    "        mu_y_given_x = mu2 + rho * (sigma2 / sigma1) * (x - mu1)\n",
    "        std_y_given_x = sigma2 * np.sqrt(1 - rho**2)\n",
    "        y = rng.normal(mu_y_given_x, std_y_given_x)\n",
    "\n",
    "        samples[t] = [x, y]\n",
    "\n",
    "    return samples[burn_in:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef98ed51",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Gibbs Sampling: Gaussiana Bivariada ===\\n\")\n",
    "\n",
    "# Definir distribución objetivo\n",
    "mu_biv = np.array([2.0, 5.0])\n",
    "sigma_biv = np.array([[1.0, 0.8], [0.8, 2.0]])\n",
    "\n",
    "print(\"Objetivo: N(μ=[2, 5], Σ=[[1, 0.8], [0.8, 2]])\\n\")\n",
    "\n",
    "# Ejecutar Gibbs\n",
    "samples_gibbs = gibbs_sampling_bivariate_normal(mu_biv, sigma_biv, n_samples=5000)\n",
    "\n",
    "print(f\"Media muestral:  {samples_gibbs.mean(axis=0)}\")\n",
    "print(f\"Media teórica:   {mu_biv}\")\n",
    "print(f\"\\nCov muestral:\\n{np.cov(samples_gibbs.T)}\")\n",
    "print(f\"\\nCov teórica:\\n{sigma_biv}\")\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Trace plots\n",
    "axes[0].plot(samples_gibbs[:500, 0], label=\"x₁\")\n",
    "axes[0].plot(samples_gibbs[:500, 1], label=\"x₂\")\n",
    "axes[0].set_xlabel(\"Iteración\")\n",
    "axes[0].set_ylabel(\"Valor\")\n",
    "axes[0].set_title(\"Trace Plots\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Scatter plot\n",
    "axes[1].scatter(samples_gibbs[:, 0], samples_gibbs[:, 1], alpha=0.3, s=5)\n",
    "axes[1].set_xlabel(\"x₁\")\n",
    "axes[1].set_ylabel(\"x₂\")\n",
    "axes[1].set_title(\"Muestras Gibbs (scatter)\")\n",
    "\n",
    "# Contour con muestras\n",
    "x_grid = np.linspace(-1, 5, 100)\n",
    "y_grid = np.linspace(1, 9, 100)\n",
    "X, Y = np.meshgrid(x_grid, y_grid)\n",
    "pos = np.dstack((X, Y))\n",
    "rv = multivariate_normal(mu_biv, sigma_biv)\n",
    "Z = rv.pdf(pos)\n",
    "\n",
    "axes[2].contour(X, Y, Z, levels=10, cmap=\"viridis\")\n",
    "axes[2].scatter(\n",
    "    samples_gibbs[::10, 0],\n",
    "    samples_gibbs[::10, 1],\n",
    "    alpha=0.3,\n",
    "    s=5,\n",
    "    c=\"red\",\n",
    "    label=\"Muestras\",\n",
    ")\n",
    "axes[2].set_xlabel(\"x₁\")\n",
    "axes[2].set_ylabel(\"x₂\")\n",
    "axes[2].set_title(\"Contornos + Muestras\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../assets/gibbs_bivariate.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3e388",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Parte 4: Diagnósticos de Convergencia (30 min)\n",
    "\n",
    "### 4.1 Effective Sample Size (ESS)\n",
    "\n",
    "ESS mide cuántas muestras independientes equivalen a nuestras muestras correlacionadas.\n",
    "\n",
    "ESS = n / (1 + 2*Σₖ ρₖ)\n",
    "\n",
    "donde ρₖ es la autocorrelación en lag k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd317c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def effective_sample_size(samples: NDArray[np.float64]) -> float:\n",
    "    \"\"\"\n",
    "    Calcula el Effective Sample Size.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    samples : NDArray\n",
    "        Muestras 1D de la cadena MCMC.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    float\n",
    "        ESS estimado.\n",
    "    \"\"\"\n",
    "    n = len(samples)\n",
    "    if n < 2:\n",
    "        return float(n)\n",
    "\n",
    "    # Calcular autocorrelación\n",
    "    mean = np.mean(samples)\n",
    "    var = np.var(samples)\n",
    "    if var == 0:\n",
    "        return float(n)\n",
    "\n",
    "    # Autocorrelación usando FFT (más eficiente)\n",
    "    samples_centered = samples - mean\n",
    "    fft_result = np.fft.fft(samples_centered, n=2 * n)\n",
    "    acf = np.fft.ifft(fft_result * np.conj(fft_result))[:n].real\n",
    "    acf = acf / acf[0]\n",
    "\n",
    "    # Sumar autocorrelaciones hasta que sean insignificantes\n",
    "    sum_acf = 0.0\n",
    "    for k in range(1, n):\n",
    "        if acf[k] < 0.05:\n",
    "            break\n",
    "        sum_acf += acf[k]\n",
    "\n",
    "    ess = n / (1 + 2 * sum_acf)\n",
    "    return float(max(1.0, ess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5aabb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Effective Sample Size ===\\n\")\n",
    "\n",
    "# Comparar diferentes proposal widths\n",
    "for width in [0.1, 1.5, 10.0]:\n",
    "    samples, acc = metropolis_hastings(log_normal, width, n_samples=5000, burn_in=500)\n",
    "    ess = effective_sample_size(samples)\n",
    "    print(\n",
    "        f\"σ_proposal = {width:>4.1f}: ESS = {ess:>7.1f} / 5000 \"\n",
    "        f\"({100*ess/5000:>5.1f}%), Aceptación = {acc:>5.1%}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84313e78",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### 4.2 R-hat (Gelman-Rubin Diagnostic)\n",
    "\n",
    "R-hat compara la varianza dentro de cadenas vs entre cadenas.\n",
    "- R-hat ≈ 1: cadenas han convergido\n",
    "- R-hat > 1.1: posibles problemas de convergencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a51a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelman_rubin_rhat(chains: list[NDArray[np.float64]]) -> float:\n",
    "    \"\"\"\n",
    "    Calcula el diagnóstico R-hat de Gelman-Rubin.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    chains : list[NDArray]\n",
    "        Lista de cadenas MCMC (cada una es 1D array).\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    float\n",
    "        Valor de R-hat.\n",
    "    \"\"\"\n",
    "    n = min(len(c) for c in chains)  # longitud de cada cadena\n",
    "\n",
    "    # Medias de cada cadena\n",
    "    chain_means = np.array([np.mean(c[:n]) for c in chains])\n",
    "\n",
    "    # Varianzas de cada cadena\n",
    "    chain_vars = np.array([np.var(c[:n], ddof=1) for c in chains])\n",
    "\n",
    "    # Media global\n",
    "    _ = np.mean(chain_means)  # grand_mean for reference\n",
    "\n",
    "    # Varianza entre cadenas (B)\n",
    "    B = n * np.var(chain_means, ddof=1)\n",
    "\n",
    "    # Varianza dentro de cadenas (W)\n",
    "    W = np.mean(chain_vars)\n",
    "\n",
    "    # Estimación de varianza\n",
    "    var_plus = ((n - 1) / n) * W + (1 / n) * B\n",
    "\n",
    "    # R-hat\n",
    "    rhat = np.sqrt(var_plus / W) if W > 0 else 1.0\n",
    "\n",
    "    return float(rhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f5938",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== R-hat Diagnostic ===\\n\")\n",
    "\n",
    "# Ejecutar múltiples cadenas con diferentes inicializaciones\n",
    "n_chains = 4\n",
    "chains: list[NDArray[np.float64]] = []\n",
    "\n",
    "for _ in range(n_chains):\n",
    "    x_init_value = float(rng.uniform(-10, 10))  # Inicialización aleatoria\n",
    "    samples, _acceptance_rate = metropolis_hastings(\n",
    "        log_normal, 1.5, n_samples=2000, x_init=x_init_value, burn_in=500\n",
    "    )\n",
    "    chains.append(samples)\n",
    "\n",
    "rhat = gelman_rubin_rhat(chains)\n",
    "print(f\"R-hat = {rhat:.4f}\")\n",
    "\n",
    "if rhat < 1.1:\n",
    "    print(\"✅ Cadenas han convergido (R-hat < 1.1)\")\n",
    "else:\n",
    "    print(\"⚠️ Posible problema de convergencia (R-hat > 1.1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0321b9",
   "metadata": {},
   "source": [
    "## Ejercicios para el Estudiante\n",
    "\n",
    "### Ejercicio 1: Muestrear de distribución bimodal\n",
    "Implementa M-H para muestrear de una mezcla de Gaussianas:\n",
    "π(x) = 0.5*N(-3, 1) + 0.5*N(3, 1)\n",
    "\n",
    "### Ejercicio 2: Gibbs para modelo jerárquico\n",
    "Implementa Gibbs Sampling para:\n",
    "μ ~ N(0, 10)\n",
    "xᵢ ~ N(μ, 1) para i = 1, ..., n\n",
    "Dado datos x, muestrea del posterior de μ.\n",
    "\n",
    "### Ejercicio 3: Comparar proposal distributions\n",
    "Compara M-H con:\n",
    "- Proposal Gaussiano\n",
    "- Proposal Uniforme\n",
    "¿Cuál es más eficiente para una Gaussiana objetivo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82dc54c",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "| Método | Cuándo usar | Ventajas | Desventajas |\n",
    "|--------|-------------|----------|-------------|\n",
    "| **Monte Carlo Simple** | Integración numérica | Simple, paralelo | Requiere muestreo directo |\n",
    "| **Metropolis-Hastings** | Distribución arbitraria | Flexible | Requiere tuning de proposal |\n",
    "| **Gibbs Sampling** | Condicionales conocidas | Sin rechazo | Solo si condicionales fáciles |\n",
    "\n",
    "### Diagnósticos clave:\n",
    "- **Trace plots**: visual, buscar \"mezcla\" estable\n",
    "- **ESS**: eficiencia del muestreo\n",
    "- **R-hat**: convergencia de múltiples cadenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602a793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FIN DEL LAB 2: MONTE CARLO Y MCMC\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
