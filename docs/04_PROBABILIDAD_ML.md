# M√≥dulo 04: Probabilidad Esencial para Machine Learning

> **Semana 8 | Prerequisito para entender Loss Functions y GMM**  
> **Filosof√≠a: Solo la probabilidad que necesitas para la L√≠nea 1**

---

## üéØ Objetivo del M√≥dulo

Dominar los **conceptos m√≠nimos de probabilidad** necesarios para:

1. Entender **Logistic Regression** como modelo probabil√≠stico
2. Comprender **Cross-Entropy Loss** y por qu√© funciona
3. Prepararte para **Gaussian Mixture Models (GMM)** en Unsupervised
4. Entender **Softmax** como distribuci√≥n de probabilidad

> ‚ö†Ô∏è **Nota:** Este NO es el curso completo de Probabilidad (L√≠nea 2). Es solo lo esencial para ML.

---

## üìö Contenido

### D√≠a 1-2: Fundamentos de Probabilidad

#### 1.1 Probabilidad B√°sica

```
P(A) = casos favorables / casos totales

Propiedades:
- 0 ‚â§ P(A) ‚â§ 1
- P(Œ©) = 1 (espacio muestral)
- P(‚àÖ) = 0 (evento imposible)
```

#### 1.2 Probabilidad Condicional

```
P(A|B) = P(A ‚à© B) / P(B)

"Probabilidad de A dado que B ocurri√≥"
```

**Ejemplo en ML:**
- P(spam | contiene "gratis") = ¬øQu√© tan probable es spam si el email dice "gratis"?

#### 1.3 Independencia

```
A y B son independientes si:
P(A ‚à© B) = P(A) ¬∑ P(B)

Equivalente a:
P(A|B) = P(A)
```

---

### D√≠a 3-4: Teorema de Bayes (Cr√≠tico para ML)

#### 2.1 La F√≥rmula

```
            P(B|A) ¬∑ P(A)
P(A|B) = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
               P(B)

Donde:
- P(A|B) = Posterior (lo que queremos calcular)
- P(B|A) = Likelihood (verosimilitud)
- P(A)   = Prior (conocimiento previo)
- P(B)   = Evidence (normalizador)
```

#### 2.2 Interpretaci√≥n para ML

```
              P(datos|clase) ¬∑ P(clase)
P(clase|datos) = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                      P(datos)

Ejemplo: Clasificaci√≥n de spam
- P(spam|palabras) = P(palabras|spam) ¬∑ P(spam) / P(palabras)
```

#### 2.3 Implementaci√≥n en Python

```python
import numpy as np

def bayes_classifier(x: np.ndarray, 
                     likelihood_spam: float,
                     likelihood_ham: float,
                     prior_spam: float = 0.3) -> str:
    """
    Clasificador Bayesiano simple.
    
    Args:
        x: Caracter√≠sticas del email (simplificado)
        likelihood_spam: P(x|spam)
        likelihood_ham: P(x|ham)
        prior_spam: P(spam) - conocimiento previo
    
    Returns:
        'spam' o 'ham'
    """
    prior_ham = 1 - prior_spam
    
    # Posterior (sin normalizar, solo comparamos)
    posterior_spam = likelihood_spam * prior_spam
    posterior_ham = likelihood_ham * prior_ham
    
    return 'spam' if posterior_spam > posterior_ham else 'ham'


# Ejemplo: Email con palabra "gratis"
# P("gratis"|spam) = 0.8, P("gratis"|ham) = 0.1
result = bayes_classifier(
    x=None,  # simplificado
    likelihood_spam=0.8,
    likelihood_ham=0.1,
    prior_spam=0.3
)
print(f"Clasificaci√≥n: {result}")  # spam
```

#### 2.4 Naive Bayes (Conexi√≥n con Supervised Learning)

```python
def naive_bayes_predict(X: np.ndarray, 
                        class_priors: np.ndarray,
                        feature_probs: dict) -> np.ndarray:
    """
    Naive Bayes asume independencia entre features:
    P(x1, x2, ..., xn | clase) = P(x1|clase) ¬∑ P(x2|clase) ¬∑ ... ¬∑ P(xn|clase)
    
    Esta "ingenuidad" simplifica mucho el c√°lculo.
    """
    n_samples = X.shape[0]
    n_classes = len(class_priors)
    
    log_posteriors = np.zeros((n_samples, n_classes))
    
    for c in range(n_classes):
        # Log para evitar underflow con muchas features
        log_prior = np.log(class_priors[c])
        log_likelihood = np.sum(np.log(feature_probs[c][X]), axis=1)
        log_posteriors[:, c] = log_prior + log_likelihood
    
    return np.argmax(log_posteriors, axis=1)
```

---

### D√≠a 5: Distribuci√≥n Gaussiana (Normal)

#### 3.1 La Distribuci√≥n M√°s Importante en ML

```
                    1              (x - Œº)¬≤
f(x) = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ¬∑ exp(- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ)
       œÉ ¬∑ ‚àö(2œÄ)                   2œÉ¬≤

Par√°metros:
- Œº (mu): Media (centro de la campana)
- œÉ (sigma): Desviaci√≥n est√°ndar (ancho)
- œÉ¬≤ (sigma¬≤): Varianza
```

#### 3.2 Por Qu√© es Importante

1. **Muchos fen√≥menos naturales** siguen esta distribuci√≥n
2. **Teorema del L√≠mite Central:** promedios de cualquier distribuci√≥n ‚Üí Normal
3. **GMM usa Gaussianas** para modelar clusters
4. **Inicializaci√≥n de pesos** en redes neuronales

#### 3.3 Implementaci√≥n

```python
import numpy as np

def gaussian_pdf(x: np.ndarray, mu: float, sigma: float) -> np.ndarray:
    """
    Probability Density Function de la Gaussiana.
    
    Args:
        x: Puntos donde evaluar
        mu: Media
        sigma: Desviaci√≥n est√°ndar
    
    Returns:
        Densidad de probabilidad en cada punto
    """
    coefficient = 1 / (sigma * np.sqrt(2 * np.pi))
    exponent = -((x - mu) ** 2) / (2 * sigma ** 2)
    return coefficient * np.exp(exponent)


# Visualizaci√≥n
import matplotlib.pyplot as plt

x = np.linspace(-5, 5, 1000)

# Diferentes Gaussianas
plt.figure(figsize=(10, 6))
plt.plot(x, gaussian_pdf(x, mu=0, sigma=1), label='Œº=0, œÉ=1 (est√°ndar)')
plt.plot(x, gaussian_pdf(x, mu=0, sigma=2), label='Œº=0, œÉ=2 (m√°s ancha)')
plt.plot(x, gaussian_pdf(x, mu=2, sigma=1), label='Œº=2, œÉ=1 (desplazada)')
plt.legend()
plt.title('Distribuciones Gaussianas')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.grid(True)
plt.savefig('gaussian_distributions.png')
```

#### 3.4 Gaussiana Multivariada (Para GMM)

```python
def multivariate_gaussian_pdf(x: np.ndarray, 
                               mu: np.ndarray, 
                               cov: np.ndarray) -> float:
    """
    Gaussiana multivariada para vectores.
    
    Args:
        x: Vector de caracter√≠sticas (d,)
        mu: Vector de medias (d,)
        cov: Matriz de covarianza (d, d)
    
    Returns:
        Densidad de probabilidad
    """
    d = len(mu)
    diff = x - mu
    
    # Determinante e inversa de la covarianza
    det_cov = np.linalg.det(cov)
    inv_cov = np.linalg.inv(cov)
    
    # Coeficiente de normalizaci√≥n
    coefficient = 1 / (np.sqrt((2 * np.pi) ** d * det_cov))
    
    # Exponente (forma cuadr√°tica)
    exponent = -0.5 * diff.T @ inv_cov @ diff
    
    return coefficient * np.exp(exponent)


# Ejemplo 2D
mu = np.array([0, 0])
cov = np.array([[1, 0.5], 
                [0.5, 1]])  # Correlaci√≥n positiva

x = np.array([0.5, 0.5])
prob = multivariate_gaussian_pdf(x, mu, cov)
print(f"P(x=[0.5, 0.5]) = {prob:.4f}")
```

---

### D√≠a 6: Maximum Likelihood Estimation (MLE)

#### 4.1 La Idea Central

```
MLE: Encontrar los par√°metros Œ∏ que maximizan la probabilidad 
     de observar los datos que tenemos.

Œ∏_MLE = argmax P(datos | Œ∏)
            Œ∏
```

#### 4.2 Por Qu√© es Fundamental

- **Logistic Regression** usa MLE para encontrar los pesos
- **Cross-Entropy Loss** viene de maximizar likelihood
- **GMM** usa MLE (via EM algorithm)

#### 4.3 MLE para Gaussiana

```python
def mle_gaussian(data: np.ndarray) -> tuple[float, float]:
    """
    Estimar par√°metros de Gaussiana con MLE.
    
    Para una Gaussiana, los estimadores MLE son:
    - Œº_MLE = media muestral
    - œÉ¬≤_MLE = varianza muestral (con n, no n-1)
    
    Args:
        data: Muestras observadas
    
    Returns:
        (mu_mle, sigma_mle)
    """
    n = len(data)
    
    # MLE de la media
    mu_mle = np.mean(data)
    
    # MLE de la varianza (dividir por n, no n-1)
    sigma_squared_mle = np.sum((data - mu_mle) ** 2) / n
    sigma_mle = np.sqrt(sigma_squared_mle)
    
    return mu_mle, sigma_mle


# Ejemplo: Generar datos y estimar
np.random.seed(42)
true_mu, true_sigma = 5.0, 2.0
samples = np.random.normal(true_mu, true_sigma, size=1000)

estimated_mu, estimated_sigma = mle_gaussian(samples)
print(f"Par√°metros reales: Œº={true_mu}, œÉ={true_sigma}")
print(f"MLE estimados:     Œº={estimated_mu:.3f}, œÉ={estimated_sigma:.3f}")
```

#### 4.4 Conexi√≥n con Cross-Entropy Loss

```python
def cross_entropy_from_mle():
    """
    Demostraci√≥n de que Cross-Entropy viene de MLE.
    
    Para clasificaci√≥n binaria con Bernoulli:
    P(y|x, Œ∏) = p^y ¬∑ (1-p)^(1-y)
    
    Donde p = œÉ(Œ∏·µÄx) (predicci√≥n del modelo)
    
    Log-likelihood:
    log P(y|x, Œ∏) = y¬∑log(p) + (1-y)¬∑log(1-p)
    
    Maximizar likelihood = Minimizar negative log-likelihood
    = Minimizar Cross-Entropy!
    """
    # Ejemplo num√©rico
    y_true = np.array([1, 0, 1, 1, 0])
    y_pred = np.array([0.9, 0.1, 0.8, 0.7, 0.2])  # Probabilidades
    
    # Cross-Entropy (negative log-likelihood promedio)
    epsilon = 1e-15  # Para evitar log(0)
    ce = -np.mean(
        y_true * np.log(y_pred + epsilon) + 
        (1 - y_true) * np.log(1 - y_pred + epsilon)
    )
    
    print(f"Cross-Entropy Loss: {ce:.4f}")
    return ce

cross_entropy_from_mle()
```

---

### D√≠a 7: Softmax como Distribuci√≥n de Probabilidad

#### 5.1 De Logits a Probabilidades

```
                    exp(z·µ¢)
softmax(z)·µ¢ = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
              Œ£‚±º exp(z‚±º)

Propiedades:
- Cada salida ‚àà (0, 1)
- Suma de salidas = 1 (distribuci√≥n v√°lida)
- Preserva el orden (mayor logit ‚Üí mayor probabilidad)
```

#### 5.2 El Problema de Estabilidad Num√©rica (v3.3)

```
‚ö†Ô∏è PROBLEMA: exp() puede causar overflow/underflow

Ejemplo peligroso:
    z = [1000, 1001, 1002]
    exp(z) = [inf, inf, inf]  ‚Üí NaN en softmax!

Ejemplo underflow:
    z = [-1000, -1001, -1002]
    exp(z) = [0, 0, 0]  ‚Üí 0/0 = NaN!
```

#### 5.3 Log-Sum-Exp Trick (Estabilidad Num√©rica)

```
TRUCO: softmax(z) = softmax(z - max(z))

Demostraci√≥n:
    softmax(z - c)·µ¢ = exp(z·µ¢ - c) / Œ£‚±º exp(z‚±º - c)
                    = exp(z·µ¢)¬∑exp(-c) / Œ£‚±º exp(z‚±º)¬∑exp(-c)
                    = exp(z·µ¢) / Œ£‚±º exp(z‚±º)
                    = softmax(z)·µ¢

Al restar max(z), todos los exponentes son ‚â§ 0, evitando overflow.
```

#### 5.4 Implementaci√≥n Num√©ricamente Estable

```python
def softmax(z: np.ndarray) -> np.ndarray:
    """
    Softmax num√©ricamente estable usando Log-Sum-Exp trick.
    
    Truco: Restar el m√°ximo para evitar overflow en exp()
    softmax(z) = softmax(z - max(z))
    
    Args:
        z: Logits (scores antes de activaci√≥n)
    
    Returns:
        Probabilidades que suman 1
    """
    # Log-Sum-Exp trick: restar el m√°ximo
    z_stable = z - np.max(z, axis=-1, keepdims=True)
    
    exp_z = np.exp(z_stable)
    return exp_z / np.sum(exp_z, axis=-1, keepdims=True)


def log_softmax(z: np.ndarray) -> np.ndarray:
    """
    Log-Softmax estable (√∫til para Cross-Entropy).
    
    log(softmax(z)) calculado de forma estable.
    Evita calcular softmax primero y luego log (pierde precisi√≥n).
    """
    z_stable = z - np.max(z, axis=-1, keepdims=True)
    log_sum_exp = np.log(np.sum(np.exp(z_stable), axis=-1, keepdims=True))
    return z_stable - log_sum_exp


# ============================================================
# DEMOSTRACI√ìN: Por qu√© el trick es necesario
# ============================================================

def demo_numerical_stability():
    """Muestra por qu√© necesitamos el Log-Sum-Exp trick."""
    
    # Caso peligroso: logits muy grandes
    z_dangerous = np.array([1000.0, 1001.0, 1002.0])
    
    # Sin el trick (INCORRECTO)
    def softmax_naive(z):
        exp_z = np.exp(z)  # ¬°Overflow!
        return exp_z / np.sum(exp_z)
    
    # Con el trick (CORRECTO)
    def softmax_stable(z):
        z_stable = z - np.max(z)
        exp_z = np.exp(z_stable)
        return exp_z / np.sum(exp_z)
    
    print("Logits peligrosos:", z_dangerous)
    print()
    
    # Naive (falla)
    import warnings
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        result_naive = softmax_naive(z_dangerous)
        print(f"Softmax NAIVE: {result_naive}")
        print(f"  ‚Üí Suma: {np.sum(result_naive)} (deber√≠a ser 1.0)")
    
    # Estable (funciona)
    result_stable = softmax_stable(z_dangerous)
    print(f"\nSoftmax ESTABLE: {result_stable}")
    print(f"  ‚Üí Suma: {np.sum(result_stable):.6f} ‚úì")

demo_numerical_stability()


# Ejemplo: Clasificaci√≥n multiclase (d√≠gitos 0-9)
logits = np.array([2.0, 1.0, 0.1, -1.0, 3.0, 0.5, -0.5, 1.5, 0.0, -2.0])
probs = softmax(logits)

print("\nLogits ‚Üí Probabilidades:")
for i, (l, p) in enumerate(zip(logits, probs)):
    print(f"  Clase {i}: logit={l:+.1f} ‚Üí prob={p:.3f}")
print(f"\nSuma de probabilidades: {np.sum(probs):.6f}")
print(f"Clase predicha: {np.argmax(probs)}")
```

#### 5.3 Categorical Cross-Entropy (Multiclase)

```python
def categorical_cross_entropy(y_true: np.ndarray, 
                               y_pred: np.ndarray) -> float:
    """
    Loss para clasificaci√≥n multiclase.
    
    Args:
        y_true: One-hot encoded labels (n_samples, n_classes)
        y_pred: Probabilidades softmax (n_samples, n_classes)
    
    Returns:
        Loss promedio
    """
    epsilon = 1e-15
    # Solo cuenta la clase correcta (donde y_true=1)
    return -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=1))


# Ejemplo
y_true = np.array([
    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],  # Clase 4
    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # Clase 0
])

y_pred = np.array([
    softmax(np.array([0, 0, 0, 0, 5, 0, 0, 0, 0, 0])),  # Confiado en 4
    softmax(np.array([3, 1, 0, 0, 0, 0, 0, 0, 0, 0])),  # Confiado en 0
])

loss = categorical_cross_entropy(y_true, y_pred)
print(f"Categorical Cross-Entropy: {loss:.4f}")
```

---

## üî® Entregables del M√≥dulo

### E1: `probability.py`

```python
"""
M√≥dulo de probabilidad esencial para ML.
Implementaciones desde cero con NumPy.
"""

import numpy as np
from typing import Tuple

def gaussian_pdf(x: np.ndarray, mu: float, sigma: float) -> np.ndarray:
    """Densidad de probabilidad Gaussiana univariada."""
    pass

def multivariate_gaussian_pdf(x: np.ndarray, 
                               mu: np.ndarray, 
                               cov: np.ndarray) -> float:
    """Densidad de probabilidad Gaussiana multivariada."""
    pass

def mle_gaussian(data: np.ndarray) -> Tuple[float, float]:
    """Estimaci√≥n MLE de par√°metros de Gaussiana."""
    pass

def softmax(z: np.ndarray) -> np.ndarray:
    """Funci√≥n softmax num√©ricamente estable."""
    pass

def cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Binary cross-entropy loss."""
    pass

def categorical_cross_entropy(y_true: np.ndarray, 
                               y_pred: np.ndarray) -> float:
    """Categorical cross-entropy loss para multiclase."""
    pass
```

### E2: Tests

```python
# tests/test_probability.py
import numpy as np
import pytest
from src.probability import (
    gaussian_pdf, mle_gaussian, softmax, 
    cross_entropy, categorical_cross_entropy
)

def test_gaussian_pdf_standard():
    """PDF de Gaussiana est√°ndar en x=0 debe ser ~0.3989."""
    result = gaussian_pdf(np.array([0.0]), mu=0, sigma=1)
    expected = 1 / np.sqrt(2 * np.pi)  # ~0.3989
    assert np.isclose(result[0], expected, rtol=1e-5)

def test_softmax_sums_to_one():
    """Softmax debe sumar 1."""
    z = np.random.randn(10)
    probs = softmax(z)
    assert np.isclose(np.sum(probs), 1.0)

def test_softmax_preserves_order():
    """Mayor logit ‚Üí mayor probabilidad."""
    z = np.array([1.0, 2.0, 3.0])
    probs = softmax(z)
    assert probs[2] > probs[1] > probs[0]

def test_mle_gaussian_accuracy():
    """MLE debe recuperar par√°metros con suficientes datos."""
    np.random.seed(42)
    true_mu, true_sigma = 10.0, 3.0
    data = np.random.normal(true_mu, true_sigma, size=10000)
    
    est_mu, est_sigma = mle_gaussian(data)
    
    assert np.isclose(est_mu, true_mu, rtol=0.05)
    assert np.isclose(est_sigma, true_sigma, rtol=0.05)

def test_cross_entropy_perfect_prediction():
    """CE debe ser ~0 para predicciones perfectas."""
    y_true = np.array([1, 0, 1])
    y_pred = np.array([0.999, 0.001, 0.999])
    
    loss = cross_entropy(y_true, y_pred)
    assert loss < 0.01
```

---

## üìä Resumen Visual

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PROBABILIDAD PARA ML - MAPA CONCEPTUAL                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  TEOREMA DE BAYES                                               ‚îÇ
‚îÇ       ‚îÇ                                                         ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚ñ∫ Naive Bayes Classifier (M√≥dulo 05)                   ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚ñ∫ Intuici√≥n de posterior vs prior                      ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  DISTRIBUCI√ìN GAUSSIANA                                         ‚îÇ
‚îÇ       ‚îÇ                                                         ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚ñ∫ GMM en Unsupervised (M√≥dulo 06)                      ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚ñ∫ Inicializaci√≥n de pesos en DL (M√≥dulo 07)            ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚ñ∫ Normalizaci√≥n de datos                               ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  MAXIMUM LIKELIHOOD (MLE)                                       ‚îÇ
‚îÇ       ‚îÇ                                                         ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚ñ∫ Cross-Entropy Loss (Logistic Regression)             ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ‚ñ∫ Categorical CE (Softmax + Multiclase)                ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚ñ∫ EM Algorithm en GMM                                  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  SOFTMAX                                                        ‚îÇ
‚îÇ       ‚îÇ                                                         ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚ñ∫ Capa de salida en clasificaci√≥n multiclase           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîó Conexiones con Otros M√≥dulos

| Concepto | D√≥nde se usa |
|----------|--------------|
| Teorema de Bayes | Naive Bayes en M√≥dulo 05 |
| Gaussiana | GMM en M√≥dulo 06, inicializaci√≥n en M√≥dulo 07 |
| MLE | Derivaci√≥n de Cross-Entropy en M√≥dulo 05 |
| Softmax | Capa de salida en M√≥dulo 07 |
| Cross-Entropy | Loss function principal en M√≥dulo 05 y 07 |

---

## ‚úÖ Checklist del M√≥dulo

- [ ] Puedo explicar el Teorema de Bayes con un ejemplo
- [ ] S√© calcular la PDF de una Gaussiana a mano
- [ ] Entiendo por qu√© MLE da Cross-Entropy como loss
- [ ] Implement√© softmax num√©ricamente estable
- [ ] Los tests de `probability.py` pasan

---

## üìñ Recursos Adicionales

### Videos
- [3Blue1Brown - Bayes Theorem](https://www.youtube.com/watch?v=HZGCoVF3YvM)
- [StatQuest - Maximum Likelihood](https://www.youtube.com/watch?v=XepXtl9YKwc)
- [StatQuest - Gaussian Distribution](https://www.youtube.com/watch?v=rzFX5NWojp0)

### Lecturas
- Mathematics for ML, Cap. 6 (Probability)
- Pattern Recognition and ML (Bishop), Cap. 1-2

---

> üí° **Nota Final:** Este m√≥dulo es deliberadamente corto (1 semana). No necesitas ser experto en probabilidad para la L√≠nea 1, pero estos conceptos son el "pegamento" que conecta las matem√°ticas con las funciones de p√©rdida que usar√°s en los siguientes m√≥dulos.

---

**[‚Üê M√≥dulo 03: C√°lculo](03_CALCULO_MULTIVARIANTE.md)** | **[M√≥dulo 05: Supervised Learning ‚Üí](05_SUPERVISED_LEARNING.md)**
