"""
Notebook 05: Sistema de RecomendaciÃ³n con SVD - MovieLens
==========================================================

MÃ³dulo 6 - Semana 15: Sistemas de RecomendaciÃ³n
Curso Alineado: CSCA 5632 - Unsupervised Learning

Objetivos:
1. Implementar factorizaciÃ³n de matrices desde cero
2. Usar la librerÃ­a Surprise para SVD
3. Construir un recomendador funcional con MovieLens
4. Evaluar con mÃ©tricas apropiadas (RMSE, Precision@K)

Dataset: MovieLens 100K
    https://grouplens.org/datasets/movielens/

Dependencias:
    pip install surprise pandas numpy matplotlib seaborn

Ejecutar como script o convertir a notebook con jupytext.
"""

# %% [markdown]
# # Sistema de RecomendaciÃ³n con FactorizaciÃ³n de Matrices
#
# En este notebook implementaremos un sistema de recomendaciÃ³n de pelÃ­culas
# usando el dataset MovieLens y la tÃ©cnica de **FactorizaciÃ³n de Matrices (SVD)**.

# %%
# Imports
import warnings
from collections import defaultdict

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

warnings.filterwarnings("ignore")

# Verificar Surprise
try:
    from surprise import NMF, SVD, Dataset, KNNBasic, SVDpp, accuracy
    from surprise.model_selection import GridSearchCV, cross_validate, train_test_split

    SURPRISE_AVAILABLE = True
except ImportError:
    SURPRISE_AVAILABLE = False
    print("âš ï¸ Surprise no instalado. Ejecutar: pip install scikit-surprise")

print("âœ… Imports completados")

# %% [markdown]
# ## 1. Cargar y Explorar MovieLens Dataset
#
# MovieLens es el dataset estÃ¡ndar para evaluaciÃ³n de sistemas de recomendaciÃ³n.
# Usaremos la versiÃ³n 100K (100,000 ratings).

# %%
if SURPRISE_AVAILABLE:
    # Cargar dataset builtin de Surprise
    print("ğŸ“¥ Cargando MovieLens 100K...")
    data = Dataset.load_builtin("ml-100k")

    # Obtener dataframe para exploraciÃ³n
    trainset = data.build_full_trainset()

    print("\nğŸ“Š EstadÃ­sticas del Dataset:")
    print(f"   Usuarios: {trainset.n_users:,}")
    print(f"   Items (pelÃ­culas): {trainset.n_items:,}")
    print(f"   Ratings: {trainset.n_ratings:,}")
    print(
        f"   Densidad: {trainset.n_ratings / (trainset.n_users * trainset.n_items) * 100:.2f}%"
    )
    print(f"   Rango de ratings: {trainset.rating_scale}")

# %%
if SURPRISE_AVAILABLE:
    # Convertir a DataFrame para anÃ¡lisis
    ratings_list = [
        (trainset.to_raw_uid(u), trainset.to_raw_iid(i), r)
        for u, i, r in trainset.all_ratings()
    ]

    df_ratings = pd.DataFrame(ratings_list, columns=["user_id", "item_id", "rating"])

    print("\nğŸ“‹ Muestra de ratings:")
    print(df_ratings.head(10))

    print("\nğŸ“ˆ DistribuciÃ³n de ratings:")
    print(df_ratings["rating"].value_counts().sort_index())

# %%
if SURPRISE_AVAILABLE:
    # Visualizar distribuciÃ³n
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    # DistribuciÃ³n de ratings
    axes[0].hist(df_ratings["rating"], bins=5, edgecolor="black", alpha=0.7)
    axes[0].set_xlabel("Rating")
    axes[0].set_ylabel("Frecuencia")
    axes[0].set_title("DistribuciÃ³n de Ratings")

    # Ratings por usuario
    ratings_per_user = df_ratings.groupby("user_id").size()
    axes[1].hist(ratings_per_user, bins=50, edgecolor="black", alpha=0.7)
    axes[1].set_xlabel("NÃºmero de ratings")
    axes[1].set_ylabel("NÃºmero de usuarios")
    axes[1].set_title("Ratings por Usuario")
    axes[1].set_xlim(0, 500)

    # Ratings por pelÃ­cula (long-tail)
    ratings_per_item = df_ratings.groupby("item_id").size().sort_values(ascending=False)
    axes[2].plot(range(len(ratings_per_item)), ratings_per_item.values)
    axes[2].set_xlabel("PelÃ­cula (ordenada por popularidad)")
    axes[2].set_ylabel("NÃºmero de ratings")
    axes[2].set_title("Long-tail de Popularidad")
    axes[2].set_yscale("log")

    plt.tight_layout()
    plt.show()

# %% [markdown]
# ## 1.5 VisualizaciÃ³n de Sparsity (Matriz Usuario Ã— Item)
#
# > **ğŸ’¡ ConexiÃ³n con M02 - Ãlgebra Lineal**: La matriz de ratings R es una matriz
# > **dispersa (sparse)** donde la mayorÃ­a de entradas son desconocidas.
# > En M02 estudiamos matrices densas vs dispersas. AquÃ­, el 98%+ estÃ¡ vacÃ­a.

# %%
if SURPRISE_AVAILABLE:
    # Crear matriz para visualizaciÃ³n (subset pequeÃ±o)
    n_users_sample, n_items_sample = 50, 100
    unique_users = df_ratings["user_id"].unique()[:n_users_sample]
    unique_items = df_ratings["item_id"].unique()[:n_items_sample]

    # Matriz de ratings (NaN para valores faltantes)
    rating_matrix = np.full((n_users_sample, n_items_sample), np.nan)
    user_map = {u: i for i, u in enumerate(unique_users)}
    item_map = {it: i for i, it in enumerate(unique_items)}

    for _, row in df_ratings.iterrows():
        if row["user_id"] in user_map and row["item_id"] in item_map:
            rating_matrix[user_map[row["user_id"]], item_map[row["item_id"]]] = row[
                "rating"
            ]

    # Visualizar sparsity
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # GrÃ¡fico 1: PatrÃ³n de sparsity
    mask = ~np.isnan(rating_matrix)
    axes[0].imshow(mask, cmap="Blues", aspect="auto")
    axes[0].set_xlabel("Items (pelÃ­culas)")
    axes[0].set_ylabel("Usuarios")
    axes[0].set_title("PatrÃ³n de Sparsity (Azul = Rating conocido)")
    sparsity = 1 - np.sum(mask) / mask.size
    axes[0].text(
        0.02,
        0.98,
        f"Sparsity: {sparsity*100:.1f}%",
        transform=axes[0].transAxes,
        fontsize=12,
        verticalalignment="top",
        bbox={"boxstyle": "round", "facecolor": "white"},
    )

    # GrÃ¡fico 2: Heatmap de ratings
    rating_display = np.ma.masked_where(np.isnan(rating_matrix), rating_matrix)
    im = axes[1].imshow(rating_display, cmap="YlOrRd", aspect="auto", vmin=1, vmax=5)
    axes[1].set_xlabel("Items (pelÃ­culas)")
    axes[1].set_ylabel("Usuarios")
    axes[1].set_title("Matriz Usuario Ã— Item (valores conocidos)")
    plt.colorbar(im, ax=axes[1], label="Rating")

    plt.tight_layout()
    plt.show()

    print("\nğŸ“Š EstadÃ­sticas de Sparsity:")
    print(f"   Matriz completa: {trainset.n_users * trainset.n_items:,} entradas")
    print(f"   Ratings conocidos: {trainset.n_ratings:,}")
    print(
        f"   Sparsity global: {(1 - trainset.n_ratings / (trainset.n_users * trainset.n_items)) * 100:.2f}%"
    )

# %% [markdown]
# ## 2. Matrix Factorization: TeorÃ­a vs PrÃ¡ctica
#
# ### 2.1 TeorÃ­a MatemÃ¡tica
#
# La idea es descomponer la matriz de ratings R (usuarios Ã— items) en dos matrices:
# - P (usuarios Ã— factores): preferencias latentes de usuarios
# - Q (factores Ã— items): caracterÃ­sticas latentes de items
#
# $$ R \approx P \times Q^T $$
#
# AdemÃ¡s, usamos biases para capturar tendencias:
# $$ \hat{r}_{ui} = \mu + b_u + b_i + p_u \cdot q_i $$
#
# ### 2.2 ğŸ’¡ SVD ClÃ¡sico vs SVD para RecomendaciÃ³n
#
# > **âš ï¸ DISTINCIÃ“N CRÃTICA**:
# >
# > | Aspecto | SVD ClÃ¡sico (M02) | TruncatedSVD (sklearn) | SVD RecomendaciÃ³n |
# > |---------|-------------------|------------------------|-------------------|
# > | FÃ³rmula | $A = U\Sigma V^T$ | AproximaciÃ³n low-rank | $R \approx PQ^T$ |
# > | Valores faltantes | âŒ Requiere matriz completa | âŒ Trata NaN como 0 | âœ… Los ignora |
# > | Uso tÃ­pico | Ãlgebra lineal | ReducciÃ³n dimensionalidad | Sistemas recomendaciÃ³n |
# > | ImplementaciÃ³n | `np.linalg.svd()` | `sklearn.TruncatedSVD` | Surprise, LightFM |
#
# **Â¿Por quÃ© NO usar TruncatedSVD de sklearn para recomendaciÃ³n?**
# - Trata valores faltantes como 0 (un rating muy bajo)
# - Esto sesga las predicciones hacia items populares

# %%
# Demo: ComparaciÃ³n TruncatedSVD vs SVD para RecomendaciÃ³n
if SURPRISE_AVAILABLE:
    from sklearn.decomposition import TruncatedSVD

    # TruncatedSVD trata NaN como 0 (INCORRECTO para recomendaciÃ³n)
    rating_matrix_filled = np.nan_to_num(rating_matrix, nan=0.0)
    truncated = TruncatedSVD(n_components=10, random_state=42)
    U_trunc = truncated.fit_transform(rating_matrix_filled)

    print("âš ï¸ TruncatedSVD (sklearn) NO es apropiado para recomendaciÃ³n:")
    print(f"   - Trata {int(np.sum(np.isnan(rating_matrix)))} valores NaN como 0")
    print(f"   - Varianza explicada: {truncated.explained_variance_ratio_.sum():.1%}")
    print(
        "\nâœ… En cambio, SVD de Surprise IGNORA valores faltantes durante el entrenamiento."
    )


# %%
class MatrixFactorizationSGD:
    """
    FactorizaciÃ³n de Matrices con Stochastic Gradient Descent.

    ImplementaciÃ³n educativa desde cero.

    ParÃ¡metros:
    -----------
    n_factors : int
        NÃºmero de factores latentes (dimensiÃ³n del embedding).
    n_epochs : int
        NÃºmero de Ã©pocas de entrenamiento.
    lr : float
        Learning rate.
    reg : float
        ParÃ¡metro de regularizaciÃ³n L2.
    verbose : bool
        Si True, imprime progreso.
    """

    def __init__(
        self,
        n_factors=50,
        n_epochs=20,
        lr=0.005,
        reg=0.02,
        verbose=True,
        random_state: int | None = None,
    ):
        self.n_factors = n_factors
        self.n_epochs = n_epochs
        self.lr = lr
        self.reg = reg
        self.verbose = verbose
        self.random_state = random_state

    def fit(
        self, ratings_df, user_col="user_id", item_col="item_id", rating_col="rating"
    ):
        """
        Entrenar el modelo.

        ParÃ¡metros:
        -----------
        ratings_df : pd.DataFrame
            DataFrame con columnas user, item, rating.
        """
        # Crear mapeos de IDs
        self.user_ids = ratings_df[user_col].unique()
        self.item_ids = ratings_df[item_col].unique()
        self.user_to_idx = {u: i for i, u in enumerate(self.user_ids)}
        self.item_to_idx = {i: j for j, i in enumerate(self.item_ids)}
        self.idx_to_user = {i: u for u, i in self.user_to_idx.items()}
        self.idx_to_item = {j: i for i, j in self.item_to_idx.items()}

        self.n_users = len(self.user_ids)
        self.n_items = len(self.item_ids)

        rng = np.random.default_rng(self.random_state)

        # Inicializar parÃ¡metros
        self.global_mean = ratings_df[rating_col].mean()
        self.b_u = np.zeros(self.n_users)  # User bias
        self.b_i = np.zeros(self.n_items)  # Item bias
        self.P = rng.normal(0, 0.1, (self.n_users, self.n_factors))  # User factors
        self.Q = rng.normal(0, 0.1, (self.n_items, self.n_factors))  # Item factors

        # Convertir a arrays
        users = ratings_df[user_col].map(self.user_to_idx).values
        items = ratings_df[item_col].map(self.item_to_idx).values
        ratings = ratings_df[rating_col].values

        # Historial de entrenamiento
        self.history: dict[str, list[float]] = {"rmse": []}

        # Entrenamiento SGD
        for epoch in range(self.n_epochs):
            # Shuffle
            indices = rng.permutation(len(ratings))
            total_error = 0

            for idx in indices:
                u, i, r = users[idx], items[idx], ratings[idx]

                # PredicciÃ³n actual
                pred = (
                    self.global_mean
                    + self.b_u[u]
                    + self.b_i[i]
                    + np.dot(self.P[u], self.Q[i])
                )
                error = r - pred
                total_error += error**2

                # Actualizar biases
                self.b_u[u] += self.lr * (error - self.reg * self.b_u[u])
                self.b_i[i] += self.lr * (error - self.reg * self.b_i[i])

                # Actualizar factores
                P_u_old = self.P[u].copy()
                self.P[u] += self.lr * (error * self.Q[i] - self.reg * self.P[u])
                self.Q[i] += self.lr * (error * P_u_old - self.reg * self.Q[i])

            rmse = np.sqrt(total_error / len(ratings))
            self.history["rmse"].append(rmse)

            if self.verbose:
                print(f"Epoch {epoch+1}/{self.n_epochs} - RMSE: {rmse:.4f}")

        return self

    def predict(self, user_id, item_id):
        """Predecir rating para un usuario e item."""
        if user_id not in self.user_to_idx or item_id not in self.item_to_idx:
            return self.global_mean

        u = self.user_to_idx[user_id]
        i = self.item_to_idx[item_id]

        pred = (
            self.global_mean + self.b_u[u] + self.b_i[i] + np.dot(self.P[u], self.Q[i])
        )
        return np.clip(pred, 1, 5)

    def recommend(self, user_id, n=10, exclude_seen=True, seen_items=None):
        """
        Generar top-N recomendaciones para un usuario.

        Retorna lista de (item_id, predicted_rating).
        """
        if user_id not in self.user_to_idx:
            return []

        predictions = []
        for item_id in self.item_ids:
            if exclude_seen and seen_items and item_id in seen_items:
                continue
            pred = self.predict(user_id, item_id)
            predictions.append((item_id, pred))

        predictions.sort(key=lambda x: x[1], reverse=True)
        return predictions[:n]

    def plot_training(self):
        """Visualizar curva de entrenamiento."""
        plt.figure(figsize=(10, 5))
        plt.plot(self.history["rmse"], marker="o")
        plt.xlabel("Epoch")
        plt.ylabel("RMSE")
        plt.title("Training Progress")
        plt.grid(True, alpha=0.3)
        plt.show()


# %%
# Entrenar modelo desde cero
print("ğŸ”¬ Entrenando modelo de factorizaciÃ³n (desde cero)...")

mf_model = MatrixFactorizationSGD(
    n_factors=50, n_epochs=20, lr=0.005, reg=0.02, verbose=True
)

mf_model.fit(df_ratings)

# %%
# Visualizar entrenamiento
mf_model.plot_training()

# %%
# Probar recomendaciones
user_test = df_ratings["user_id"].iloc[0]
seen_movies = set(df_ratings[df_ratings["user_id"] == user_test]["item_id"])

print(f"\nğŸ¬ Recomendaciones para usuario '{user_test}':")
print(f"   PelÃ­culas ya vistas: {len(seen_movies)}")

recommendations = mf_model.recommend(user_test, n=10, seen_items=seen_movies)
print("\n   Top 10 recomendaciones:")
for i, (item_id, score) in enumerate(recommendations, 1):
    print(f"   {i}. Item {item_id}: {score:.2f}")

# %% [markdown]
# ## 3. SVD con Surprise Library
#
# Ahora usaremos la librerÃ­a **Surprise** que implementa SVD optimizado
# y otras variantes como SVD++ y NMF.

# %%
if SURPRISE_AVAILABLE:
    # Split train/test
    trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

    print("ğŸ“Š Split:")
    print(f"   Train: {trainset.n_ratings:,} ratings")
    print(f"   Test: {len(testset):,} ratings")

# %%
if SURPRISE_AVAILABLE:
    # Entrenar SVD
    print("\nğŸ”¬ Entrenando SVD (Surprise)...")

    svd = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02, random_state=42)

    svd.fit(trainset)

    # Evaluar
    predictions = svd.test(testset)
    rmse = accuracy.rmse(predictions)
    mae = accuracy.mae(predictions)

    print("\nğŸ“ˆ Resultados:")
    print(f"   RMSE: {rmse:.4f}")
    print(f"   MAE: {mae:.4f}")

# %%
if SURPRISE_AVAILABLE:
    # Cross-validation para comparar algoritmos
    print("\nğŸ”¬ Comparando algoritmos (5-fold CV)...")

    algorithms = {
        "SVD": SVD(random_state=42),
        "SVD++": SVDpp(random_state=42),
        "NMF": NMF(random_state=42),
        "KNN (user-based)": KNNBasic(sim_options={"user_based": True}),
    }

    results = {}

    for name, algo in algorithms.items():
        print(f"\n   Evaluando {name}...")
        cv_results = cross_validate(
            algo, data, measures=["RMSE", "MAE"], cv=5, verbose=False
        )
        results[name] = {
            "RMSE": cv_results["test_rmse"].mean(),
            "MAE": cv_results["test_mae"].mean(),
            "Fit Time": cv_results["fit_time"].mean(),
        }
        print(f"   RMSE: {results[name]['RMSE']:.4f} | MAE: {results[name]['MAE']:.4f}")

    # Tabla comparativa
    results_df = pd.DataFrame(results).T
    print("\nğŸ“Š Tabla Comparativa:")
    print(results_df.to_string())

# %%
if SURPRISE_AVAILABLE:
    # Visualizar comparaciÃ³n
    fig, ax = plt.subplots(figsize=(10, 5))

    x = np.arange(len(results))
    width = 0.35

    rmse_vals = [results[name]["RMSE"] for name in results]
    mae_vals = [results[name]["MAE"] for name in results]

    ax.bar(x - width / 2, rmse_vals, width, label="RMSE", color="steelblue")
    ax.bar(x + width / 2, mae_vals, width, label="MAE", color="coral")

    ax.set_ylabel("Error")
    ax.set_title("ComparaciÃ³n de Algoritmos de RecomendaciÃ³n")
    ax.set_xticks(x)
    ax.set_xticklabels(results.keys(), rotation=45, ha="right")
    ax.legend()
    ax.grid(True, alpha=0.3, axis="y")

    plt.tight_layout()
    plt.show()

# %% [markdown]
# ## 4. Grid Search para Optimizar HiperparÃ¡metros

# %%
if SURPRISE_AVAILABLE:
    print("ğŸ” Grid Search para SVD...")

    param_grid = {
        "n_factors": [50, 100],
        "n_epochs": [20, 30],
        "lr_all": [0.002, 0.005],
        "reg_all": [0.02, 0.1],
    }

    gs = GridSearchCV(SVD, param_grid, measures=["rmse", "mae"], cv=3, n_jobs=-1)
    gs.fit(data)

    print("\nğŸ“ˆ Mejores resultados:")
    print(f"   Mejor RMSE: {gs.best_score['rmse']:.4f}")
    print(f"   Mejores parÃ¡metros: {gs.best_params['rmse']}")

# %% [markdown]
# ## 5. MÃ©tricas de Ranking: Precision@K y NDCG


# %%
def precision_recall_at_k(predictions, k=10, threshold=4.0):
    """
    Calcular Precision y Recall @K.

    Considera como "relevante" un item con rating >= threshold.
    """
    # Agrupar predicciones por usuario
    user_est_true = defaultdict(list)
    for uid, _iid, true_r, est, _ in predictions:
        user_est_true[uid].append((est, true_r))

    precisions = []
    recalls = []

    for _uid, user_ratings in user_est_true.items():
        # Ordenar por rating predicho
        user_ratings.sort(key=lambda x: x[0], reverse=True)

        # Top K predicciones
        top_k = user_ratings[:k]

        # NÃºmero de items relevantes en top K
        n_relevant_in_k = sum(1 for (_, true_r) in top_k if true_r >= threshold)

        # Total de items relevantes
        n_relevant_total = sum(1 for (_, true_r) in user_ratings if true_r >= threshold)

        # Precision@K
        precision = n_relevant_in_k / k
        precisions.append(precision)

        # Recall@K
        recall = n_relevant_in_k / n_relevant_total if n_relevant_total > 0 else 0
        recalls.append(recall)

    return np.mean(precisions), np.mean(recalls)


# %%
if SURPRISE_AVAILABLE:
    # Calcular mÃ©tricas de ranking
    print("ğŸ“Š MÃ©tricas de Ranking:")

    for k in [5, 10, 20]:
        precision, recall = precision_recall_at_k(predictions, k=k, threshold=4.0)
        print(f"   @{k}: Precision={precision:.4f}, Recall={recall:.4f}")

# %% [markdown]
# ## 6. Generar Recomendaciones Personalizadas

# %%
if SURPRISE_AVAILABLE:
    # Reentrenar en todo el dataset
    full_trainset = data.build_full_trainset()
    svd_final = SVD(n_factors=100, n_epochs=30, random_state=42)
    svd_final.fit(full_trainset)

    def get_top_n_recommendations(model, trainset, user_id, n=10):
        """
        Obtener top-N recomendaciones para un usuario.
        """
        # Items que el usuario ya ha calificado
        try:
            inner_uid = trainset.to_inner_uid(user_id)
            rated_items = set(trainset.ur[inner_uid])
            rated_items = {trainset.to_raw_iid(iid) for iid, _ in rated_items}
        except ValueError:
            rated_items = set()

        # Predecir para todos los items no vistos
        predictions = []
        for inner_iid in range(trainset.n_items):
            raw_iid = trainset.to_raw_iid(inner_iid)
            if raw_iid not in rated_items:
                pred = model.predict(user_id, raw_iid)
                predictions.append((raw_iid, pred.est))

        # Ordenar y retornar top N
        predictions.sort(key=lambda x: x[1], reverse=True)
        return predictions[:n]

    # Ejemplo
    test_user = "196"
    recommendations = get_top_n_recommendations(
        svd_final, full_trainset, test_user, n=10
    )

    print(f"\nğŸ¬ Top 10 recomendaciones para usuario '{test_user}':")
    for i, (item_id, score) in enumerate(recommendations, 1):
        print(f"   {i}. PelÃ­cula {item_id}: {score:.2f} â­")

# %% [markdown]
# ## 7. AnÃ¡lisis de Factores Latentes

# %%
if SURPRISE_AVAILABLE:
    # Explorar factores latentes
    print("ğŸ”¬ AnÃ¡lisis de Factores Latentes")

    # Obtener matrices de factores
    P = svd_final.pu  # User factors
    Q = svd_final.qi  # Item factors

    print(f"\n   Shape de P (usuarios): {P.shape}")
    print(f"   Shape de Q (items): {Q.shape}")

    # Visualizar primeros 2 factores de items
    plt.figure(figsize=(10, 8))

    # Seleccionar subset de items
    n_items_plot = 100
    plt.scatter(Q[:n_items_plot, 0], Q[:n_items_plot, 1], alpha=0.6)

    plt.xlabel("Factor 1")
    plt.ylabel("Factor 2")
    plt.title("ProyecciÃ³n de Items en Espacio Latente (primeros 2 factores)")
    plt.grid(True, alpha=0.3)
    plt.show()

    print("\n   Los factores latentes capturan caracterÃ­sticas 'ocultas'")
    print("   como gÃ©nero, Ã©poca, estilo, que correlacionan con preferencias.")

# %% [markdown]
# ## 8. Ejercicios

# %%
print(
    """
ğŸ“ EJERCICIOS

1. COLD-START:
   - Simular usuario nuevo (sin ratings)
   - Â¿CÃ³mo podemos recomendar? Implementar estrategia basada en popularidad

2. EVALUACIÃ“N OFFLINE:
   - Implementar NDCG@K desde cero
   - Comparar con Precision@K: Â¿cuÃ¡l penaliza mÃ¡s errores en top posiciones?

3. HÃBRIDO:
   - Combinar predicciones de SVD y KNN (weighted average)
   - Â¿Mejora el RMSE?

4. VISUALIZACIÃ“N:
   - Usar t-SNE para visualizar items en 2D basado en factores latentes
   - Â¿Se agrupan por gÃ©nero?

5. IMPLICIT FEEDBACK:
   - Convertir ratings a feedback implÃ­cito (1 si rating >= 4, 0 sino)
   - Entrenar modelo ALS para implicit feedback
"""
)

# %% [markdown]
# ## 9. Resumen

# %%
print(
    """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        RESUMEN                                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘  FACTORIZACIÃ“N DE MATRICES:                                      â•‘
â•‘  - Descompone R â‰ˆ P Ã— Qáµ€ en espacio latente                      â•‘
â•‘  - Captura patrones no observables directamente                  â•‘
â•‘  - Base del Netflix Prize (2009)                                 â•‘
â•‘                                                                  â•‘
â•‘  ALGORITMOS:                                                     â•‘
â•‘  - SVD: FactorizaciÃ³n bÃ¡sica con biases                          â•‘
â•‘  - SVD++: Incorpora feedback implÃ­cito                           â•‘
â•‘  - NMF: Factores no negativos (mÃ¡s interpretables)               â•‘
â•‘  - ALS: Paralelizable (usado en Spark)                           â•‘
â•‘                                                                  â•‘
â•‘  MÃ‰TRICAS:                                                       â•‘
â•‘  - RMSE/MAE: PredicciÃ³n de rating                                â•‘
â•‘  - Precision/Recall@K: Calidad de ranking                        â•‘
â•‘  - NDCG: Penaliza errores en top posiciones                      â•‘
â•‘                                                                  â•‘
â•‘  PROBLEMAS:                                                      â•‘
â•‘  - Cold-start: Usuarios/items nuevos sin datos                   â•‘
â•‘  - Sparsity: >99% de la matriz vacÃ­a                             â•‘
â•‘  - Scalability: Millones de usuarios/items                       â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
)

# %%
print("\nâœ… Notebook completado!")
print("   Este mÃ³dulo es CRÃTICO para CSCA 5632.")
print("   AsegÃºrate de entender la matemÃ¡tica detrÃ¡s de SVD.")
