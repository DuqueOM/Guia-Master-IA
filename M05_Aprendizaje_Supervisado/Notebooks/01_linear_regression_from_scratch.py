#!/usr/bin/env python3
"""
Notebook M05: RegresiÃ³n Lineal desde Cero con Celdas de ValidaciÃ³n
===================================================================

MÃ³dulo 5 - Semana 9: Fundamentos de Aprendizaje Supervisado
Curso Alineado: CSCA 5622 - Supervised Learning

Objetivos:
1. Implementar RegresiÃ³n Lineal con Gradiente Descendente desde cero
2. Comprender la conexiÃ³n matemÃ¡tica con M02 (Ãlgebra Lineal) y M03 (CÃ¡lculo)
3. Aplicar RegularizaciÃ³n L2 (Ridge) y entender su relaciÃ³n con normas vectoriales
4. Validar implementaciones con celdas de autograding

Dependencias:
    pip install numpy matplotlib scikit-learn

Ejecutar como script o convertir a notebook con jupytext.
"""
from __future__ import annotations

import numpy as np

rng = np.random.default_rng(seed=42)

# =============================================================================
# PARTE 1: RegresiÃ³n Lineal - Fundamentos MatemÃ¡ticos
# =============================================================================

print("=" * 70)
print("PARTE 1: RegresiÃ³n Lineal desde Cero")
print("=" * 70)

# %% [markdown]
# ## ğŸ’¡ ConexiÃ³n con M02 - Ãlgebra Lineal
#
# La regresiÃ³n lineal busca encontrar el vector de pesos $\mathbf{w}$ que minimiza:
#
# $$\min_{\mathbf{w}} \|\mathbf{Xw} - \mathbf{y}\|_2^2$$
#
# Donde $\|\cdot\|_2$ es la **norma L2** (norma Euclidiana) que estudiaste en M02.
# La soluciÃ³n analÃ­tica usa la **ecuaciÃ³n normal**:
#
# $$\mathbf{w}^* = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$$
#
# Esto requiere que $\mathbf{X}^T\mathbf{X}$ sea **invertible** (matriz no singular).


def linear_regression_closed_form(
    X: np.ndarray,
    y: np.ndarray,
) -> np.ndarray:
    """
    RegresiÃ³n Lineal usando la soluciÃ³n de forma cerrada (ecuaciÃ³n normal).

    ParÃ¡metros
    ----------
    X : np.ndarray
        Matriz de caracterÃ­sticas (n_samples, n_features).
        Debe incluir columna de 1s para el bias si se desea intercepto.
    y : np.ndarray
        Vector objetivo (n_samples,).

    Retorna
    -------
    np.ndarray
        Vector de pesos (n_features,) incluyendo bias si X tiene columna de 1s.

    Notas
    -----
    ğŸ’¡ ConexiÃ³n M02: Esta funciÃ³n implementa la soluciÃ³n $w = (X^TX)^{-1}X^Ty$.
    Usa np.linalg.pinv (pseudo-inversa) para mayor estabilidad numÃ©rica.
    """
    # Pseudo-inversa de Moore-Penrose para estabilidad numÃ©rica
    # Equivalente a (X^T X)^{-1} X^T cuando X tiene rango completo
    return np.linalg.pinv(X) @ y


# %% [markdown]
# ## ğŸ’¡ ConexiÃ³n con M03 - CÃ¡lculo y OptimizaciÃ³n
#
# El gradiente del MSE respecto a $\mathbf{w}$ es:
#
# $$\nabla_{\mathbf{w}} \text{MSE} = \frac{2}{n}\mathbf{X}^T(\mathbf{Xw} - \mathbf{y})$$
#
# El **Descenso de Gradiente** actualiza iterativamente:
#
# $$\mathbf{w}_{t+1} = \mathbf{w}_t - \alpha \nabla_{\mathbf{w}} \text{MSE}$$
#
# El signo negativo es porque descendemos en la direcciÃ³n opuesta al gradiente.


def linear_regression_gradient_descent(
    X: np.ndarray,
    y: np.ndarray,
    learning_rate: float = 0.01,
    n_iterations: int = 1000,
    tolerance: float = 1e-6,
    verbose: bool = False,
) -> tuple[np.ndarray, list[float]]:
    """
    RegresiÃ³n Lineal usando Descenso de Gradiente.

    ParÃ¡metros
    ----------
    X : np.ndarray
        Matriz de caracterÃ­sticas (n_samples, n_features).
    y : np.ndarray
        Vector objetivo (n_samples,).
    learning_rate : float
        Tasa de aprendizaje Î± (default: 0.01).
    n_iterations : int
        NÃºmero mÃ¡ximo de iteraciones (default: 1000).
    tolerance : float
        Criterio de convergencia basado en cambio de loss (default: 1e-6).
    verbose : bool
        Si True, imprime progreso cada 100 iteraciones.

    Retorna
    -------
    tuple[np.ndarray, list[float]]
        - weights: Vector de pesos optimizados (n_features,)
        - loss_history: Lista con el MSE en cada iteraciÃ³n

    Notas
    -----
    ğŸ’¡ ConexiÃ³n M03: Este algoritmo implementa el descenso de gradiente
    que derivaste manualmente. La actualizaciÃ³n w = w - Î±âˆ‡MSE usa el
    gradiente para moverse hacia el mÃ­nimo de la funciÃ³n de costo.
    """
    n_samples, n_features = X.shape

    # InicializaciÃ³n de pesos (pequeÃ±os valores aleatorios)
    weights = rng.standard_normal(n_features) * 0.01

    loss_history: list[float] = []
    prev_loss = float("inf")

    for iteration in range(n_iterations):
        # Forward pass: predicciÃ³n
        y_pred = X @ weights  # (n_samples,)

        # Calcular error
        error = y_pred - y  # (n_samples,)

        # Calcular MSE loss
        mse = float(np.mean(error**2))
        loss_history.append(mse)

        # Verificar convergencia
        if abs(prev_loss - mse) < tolerance:
            if verbose:
                print(f"  ConvergiÃ³ en iteraciÃ³n {iteration}")
            break
        prev_loss = mse

        # Calcular gradiente: âˆ‡MSE = (2/n) * X^T * (Xw - y)
        gradient = (2 / n_samples) * (X.T @ error)  # (n_features,)

        # Actualizar pesos: w = w - Î± * âˆ‡MSE
        weights = weights - learning_rate * gradient

        if verbose and iteration % 100 == 0:
            print(f"  IteraciÃ³n {iteration}: MSE = {mse:.6f}")

    return weights, loss_history


# =============================================================================
# PARTE 2: RegularizaciÃ³n L2 (Ridge Regression)
# =============================================================================

print("\n" + "=" * 70)
print("PARTE 2: RegularizaciÃ³n L2 (Ridge Regression)")
print("=" * 70)

# %% [markdown]
# ## ğŸ’¡ ConexiÃ³n con M02 - Normas Vectoriales y RegularizaciÃ³n L2
#
# > **âš ï¸ CONCEPTO CLAVE**: La regularizaciÃ³n L2 penaliza la **norma L2** del vector
# > de pesos, evitando que crezcan demasiado (overfitting).
#
# La funciÃ³n de costo con regularizaciÃ³n L2 es:
#
# $$J(\mathbf{w}) = \text{MSE} + \lambda \|\mathbf{w}\|_2^2
#                = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p}w_j^2$$
#
# Donde:
# - $\|\mathbf{w}\|_2^2 = w_1^2 + w_2^2 + ... + w_p^2$ es la **norma L2 al cuadrado**
# - $\lambda$ es el hiperparÃ¡metro de regularizaciÃ³n
#
# ### Â¿Por quÃ© funciona?
#
# En el espacio de pesos, la regularizaciÃ³n L2 restringe la soluciÃ³n a una
# **hiperesfera** centrada en el origen. Esto es exactamente la definiciÃ³n
# geomÃ©trica de la norma L2 que viste en M02: todos los puntos a distancia
# constante del origen forman una esfera.
#
# ### Gradiente con RegularizaciÃ³n
#
# $$\nabla_{\mathbf{w}} J = \frac{2}{n}\mathbf{X}^T(\mathbf{Xw} - \mathbf{y}) + 2\lambda\mathbf{w}$$


def ridge_regression_gradient_descent(
    X: np.ndarray,
    y: np.ndarray,
    lambda_reg: float = 0.1,
    learning_rate: float = 0.01,
    n_iterations: int = 1000,
    tolerance: float = 1e-6,
    verbose: bool = False,
) -> tuple[np.ndarray, list[float]]:
    """
    Ridge Regression (L2 Regularization) usando Descenso de Gradiente.

    ParÃ¡metros
    ----------
    X : np.ndarray
        Matriz de caracterÃ­sticas (n_samples, n_features).
    y : np.ndarray
        Vector objetivo (n_samples,).
    lambda_reg : float
        ParÃ¡metro de regularizaciÃ³n Î» (default: 0.1).
        Mayor Î» â†’ mÃ¡s regularizaciÃ³n â†’ pesos mÃ¡s pequeÃ±os.
    learning_rate : float
        Tasa de aprendizaje Î± (default: 0.01).
    n_iterations : int
        NÃºmero mÃ¡ximo de iteraciones (default: 1000).
    tolerance : float
        Criterio de convergencia (default: 1e-6).
    verbose : bool
        Si True, imprime progreso.

    Retorna
    -------
    tuple[np.ndarray, list[float]]
        - weights: Vector de pesos regularizados
        - loss_history: Lista con el costo total (MSE + penalizaciÃ³n L2)

    Notas
    -----
    ğŸ’¡ ConexiÃ³n M02: El tÃ©rmino Î»||w||â‚‚Â² penaliza la norma L2 del vector w.
    GeomÃ©tricamente, esto restringe w a estar dentro de una hiperesfera.
    """
    n_samples, n_features = X.shape
    weights = rng.standard_normal(n_features) * 0.01
    loss_history: list[float] = []
    prev_loss = float("inf")

    for iteration in range(n_iterations):
        # Forward pass
        y_pred = X @ weights
        error = y_pred - y

        # Calcular costo total: MSE + Î»||w||Â²
        mse = float(np.mean(error**2))
        l2_penalty = lambda_reg * float(np.sum(weights**2))
        total_cost = mse + l2_penalty
        loss_history.append(total_cost)

        # Verificar convergencia
        if abs(prev_loss - total_cost) < tolerance:
            if verbose:
                print(f"  ConvergiÃ³ en iteraciÃ³n {iteration}")
            break
        prev_loss = total_cost

        # Gradiente con regularizaciÃ³n: âˆ‡J = (2/n)X^T(Xw-y) + 2Î»w
        gradient = (2 / n_samples) * (X.T @ error) + 2 * lambda_reg * weights

        # Actualizar pesos
        weights = weights - learning_rate * gradient

        if verbose and iteration % 100 == 0:
            print(
                f"  IteraciÃ³n {iteration}: Costo = {total_cost:.6f} (MSE={mse:.6f}, L2={l2_penalty:.6f})"
            )

    return weights, loss_history


# =============================================================================
# PARTE 3: Demo y VisualizaciÃ³n
# =============================================================================

print("\n" + "=" * 70)
print("PARTE 3: Demo con Datos SintÃ©ticos")
print("=" * 70)

# Generar datos sintÃ©ticos
n_samples = 100
n_features = 3

# Crear matriz de caracterÃ­sticas con columna de 1s para bias
X_raw = rng.standard_normal((n_samples, n_features))
X = np.column_stack([np.ones(n_samples), X_raw])  # AÃ±adir intercepto

# Pesos verdaderos (incluyendo bias)
true_weights = np.array([2.0, 1.5, -0.5, 0.3])  # [bias, w1, w2, w3]

# Generar y con ruido
noise = rng.standard_normal(n_samples) * 0.5
y = X @ true_weights + noise

print(f"Datos generados: X.shape={X.shape}, y.shape={y.shape}")
print(f"Pesos verdaderos: {true_weights}")

# --- MÃ©todo 1: Forma Cerrada ---
print("\n--- MÃ©todo 1: Forma Cerrada (EcuaciÃ³n Normal) ---")
weights_closed = linear_regression_closed_form(X, y)
print(f"Pesos estimados: {weights_closed}")
print(f"Error vs verdaderos: {np.abs(weights_closed - true_weights)}")

# --- MÃ©todo 2: Gradiente Descendente ---
print("\n--- MÃ©todo 2: Gradiente Descendente ---")
weights_gd, loss_gd = linear_regression_gradient_descent(
    X, y, learning_rate=0.1, n_iterations=1000, verbose=True
)
print(f"Pesos estimados: {weights_gd}")
print(f"MSE final: {loss_gd[-1]:.6f}")

# --- MÃ©todo 3: Ridge Regression ---
print("\n--- MÃ©todo 3: Ridge Regression (Î»=0.1) ---")
weights_ridge, loss_ridge = ridge_regression_gradient_descent(
    X, y, lambda_reg=0.1, learning_rate=0.1, n_iterations=1000, verbose=True
)
print(f"Pesos estimados: {weights_ridge}")
print(f"Norma L2 de pesos (GD): {np.linalg.norm(weights_gd):.4f}")
print(f"Norma L2 de pesos (Ridge): {np.linalg.norm(weights_ridge):.4f}")
print("â†’ Nota: Ridge produce pesos con menor norma (mÃ¡s regularizados)")


# =============================================================================
# PARTE 4: CELDAS DE VALIDACIÃ“N (AUTOGRADERS)
# =============================================================================

print("\n" + "=" * 70)
print("PARTE 4: Celdas de ValidaciÃ³n para Estudiantes")
print("=" * 70)


def validar_regresion_lineal(
    weights_estudiante: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
    mse_threshold: float = 1.0,
    weights_reference: np.ndarray | None = None,
    tolerance: float = 0.1,
) -> bool:
    """
    Celda de ValidaciÃ³n para RegresiÃ³n Lineal.

    Verifica que la implementaciÃ³n del estudiante:
    1. Produce predicciones con forma correcta
    2. Alcanza un MSE razonable
    3. Pesos cercanos a la referencia (si se proporciona)

    ParÃ¡metros
    ----------
    weights_estudiante : np.ndarray
        Vector de pesos del estudiante (n_features,).
    X_test : np.ndarray
        Datos de prueba (n_samples, n_features).
    y_test : np.ndarray
        Valores objetivo de prueba (n_samples,).
    mse_threshold : float
        MSE mÃ¡ximo aceptable (default: 1.0).
    weights_reference : np.ndarray, opcional
        Pesos de referencia para comparar.
    tolerance : float
        Tolerancia para comparaciÃ³n de pesos (default: 0.1).

    Retorna
    -------
    bool
        True si pasa todas las validaciones.

    Raises
    ------
    AssertionError
        Si alguna validaciÃ³n falla, con mensaje descriptivo.
    """
    print("ğŸ” Ejecutando validaciones...")

    # Test 1: Verificar dimensiones
    expected_features = X_test.shape[1]
    assert weights_estudiante.shape == (expected_features,), (
        f"âŒ Error de dimensiones: "
        f"Esperado ({expected_features},), "
        f"Obtenido {weights_estudiante.shape}"
    )
    print("  âœ… Test 1: Dimensiones correctas")

    # Test 2: Calcular predicciones y MSE
    y_pred = X_test @ weights_estudiante
    assert y_pred.shape == y_test.shape, (
        f"âŒ Error: Predicciones con forma incorrecta. "
        f"Esperado {y_test.shape}, Obtenido {y_pred.shape}"
    )
    print("  âœ… Test 2: Forma de predicciones correcta")

    # Test 3: Verificar MSE
    mse = float(np.mean((y_pred - y_test) ** 2))
    assert mse < mse_threshold, (
        f"âŒ Error: MSE demasiado alto. " f"MSE={mse:.4f}, Umbral={mse_threshold}"
    )
    print(f"  âœ… Test 3: MSE aceptable ({mse:.4f} < {mse_threshold})")

    # Test 4: Comparar con referencia (opcional)
    if weights_reference is not None:
        diff = np.abs(weights_estudiante - weights_reference)
        max_diff = float(np.max(diff))
        assert max_diff < tolerance, (
            f"âŒ Error: Pesos difieren de la referencia. "
            f"MÃ¡xima diferencia={max_diff:.4f}, Tolerancia={tolerance}"
        )
        print(f"  âœ… Test 4: Pesos cercanos a referencia (max_diff={max_diff:.4f})")

    print("\nâœ… Â¡EXCELENTE! Tu implementaciÃ³n pasa todas las validaciones.")
    return True


def validar_ridge_regression(
    weights_estudiante: np.ndarray,
    weights_sin_regularizar: np.ndarray,
    lambda_reg: float,
) -> bool:
    """
    ValidaciÃ³n especÃ­fica para Ridge Regression.

    Verifica que:
    1. Los pesos regularizados tienen menor norma L2
    2. El efecto de regularizaciÃ³n es proporcional a Î»

    ParÃ¡metros
    ----------
    weights_estudiante : np.ndarray
        Pesos de Ridge Regression del estudiante.
    weights_sin_regularizar : np.ndarray
        Pesos de regresiÃ³n lineal sin regularizar (para comparar).
    lambda_reg : float
        Valor de Î» usado en la regularizaciÃ³n.

    Retorna
    -------
    bool
        True si pasa las validaciones.
    """
    print("ğŸ” Validando Ridge Regression...")

    norma_estudiante = float(np.linalg.norm(weights_estudiante))
    norma_sin_reg = float(np.linalg.norm(weights_sin_regularizar))

    # Test 1: Ridge debe producir pesos con menor norma
    assert norma_estudiante < norma_sin_reg * 1.1, (
        f"âŒ Error: La regularizaciÃ³n L2 deberÃ­a reducir la norma de los pesos. "
        f"Norma Ridge={norma_estudiante:.4f}, Norma sin regularizar={norma_sin_reg:.4f}"
    )
    print(f"  âœ… Test 1: Norma reducida ({norma_estudiante:.4f} < {norma_sin_reg:.4f})")

    # Test 2: Con Î» > 0, los pesos no deberÃ­an ser idÃ©nticos
    if lambda_reg > 0:
        diff = float(np.linalg.norm(weights_estudiante - weights_sin_regularizar))
        assert diff > 1e-6, (
            "âŒ Error: Los pesos regularizados son idÃ©nticos a los no regularizados. "
            "Verifica que estÃ¡s aplicando el tÃ©rmino de penalizaciÃ³n Î»||w||Â²"
        )
        print(f"  âœ… Test 2: RegularizaciÃ³n aplicada correctamente (diff={diff:.6f})")

    print("\nâœ… Â¡CORRECTO! Tu Ridge Regression estÃ¡ bien implementado.")
    return True


# --- Ejecutar Validaciones ---
print("\n--- ValidaciÃ³n de tu implementaciÃ³n de RegresiÃ³n Lineal ---")
try:
    validar_regresion_lineal(
        weights_estudiante=weights_gd,
        X_test=X,
        y_test=y,
        mse_threshold=1.0,
        weights_reference=weights_closed,
        tolerance=0.5,
    )
except AssertionError as e:
    print(f"\n{e}")

print("\n--- ValidaciÃ³n de tu implementaciÃ³n de Ridge Regression ---")
try:
    validar_ridge_regression(
        weights_estudiante=weights_ridge,
        weights_sin_regularizar=weights_gd,
        lambda_reg=0.1,
    )
except AssertionError as e:
    print(f"\n{e}")


# =============================================================================
# PARTE 5: Ejercicios para el Estudiante
# =============================================================================

print("\n" + "=" * 70)
print("PARTE 5: Ejercicios para Practicar")
print("=" * 70)

# %% [markdown]
# ## ğŸ“ Ejercicio 1: Implementa tu propia funciÃ³n de RegresiÃ³n Lineal
#
# Completa la funciÃ³n `mi_regresion_lineal()` usando gradiente descendente.
# Luego ejecuta la celda de validaciÃ³n para verificar tu implementaciÃ³n.
#
# ```python
# def mi_regresion_lineal(X, y, lr=0.01, epochs=1000):
#     """
#     Tu implementaciÃ³n aquÃ­.
#
#     Pistas:
#     1. Inicializa pesos con valores pequeÃ±os aleatorios
#     2. En cada Ã©poca:
#        a. Calcula predicciones: y_pred = X @ weights
#        b. Calcula error: error = y_pred - y
#        c. Calcula gradiente: grad = (2/n) * X.T @ error
#        d. Actualiza pesos: weights = weights - lr * grad
#     3. Retorna los pesos finales
#     """
#     n_samples, n_features = X.shape
#     weights = ...  # Inicializar
#
#     for epoch in range(epochs):
#         # Tu cÃ³digo aquÃ­
#         pass
#
#     return weights
# ```
#
# ## ğŸ“ Ejercicio 2: Experimenta con diferentes valores de Î»
#
# Ejecuta Ridge Regression con Î» âˆˆ {0.01, 0.1, 1.0, 10.0} y observa:
# 1. Â¿CÃ³mo cambia la norma L2 de los pesos?
# 2. Â¿CÃ³mo cambia el MSE en datos de entrenamiento?
# 3. Â¿Hay un punto donde demasiada regularizaciÃ³n perjudica?
#
# ## ğŸ“ Ejercicio 3: ConexiÃ³n con M02
#
# Demuestra geomÃ©tricamente por quÃ© la regularizaciÃ³n L2 produce pesos
# dentro de una hiperesfera. Pista: grafica el contorno de MSE vs el
# contorno de ||w||Â² = constante para un problema 2D.


print(
    """
ğŸ“š RESUMEN DE CONEXIONES TEÃ“RICO-PRÃCTICAS:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MÃ“DULO    â”‚ CONCEPTO              â”‚ APLICACIÃ“N EN REGRESIÃ“N             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ M02       â”‚ Norma L2 (Euclidiana) â”‚ RegularizaciÃ³n Ridge: Î»||w||â‚‚Â²      â”‚
â”‚ M02       â”‚ Producto interno      â”‚ PredicciÃ³n: Å· = Xw = Î£ xáµ¢wáµ¢        â”‚
â”‚ M02       â”‚ InversiÃ³n matricial   â”‚ EcuaciÃ³n normal: w = (X^TX)^{-1}X^Tyâ”‚
â”‚ M03       â”‚ Gradiente             â”‚ âˆ‡MSE = (2/n)X^T(Xw - y)             â”‚
â”‚ M03       â”‚ Descenso de gradiente â”‚ w = w - Î±âˆ‡MSE                       â”‚
â”‚ M03       â”‚ Derivadas parciales   â”‚ âˆ‚MSE/âˆ‚wâ±¼ para cada peso             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""
)

print("\nâœ… Notebook completado. Â¡Ahora implementa tus propias funciones y valÃ­dalas!")
