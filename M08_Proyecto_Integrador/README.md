# MÃ³dulo 08: Proyecto Capstone - NLP Disaster Analysis Pipeline

> **Semanas:** 21-24 | **Fase:** IntegraciÃ³n ğŸ¯ | **Nivel:** MaestrÃ­a

---

## ğŸ¯ DescripciÃ³n del Proyecto

### "Natural Language Processing with Disaster Tweets"

**Dataset:** [Kaggle - Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started)

Este proyecto final integra **los 3 cursos del MS-AI Pathway** en un pipeline completo:
- **CSCA 5622 (Supervisado):** RegresiÃ³n LogÃ­stica, Naive Bayes, mÃ©tricas de evaluaciÃ³n
- **CSCA 5632 (No Supervisado):** Word Embeddings como representaciones latentes
- **CSCA 5642 (Deep Learning):** LSTMs bidireccionales, Transfer Learning con BERT

### Â¿Por quÃ© este proyecto?

| Aspecto | MNIST (Anterior) | Disaster Tweets (Nuevo) |
|---------|------------------|-------------------------|
| Datos | Limpios, estructurados | Sucios, texto no estructurado |
| Preprocesamiento | MÃ­nimo | Regex, tokenizaciÃ³n, lematizaciÃ³n |
| Complejidad | Introductorio | Nivel de maestrÃ­a |
| EvaluaciÃ³n | Accuracy simple | F1-Score, datos desbalanceados |
| ComunicaciÃ³n | Opcional | REPORT.md obligatorio |

---

## ğŸ“ Estructura del Proyecto

```
M08_Proyecto_Integrador/
â”œâ”€â”€ README.md                              # Este archivo
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                              # Datos originales de Kaggle
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â””â”€â”€ test.csv
â”‚   â”œâ”€â”€ processed/                        # Datos limpios
â”‚   â”‚   â””â”€â”€ train_clean.csv
â”‚   â””â”€â”€ README_data.md                    # Instrucciones de descarga
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_EDA_Preprocessing.ipynb        # Semana 21
â”‚   â”œâ”€â”€ 02_Baseline_Models.ipynb          # Semana 22
â”‚   â”œâ”€â”€ 03_Deep_Learning_LSTM.ipynb       # Semana 23
â”‚   â””â”€â”€ 04_Transfer_Learning_BERT.ipynb   # Semana 24
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessing.py                  # Funciones de limpieza
â”‚   â”œâ”€â”€ features.py                       # TF-IDF, embeddings
â”‚   â”œâ”€â”€ models.py                         # Clases de modelos
â”‚   â””â”€â”€ evaluation.py                     # MÃ©tricas y visualizaciÃ³n
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ baseline_logreg.pkl
â”‚   â”œâ”€â”€ lstm_best.h5
â”‚   â””â”€â”€ bert_finetuned/
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ REPORT.md                         # Reporte final acadÃ©mico
â”‚   â”œâ”€â”€ figures/                          # GrÃ¡ficas para el reporte
â”‚   â””â”€â”€ MODEL_COMPARISON.md               # Benchmarks de modelos
â”œâ”€â”€ Archive_MNIST/                        # Proyecto MNIST archivado
â”‚   â””â”€â”€ README.md                         # Referencia como tarea introductoria
â””â”€â”€ requirements_capstone.txt             # Dependencias especÃ­ficas
```

---

## ğŸ““ Notebooks del Proyecto

### Notebook 1: EDA & Preprocessing (Semana 21)

**Archivo:** `notebooks/01_EDA_Preprocessing.ipynb`

| Tarea | TÃ©cnica | LibrerÃ­a |
|-------|---------|----------|
| Carga y exploraciÃ³n | `df.info()`, `df.describe()` | Pandas |
| Limpieza de texto | Regex para URLs, HTML tags, menciones | `re` |
| TokenizaciÃ³n | Word tokenization | NLTK / SpaCy |
| LematizaciÃ³n | Reducir a raÃ­z | NLTK WordNetLemmatizer |
| VisualizaciÃ³n | WordClouds comparativas | `wordcloud`, Matplotlib |
| AnÃ¡lisis de desbalance | ProporciÃ³n de clases | Pandas |

**Entregables:**
- [ ] `train_clean.csv` generado
- [ ] WordCloud de tweets reales vs falsos
- [ ] AnÃ¡lisis estadÃ­stico de longitud de tweets

---

### Notebook 2: Baseline Models (Semana 22)

**Archivo:** `notebooks/02_Baseline_Models.ipynb`

| Modelo | VectorizaciÃ³n | HiperparÃ¡metros |
|--------|---------------|-----------------|
| Logistic Regression | TF-IDF | C, max_iter |
| Multinomial Naive Bayes | Bag of Words | alpha |
| SVM | TF-IDF | kernel, C |

**MÃ©tricas de EvaluaciÃ³n (CRÃTICO):**

```python
from sklearn.metrics import classification_report, confusion_matrix, f1_score

# Â¿Por quÃ© NO usar accuracy en datos desbalanceados?
# Si 70% son clase 0, un modelo que prediga siempre 0 tiene 70% accuracy
# pero es completamente inÃºtil. F1-Score balancea Precision y Recall.

print(classification_report(y_true, y_pred))
print(f"F1-Score (macro): {f1_score(y_true, y_pred, average='macro'):.4f}")
```

**Entregables:**
- [ ] Pipeline de vectorizaciÃ³n + modelo
- [ ] Matriz de confusiÃ³n visualizada
- [ ] ComparaciÃ³n F1-Score de baselines
- [ ] AnÃ¡lisis de errores (falsos positivos/negativos)

---

### Notebook 3: Deep Learning - LSTM (Semana 23)

**Archivo:** `notebooks/03_Deep_Learning_LSTM.ipynb`

**Arquitectura:**

```python
from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Dropout
from tensorflow.keras.models import Model

# Arquitectura Bidirectional LSTM
inputs = Input(shape=(max_length,))
x = Embedding(vocab_size, embedding_dim)(inputs)
x = Bidirectional(LSTM(64, return_sequences=True))(x)
x = Bidirectional(LSTM(32))(x)
x = Dropout(0.5)(x)
x = Dense(64, activation='relu')(x)
x = Dropout(0.3)(x)
outputs = Dense(1, activation='sigmoid')(x)

model = Model(inputs=inputs, outputs=outputs)
```

**Embeddings:**
- OpciÃ³n A: Entrenar desde cero
- OpciÃ³n B: Usar GloVe preentrenados (recomendado)

**RegularizaciÃ³n (CRÃTICO para calificaciÃ³n):**
- `Dropout` entre capas
- `EarlyStopping` con `patience=5`
- `ModelCheckpoint` para guardar mejor modelo

**Entregables:**
- [ ] LSTM bidireccional funcionando
- [ ] Curvas de learning (loss, accuracy)
- [ ] ComparaciÃ³n con/sin GloVe embeddings
- [ ] AnÃ¡lisis de overfitting

---

### Notebook 4: Transfer Learning & Reporte (Semana 24)

**Archivo:** `notebooks/04_Transfer_Learning_BERT.ipynb`

**Bonus Track - BERT con HuggingFace:**

```python
from transformers import BertTokenizer, TFBertForSequenceClassification
import tensorflow as tf

# Cargar modelo preentrenado
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Tokenizar datos
encodings = tokenizer(texts, truncation=True, padding=True, max_length=128, return_tensors='tf')

# Fine-tuning
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)
```

**Entregables:**
- [ ] BERT fine-tuned en disaster tweets
- [ ] ComparaciÃ³n BERT vs LSTM vs Baselines
- [ ] **REPORT.md** completo

---

## ğŸ“„ REPORT.md - Estructura AcadÃ©mica

El reporte final debe seguir esta estructura:

```markdown
# NLP Disaster Tweet Classification - Final Report

## Abstract (150 palabras)
Resumen ejecutivo del proyecto, metodologÃ­a y resultados principales.

## 1. Introduction
- Contexto del problema
- Relevancia en aplicaciones reales (detecciÃ³n de emergencias)
- Objetivos del proyecto

## 2. Dataset Description
- EstadÃ­sticas descriptivas
- AnÃ¡lisis de desbalance de clases
- Ejemplos de tweets difÃ­ciles

## 3. Methodology
### 3.1 Preprocessing Pipeline
### 3.2 Feature Engineering
### 3.3 Model Architectures

## 4. Experiments
### 4.1 Baseline Models
### 4.2 LSTM Results
### 4.3 BERT Results

## 5. Results & Discussion
- Tabla comparativa de todos los modelos
- AnÃ¡lisis de errores
- Limitaciones del estudio

## 6. Conclusion
- Resumen de hallazgos
- Modelo recomendado para producciÃ³n
- Trabajo futuro

## References
- Papers citados
- DocumentaciÃ³n de librerÃ­as
```

---

## âš¡ Inicio RÃ¡pido

```bash
# 1. Descargar datos de Kaggle
# Ir a https://www.kaggle.com/c/nlp-getting-started/data
# Descargar train.csv y test.csv â†’ data/raw/

# 2. Instalar dependencias adicionales
pip install -r requirements_capstone.txt

# 3. Ejecutar notebooks en orden
jupyter notebook notebooks/01_EDA_Preprocessing.ipynb
jupyter notebook notebooks/02_Baseline_Models.ipynb
jupyter notebook notebooks/03_Deep_Learning_LSTM.ipynb
jupyter notebook notebooks/04_Transfer_Learning_BERT.ipynb
```

---

## ğŸ“¦ Dependencias EspecÃ­ficas

```txt
# requirements_capstone.txt
nltk>=3.8
spacy>=3.5
wordcloud>=1.9
transformers>=4.30
datasets>=2.12
scikit-learn>=1.2
tensorflow>=2.12
```

---

## âœ… Checklist Final de Entrega

### Semana 21
- [ ] Dataset descargado y explorado
- [ ] Pipeline de preprocesamiento completo
- [ ] WordClouds generados

### Semana 22
- [ ] Baselines entrenados (LogReg, NB)
- [ ] F1-Score documentado
- [ ] Matriz de confusiÃ³n analizada

### Semana 23
- [ ] LSTM bidireccional entrenado
- [ ] Embeddings GloVe integrados
- [ ] RegularizaciÃ³n implementada (Dropout, EarlyStopping)

### Semana 24
- [ ] BERT fine-tuned (bonus)
- [ ] MODEL_COMPARISON.md completo
- [ ] **REPORT.md entregado**
- [ ] CÃ³digo limpio y documentado

---

## ğŸ† Criterios de Ã‰xito

| Criterio | MÃ­nimo | Excelente |
|----------|--------|-----------|
| F1-Score Baseline | > 0.70 | > 0.78 |
| F1-Score LSTM | > 0.75 | > 0.80 |
| F1-Score BERT | > 0.80 | > 0.85 |
| REPORT.md | Completo | Publicable |
| CÃ³digo | Funcional | Modular y testeado |

---

## ğŸ“š Recursos

### Dataset
- [Kaggle Competition](https://www.kaggle.com/c/nlp-getting-started)
- [Dataset Paper](https://arxiv.org/abs/1907.11692)

### NLP
- [NLTK Book](https://www.nltk.org/book/)
- [SpaCy Course](https://course.spacy.io/)

### Deep Learning NLP
- [CS224n Stanford](https://web.stanford.edu/class/cs224n/)
- [HuggingFace Course](https://huggingface.co/course)

### GloVe Embeddings
- [Download GloVe](https://nlp.stanford.edu/projects/glove/)
- Usar `glove.6B.100d.txt` (100 dimensiones)

---

## ğŸ”— NavegaciÃ³n

| Anterior | Ãndice | Final |
|----------|--------|-------|
| [M07 Deep Learning](../M07_Deep_Learning/) | [README](../README.md) | ğŸ“ Completado | |
