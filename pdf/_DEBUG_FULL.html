
    <!DOCTYPE html>
    <html>
    <head><meta charset="UTF-8"></head>
    <body>
        
            <div class="cover-page global-cover">
                <div class="cover-content" style="border-color:rgba(96,165,250,0.3);background:rgba(15,23,42,0.6);">
                    <div style="font-size:60pt;margin-bottom:30px;">ğŸš€</div>
                    <div class="global-title">MS AI PATHWAY</div>
                    <div class="cover-subtitle" style="color:#94a3b8;margin-bottom:40px;">ML Specialist v3.3 | 24 Semanas</div>
                    <div class="cover-badge" style="background:linear-gradient(90deg,#3b82f6,#8b5cf6);border:none;padding:10px 30px;">LÃNEA 1: MACHINE LEARNING</div>
                </div>
                <div style="position:absolute;bottom:30px;font-size:11pt;color:#64748b;">DUQUEOM | 2025</div>
            </div>
        
        
            <!-- MÃ“DULO: 00_INDICE.md -->
            <div class="cover-page">
                <div class="cover-content">
                    <div style="font-size:40pt;margin-bottom:20px;">ğŸ’</div>
                    <div id="mod_00_INDICE" class="cover-title">ÃNDICE GENERAL</div>
                    <div class="cover-subtitle">MS in AI Pathway - ML Specialist v3.0</div>
                    <div class="cover-badge">MATEMÃTICAS APLICADAS A CÃ“DIGO</div>
                </div>
                <div style="position:absolute;bottom:30px;font-size:10pt;opacity:0.6;">DUQUEOM | 2025</div>
            </div>
            <div class="content">
                <h1>ğŸ“š GUÃA MAESTRA: MS AI PATHWAY - ML SPECIALIST (v3.1)</h1>
<blockquote>
<p><strong>De Python BÃ¡sico a Candidato del MS in AI de CU Boulder</strong><br />
<strong>24 Semanas (6 Meses Exactos) | Enfoque: LÃ­nea 1 - Machine Learning</strong><br />
<strong>FilosofÃ­a: "MatemÃ¡ticas Aplicadas a CÃ³digo"</strong></p>
</blockquote>
<h2>ğŸ¯ Objetivo de Esta GuÃ­a</h2>
<p><strong>Dominio absoluto de las 3 materias de la LÃ­nea de Machine Learning</strong> del Performance-Based Admission Pathway:</p>
<h3>â­ LÃNEA 1: Machine Learning (3 crÃ©ditos) - FOCO PRINCIPAL</h3>
<table>
<thead>
<tr>
<th>Curso del Pathway</th>
<th>MÃ³dulo de Esta GuÃ­a</th>
</tr>
</thead>
<tbody>
<tr>
<td>Introduction to Machine Learning: Supervised Learning</td>
<td><strong>MÃ³dulo 05</strong></td>
</tr>
<tr>
<td>Unsupervised Algorithms in Machine Learning</td>
<td><strong>MÃ³dulo 06</strong></td>
</tr>
<tr>
<td>Introduction to Deep Learning</td>
<td><strong>MÃ³dulo 07</strong></td>
</tr>
</tbody>
</table>
<h3>ğŸ“– LÃNEA 2: Probabilidad y EstadÃ­stica (Lectura Opcional)</h3>
<table>
<thead>
<tr>
<th>Curso del Pathway</th>
<th>Estado</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probability Foundations for Data Science and AI</td>
<td>Lectura opcional</td>
</tr>
<tr>
<td>Discrete-Time Markov Chains and Monte Carlo Methods</td>
<td>Lectura opcional</td>
</tr>
<tr>
<td>Statistical Estimation for Data Science and AI</td>
<td>Lectura opcional</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Nota:</strong> La LÃ­nea 2 pertenece a la especializaciÃ³n de EstadÃ­stica. Esta guÃ­a incluye solo la probabilidad esencial para ML (MÃ³dulo 04).</p>
</blockquote>
<h2>ğŸ—ºï¸ El Mapa de Ruta: 3 Fases CrÃ­ticas</h2>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FASE 1: FUNDAMENTOS (Semanas 1-8)                                          â”‚
â”‚  Objetivo: Python cientÃ­fico + matemÃ¡ticas para leer papers de ML           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MÃ³dulo 01  Python + Pandas + NumPy   Carga de datos, vectorizaciÃ³n [2 sem] â”‚
â”‚  MÃ³dulo 02  Ãlgebra Lineal para ML    Matrices, normas, SVD, eigen  [3 sem] â”‚
â”‚  MÃ³dulo 03  CÃ¡lculo Multivariante     Gradientes, Chain Rule        [2 sem] â”‚
â”‚  MÃ³dulo 04  Probabilidad para ML      Bayes, Gaussiana, MLE         [1 sem] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FASE 2: NÃšCLEO DE MACHINE LEARNING (Semanas 9-20)                          â”‚
â”‚  â­ SIMULACIÃ“N DEL PATHWAY - LÃNEA 1                                        â”‚
â”‚  Objetivo: Implementar desde cero los algoritmos de los 3 cursos            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MÃ³dulo 05  Supervised Learning       RegresiÃ³n, LogÃ­stica, CV      [4 sem] â”‚
â”‚  MÃ³dulo 06  Unsupervised Learning     K-Means, PCA, GMM             [4 sem] â”‚
â”‚  MÃ³dulo 07  Deep Learning             MLP, Backprop, CNNs           [4 sem] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FASE 3: PROYECTO INTEGRADOR &quot;MNIST ANALYST&quot; (Semanas 21-24)                â”‚
â”‚  Objetivo: Un proyecto que demuestra competencia en las 3 Ã¡reas             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MÃ³dulo 08  MNIST End-to-End Pipeline                               [4 sem] â”‚
â”‚             PCA + K-Means + Logistic Regression + MLP desde cero            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p><strong>Total: 8 mÃ³dulos obligatorios | 24 semanas | ~864 horas</strong></p>
<h2>ğŸ‘¤ Perfil de Entrada</h2>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PERFIL IDEAL DE ENTRADA                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ… Python bÃ¡sico (variables, funciones, listas, diccionarios) â”‚
â”‚  âœ… LÃ³gica de programaciÃ³n (if/else, loops)                    â”‚
â”‚  âœ… MatemÃ¡ticas de bachillerato (Ã¡lgebra bÃ¡sica)               â”‚
â”‚  âœ… Ganas de entender &quot;cÃ³mo funciona por dentro&quot;               â”‚
â”‚  âš ï¸  NO se requiere: numpy, pandas, sklearn, ML previo         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h2>ğŸ“– MÃ³dulos Obligatorios</h2>
<h3>FASE 1: Fundamentos (Semanas 1-8)</h3>
<p><em>Python cientÃ­fico con Pandas, matemÃ¡ticas esenciales y probabilidad bÃ¡sica para ML.</em></p>
<table>
<thead>
<tr>
<th>#</th>
<th>MÃ³dulo</th>
<th>DescripciÃ³n</th>
<th>Tiempo</th>
<th>Archivo</th>
</tr>
</thead>
<tbody>
<tr>
<td>01</td>
<td><strong>Python + Pandas + NumPy</strong></td>
<td>Carga de datos, limpieza, vectorizaciÃ³n</td>
<td>2 sem</td>
<td><a href="#mod_01_PYTHON_CIENTIFICO">01_PYTHON_CIENTIFICO.md</a></td>
</tr>
<tr>
<td>02</td>
<td><strong>Ãlgebra Lineal para ML</strong></td>
<td>Vectores, matrices, normas, SVD, eigenvalues</td>
<td>3 sem</td>
<td><a href="#mod_02_ALGEBRA_LINEAL_ML">02_ALGEBRA_LINEAL_ML.md</a></td>
</tr>
<tr>
<td>03</td>
<td><strong>CÃ¡lculo Multivariante</strong></td>
<td>Derivadas parciales, gradiente, Chain Rule</td>
<td>2 sem</td>
<td><a href="#mod_03_CALCULO_MULTIVARIANTE">03_CALCULO_MULTIVARIANTE.md</a></td>
</tr>
<tr>
<td>04</td>
<td><strong>Probabilidad para ML</strong></td>
<td>Teorema de Bayes, Gaussiana, MLE</td>
<td>1 sem</td>
<td><a href="#mod_04_PROBABILIDAD_ML">04_PROBABILIDAD_ML.md</a></td>
</tr>
</tbody>
</table>
<p><strong>Entregables Fase 1:</strong><br />
- Script de carga y limpieza de CSV con Pandas<br />
- LibrerÃ­a <code>linear_algebra.py</code> con proyecciones y distancias<br />
- Gradient Descent manual para minimizar funciones<br />
- ImplementaciÃ³n de MLE para estimar parÃ¡metros de Gaussiana</p>
<h3>FASE 2: NÃºcleo de Machine Learning (Semanas 9-20) â­ PATHWAY LÃNEA 1</h3>
<p><em>Los 3 cursos del Pathway implementados desde cero.</em></p>
<table>
<thead>
<tr>
<th>#</th>
<th>MÃ³dulo</th>
<th>Curso del Pathway</th>
<th>Tiempo</th>
<th>Archivo</th>
</tr>
</thead>
<tbody>
<tr>
<td>05</td>
<td><strong>Supervised Learning</strong></td>
<td>Introduction to ML: Supervised Learning</td>
<td>4 sem</td>
<td><a href="#mod_05_SUPERVISED_LEARNING">05_SUPERVISED_LEARNING.md</a></td>
</tr>
<tr>
<td>06</td>
<td><strong>Unsupervised Learning</strong></td>
<td>Unsupervised Algorithms in ML</td>
<td>4 sem</td>
<td><a href="#mod_06_UNSUPERVISED_LEARNING">06_UNSUPERVISED_LEARNING.md</a></td>
</tr>
<tr>
<td>07</td>
<td><strong>Deep Learning</strong></td>
<td>Introduction to Deep Learning</td>
<td>4 sem</td>
<td><a href="#mod_07_DEEP_LEARNING">07_DEEP_LEARNING.md</a></td>
</tr>
</tbody>
</table>
<p><strong>Entregables Fase 2:</strong><br />
- <code>logistic_regression.py</code> con regularizaciÃ³n L2<br />
- <code>kmeans.py</code> y <code>pca.py</code> funcionales<br />
- <code>neural_network.py</code> con backprop manual (MLP)<br />
- TeorÃ­a de CNNs (convoluciÃ³n, pooling, stride)</p>
<h3>FASE 3: Proyecto Final MNIST Analyst (Semanas 21-24)</h3>
<p><em>Pipeline completo en 4 semanas. MNIST es simple, no necesita mÃ¡s.</em></p>
<table>
<thead>
<tr>
<th>#</th>
<th>MÃ³dulo</th>
<th>DescripciÃ³n</th>
<th>Tiempo</th>
<th>Archivo</th>
</tr>
</thead>
<tbody>
<tr>
<td>08</td>
<td><strong>MNIST Analyst</strong></td>
<td>Pipeline end-to-end de clasificaciÃ³n de dÃ­gitos</td>
<td>4 sem</td>
<td><a href="#mod_08_PROYECTO_MNIST">08_PROYECTO_MNIST.md</a></td>
</tr>
</tbody>
</table>
<p><strong>Proyecto: "End-to-End Handwritten Digit Analysis Pipeline"</strong></p>
<table>
<thead>
<tr>
<th>Semana</th>
<th>Componente</th>
<th>Materia Demostrada</th>
</tr>
</thead>
<tbody>
<tr>
<td>21</td>
<td>EDA + PCA + K-Means</td>
<td>Unsupervised Algorithms</td>
</tr>
<tr>
<td>22</td>
<td>RegresiÃ³n LogÃ­stica One-vs-All</td>
<td>Supervised Learning</td>
</tr>
<tr>
<td>23</td>
<td>MLP con Backprop desde cero</td>
<td>Deep Learning</td>
</tr>
<tr>
<td>24</td>
<td>Informe + ComparaciÃ³n de Modelos</td>
<td>IntegraciÃ³n</td>
</tr>
</tbody>
</table>
<h2>ğŸ”¨ Estructura del Proyecto Final</h2>
<pre><code>mnist-analyst/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ # FASE 1: FUNDAMENTOS
â”‚   â”œâ”€â”€ data_loader.py         # Carga con Pandas, limpieza (MÃ³dulo 01)
â”‚   â”œâ”€â”€ linear_algebra.py      # Vectores, matrices, normas (MÃ³dulo 02)
â”‚   â”œâ”€â”€ calculus.py            # Gradientes, derivadas (MÃ³dulo 03)
â”‚   â”œâ”€â”€ probability.py         # Bayes, Gaussiana, MLE (MÃ³dulo 04)
â”‚   â”‚
â”‚   â”œâ”€â”€ # FASE 2: ML CORE
â”‚   â”œâ”€â”€ logistic_regression.py # ClasificaciÃ³n binaria/multiclase (MÃ³dulo 05)
â”‚   â”œâ”€â”€ metrics.py             # Accuracy, Precision, Recall, F1 (MÃ³dulo 05)
â”‚   â”œâ”€â”€ kmeans.py              # Clustering K-Means++ (MÃ³dulo 06)
â”‚   â”œâ”€â”€ pca.py                 # ReducciÃ³n dimensional SVD (MÃ³dulo 06)
â”‚   â”œâ”€â”€ neural_network.py      # MLP con backprop (MÃ³dulo 07)
â”‚   â”œâ”€â”€ activations.py         # Sigmoid, ReLU, Softmax (MÃ³dulo 07)
â”‚   â”œâ”€â”€ optimizers.py          # SGD, Adam (MÃ³dulo 07)
â”‚   â”‚
â”‚   â””â”€â”€ # INTEGRACIÃ“N
â”‚   â””â”€â”€ mnist_pipeline.py      # Pipeline completo (MÃ³dulo 08)
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_linear_algebra.py
â”‚   â”œâ”€â”€ test_logistic_regression.py
â”‚   â”œâ”€â”€ test_kmeans.py
â”‚   â”œâ”€â”€ test_pca.py
â”‚   â”œâ”€â”€ test_neural_network.py
â”‚   â””â”€â”€ test_pipeline.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ mnist/                 # Dataset MNIST (28x28 imÃ¡genes)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda_visualization.ipynb
â”‚   â”œâ”€â”€ 02_pca_kmeans.ipynb
â”‚   â”œâ”€â”€ 03_logistic_ova.ipynb
â”‚   â””â”€â”€ 04_mlp_benchmark.ipynb
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ MATHEMATICAL_FOUNDATIONS.md
â”‚   â””â”€â”€ MODEL_COMPARISON.md
â”‚
â”œâ”€â”€ README.md                  # DocumentaciÃ³n (inglÃ©s)
â”œâ”€â”€ pyproject.toml
â””â”€â”€ requirements.txt           # numpy, pandas, matplotlib, pytest
</code></pre>
<h2>â±ï¸ Tiempo Total</h2>
<table>
<thead>
<tr>
<th>Fase</th>
<th>Semanas</th>
<th>Horas (~36h/sem)</th>
<th>Enfoque</th>
</tr>
</thead>
<tbody>
<tr>
<td>Fundamentos (01-04)</td>
<td>8</td>
<td>~288h</td>
<td>Python + MatemÃ¡ticas + Probabilidad</td>
</tr>
<tr>
<td>ML Core (05-07)</td>
<td>12</td>
<td>~432h</td>
<td>Algoritmos del Pathway</td>
</tr>
<tr>
<td>Proyecto MNIST (08)</td>
<td>4</td>
<td>~144h</td>
<td>IntegraciÃ³n y demo</td>
</tr>
<tr>
<td><strong>TOTAL</strong></td>
<td><strong>24</strong></td>
<td><strong>~864h</strong></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>DuraciÃ³n:</strong> 6 meses exactos con 6h/dÃ­a (L-S)</p>
<h2>âŒ QuÃ© Se EliminÃ³ del Plan Original</h2>
<p>Para que esto quepa en 6 meses y sea efectivo para la <strong>LÃ­nea 1 de ML</strong>:</p>
<table>
<thead>
<tr>
<th>Eliminado</th>
<th>RazÃ³n</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linked Lists, Stacks, Queues</td>
<td>Irrelevante para matemÃ¡ticas del Pathway</td>
</tr>
<tr>
<td>Binary Trees, BST</td>
<td>No se usa en los 3 cursos de ML</td>
</tr>
<tr>
<td>Grafos (BFS/DFS)</td>
<td>No es parte del currÃ­culo</td>
</tr>
<tr>
<td>QuickSort, MergeSort</td>
<td>En ML usas <code>numpy.sort()</code></td>
</tr>
<tr>
<td>Inverted Index, TF-IDF</td>
<td>Proyecto de IR, no de CV/ML</td>
</tr>
<tr>
<td>Cadenas de Markov</td>
<td>Pertenece a LÃ­nea 2 (EstadÃ­stica)</td>
</tr>
<tr>
<td>Motor de BÃºsqueda</td>
<td>Reemplazado por MNIST Pipeline</td>
</tr>
</tbody>
</table>
<h2>ğŸ“¦ Material de Referencia</h2>
<table>
<thead>
<tr>
<th>Documento</th>
<th>DescripciÃ³n</th>
<th>Uso</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="#mod_GLOSARIO">GLOSARIO.md</a></td>
<td>Definiciones tÃ©cnicas de ML</td>
<td>Consulta</td>
</tr>
<tr>
<td><a href="#mod_RECURSOS">RECURSOS.md</a></td>
<td>Cursos y libros externos</td>
<td>Profundizar</td>
</tr>
<tr>
<td><a href="#mod_CHECKLIST">CHECKLIST.md</a></td>
<td>VerificaciÃ³n de entregables</td>
<td>Seguimiento</td>
</tr>
</tbody>
</table>
<h2>ğŸš€ Comenzar</h2>
<p><strong><a href="#mod_01_PYTHON_CIENTIFICO">â†’ MÃ³dulo 01: Python + Pandas + NumPy</a></strong></p>
<h2>ğŸ“Œ Restricciones del Proyecto</h2>
<ul>
<li>âœ… <strong>NumPy + Pandas permitidos</strong> - Herramientas reales de ML</li>
<li>âŒ <strong>Sin sklearn/tensorflow/pytorch</strong> - Algoritmos desde cero</li>
<li>âœ… <strong>100% local</strong> - Todo se ejecuta en tu mÃ¡quina</li>
<li>âœ… <strong>MatemÃ¡ticas primero</strong> - Entender antes de implementar</li>
<li>âœ… <strong>MNIST como benchmark</strong> - Dataset estÃ¡ndar de la industria</li>
</ul>
<h2>ğŸ¯ VerificaciÃ³n de Competencias del Pathway</h2>
<table>
<thead>
<tr>
<th>Curso del Pathway</th>
<th>Â¿Cubierto?</th>
<th>Evidencia en el Proyecto</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ML: Supervised Learning</strong></td>
<td>âœ…</td>
<td>Logistic Regression OvA, mÃ©tricas, CV</td>
</tr>
<tr>
<td><strong>ML: Unsupervised Algorithms</strong></td>
<td>âœ…</td>
<td>K-Means++, PCA con SVD desde cero</td>
</tr>
<tr>
<td><strong>ML: Deep Learning</strong></td>
<td>âœ…</td>
<td>MLP con Backprop + teorÃ­a CNNs</td>
</tr>
</tbody>
</table>
<h2>âœ¨ Cambios en v3.1 (vs v3.0)</h2>
<table>
<thead>
<tr>
<th>Cambio</th>
<th>RazÃ³n</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>24 semanas</strong> (antes 26)</td>
<td>Proyecto MNIST reducido a 4 sem (es dataset simple)</td>
</tr>
<tr>
<td><strong>Pandas en MÃ³dulo 01</strong></td>
<td>Necesario para cargar y limpiar datos reales</td>
</tr>
<tr>
<td><strong>Probabilidad para ML (MÃ³dulo 04)</strong></td>
<td>Bayes y MLE son esenciales para entender loss functions</td>
</tr>
<tr>
<td><strong>CNNs en MÃ³dulo 07</strong></td>
<td>El curso de Deep Learning de CU Boulder las cubre</td>
</tr>
</tbody>
</table>
<h2>âœ¨ Cambios en v3.2 (vs v3.1)</h2>
<table>
<thead>
<tr>
<th>Cambio</th>
<th>RazÃ³n</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Debugging NumPy (M01)</strong></td>
<td>5 errores comunes que causan horas de frustraciÃ³n</td>
</tr>
<tr>
<td><strong>EstÃ¡ndares Profesionales</strong></td>
<td><code>mypy</code>, <code>ruff</code>, <code>pytest</code> obligatorios desde Semana 2</td>
</tr>
<tr>
<td><strong>MetodologÃ­a Feynman</strong></td>
<td>"Reto del Tablero Blanco" en cada mÃ³dulo</td>
</tr>
<tr>
<td><strong>DerivaciÃ³n AnalÃ­tica (M05, M07)</strong></td>
<td>Simula exÃ¡menes de posgrado: derivar gradientes a mano</td>
</tr>
<tr>
<td><strong>AnÃ¡lisis Bias-Variance (M08)</strong></td>
<td>Concepto central de ML para diseÃ±o de modelos</td>
</tr>
<tr>
<td><strong>Formato Paper (M08)</strong></td>
<td>Notebook final con estructura acadÃ©mica</td>
</tr>
</tbody>
</table>
<h2>âœ¨ Cambios en v3.3 (vs v3.2)</h2>
<table>
<thead>
<tr>
<th>Cambio</th>
<th>RazÃ³n</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Gradient Checking (M03)</strong></td>
<td>ValidaciÃ³n matemÃ¡tica de derivadas (tÃ©cnica CS231n Stanford)</td>
</tr>
<tr>
<td><strong>Log-Sum-Exp Trick (M04)</strong></td>
<td>Softmax numÃ©ricamente estable (evita NaN)</td>
</tr>
<tr>
<td><strong>Shadow Mode (M05)</strong></td>
<td>Validar implementaciones vs sklearn</td>
</tr>
<tr>
<td><strong>Overfit Test (M07)</strong></td>
<td>Si no hace overfit en 10 ejemplos, tiene bug</td>
</tr>
<tr>
<td><strong>AnÃ¡lisis de Errores (M08)</strong></td>
<td>Visualizar y explicar fallos (nivel senior)</td>
</tr>
<tr>
<td><strong>Curvas de Aprendizaje (M08)</strong></td>
<td>DiagnÃ³stico grÃ¡fico de Bias-Variance</td>
</tr>
</tbody>
</table>
<h3>Nuevos Entregables v3.3</h3>
<table>
<thead>
<tr>
<th>MÃ³dulo</th>
<th>Nuevo Entregable</th>
</tr>
</thead>
<tbody>
<tr>
<td>03</td>
<td><code>grad_check.py</code> - validaciÃ³n numÃ©rica de derivadas</td>
</tr>
<tr>
<td>04</td>
<td><code>softmax</code> con log-sum-exp trick</td>
</tr>
<tr>
<td>05</td>
<td>Comparativa Shadow Mode vs sklearn</td>
</tr>
<tr>
<td>07</td>
<td><code>overfit_test.py</code> - debugging de redes</td>
</tr>
<tr>
<td>08</td>
<td>SecciÃ³n "Error Analysis" + Learning Curves</td>
</tr>
</tbody>
</table>
<blockquote>
<p>ğŸ’¡ <strong>FilosofÃ­a v3.3:</strong> Esta guÃ­a incluye <strong>validaciÃ³n matemÃ¡tica rigurosa</strong> en cada paso. No confÃ­es en que tu cÃ³digo "parece funcionar"â€”valÃ­dalo con gradient checking, shadow mode y overfit tests. Si completas v3.3, tu cÃ³digo es <strong>matemÃ¡ticamente correcto y profesionalmente validado</strong>.</p>
</blockquote>
            </div>
        
            <!-- MÃ“DULO: PLAN_ESTUDIOS.md -->
            <div class="cover-page">
                <div class="cover-content">
                    <div style="font-size:40pt;margin-bottom:20px;">ğŸ’</div>
                    <div id="mod_PLAN_ESTUDIOS" class="cover-title">PLAN DE ESTUDIOS</div>
                    <div class="cover-subtitle">MS in AI Pathway - ML Specialist v3.0</div>
                    <div class="cover-badge">MATEMÃTICAS APLICADAS A CÃ“DIGO</div>
                </div>
                <div style="position:absolute;bottom:30px;font-size:10pt;opacity:0.6;">DUQUEOM | 2025</div>
            </div>
            <div class="content">
                <h1>ğŸ“… Plan de Estudios - ML SPECIALIST v3.1</h1>
<blockquote>
<p><strong>24 Semanas | 6 horas/dÃ­a | Lunes a SÃ¡bado</strong><br />
<strong>PreparaciÃ³n para MS in AI Pathway - LÃ­nea 1: Machine Learning</strong></p>
</blockquote>
<h2>ğŸ—“ï¸ Vista General: 24 Semanas</h2>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SEMANAS 1-8        â”‚ SEMANAS 9-20       â”‚ SEMANAS 21-24                     â”‚
â”‚ FUNDAMENTOS        â”‚ ML CORE â­         â”‚ PROYECTO MNIST                    â”‚
â”‚ Python + Mate      â”‚ PATHWAY LÃNEA 1    â”‚ INTEGRACIÃ“N                       â”‚
â”‚ + Probabilidad     â”‚ Supervised +       â”‚ Pipeline End-to-End               â”‚
â”‚ MÃ³dulos 01-04      â”‚ Unsupervised + DL  â”‚ MÃ³dulo 08                         â”‚
â”‚                    â”‚ MÃ³dulos 05-07      â”‚ 4 semanas intensivas              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p><strong>DedicaciÃ³n total:</strong> 36 horas/semana Ã— 24 semanas = <strong>~864 horas</strong></p>
<h3>Los 8 MÃ³dulos Obligatorios</h3>
<table>
<thead>
<tr>
<th>Semanas</th>
<th>MÃ³dulo</th>
<th>Tema</th>
<th>Curso del Pathway</th>
</tr>
</thead>
<tbody>
<tr>
<td>1-2</td>
<td>01</td>
<td>Python + Pandas + NumPy</td>
<td>- (Fundamento)</td>
</tr>
<tr>
<td>3-5</td>
<td>02</td>
<td>Ãlgebra Lineal para ML</td>
<td>- (Fundamento)</td>
</tr>
<tr>
<td>6-7</td>
<td>03</td>
<td>CÃ¡lculo Multivariante</td>
<td>- (Fundamento)</td>
</tr>
<tr>
<td>8</td>
<td>04</td>
<td>Probabilidad para ML</td>
<td>- (Fundamento)</td>
</tr>
<tr>
<td>9-12</td>
<td>05</td>
<td>Supervised Learning</td>
<td>Introduction to ML: Supervised Learning</td>
</tr>
<tr>
<td>13-16</td>
<td>06</td>
<td>Unsupervised Learning</td>
<td>Unsupervised Algorithms in ML</td>
</tr>
<tr>
<td>17-20</td>
<td>07</td>
<td>Deep Learning + CNNs</td>
<td>Introduction to Deep Learning</td>
</tr>
<tr>
<td>21-24</td>
<td>08</td>
<td>Proyecto MNIST Analyst</td>
<td>IntegraciÃ³n de las 3 materias</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>FilosofÃ­a v3.1:</strong> "MatemÃ¡ticas Aplicadas a CÃ³digo". Pandas para datos, NumPy para matemÃ¡ticas, probabilidad para loss functions.</p>
</blockquote>
<h2>ğŸ“Œ DistribuciÃ³n Diaria TÃ­pica</h2>
<table>
<thead>
<tr>
<th>Bloque</th>
<th>Horario</th>
<th>Actividad</th>
<th>DuraciÃ³n</th>
</tr>
</thead>
<tbody>
<tr>
<td>ğŸŒ… MaÃ±ana</td>
<td>08:00 - 10:30</td>
<td>TeorÃ­a matemÃ¡tica + notaciÃ³n</td>
<td>2.5 h</td>
</tr>
<tr>
<td>â˜• Pausa</td>
<td>10:30 - 11:00</td>
<td>Descanso</td>
<td>30 min</td>
</tr>
<tr>
<td>ğŸŒ‡ MediodÃ­a</td>
<td>11:00 - 13:30</td>
<td>ImplementaciÃ³n en NumPy</td>
<td>2.5 h</td>
</tr>
<tr>
<td>ğŸŒ™ Tarde</td>
<td>15:00 - 16:00</td>
<td>Ejercicios + visualizaciÃ³n</td>
<td>1 h</td>
</tr>
</tbody>
</table>
<h1>ğŸ”· FASE 1: FUNDAMENTOS MATEMÃTICOS (Semanas 1-8)</h1>
<p><em>Objetivo: Leer notaciÃ³n matemÃ¡tica y traducirla a Python/NumPy</em></p>
<h2>ğŸ—“ï¸ SEMANA 1-2: Python + Pandas + NumPy (MÃ³dulo 01)</h2>
<p><strong>Objetivo:</strong> Dominar Pandas para datos reales + NumPy para matemÃ¡ticas<br />
<strong>Por quÃ©:</strong> En el mundo real, los datos vienen en CSVs sucios. Pandas es esencial para cargar y limpiar datos antes de aplicar ML.</p>
<h3>Semana 1: Pandas + NumPy BÃ¡sico</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>Pandas: DataFrame y Series</td>
<td><code>pd.read_csv()</code>, <code>.head()</code>, <code>.info()</code></td>
<td>Ejercicio 1.1</td>
</tr>
<tr>
<td>M</td>
<td>Limpieza de datos</td>
<td><code>dropna()</code>, <code>fillna()</code>, dtypes</td>
<td>Ejercicio 1.2</td>
</tr>
<tr>
<td>X</td>
<td>SelecciÃ³n y filtrado</td>
<td><code>.loc[]</code>, <code>.iloc[]</code>, condiciones</td>
<td>Ejercicio 1.3</td>
</tr>
<tr>
<td>J</td>
<td>NumPy: Arrays vs listas</td>
<td>Crear arrays, dtypes</td>
<td>Ejercicio 1.4</td>
</tr>
<tr>
<td>V</td>
<td>NumPy: Indexing y slicing</td>
<td>Extraer submatrices</td>
<td>Ejercicio 1.5</td>
</tr>
<tr>
<td>S</td>
<td><strong>Checkpoint</strong></td>
<td>Pandas â†’ NumPy: <code>.to_numpy()</code></td>
<td>Pipeline de carga</td>
</tr>
</tbody>
</table>
<h3>Semana 2: NumPy Vectorizado</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>Broadcasting (concepto)</td>
<td>Operaciones elemento a elemento</td>
<td>Ejercicio 1.6</td>
</tr>
<tr>
<td>M</td>
<td>np.dot, np.matmul, @</td>
<td>Producto matricial</td>
<td>Ejercicio 1.7</td>
</tr>
<tr>
<td>X</td>
<td>Reshape, flatten, transpose</td>
<td>ManipulaciÃ³n de formas</td>
<td>Ejercicio 1.8</td>
</tr>
<tr>
<td>J</td>
<td>Agregaciones y ejes</td>
<td>sum, mean, std con axis</td>
<td>Ejercicio 1.9</td>
</tr>
<tr>
<td>V</td>
<td>Random en NumPy</td>
<td>GeneraciÃ³n de datos sintÃ©ticos</td>
<td>Ejercicio 1.10</td>
</tr>
<tr>
<td>S</td>
<td><strong>Checkpoint</strong></td>
<td>Pipeline completo CSV â†’ NumPy</td>
<td>Entregable</td>
</tr>
</tbody>
</table>
<p><strong>Entregable:</strong> Script que carga CSV con Pandas, limpia datos, y convierte a NumPy para anÃ¡lisis.</p>
<p><strong>Recursos:</strong><br />
- <a href="https://pandas.pydata.org/docs/getting_started/">Pandas Getting Started</a><br />
- <a href="https://numpy.org/doc/stable/user/quickstart.html">NumPy Quickstart</a></p>
<h2>ğŸ—“ï¸ SEMANA 3-5: Ãlgebra Lineal para ML (MÃ³dulo 02)</h2>
<p><strong>Objetivo:</strong> Vectores, matrices, normas, autovectores<br />
<strong>ConexiÃ³n con Pathway:</strong> Vital para Unsupervised Learning (PCA requiere Eigenvalues) y Deep Learning (multiplicaciones de matrices)</p>
<h3>Semana 3: Vectores y Operaciones BÃ¡sicas</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>Vectores: definiciÃ³n geomÃ©trica</td>
<td>Crear vectores en NumPy</td>
<td>Ejercicio 2.1</td>
</tr>
<tr>
<td>M</td>
<td>Suma, resta de vectores</td>
<td>Implementar operaciones</td>
<td>Visualizar con matplotlib</td>
</tr>
<tr>
<td>X</td>
<td>Producto escalar (scalar mult)</td>
<td><code>c * v</code> en NumPy</td>
<td>Ejercicio 2.2</td>
</tr>
<tr>
<td>J</td>
<td>Producto punto (dot product)</td>
<td>FÃ³rmula $\vec{a} \cdot \vec{b}$</td>
<td>np.dot()</td>
</tr>
<tr>
<td>V</td>
<td>InterpretaciÃ³n geomÃ©trica</td>
<td>ProyecciÃ³n, Ã¡ngulo</td>
<td>Diagrama</td>
</tr>
<tr>
<td>S</td>
<td><strong>Repaso</strong></td>
<td>Funciones vectoriales</td>
<td>Test</td>
</tr>
</tbody>
</table>
<h3>Semana 4: Normas y Distancias</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>Norma L2 (Euclidiana)</td>
<td>$|x|_2 = \sqrt{\sum x_i^2}$</td>
<td>np.linalg.norm()</td>
</tr>
<tr>
<td>M</td>
<td>Norma L1 (Manhattan)</td>
<td>$|x|_1 = \sum</td>
<td>x_i</td>
</tr>
<tr>
<td>X</td>
<td>Distancia Euclidiana</td>
<td>$d(a,b) = |a - b|_2$</td>
<td>FunciÃ³n distancia</td>
</tr>
<tr>
<td>J</td>
<td>Distancia coseno</td>
<td>$1 - \cos(\theta)$</td>
<td>Similitud coseno</td>
</tr>
<tr>
<td>V</td>
<td>AplicaciÃ³n: KNN concepto</td>
<td>Vecino mÃ¡s cercano</td>
<td>Demo simple</td>
</tr>
<tr>
<td>S</td>
<td><strong>Repaso</strong></td>
<td>LibrerÃ­a de distancias</td>
<td>Test</td>
</tr>
</tbody>
</table>
<h3>Semana 5: Matrices y DescomposiciÃ³n</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>Matrices: suma, multiplicaciÃ³n</td>
<td>np.matmul, @</td>
<td>Ejercicio 2.3</td>
</tr>
<tr>
<td>M</td>
<td>Transpuesta, inversa</td>
<td>A.T, np.linalg.inv()</td>
<td>Ejercicio 2.4</td>
</tr>
<tr>
<td>X</td>
<td>Autovalores/Autovectores (intro)</td>
<td>QuÃ© son y por quÃ© importan</td>
<td>np.linalg.eig()</td>
</tr>
<tr>
<td>J</td>
<td>SVD (concepto)</td>
<td>DescomposiciÃ³n de matrices</td>
<td>np.linalg.svd()</td>
</tr>
<tr>
<td>V</td>
<td>AplicaciÃ³n: PCA preview</td>
<td>ReducciÃ³n dimensional</td>
<td>Demo visual</td>
</tr>
<tr>
<td>S</td>
<td><strong>Checkpoint</strong></td>
<td><code>linear_algebra.py</code> completo</td>
<td>Entregable</td>
</tr>
</tbody>
</table>
<p><strong>Entregable:</strong> LibrerÃ­a <code>linear_algebra.py</code> que implementa:<br />
- Producto punto, normas L1/L2<br />
- Distancia euclidiana y coseno<br />
- ProyecciÃ³n de vectores<br />
- Wrapper para eigenvalues</p>
<h2>ğŸ—“ï¸ SEMANA 6-7: CÃ¡lculo Multivariante (MÃ³dulo 03) [CRÃTICO]</h2>
<p><strong>Objetivo:</strong> Derivadas, gradiente, Chain Rule<br />
<strong>ConexiÃ³n con Pathway:</strong> Es el lenguaje del Deep Learning. Sin la Regla de la Cadena, no entenderÃ¡s Backpropagation.</p>
<h3>Semana 6: Derivadas, Gradiente y GD</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>Derivada: tasa de cambio</td>
<td>Calcular derivadas simples</td>
<td>Ejercicio 3.1</td>
</tr>
<tr>
<td>M</td>
<td>Derivadas parciales</td>
<td>$\frac{\partial f}{\partial x}$ para $f(x,y)$</td>
<td>Ejercicio 3.2</td>
</tr>
<tr>
<td>X</td>
<td>Gradiente: vector de parciales</td>
<td>$\nabla f$</td>
<td>Implementar</td>
</tr>
<tr>
<td>J</td>
<td>Gradient Descent (concepto)</td>
<td>Algoritmo bÃ¡sico</td>
<td>PseudocÃ³digo</td>
</tr>
<tr>
<td>V</td>
<td>Gradient Descent (cÃ³digo)</td>
<td>Minimizar $f(x,y) = x^2 + y^2$</td>
<td>Implementar</td>
</tr>
<tr>
<td>S</td>
<td><strong>Repaso</strong></td>
<td>Learning rate y convergencia</td>
<td>VisualizaciÃ³n</td>
</tr>
</tbody>
</table>
<h3>Semana 7: Chain Rule</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>Regla de la Cadena (1D)</td>
<td>$\frac{d}{dx}f(g(x))$</td>
<td>Ejercicio 3.3</td>
</tr>
<tr>
<td>M</td>
<td>Regla de la Cadena (multi)</td>
<td>ComposiciÃ³n de funciones</td>
<td>Ejercicio 3.4</td>
</tr>
<tr>
<td>X</td>
<td>AplicaciÃ³n: funciÃ³n de pÃ©rdida</td>
<td>Derivar MSE</td>
<td>Ejercicio 3.5</td>
</tr>
<tr>
<td>J</td>
<td>Preview Backpropagation</td>
<td>CÃ³mo fluyen gradientes</td>
<td>Diagrama</td>
</tr>
<tr>
<td>V</td>
<td>Derivar Cross-Entropy</td>
<td>PreparaciÃ³n para logÃ­stica</td>
<td>Ejercicio 3.6</td>
</tr>
<tr>
<td>S</td>
<td><strong>Checkpoint</strong></td>
<td>GD + Chain Rule documentado</td>
<td>Entregable</td>
</tr>
</tbody>
</table>
<p><strong>Entregable:</strong> Gradient Descent manual con visualizaciÃ³n de trayectoria.</p>
<p><strong>Recursos:</strong><br />
- <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">3Blue1Brown: Essence of Calculus</a></p>
<h2>ğŸ—“ï¸ SEMANA 8: Probabilidad para ML (MÃ³dulo 04)</h2>
<p><strong>Objetivo:</strong> Bayes, Gaussiana, MLE - lo mÃ­nimo para entender loss functions<br />
<strong>ConexiÃ³n con Pathway:</strong> Cross-Entropy viene de MLE. GMM usa Gaussianas.</p>
<h3>Semana 8: Probabilidad Esencial</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>Probabilidad bÃ¡sica</td>
<td>P(A), P(A|B)</td>
<td>Ejercicios</td>
</tr>
<tr>
<td>M</td>
<td>Teorema de Bayes</td>
<td>Prior, Likelihood, Posterior</td>
<td>Implementar</td>
</tr>
<tr>
<td>X</td>
<td>DistribuciÃ³n Gaussiana</td>
<td>PDF, Î¼, Ïƒ</td>
<td><code>gaussian_pdf()</code></td>
</tr>
<tr>
<td>J</td>
<td>Gaussiana multivariada</td>
<td>Matriz de covarianza</td>
<td>Implementar</td>
</tr>
<tr>
<td>V</td>
<td>MLE (Maximum Likelihood)</td>
<td>Por quÃ© da Cross-Entropy</td>
<td>DemostraciÃ³n</td>
</tr>
<tr>
<td>S</td>
<td><strong>Checkpoint Fase 1</strong></td>
<td><code>probability.py</code> completo</td>
<td>Entregable</td>
</tr>
</tbody>
</table>
<p><strong>Entregable:</strong> LibrerÃ­a <code>probability.py</code> con Gaussiana, MLE y softmax.</p>
<p><strong>Recursos:</strong><br />
- <a href="https://www.youtube.com/watch?v=HZGCoVF3YvM">3Blue1Brown: Bayes Theorem</a><br />
- <a href="https://www.youtube.com/watch?v=XepXtl9YKwc">StatQuest: Maximum Likelihood</a></p>
<h1>ğŸ”· FASE 2: NÃšCLEO DE MACHINE LEARNING (Semanas 9-20)</h1>
<p><em>Objetivo: Implementar desde cero los algoritmos exactos de los 3 cursos del Pathway</em></p>
<h2>ğŸ—“ï¸ SEMANA 9-12: Supervised Learning (MÃ³dulo 05)</h2>
<p><strong>Materia:</strong> Introduction to Machine Learning: Supervised Learning</p>
<h3>Semana 9: RegresiÃ³n Lineal</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>RegresiÃ³n: concepto</td>
<td>LÃ­nea de mejor ajuste</td>
<td>Visualizar datos</td>
</tr>
<tr>
<td>M</td>
<td>MÃ­nimos cuadrados</td>
<td>FÃ³rmula cerrada: $(X^TX)^{-1}X^Ty$</td>
<td>Implementar</td>
</tr>
<tr>
<td>X</td>
<td>MSE como funciÃ³n de costo</td>
<td>$J(\theta) = \frac{1}{n}\sum(y - \hat{y})^2$</td>
<td>Calcular MSE</td>
</tr>
<tr>
<td>J</td>
<td>GD para regresiÃ³n</td>
<td>Derivar gradiente de MSE</td>
<td>Implementar</td>
</tr>
<tr>
<td>V</td>
<td>RegresiÃ³n mÃºltiple</td>
<td>MÃ¡s de una feature</td>
<td>Extender cÃ³digo</td>
</tr>
<tr>
<td>S</td>
<td><strong>Repaso</strong></td>
<td><code>linear_regression.py</code> v1</td>
<td>Test</td>
</tr>
</tbody>
</table>
<h3>Semana 10: RegresiÃ³n LogÃ­stica</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>ClasificaciÃ³n binaria</td>
<td>0/1, sÃ­/no</td>
<td>Dataset simple</td>
</tr>
<tr>
<td>M</td>
<td>FunciÃ³n sigmoid</td>
<td>$\sigma(z) = \frac{1}{1+e^{-z}}$</td>
<td>Implementar</td>
</tr>
<tr>
<td>X</td>
<td>HipÃ³tesis logÃ­stica</td>
<td>$h_\theta(x) = \sigma(\theta^T x)$</td>
<td>Implementar</td>
</tr>
<tr>
<td>J</td>
<td>Cross-Entropy Loss</td>
<td>$-[y\log(\hat{y}) + (1-y)\log(1-\hat{y})]$</td>
<td>Implementar</td>
</tr>
<tr>
<td>V</td>
<td>GD para logÃ­stica</td>
<td>Derivar gradiente</td>
<td>Implementar</td>
</tr>
<tr>
<td>S</td>
<td><strong>Repaso</strong></td>
<td><code>logistic_regression.py</code></td>
<td>Test</td>
</tr>
</tbody>
</table>
<h3>Semana 11: EvaluaciÃ³n y MÃ©tricas</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>Train/Test split</td>
<td>Por quÃ© separar datos</td>
<td>Implementar split</td>
</tr>
<tr>
<td>M</td>
<td>Accuracy</td>
<td>Porcentaje correcto</td>
<td>Implementar</td>
</tr>
<tr>
<td>X</td>
<td>Precision y Recall</td>
<td>TP, FP, FN, TN</td>
<td>Implementar</td>
</tr>
<tr>
<td>J</td>
<td>F1-Score</td>
<td>Media armÃ³nica</td>
<td>Implementar</td>
</tr>
<tr>
<td>V</td>
<td>Matriz de confusiÃ³n</td>
<td>VisualizaciÃ³n</td>
<td>matplotlib</td>
</tr>
<tr>
<td>S</td>
<td><strong>Repaso</strong></td>
<td><code>metrics.py</code> completo</td>
<td>Test</td>
</tr>
</tbody>
</table>
<h3>Semana 12: ValidaciÃ³n Cruzada y RegularizaciÃ³n</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>Overfitting vs Underfitting</td>
<td>Bias-Variance tradeoff</td>
<td>Diagrama</td>
</tr>
<tr>
<td>M</td>
<td>K-Fold Cross Validation</td>
<td>ValidaciÃ³n robusta</td>
<td>Implementar</td>
</tr>
<tr>
<td>X</td>
<td>RegularizaciÃ³n L2 (Ridge)</td>
<td>$\lambda|\theta|^2$</td>
<td>AÃ±adir a regresiÃ³n</td>
</tr>
<tr>
<td>J</td>
<td>RegularizaciÃ³n L1 (Lasso)</td>
<td>$\lambda|\theta|_1$</td>
<td>Comparar</td>
</tr>
<tr>
<td>V</td>
<td>SelecciÃ³n de hiperparÃ¡metros</td>
<td>Grid search simple</td>
<td>Implementar</td>
</tr>
<tr>
<td>S</td>
<td><strong>Checkpoint</strong></td>
<td>Supervisado completo</td>
<td>Entregable</td>
</tr>
</tbody>
</table>
<p><strong>Entregable:</strong> <code>logistic_regression.py</code> desde cero usando NumPy para clasificar datos simples, con mÃ©tricas y cross-validation.</p>
<h2>ğŸ—“ï¸ SEMANA 13-16: Unsupervised Learning (MÃ³dulo 06)</h2>
<p><strong>Materia:</strong> Unsupervised Algorithms in Machine Learning</p>
<h3>Semana 13: K-Means Clustering</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>Clustering: concepto</td>
<td>Agrupar sin etiquetas</td>
<td>Visualizar clusters</td>
</tr>
<tr>
<td>M</td>
<td>Algoritmo Lloyd (K-Means)</td>
<td>Asignar, actualizar, repetir</td>
<td>PseudocÃ³digo</td>
</tr>
<tr>
<td>X</td>
<td>Implementar K-Means</td>
<td>VersiÃ³n bÃ¡sica</td>
<td>CÃ³digo</td>
</tr>
<tr>
<td>J</td>
<td>K-Means++ inicializaciÃ³n</td>
<td>Mejor selecciÃ³n de centroides</td>
<td>Implementar</td>
</tr>
<tr>
<td>V</td>
<td>Criterio de parada</td>
<td>Convergencia</td>
<td>Implementar</td>
</tr>
<tr>
<td>S</td>
<td><strong>Repaso</strong></td>
<td><code>kmeans.py</code> funcional</td>
<td>Test</td>
</tr>
</tbody>
</table>
<h3>Semana 14: EvaluaciÃ³n de Clusters</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>Inercia (within-cluster)</td>
<td>Suma de distancias al centroide</td>
<td>Implementar</td>
</tr>
<tr>
<td>M</td>
<td>MÃ©todo del codo</td>
<td>Elegir K Ã³ptimo</td>
<td>Visualizar</td>
</tr>
<tr>
<td>X</td>
<td>Silhouette Score</td>
<td>Calidad de clusters</td>
<td>Implementar</td>
</tr>
<tr>
<td>J</td>
<td>Limitaciones K-Means</td>
<td>Clusters no esfÃ©ricos</td>
<td>Ejemplos</td>
</tr>
<tr>
<td>V</td>
<td>Generar datos sintÃ©ticos</td>
<td>make_blobs equivalente</td>
<td>FunciÃ³n propia</td>
</tr>
<tr>
<td>S</td>
<td><strong>Repaso</strong></td>
<td>EvaluaciÃ³n completa</td>
<td>Documento</td>
</tr>
</tbody>
</table>
<h3>Semana 15: PCA (Principal Component Analysis)</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>ReducciÃ³n dimensional</td>
<td>Por quÃ© reducir</td>
<td>Visualizar</td>
</tr>
<tr>
<td>M</td>
<td>PCA: concepto</td>
<td>DirecciÃ³n de mÃ¡xima varianza</td>
<td>Diagrama</td>
</tr>
<tr>
<td>X</td>
<td>PCA con eigenvalues</td>
<td>Autovectores de covarianza</td>
<td>np.linalg.eig()</td>
</tr>
<tr>
<td>J</td>
<td>PCA con SVD</td>
<td>MÃ¡s estable numÃ©ricamente</td>
<td>np.linalg.svd()</td>
</tr>
<tr>
<td>V</td>
<td>Varianza explicada</td>
<td>CuÃ¡nta info se pierde</td>
<td>Implementar</td>
</tr>
<tr>
<td>S</td>
<td><strong>Repaso</strong></td>
<td><code>pca.py</code> v1</td>
<td>Test</td>
</tr>
</tbody>
</table>
<h3>Semana 16: PCA Aplicado y GMM</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>ReconstrucciÃ³n desde PCA</td>
<td>Proyectar y reconstruir</td>
<td>Implementar</td>
</tr>
<tr>
<td>M</td>
<td>CompresiÃ³n de imÃ¡genes</td>
<td>PCA para reducir</td>
<td>Demo visual</td>
</tr>
<tr>
<td>X</td>
<td>GMM (concepto)</td>
<td>Mezcla de Gaussianas</td>
<td>TeorÃ­a</td>
</tr>
<tr>
<td>J</td>
<td>EM Algorithm (intro)</td>
<td>Expectation-Maximization</td>
<td>PseudocÃ³digo</td>
</tr>
<tr>
<td>V</td>
<td>DetecciÃ³n de anomalÃ­as</td>
<td>Outliers con GMM</td>
<td>Concepto</td>
</tr>
<tr>
<td>S</td>
<td><strong>Checkpoint</strong></td>
<td>No supervisado completo</td>
<td>Entregable</td>
</tr>
</tbody>
</table>
<p><strong>Entregable:</strong> <code>kmeans.py</code> y <code>pca.py</code>. Usar PCA para comprimir una imagen y visualizar cuÃ¡nta varianza se pierde con diferentes nÃºmeros de componentes.</p>
<h2>ğŸ—“ï¸ SEMANA 17-20: Deep Learning + CNNs (MÃ³dulo 07)</h2>
<p><strong>Materia:</strong> Introduction to Deep Learning</p>
<h3>Semana 17: PerceptrÃ³n y Fundamentos</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>Neurona artificial</td>
<td>AnalogÃ­a biolÃ³gica</td>
<td>Diagrama</td>
</tr>
<tr>
<td>M</td>
<td>PerceptrÃ³n simple</td>
<td>$y = \text{sign}(w \cdot x + b)$</td>
<td>Implementar</td>
</tr>
<tr>
<td>X</td>
<td>Funciones de activaciÃ³n</td>
<td>Sigmoid, ReLU, Tanh</td>
<td>Implementar todas</td>
</tr>
<tr>
<td>J</td>
<td>LimitaciÃ³n del perceptrÃ³n</td>
<td>No puede resolver XOR</td>
<td>Demostrar</td>
</tr>
<tr>
<td>V</td>
<td>Necesidad de capas</td>
<td>Redes multicapa</td>
<td>Diagrama</td>
</tr>
<tr>
<td>S</td>
<td><strong>Repaso</strong></td>
<td><code>activations.py</code></td>
<td>Test</td>
</tr>
</tbody>
</table>
<h3>Semana 18: Forward Propagation</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>MLP: arquitectura</td>
<td>Capas ocultas</td>
<td>Diagrama</td>
</tr>
<tr>
<td>M</td>
<td>Forward pass</td>
<td>PropagaciÃ³n hacia adelante</td>
<td>PseudocÃ³digo</td>
</tr>
<tr>
<td>X</td>
<td>Implementar forward</td>
<td>Clase NeuralNetwork</td>
<td>CÃ³digo</td>
</tr>
<tr>
<td>J</td>
<td>FunciÃ³n de pÃ©rdida DL</td>
<td>Cross-entropy para multiclase</td>
<td>Softmax</td>
</tr>
<tr>
<td>V</td>
<td>InicializaciÃ³n de pesos</td>
<td>Xavier, He</td>
<td>Implementar</td>
</tr>
<tr>
<td>S</td>
<td><strong>Repaso</strong></td>
<td>Forward funcional</td>
<td>Test</td>
</tr>
</tbody>
</table>
<h3>Semana 19: CNNs - TeorÃ­a (NO implementaciÃ³n)</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>Por quÃ© CNNs para imÃ¡genes</td>
<td>Problema de MLP con imÃ¡genes</td>
<td>Diagrama</td>
</tr>
<tr>
<td>M</td>
<td>OperaciÃ³n de convoluciÃ³n</td>
<td>Kernel, filtro, feature map</td>
<td>Demo visual</td>
</tr>
<tr>
<td>X</td>
<td>Stride, padding, pooling</td>
<td>CÃ¡lculo de dimensiones output</td>
<td>Ejercicios</td>
</tr>
<tr>
<td>J</td>
<td>Arquitectura LeNet-5</td>
<td>Capas CONV + POOL + FC</td>
<td>Diagrama</td>
</tr>
<tr>
<td>V</td>
<td>Max pooling implementaciÃ³n</td>
<td>Concepto simple</td>
<td>CÃ³digo bÃ¡sico</td>
</tr>
<tr>
<td>S</td>
<td><strong>Repaso</strong></td>
<td>Quiz de dimensiones CNN</td>
<td>Test teÃ³rico</td>
</tr>
</tbody>
</table>
<h3>Semana 20: Optimizadores y Entrenamiento</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>MaÃ±ana (TeorÃ­a)</th>
<th>MediodÃ­a (CÃ³digo)</th>
<th>Tarde (PrÃ¡ctica)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>SGD (Stochastic GD)</td>
<td>Mini-batches</td>
<td>Implementar</td>
</tr>
<tr>
<td>M</td>
<td>Momentum</td>
<td>Acelerar convergencia</td>
<td>Implementar</td>
</tr>
<tr>
<td>X</td>
<td>Adam optimizer</td>
<td>Adaptive learning rate</td>
<td>Implementar</td>
</tr>
<tr>
<td>J</td>
<td>RegularizaciÃ³n DL</td>
<td>Dropout (concepto), L2</td>
<td>Implementar L2</td>
</tr>
<tr>
<td>V</td>
<td>Training loop completo</td>
<td>Epochs, batches, logging</td>
<td>Implementar</td>
</tr>
<tr>
<td>S</td>
<td><strong>Checkpoint</strong></td>
<td>MLP resuelve XOR</td>
<td>Entregable</td>
</tr>
</tbody>
</table>
<p><strong>Entregable:</strong> <code>neural_net.py</code> - Una red neuronal que resuelve el problema XOR y clasifica dÃ­gitos simples, implementando <code>backward()</code> manualmente.</p>
<h1>ğŸ”· FASE 3: PROYECTO FINAL "MNIST ANALYST" (Semanas 21-24)</h1>
<p><em>Objetivo: Un proyecto intensivo de 4 semanas que demuestra competencia en las 3 Ã¡reas</em></p>
<p><strong>Dataset:</strong> MNIST (imÃ¡genes de 28x28 pÃ­xeles de dÃ­gitos escritos a mano)</p>
<blockquote>
<p>ğŸ’¡ <strong>v3.1:</strong> MNIST es un dataset simple (solo 10 clases, imÃ¡genes pequeÃ±as). 4 semanas son suficientes.</p>
</blockquote>
<h2>ğŸ—“ï¸ SEMANA 21: EDA + No Supervisado</h2>
<p><strong>Materia demostrada:</strong> Unsupervised Algorithms in Machine Learning</p>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>Actividad</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>Cargar MNIST, entender estructura (784 dimensiones)</td>
</tr>
<tr>
<td>M</td>
<td>Implementar PCA desde cero, reducir a 2-3 componentes</td>
</tr>
<tr>
<td>X</td>
<td>Visualizar dÃ­gitos en grÃ¡fico 2D</td>
</tr>
<tr>
<td>J</td>
<td>Implementar K-Means, agrupar dÃ­gitos SIN etiquetas</td>
</tr>
<tr>
<td>V</td>
<td>Visualizar centroides como imÃ¡genes 28x28</td>
</tr>
<tr>
<td>S</td>
<td><strong>Checkpoint:</strong> Notebook PCA + K-Means</td>
</tr>
</tbody>
</table>
<p><strong>Entregable:</strong> Jupyter notebook con PCA 2D y K-Means clustering.</p>
<h2>ğŸ—“ï¸ SEMANA 22: ClasificaciÃ³n Supervisada</h2>
<p><strong>Materia demostrada:</strong> Introduction to ML: Supervised Learning</p>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>Actividad</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>Train/test split, normalizaciÃ³n</td>
</tr>
<tr>
<td>M</td>
<td>Implementar Logistic Regression One-vs-All (10 clasificadores)</td>
</tr>
<tr>
<td>X</td>
<td>Entrenar y medir Accuracy global</td>
</tr>
<tr>
<td>J</td>
<td>Precision, Recall, F1, matriz de confusiÃ³n</td>
</tr>
<tr>
<td>V</td>
<td>Visualizar errores (imÃ¡genes mal clasificadas)</td>
</tr>
<tr>
<td>S</td>
<td><strong>Checkpoint:</strong> Logistic Regression completo</td>
</tr>
</tbody>
</table>
<p><strong>Entregable:</strong> <code>logistic_mnist.py</code> con mÃ©tricas completas.</p>
<h2>ğŸ—“ï¸ SEMANA 23: Deep Learning</h2>
<p><strong>Materia demostrada:</strong> Introduction to Deep Learning</p>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>Actividad</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>DiseÃ±ar arquitectura MLP (784â†’128â†’64â†’10)</td>
</tr>
<tr>
<td>M</td>
<td>Implementar forward pass + softmax</td>
</tr>
<tr>
<td>X</td>
<td>Implementar backprop con cross-entropy</td>
</tr>
<tr>
<td>J</td>
<td>Training loop con mini-batches</td>
</tr>
<tr>
<td>V</td>
<td>Entrenar y ajustar hiperparÃ¡metros</td>
</tr>
<tr>
<td>S</td>
<td><strong>Checkpoint:</strong> MLP funcional &gt;90% accuracy</td>
</tr>
</tbody>
</table>
<p><strong>Entregable:</strong> <code>neural_network_mnist.py</code> con backprop manual.</p>
<h2>ğŸ—“ï¸ SEMANA 24: Benchmark + Informe Final</h2>
<p><strong>Objetivo:</strong> Comparar modelos y documentar</p>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>Actividad</th>
</tr>
</thead>
<tbody>
<tr>
<td>L</td>
<td>Comparar rendimiento: Logistic vs MLP</td>
</tr>
<tr>
<td>M</td>
<td>AnÃ¡lisis: Â¿por quÃ© MLP es mejor? (no linealidad)</td>
</tr>
<tr>
<td>X</td>
<td>Escribir MODEL_COMPARISON.md</td>
</tr>
<tr>
<td>J</td>
<td>Crear README.md profesional (inglÃ©s)</td>
</tr>
<tr>
<td>V</td>
<td>Demo final: Jupyter notebook completo</td>
</tr>
<tr>
<td>S</td>
<td><strong>Entrega final + AutoevaluaciÃ³n</strong></td>
</tr>
</tbody>
</table>
<p><strong>Entregable Final:</strong> </p>
<pre><code>mnist-analyst/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ linear_algebra.py
â”‚   â”œâ”€â”€ probability.py
â”‚   â”œâ”€â”€ pca.py
â”‚   â”œâ”€â”€ kmeans.py
â”‚   â”œâ”€â”€ logistic_regression.py
â”‚   â”œâ”€â”€ neural_network.py
â”‚   â””â”€â”€ mnist_pipeline.py
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda_pca_kmeans.ipynb
â”‚   â”œâ”€â”€ 02_logistic_classification.ipynb
â”‚   â””â”€â”€ 03_neural_network_benchmark.ipynb
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ MODEL_COMPARISON.md
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_*.py
â””â”€â”€ README.md
</code></pre>
<h2>âœ… Checklist de FinalizaciÃ³n - ML SPECIALIST v3.1</h2>
<h3>Fase 1: Fundamentos (MÃ³dulos 01-04)</h3>
<ul>
<li>[ ] Python + Pandas + NumPy dominado</li>
<li>[ ] Ãlgebra lineal: normas, distancias, SVD, eigenvalues</li>
<li>[ ] CÃ¡lculo: gradientes, chain rule, gradient descent</li>
<li>[ ] Probabilidad: Bayes, Gaussiana, MLE, softmax</li>
</ul>
<h3>Fase 2: ML Core (MÃ³dulos 05-07) â­ PATHWAY</h3>
<ul>
<li>[ ] <strong>Supervised (05):</strong> Logistic Regression con mÃ©tricas</li>
<li>[ ] <strong>Unsupervised (06):</strong> K-Means y PCA desde cero</li>
<li>[ ] <strong>Deep Learning (07):</strong> MLP con backprop + teorÃ­a CNNs</li>
</ul>
<h3>Fase 3: Proyecto MNIST (MÃ³dulo 08)</h3>
<ul>
<li>[ ] PCA reduce MNIST a 2D con visualizaciÃ³n</li>
<li>[ ] K-Means agrupa dÃ­gitos sin etiquetas</li>
<li>[ ] Logistic Regression clasifica con &gt;85% accuracy</li>
<li>[ ] MLP supera a Logistic con &gt;90% accuracy</li>
<li>[ ] MODEL_COMPARISON.md explica matemÃ¡ticamente las diferencias</li>
<li>[ ] README.md profesional en inglÃ©s</li>
</ul>
<h3>VerificaciÃ³n Final</h3>
<ul>
<li>[ ] Puedo explicar matemÃ¡ticamente por quÃ© funciona cada algoritmo</li>
<li>[ ] Puedo derivar las fÃ³rmulas de gradiente a mano</li>
<li>[ ] Puedo implementar desde cero sin copiar cÃ³digo</li>
<li>[ ] Entiendo convoluciÃ³n, stride, padding, pooling para CNNs</li>
<li>[ ] Listo para los 3 cursos del Pathway LÃ­nea 1</li>
</ul>
<h2>ğŸ“š Recursos Recomendados</h2>
<h3>MatemÃ¡ticas</h3>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">3Blue1Brown: Linear Algebra</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">3Blue1Brown: Calculus</a></li>
</ul>
<h3>Machine Learning</h3>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU">Stanford CS229</a></li>
<li><a href="https://www.coursera.org/learn/machine-learning">Coursera: Machine Learning (Andrew Ng)</a></li>
</ul>
<h3>Deep Learning</h3>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">3Blue1Brown: Neural Networks</a></li>
<li><a href="http://cs231n.stanford.edu/">CS231n: CNNs for Visual Recognition</a></li>
</ul>
<blockquote>
<p>ğŸ’¡ <strong>FilosofÃ­a v3.1:</strong> Esta guÃ­a te lleva de Python bÃ¡sico a candidato competitivo del MS in AI en exactamente 6 meses (24 semanas). Si puedes implementar PCA, K-Means, Logistic Regression y un MLP desde cero sobre MNIST, y entiendes la teorÃ­a de CNNs, <strong>dominas la LÃ­nea 1 del Pathway</strong>.</p>
</blockquote>
            </div>
        
            <!-- MÃ“DULO: 01_PYTHON_CIENTIFICO.md -->
            <div class="cover-page">
                <div class="cover-content">
                    <div style="font-size:40pt;margin-bottom:20px;">ğŸ’</div>
                    <div id="mod_01_PYTHON_CIENTIFICO" class="cover-title">MÃ“DULO 01 - PYTHON + PANDAS + NUMPY</div>
                    <div class="cover-subtitle">MS in AI Pathway - ML Specialist v3.0</div>
                    <div class="cover-badge">MATEMÃTICAS APLICADAS A CÃ“DIGO</div>
                </div>
                <div style="position:absolute;bottom:30px;font-size:10pt;opacity:0.6;">DUQUEOM | 2025</div>
            </div>
            <div class="content">
                <h1>MÃ³dulo 01 - Python CientÃ­fico + Pandas</h1>
<blockquote>
<p><strong>ğŸ¯ Objetivo:</strong> Dominar Pandas para datos + NumPy para matemÃ¡ticas<br />
<strong>Fase:</strong> 1 - Fundamentos | <strong>Semanas 1-2</strong><br />
<strong>Prerrequisitos:</strong> Python bÃ¡sico (variables, funciones, listas, loops)</p>
</blockquote>
<h2>ğŸ§  Â¿Por QuÃ© Este MÃ³dulo?</h2>
<h3>El Problema con Python Puro para ML</h3>
<pre><code class="language-python"># âŒ AsÃ­ NO se hace en Machine Learning
def dot_product_slow(a: list, b: list) -&gt; float:
    &quot;&quot;&quot;Producto punto con loop - LENTO.&quot;&quot;&quot;
    result = 0
    for i in range(len(a)):
        result += a[i] * b[i]
    return result

# Para vectores de 1 millÃ³n de elementos:
# Tiempo: ~200ms
</code></pre>
<pre><code class="language-python"># âœ… AsÃ­ SÃ se hace en Machine Learning
import numpy as np

def dot_product_fast(a: np.ndarray, b: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Producto punto vectorizado - RÃPIDO.&quot;&quot;&quot;
    return np.dot(a, b)

# Para vectores de 1 millÃ³n de elementos:
# Tiempo: ~2ms (100x mÃ¡s rÃ¡pido)
</code></pre>
<h3>ConexiÃ³n con el Pathway</h3>
<p>En los cursos de CU Boulder:<br />
- <strong>Supervised Learning:</strong> Multiplicaciones de matrices para regresiÃ³n<br />
- <strong>Unsupervised Learning:</strong> PCA requiere descomposiciÃ³n de matrices<br />
- <strong>Deep Learning:</strong> Forward/backward pass son operaciones matriciales</p>
<p><strong>Sin NumPy, no puedes hacer ML eficiente.</strong></p>
<h2>ğŸ“š Contenido del MÃ³dulo</h2>
<h3>Semana 1: Pandas + NumPy BÃ¡sico</h3>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DÃA 1: Pandas - DataFrame y Series                             â”‚
â”‚  DÃA 2: Pandas - Carga de CSVs (read_csv, head, info)           â”‚
â”‚  DÃA 3: Pandas - Limpieza (dropna, fillna, dtypes)              â”‚
â”‚  DÃA 4: NumPy - Arrays y dtypes                                 â”‚
â”‚  DÃA 5: NumPy - Indexing y Slicing                              â”‚
â”‚  DÃA 6: Pandas â†’ NumPy (df.values, df.to_numpy())               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h3>Semana 2: NumPy Vectorizado</h3>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DÃA 1: Broadcasting                                            â”‚
â”‚  DÃA 2: Producto matricial (@, np.dot, np.matmul)               â”‚
â”‚  DÃA 3: Reshape, flatten, transpose                             â”‚
â”‚  DÃA 4: Agregaciones y operaciones con ejes                     â”‚
â”‚  DÃA 5: Random y generaciÃ³n de datos sintÃ©ticos                 â”‚
â”‚  DÃA 6: Entregable: Pipeline Pandas â†’ NumPy                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h2>ğŸ’» Conceptos Clave</h2>
<h3>0. Pandas Esencial (DÃ­as 1-3)</h3>
<h4>Â¿Por QuÃ© Pandas?</h4>
<p>En el mundo real de ML, los datos vienen en CSVs sucios, no en arrays NumPy perfectos. Antes de aplicar cualquier algoritmo necesitas:</p>
<ol>
<li><strong>Cargar datos</strong> desde archivos</li>
<li><strong>Explorar</strong> estructura y tipos</li>
<li><strong>Limpiar</strong> valores faltantes y errores</li>
<li><strong>Convertir</strong> a NumPy para el modelo</li>
</ol>
<pre><code class="language-python">import pandas as pd
import numpy as np

# ========== CARGA DE DATOS ==========
# Cargar CSV
df = pd.read_csv('data/iris.csv')

# Primeras filas
print(df.head())

# InformaciÃ³n del DataFrame
print(df.info())
#  Column         Non-Null Count  Dtype  
# ---  ------         --------------  -----  
#  0   sepal_length   150 non-null    float64
#  1   sepal_width    150 non-null    float64
#  2   petal_length   150 non-null    float64
#  3   petal_width    150 non-null    float64
#  4   species        150 non-null    object 

# EstadÃ­sticas bÃ¡sicas
print(df.describe())
</code></pre>
<h4>Limpieza de Datos</h4>
<pre><code class="language-python">import pandas as pd

# Crear DataFrame con datos sucios
df = pd.DataFrame({
    'edad': [25, 30, None, 45, 50],
    'salario': [50000, 60000, 70000, None, 90000],
    'ciudad': ['Madrid', 'Barcelona', 'Madrid', 'Sevilla', None]
})

# ========== DETECTAR NULOS ==========
print(df.isnull().sum())
# edad       1
# salario    1
# ciudad     1

# ========== ELIMINAR FILAS CON NULOS ==========
df_clean = df.dropna()  # Elimina filas con cualquier nulo
print(f&quot;Filas despuÃ©s de dropna: {len(df_clean)}&quot;)  # 2

# ========== RELLENAR NULOS ==========
df_filled = df.copy()
df_filled['edad'] = df_filled['edad'].fillna(df_filled['edad'].mean())
df_filled['salario'] = df_filled['salario'].fillna(df_filled['salario'].median())
df_filled['ciudad'] = df_filled['ciudad'].fillna('Desconocido')

print(df_filled)
</code></pre>
<h4>SelecciÃ³n y Filtrado</h4>
<pre><code class="language-python">import pandas as pd

df = pd.read_csv('data/iris.csv')

# ========== SELECCIONAR COLUMNAS ==========
# Una columna (Serie)
sepal_length = df['sepal_length']

# MÃºltiples columnas (DataFrame)
features = df[['sepal_length', 'sepal_width']]

# ========== FILTRAR FILAS ==========
# CondiciÃ³n simple
setosa = df[df['species'] == 'setosa']

# MÃºltiples condiciones
large_setosa = df[(df['species'] == 'setosa') &amp; (df['sepal_length'] &gt; 5)]

# ========== LOC e ILOC ==========
# loc: por etiquetas
df.loc[0:5, ['sepal_length', 'species']]

# iloc: por posiciÃ³n (como NumPy)
df.iloc[0:5, 0:2]
</code></pre>
<h4>De Pandas a NumPy (DÃ­a 6)</h4>
<pre><code class="language-python">import pandas as pd
import numpy as np

df = pd.read_csv('data/iris.csv')

# ========== SEPARAR FEATURES Y TARGET ==========
# Features (X) - todas las columnas numÃ©ricas
X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].to_numpy()
print(f&quot;X shape: {X.shape}&quot;)  # (150, 4)
print(f&quot;X dtype: {X.dtype}&quot;)  # float64

# Target (y) - convertir categorÃ­as a nÃºmeros
y = df['species'].map({'setosa': 0, 'versicolor': 1, 'virginica': 2}).to_numpy()
print(f&quot;y shape: {y.shape}&quot;)  # (150,)

# ========== VERIFICAR ==========
print(f&quot;Tipo X: {type(X)}&quot;)  # &lt;class 'numpy.ndarray'&gt;
print(f&quot;Tipo y: {type(y)}&quot;)  # &lt;class 'numpy.ndarray'&gt;

# Ahora X e y estÃ¡n listos para algoritmos de ML
</code></pre>
<h3>1. Arrays vs Listas</h3>
<pre><code class="language-python">import numpy as np

# Lista de Python
lista = [1, 2, 3, 4, 5]

# Array de NumPy
array = np.array([1, 2, 3, 4, 5])

# Diferencias clave:
# 1. Tipo homogÃ©neo (todos los elementos del mismo tipo)
# 2. TamaÃ±o fijo despuÃ©s de creaciÃ³n
# 3. Operaciones vectorizadas
# 4. Almacenamiento contiguo en memoria
</code></pre>
<h3>2. CreaciÃ³n de Arrays</h3>
<pre><code class="language-python">import numpy as np

# Desde lista
a = np.array([1, 2, 3])

# Arrays especiales
zeros = np.zeros((3, 4))        # Matriz 3x4 de ceros
ones = np.ones((2, 3))          # Matriz 2x3 de unos
identity = np.eye(4)            # Matriz identidad 4x4
random = np.random.randn(3, 3)  # Matriz 3x3 valores normales

# Secuencias
rango = np.arange(0, 10, 2)     # [0, 2, 4, 6, 8]
linspace = np.linspace(0, 1, 5) # [0, 0.25, 0.5, 0.75, 1]

print(f&quot;Shape de zeros: {zeros.shape}&quot;)  # (3, 4)
print(f&quot;Dtype de zeros: {zeros.dtype}&quot;)  # float64
</code></pre>
<h3>3. Indexing y Slicing</h3>
<pre><code class="language-python">import numpy as np

# Crear matriz 2D
matrix = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
])

# Acceso a elementos
print(matrix[0, 0])      # 1 (fila 0, columna 0)
print(matrix[1, 2])      # 6 (fila 1, columna 2)

# Slicing
print(matrix[0, :])      # [1, 2, 3] (toda la fila 0)
print(matrix[:, 1])      # [2, 5, 8] (toda la columna 1)
print(matrix[0:2, 1:3])  # [[2, 3], [5, 6]] (submatriz)

# Indexing booleano
print(matrix[matrix &gt; 5])  # [6, 7, 8, 9]
</code></pre>
<h3>4. Broadcasting</h3>
<pre><code class="language-python">import numpy as np

# Broadcasting: operar arrays de diferentes shapes

# Escalar + Array
a = np.array([1, 2, 3])
print(a + 10)  # [11, 12, 13]

# Vector + Matriz (broadcasting automÃ¡tico)
matrix = np.array([
    [1, 2, 3],
    [4, 5, 6]
])
vector = np.array([10, 20, 30])

# El vector se &quot;expande&quot; para coincidir con la matriz
print(matrix + vector)
# [[11, 22, 33],
#  [14, 25, 36]]

# Regla de broadcasting:
# Las dimensiones deben ser iguales O una de ellas debe ser 1
</code></pre>
<h3>5. Agregaciones y Ejes</h3>
<pre><code class="language-python">import numpy as np

matrix = np.array([
    [1, 2, 3],
    [4, 5, 6]
])

# Agregaciones globales
print(np.sum(matrix))   # 21 (suma de todos)
print(np.mean(matrix))  # 3.5 (promedio de todos)
print(np.std(matrix))   # 1.707... (desviaciÃ³n estÃ¡ndar)

# Agregaciones por eje
# axis=0: colapsar filas (operar columnas)
print(np.sum(matrix, axis=0))  # [5, 7, 9]

# axis=1: colapsar columnas (operar filas)
print(np.sum(matrix, axis=1))  # [6, 15]

# VisualizaciÃ³n de ejes:
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ axis=0 â†“    â”‚
# â”‚ [1, 2, 3]   â”‚ â†’ axis=1
# â”‚ [4, 5, 6]   â”‚ â†’ axis=1
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h3>6. Operaciones Matriciales</h3>
<pre><code class="language-python">import numpy as np

A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# Operaciones elemento a elemento
print(A + B)   # Suma
print(A * B)   # MultiplicaciÃ³n elemento a elemento (Hadamard)
print(A / B)   # DivisiÃ³n elemento a elemento

# Producto matricial (lo que usarÃ¡s en ML)
print(A @ B)           # Operador @ (Python 3.5+)
print(np.matmul(A, B)) # FunciÃ³n matmul
print(np.dot(A, B))    # FunciÃ³n dot

# Resultado:
# [[19, 22],
#  [43, 50]]

# Transpuesta
print(A.T)
# [[1, 3],
#  [2, 4]]
</code></pre>
<h3>7. VectorizaciÃ³n: Eliminar Loops</h3>
<pre><code class="language-python">import numpy as np

# âŒ CON LOOP (lento)
def normalize_loop(data: list) -&gt; list:
    &quot;&quot;&quot;Normalizar datos con loop.&quot;&quot;&quot;
    mean = sum(data) / len(data)
    std = (sum((x - mean)**2 for x in data) / len(data)) ** 0.5
    return [(x - mean) / std for x in data]

# âœ… VECTORIZADO (rÃ¡pido)
def normalize_vectorized(data: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;Normalizar datos vectorizado.&quot;&quot;&quot;
    return (data - np.mean(data)) / np.std(data)

# Ejemplo
data = np.random.randn(1000000)

# La versiÃ³n vectorizada es ~100x mÃ¡s rÃ¡pida
normalized = normalize_vectorized(data)
</code></pre>
<h3>8. Funciones Universales (ufuncs)</h3>
<pre><code class="language-python">import numpy as np

x = np.array([1, 2, 3, 4, 5])

# Funciones matemÃ¡ticas (aplicadas elemento a elemento)
print(np.exp(x))      # e^x
print(np.log(x))      # ln(x)
print(np.sqrt(x))     # âˆšx
print(np.sin(x))      # sin(x)

# Importante para ML:
# Sigmoid: Ïƒ(x) = 1 / (1 + e^(-x))
def sigmoid(x: np.ndarray) -&gt; np.ndarray:
    return 1 / (1 + np.exp(-x))

# ReLU: max(0, x)
def relu(x: np.ndarray) -&gt; np.ndarray:
    return np.maximum(0, x)

print(sigmoid(np.array([-2, -1, 0, 1, 2])))
# [0.119, 0.269, 0.5, 0.731, 0.881]
</code></pre>
<h3>9. Reshape y ManipulaciÃ³n de Forma</h3>
<pre><code class="language-python">import numpy as np

# Crear array 1D
a = np.arange(12)  # [0, 1, 2, ..., 11]

# Reshape a 2D
matrix = a.reshape(3, 4)
print(matrix.shape)  # (3, 4)
# [[ 0,  1,  2,  3],
#  [ 4,  5,  6,  7],
#  [ 8,  9, 10, 11]]

# Reshape a 3D
tensor = a.reshape(2, 2, 3)
print(tensor.shape)  # (2, 2, 3)

# Flatten: volver a 1D
flat = matrix.flatten()
print(flat.shape)  # (12,)

# -1 para inferir dimensiÃ³n automÃ¡ticamente
auto = a.reshape(4, -1)  # (4, 3)
auto = a.reshape(-1, 6)  # (2, 6)
</code></pre>
<h3>10. GeneraciÃ³n de Datos Aleatorios</h3>
<pre><code class="language-python">import numpy as np

# Fijar semilla para reproducibilidad
np.random.seed(42)

# DistribuciÃ³n uniforme [0, 1)
uniform = np.random.rand(3, 3)

# DistribuciÃ³n normal (media=0, std=1)
normal = np.random.randn(3, 3)

# DistribuciÃ³n normal personalizada
custom_normal = np.random.normal(loc=5, scale=2, size=(100,))

# Enteros aleatorios
integers = np.random.randint(0, 10, size=(3, 3))

# Shuffle (mezclar)
data = np.arange(10)
np.random.shuffle(data)

# Muestreo sin reemplazo
sample = np.random.choice(data, size=5, replace=False)
</code></pre>
<h2>ğŸ“Š Type Hints con NumPy</h2>
<pre><code class="language-python">import numpy as np
from numpy.typing import NDArray

# Type hints para arrays
def normalize(data: NDArray[np.float64]) -&gt; NDArray[np.float64]:
    &quot;&quot;&quot;Normaliza un array de floats.&quot;&quot;&quot;
    return (data - np.mean(data)) / np.std(data)

# Type hints genÃ©ricos
def dot_product(a: np.ndarray, b: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Calcula el producto punto de dos vectores.&quot;&quot;&quot;
    return float(np.dot(a, b))

# Con mypy
# pip install numpy-stubs
</code></pre>
<h2>âš¡ Benchmark: Lista vs NumPy</h2>
<pre><code class="language-python">import numpy as np
import time
from typing import List

def benchmark_dot_product():
    &quot;&quot;&quot;Compara rendimiento de lista vs NumPy.&quot;&quot;&quot;
    size = 1_000_000

    # Crear datos
    list_a: List[float] = [float(i) for i in range(size)]
    list_b: List[float] = [float(i) for i in range(size)]
    array_a = np.array(list_a)
    array_b = np.array(list_b)

    # Benchmark lista
    start = time.time()
    result_list = sum(a * b for a, b in zip(list_a, list_b))
    time_list = time.time() - start

    # Benchmark NumPy
    start = time.time()
    result_numpy = np.dot(array_a, array_b)
    time_numpy = time.time() - start

    print(f&quot;Lista:  {time_list:.4f}s&quot;)
    print(f&quot;NumPy:  {time_numpy:.4f}s&quot;)
    print(f&quot;Speedup: {time_list/time_numpy:.1f}x&quot;)

    # Verificar resultados iguales
    assert abs(result_list - result_numpy) &lt; 1e-6

if __name__ == &quot;__main__&quot;:
    benchmark_dot_product()

# Output tÃ­pico:
# Lista:  0.1523s
# NumPy:  0.0015s
# Speedup: 101.5x
</code></pre>
<h2>ğŸ¯ Ejercicios</h2>
<h3>Ejercicio 1.1: Crear Arrays</h3>
<pre><code class="language-python"># Crear:
# 1. Vector de 10 ceros
# 2. Matriz 3x3 de unos
# 3. Matriz identidad 4x4
# 4. Vector de 0 a 99
# 5. 20 valores equiespaciados entre 0 y 2Ï€
</code></pre>
<h3>Ejercicio 1.2: Indexing</h3>
<pre><code class="language-python"># Dada la matriz:
matrix = np.arange(20).reshape(4, 5)

# Extraer:
# 1. Elemento en fila 2, columna 3
# 2. Toda la fila 1
# 3. Toda la columna 4
# 4. Submatriz filas 1-2, columnas 2-4
# 5. Elementos mayores que 10
</code></pre>
<h3>Ejercicio 1.3: Broadcasting</h3>
<pre><code class="language-python"># Sin usar loops:
# 1. Sumar 100 a cada elemento de una matriz 3x3
# 2. Multiplicar cada fila por un vector diferente
# 3. Normalizar cada columna (restar media, dividir por std)
</code></pre>
<h3>Ejercicio 1.4: VectorizaciÃ³n</h3>
<pre><code class="language-python"># Reescribir sin loops:
def euclidean_distance_loop(a: list, b: list) -&gt; float:
    total = 0
    for i in range(len(a)):
        total += (a[i] - b[i]) ** 2
    return total ** 0.5

# Tu versiÃ³n vectorizada:
def euclidean_distance_vectorized(a: np.ndarray, b: np.ndarray) -&gt; float:
    pass  # Implementar
</code></pre>
<h3>Ejercicio 1.5: Funciones de ActivaciÃ³n</h3>
<pre><code class="language-python"># Implementar las siguientes funciones de activaciÃ³n:

def sigmoid(x: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;Ïƒ(x) = 1 / (1 + e^(-x))&quot;&quot;&quot;
    pass

def relu(x: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;ReLU(x) = max(0, x)&quot;&quot;&quot;
    pass

def softmax(x: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;softmax(x)_i = e^(x_i) / Î£ e^(x_j)&quot;&quot;&quot;
    pass

# Verificar:
# sigmoid(0) â‰ˆ 0.5
# relu(-5) = 0, relu(5) = 5
# softmax([1,2,3]).sum() â‰ˆ 1.0
</code></pre>
<h2>ğŸ“¦ Entregable del MÃ³dulo</h2>
<h3>Script: <code>benchmark_vectorization.py</code></h3>
<pre><code class="language-python">&quot;&quot;&quot;
Benchmark: Operaciones vectoriales Lista vs NumPy

Este script compara el rendimiento de operaciones comunes
usando listas de Python puras vs arrays de NumPy.

Operaciones comparadas:
1. Producto punto
2. NormalizaciÃ³n
3. Distancia euclidiana
4. Suma de matrices

Autor: [Tu nombre]
Fecha: [Fecha]
&quot;&quot;&quot;

import numpy as np
import time
from typing import List, Tuple, Callable
from dataclasses import dataclass


@dataclass
class BenchmarkResult:
    &quot;&quot;&quot;Resultado de un benchmark.&quot;&quot;&quot;
    operation: str
    time_list: float
    time_numpy: float
    speedup: float


def benchmark(
    func_list: Callable,
    func_numpy: Callable,
    args_list: Tuple,
    args_numpy: Tuple,
    operation_name: str,
    iterations: int = 100
) -&gt; BenchmarkResult:
    &quot;&quot;&quot;Ejecuta benchmark comparativo.&quot;&quot;&quot;

    # Benchmark lista
    start = time.time()
    for _ in range(iterations):
        func_list(*args_list)
    time_list = (time.time() - start) / iterations

    # Benchmark NumPy
    start = time.time()
    for _ in range(iterations):
        func_numpy(*args_numpy)
    time_numpy = (time.time() - start) / iterations

    return BenchmarkResult(
        operation=operation_name,
        time_list=time_list,
        time_numpy=time_numpy,
        speedup=time_list / time_numpy
    )


# === IMPLEMENTAR TUS FUNCIONES AQUÃ ===

def dot_product_list(a: List[float], b: List[float]) -&gt; float:
    &quot;&quot;&quot;Producto punto con listas.&quot;&quot;&quot;
    # TODO: Implementar
    pass


def dot_product_numpy(a: np.ndarray, b: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Producto punto con NumPy.&quot;&quot;&quot;
    # TODO: Implementar
    pass


def normalize_list(data: List[float]) -&gt; List[float]:
    &quot;&quot;&quot;Normalizar con listas.&quot;&quot;&quot;
    # TODO: Implementar
    pass


def normalize_numpy(data: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;Normalizar con NumPy.&quot;&quot;&quot;
    # TODO: Implementar
    pass


def euclidean_distance_list(a: List[float], b: List[float]) -&gt; float:
    &quot;&quot;&quot;Distancia euclidiana con listas.&quot;&quot;&quot;
    # TODO: Implementar
    pass


def euclidean_distance_numpy(a: np.ndarray, b: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Distancia euclidiana con NumPy.&quot;&quot;&quot;
    # TODO: Implementar
    pass


def matrix_sum_list(A: List[List[float]], B: List[List[float]]) -&gt; List[List[float]]:
    &quot;&quot;&quot;Suma de matrices con listas.&quot;&quot;&quot;
    # TODO: Implementar
    pass


def matrix_sum_numpy(A: np.ndarray, B: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;Suma de matrices con NumPy.&quot;&quot;&quot;
    # TODO: Implementar
    pass


def main():
    &quot;&quot;&quot;Ejecutar todos los benchmarks.&quot;&quot;&quot;
    size = 10000

    # Crear datos de prueba
    list_a = [float(i) for i in range(size)]
    list_b = [float(i) for i in range(size)]
    array_a = np.array(list_a)
    array_b = np.array(list_b)

    matrix_size = 100
    list_matrix_a = [[float(i*j) for j in range(matrix_size)] 
                     for i in range(matrix_size)]
    list_matrix_b = [[float(i+j) for j in range(matrix_size)] 
                     for i in range(matrix_size)]
    array_matrix_a = np.array(list_matrix_a)
    array_matrix_b = np.array(list_matrix_b)

    # Ejecutar benchmarks
    results = []

    results.append(benchmark(
        dot_product_list, dot_product_numpy,
        (list_a, list_b), (array_a, array_b),
        &quot;Producto Punto&quot;
    ))

    results.append(benchmark(
        normalize_list, normalize_numpy,
        (list_a,), (array_a,),
        &quot;NormalizaciÃ³n&quot;
    ))

    results.append(benchmark(
        euclidean_distance_list, euclidean_distance_numpy,
        (list_a, list_b), (array_a, array_b),
        &quot;Distancia Euclidiana&quot;
    ))

    results.append(benchmark(
        matrix_sum_list, matrix_sum_numpy,
        (list_matrix_a, list_matrix_b), (array_matrix_a, array_matrix_b),
        &quot;Suma de Matrices&quot;
    ))

    # Mostrar resultados
    print(&quot;\n&quot; + &quot;=&quot;*60)
    print(&quot;BENCHMARK: Lista vs NumPy&quot;)
    print(&quot;=&quot;*60)
    print(f&quot;{'OperaciÃ³n':&lt;25} {'Lista (ms)':&lt;12} {'NumPy (ms)':&lt;12} {'Speedup':&lt;10}&quot;)
    print(&quot;-&quot;*60)

    for r in results:
        print(f&quot;{r.operation:&lt;25} {r.time_list*1000:&lt;12.4f} {r.time_numpy*1000:&lt;12.4f} {r.speedup:&lt;10.1f}x&quot;)

    print(&quot;=&quot;*60)
    print(f&quot;\nSpeedup promedio: {sum(r.speedup for r in results)/len(results):.1f}x&quot;)


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<h2>ğŸ› Debugging NumPy: Errores que te HarÃ¡n Perder el Tiempo (v3.2)</h2>
<blockquote>
<p>âš ï¸ <strong>CRÃTICO:</strong> Estos 5 errores son los mÃ¡s frecuentes en las Fases 1 y 2. Resolverlos ahora previene horas de frustraciÃ³n.</p>
</blockquote>
<h3>Error 1: Shape Mismatch - <code>(5,)</code> vs <code>(5,1)</code></h3>
<pre><code class="language-python">import numpy as np

# PROBLEMA: Vector 1D vs Vector Columna
v1 = np.array([1, 2, 3, 4, 5])      # Shape: (5,) - Vector 1D
v2 = np.array([[1], [2], [3], [4], [5]])  # Shape: (5, 1) - Vector columna

print(f&quot;v1.shape: {v1.shape}&quot;)  # (5,)
print(f&quot;v2.shape: {v2.shape}&quot;)  # (5, 1)

# ESTO FALLA en RegresiÃ³n Lineal:
# Si X tiene shape (100, 5) y theta tiene shape (5,), el resultado es (100,)
# Si theta tiene shape (5, 1), el resultado es (100, 1)

# SOLUCIÃ“N: Usar reshape o keepdims
v1_columna = v1.reshape(-1, 1)  # (5,) â†’ (5, 1)
v1_columna_alt = v1[:, np.newaxis]  # Alternativa

# REGLA: Para ML, los vectores de features deben ser (n, 1), no (n,)
</code></pre>
<h3>Error 2: Broadcasting Silencioso Incorrecto</h3>
<pre><code class="language-python">import numpy as np

# PROBLEMA: Broadcasting no falla, pero da resultados incorrectos
X = np.random.randn(100, 5)  # 100 samples, 5 features
mean_wrong = np.mean(X)      # Â¡INCORRECTO! Media de TODO el array
mean_correct = np.mean(X, axis=0)  # Correcto: media por feature (shape: (5,))

print(f&quot;mean_wrong shape: {np.array(mean_wrong).shape}&quot;)  # () - escalar
print(f&quot;mean_correct shape: {mean_correct.shape}&quot;)  # (5,)

# REGLA: Siempre especifica axis= en agregaciones
# axis=0: opera sobre filas (resultado por columna)
# axis=1: opera sobre columnas (resultado por fila)
</code></pre>
<h3>Error 3: ModificaciÃ³n In-Place Inesperada</h3>
<pre><code class="language-python">import numpy as np

# PROBLEMA: Los slices de NumPy son VISTAS, no copias
original = np.array([1, 2, 3, 4, 5])
slice_view = original[1:4]
slice_view[0] = 999

print(original)  # [1, 999, 3, 4, 5] - Â¡ORIGINAL MODIFICADO!

# SOLUCIÃ“N: Usar .copy() explÃ­citamente
original = np.array([1, 2, 3, 4, 5])
slice_copy = original[1:4].copy()
slice_copy[0] = 999

print(original)  # [1, 2, 3, 4, 5] - Original intacto
</code></pre>
<h3>Error 4: DivisiÃ³n por Cero en NormalizaciÃ³n</h3>
<pre><code class="language-python">import numpy as np

# PROBLEMA: DivisiÃ³n por cero cuando std = 0
data = np.array([5, 5, 5, 5, 5])
std = np.std(data)  # 0.0
normalized = (data - np.mean(data)) / std  # RuntimeWarning: divide by zero

# SOLUCIÃ“N: AÃ±adir epsilon
epsilon = 1e-8
normalized_safe = (data - np.mean(data)) / (std + epsilon)

# REGLA: Siempre usar epsilon en divisiones (especialmente en softmax, normalizaciones)
</code></pre>
<h3>Error 5: Tipos de Datos Incorrectos</h3>
<pre><code class="language-python">import numpy as np

# PROBLEMA: Operaciones con int cuando necesitas float
a = np.array([1, 2, 3])  # dtype: int64
b = a / 2  # dtype: float64 (OK en Python 3)

# PERO en operaciones in-place:
a = np.array([1, 2, 3])
a /= 2  # a sigue siendo int64, se trunca!
print(a)  # [0, 1, 1] - Â¡TRUNCADO!

# SOLUCIÃ“N: Especificar dtype al crear
a = np.array([1, 2, 3], dtype=np.float64)
a /= 2
print(a)  # [0.5, 1.0, 1.5] - Correcto

# REGLA: Para ML, siempre usar dtype=np.float64 o np.float32
</code></pre>
<h2>ğŸ› ï¸ EstÃ¡ndares de CÃ³digo Profesional (v3.2)</h2>
<blockquote>
<p>ğŸ’ <strong>FilosofÃ­a v3.2:</strong> El cÃ³digo no se considera terminado hasta que pase <code>mypy</code>, <code>ruff</code> y <code>pytest</code>.</p>
</blockquote>
<h3>ConfiguraciÃ³n del Entorno Profesional</h3>
<pre><code class="language-bash"># Crear entorno virtual
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate   # Windows

# Instalar herramientas de calidad
pip install numpy pandas matplotlib
pip install mypy ruff pytest

# Archivo pyproject.toml (crear en la raÃ­z del proyecto)
</code></pre>
<pre><code class="language-toml"># pyproject.toml
[tool.mypy]
python_version = &quot;3.11&quot;
warn_return_any = true
warn_unused_ignores = true
disallow_untyped_defs = true

[tool.ruff]
line-length = 100
select = [&quot;E&quot;, &quot;F&quot;, &quot;W&quot;, &quot;I&quot;, &quot;UP&quot;]

[tool.pytest.ini_options]
testpaths = [&quot;tests&quot;]
python_files = &quot;test_*.py&quot;
</code></pre>
<h3>Ejemplo: CÃ³digo con Type Hints</h3>
<pre><code class="language-python"># src/linear_algebra.py
&quot;&quot;&quot;Operaciones de Ã¡lgebra lineal desde cero.&quot;&quot;&quot;
import numpy as np
from numpy.typing import NDArray


def dot_product(a: NDArray[np.float64], b: NDArray[np.float64]) -&gt; float:
    &quot;&quot;&quot;
    Calcula el producto punto de dos vectores.

    Args:
        a: Primer vector (n,)
        b: Segundo vector (n,)

    Returns:
        El producto punto (escalar)

    Raises:
        ValueError: Si los vectores tienen shapes diferentes
    &quot;&quot;&quot;
    if a.shape != b.shape:
        raise ValueError(f&quot;Shapes incompatibles: {a.shape} vs {b.shape}&quot;)
    return float(np.sum(a * b))


def norm_l2(v: NDArray[np.float64]) -&gt; float:
    &quot;&quot;&quot;Calcula la norma L2 (euclidiana) de un vector.&quot;&quot;&quot;
    return float(np.sqrt(np.sum(v ** 2)))
</code></pre>
<h3>Ejemplo: Tests con pytest</h3>
<pre><code class="language-python"># tests/test_linear_algebra.py
&quot;&quot;&quot;Tests unitarios para linear_algebra.py&quot;&quot;&quot;
import numpy as np
import pytest
from src.linear_algebra import dot_product, norm_l2


class TestDotProduct:
    &quot;&quot;&quot;Tests para la funciÃ³n dot_product.&quot;&quot;&quot;

    def test_dot_product_basic(self) -&gt; None:
        &quot;&quot;&quot;Test bÃ¡sico: [1,2,3] Â· [4,5,6] = 32&quot;&quot;&quot;
        a = np.array([1.0, 2.0, 3.0])
        b = np.array([4.0, 5.0, 6.0])
        assert dot_product(a, b) == 32.0

    def test_dot_product_orthogonal(self) -&gt; None:
        &quot;&quot;&quot;Vectores ortogonales tienen producto punto = 0&quot;&quot;&quot;
        a = np.array([1.0, 0.0])
        b = np.array([0.0, 1.0])
        assert dot_product(a, b) == 0.0

    def test_dot_product_shape_mismatch(self) -&gt; None:
        &quot;&quot;&quot;Debe lanzar ValueError si shapes no coinciden&quot;&quot;&quot;
        a = np.array([1.0, 2.0])
        b = np.array([1.0, 2.0, 3.0])
        with pytest.raises(ValueError):
            dot_product(a, b)


class TestNormL2:
    &quot;&quot;&quot;Tests para la funciÃ³n norm_l2.&quot;&quot;&quot;

    def test_norm_unit_vector(self) -&gt; None:
        &quot;&quot;&quot;Vector unitario tiene norma 1&quot;&quot;&quot;
        v = np.array([1.0, 0.0, 0.0])
        assert norm_l2(v) == 1.0

    def test_norm_345(self) -&gt; None:
        &quot;&quot;&quot;TriÃ¡ngulo 3-4-5: norma de [3,4] = 5&quot;&quot;&quot;
        v = np.array([3.0, 4.0])
        assert norm_l2(v) == 5.0
</code></pre>
<h3>Comandos de VerificaciÃ³n</h3>
<pre><code class="language-bash"># Ejecutar en la raÃ­z del proyecto:

# 1. Verificar tipos (mypy)
mypy src/

# 2. Verificar estilo (ruff)
ruff check src/
ruff format src/  # Auto-formatear

# 3. Ejecutar tests (pytest)
pytest tests/ -v

# 4. Todo junto (antes de cada commit)
mypy src/ &amp;&amp; ruff check src/ &amp;&amp; pytest tests/ -v
</code></pre>
<h2>ğŸ¯ El Reto del Tablero Blanco (MetodologÃ­a Feynman)</h2>
<blockquote>
<p>ğŸ“ <strong>InstrucciÃ³n:</strong> DespuÃ©s de implementar cÃ³digo, debes poder explicar el algoritmo en <strong>mÃ¡ximo 5 lÃ­neas</strong> sin usar jerga tÃ©cnica. Si no puedes, vuelve a la teorÃ­a.</p>
</blockquote>
<h3>Ejemplo: Broadcasting</h3>
<p><strong>âŒ ExplicaciÃ³n tÃ©cnica (mala):</strong><br />
"Broadcasting es la capacidad de NumPy de realizar operaciones elemento a elemento entre arrays de diferentes shapes mediante la expansiÃ³n implÃ­cita de dimensiones segÃºn reglas de compatibilidad."</p>
<p><strong>âœ… ExplicaciÃ³n Feynman (buena):</strong><br />
"Cuando sumas un nÃºmero a una lista, NumPy automÃ¡ticamente suma ese nÃºmero a CADA elemento. Es como si el nÃºmero se 'copiara' para que tenga el mismo tamaÃ±o que la lista. Lo mismo pasa entre listas de diferentes tamaÃ±os, siempre que una de ellas tenga tamaÃ±o 1 en alguna dimensiÃ³n."</p>
<h3>Tu Reto para el MÃ³dulo 01:</h3>
<p>Explica en 5 lÃ­neas o menos:<br />
1. Â¿Por quÃ© NumPy es mÃ¡s rÃ¡pido que listas de Python?<br />
2. Â¿QuÃ© significa <code>axis=0</code> vs <code>axis=1</code>?<br />
3. Â¿Por quÃ© <code>.copy()</code> es importante?</p>
<h2>âœ… Checklist de FinalizaciÃ³n (v3.2)</h2>
<h3>Conocimiento</h3>
<ul>
<li>[ ] Puedo crear arrays 1D, 2D y 3D con NumPy</li>
<li>[ ] Entiendo indexing y slicing de arrays</li>
<li>[ ] Puedo explicar broadcasting y usarlo</li>
<li>[ ] SÃ© calcular agregaciones por eje (axis)</li>
<li>[ ] Puedo reescribir loops como operaciones vectorizadas</li>
<li>[ ] Conozco las diferencias entre <code>@</code>, <code>np.dot</code>, <code>np.matmul</code></li>
<li>[ ] Conozco los 5 errores comunes de NumPy y sus soluciones</li>
</ul>
<h3>Entregables de CÃ³digo</h3>
<ul>
<li>[ ] <code>benchmark_vectorization.py</code> implementado</li>
<li>[ ] El speedup de NumPy vs lista es &gt;50x en mis pruebas</li>
<li>[ ] <code>mypy src/</code> pasa sin errores</li>
<li>[ ] <code>ruff check src/</code> pasa sin errores</li>
<li>[ ] Al menos 3 tests con <code>pytest</code> pasando</li>
</ul>
<h3>MetodologÃ­a Feynman</h3>
<ul>
<li>[ ] Puedo explicar broadcasting en 5 lÃ­neas sin jerga</li>
<li>[ ] Puedo explicar axis=0 vs axis=1 en 5 lÃ­neas sin jerga</li>
<li>[ ] Puedo explicar por quÃ© .copy() es importante</li>
</ul>
<h2>ğŸ”— NavegaciÃ³n</h2>
<table>
<thead>
<tr>
<th>Anterior</th>
<th>Ãndice</th>
<th>Siguiente</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td><a href="#mod_00_INDICE">00_INDICE</a></td>
<td><a href="#mod_02_ALGEBRA_LINEAL_ML">02_ALGEBRA_LINEAL_ML</a></td>
</tr>
</tbody>
</table>
            </div>
        
            <!-- MÃ“DULO: 02_ALGEBRA_LINEAL_ML.md -->
            <div class="cover-page">
                <div class="cover-content">
                    <div style="font-size:40pt;margin-bottom:20px;">ğŸ’</div>
                    <div id="mod_02_ALGEBRA_LINEAL_ML" class="cover-title">MÃ“DULO 02 - ÃLGEBRA LINEAL PARA ML</div>
                    <div class="cover-subtitle">MS in AI Pathway - ML Specialist v3.0</div>
                    <div class="cover-badge">MATEMÃTICAS APLICADAS A CÃ“DIGO</div>
                </div>
                <div style="position:absolute;bottom:30px;font-size:10pt;opacity:0.6;">DUQUEOM | 2025</div>
            </div>
            <div class="content">
                <h1>MÃ³dulo 02 - Ãlgebra Lineal para Machine Learning</h1>
<blockquote>
<p><strong>ğŸ¯ Objetivo:</strong> Dominar vectores, matrices, normas y eigenvalues para ML<br />
<strong>Fase:</strong> 1 - Fundamentos MatemÃ¡ticos | <strong>Semanas 3-5</strong><br />
<strong>Prerrequisitos:</strong> MÃ³dulo 01 (Python CientÃ­fico con NumPy)</p>
</blockquote>
<h2>ğŸ§  Â¿Por QuÃ© Ãlgebra Lineal para ML?</h2>
<h3>Conexiones Directas con el Pathway</h3>
<table>
<thead>
<tr>
<th>Concepto</th>
<th>Uso en ML</th>
<th>Curso del Pathway</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Producto punto</strong></td>
<td>Similitud, predicciones</td>
<td>Supervised Learning</td>
</tr>
<tr>
<td><strong>Normas L1/L2</strong></td>
<td>RegularizaciÃ³n, distancias</td>
<td>Supervised Learning</td>
</tr>
<tr>
<td><strong>Eigenvalues</strong></td>
<td>PCA, reducciÃ³n dimensional</td>
<td>Unsupervised Learning</td>
</tr>
<tr>
<td><strong>MultiplicaciÃ³n matricial</strong></td>
<td>Forward pass en redes</td>
<td>Deep Learning</td>
</tr>
<tr>
<td><strong>SVD</strong></td>
<td>CompresiÃ³n, PCA</td>
<td>Unsupervised Learning</td>
</tr>
</tbody>
</table>
<h3>La MatemÃ¡tica DetrÃ¡s de ML</h3>
<pre><code>RegresiÃ³n Lineal:     Å· = XÎ¸           (multiplicaciÃ³n matriz-vector)
Logistic Regression:  Å· = Ïƒ(XÎ¸)        (+ funciÃ³n de activaciÃ³n)
Neural Network:       Å· = Ïƒ(Wâ‚ƒÏƒ(Wâ‚‚Ïƒ(Wâ‚x)))  (capas de multiplicaciones)
PCA:                  X_reduced = XV    (proyecciÃ³n a eigenvectors)
</code></pre>
<h2>ğŸ“š Contenido del MÃ³dulo</h2>
<h3>Semana 3: Vectores y Operaciones BÃ¡sicas</h3>
<h3>Semana 4: Normas y Distancias</h3>
<h3>Semana 5: Matrices, Eigenvalues y SVD</h3>
<h2>ğŸ’» Parte 1: Vectores</h2>
<h3>1.1 DefiniciÃ³n GeomÃ©trica y Algebraica</h3>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Un vector es una lista ordenada de nÃºmeros
# GeomÃ©tricamente: flecha con direcciÃ³n y magnitud

# Vector en RÂ² (2 dimensiones)
v = np.array([3, 4])

# Vector en RÂ³ (3 dimensiones)
w = np.array([1, 2, 3])

# Vector en R^n (n dimensiones) - comÃºn en ML
# Ejemplo: imagen 28x28 = 784 dimensiones
image_vector = np.random.randn(784)

# VisualizaciÃ³n 2D
def plot_vector(v, origin=[0, 0], color='blue', label=None):
    &quot;&quot;&quot;Dibuja un vector desde el origen.&quot;&quot;&quot;
    plt.quiver(*origin, *v, angles='xy', scale_units='xy', scale=1, color=color, label=label)

plt.figure(figsize=(8, 8))
plot_vector(np.array([3, 4]), color='blue', label='v = [3, 4]')
plot_vector(np.array([2, 1]), color='red', label='w = [2, 1]')
plt.xlim(-1, 5)
plt.ylim(-1, 5)
plt.grid(True)
plt.axhline(y=0, color='k', linewidth=0.5)
plt.axvline(x=0, color='k', linewidth=0.5)
plt.legend()
plt.title('Vectores en RÂ²')
plt.show()
</code></pre>
<h3>1.2 Operaciones con Vectores</h3>
<pre><code class="language-python">import numpy as np

# Vectores de ejemplo
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

# === SUMA DE VECTORES ===
# (a + b)áµ¢ = aáµ¢ + báµ¢
suma = a + b
print(f&quot;a + b = {suma}&quot;)  # [5, 7, 9]

# === RESTA DE VECTORES ===
resta = a - b
print(f&quot;a - b = {resta}&quot;)  # [-3, -3, -3]

# === MULTIPLICACIÃ“N POR ESCALAR ===
# (cÂ·a)áµ¢ = cÂ·aáµ¢
escalar = 2 * a
print(f&quot;2Â·a = {escalar}&quot;)  # [2, 4, 6]

# === PRODUCTO PUNTO (DOT PRODUCT) ===
# aÂ·b = Î£áµ¢ aáµ¢Â·báµ¢
# Resultado: escalar
dot = np.dot(a, b)
print(f&quot;aÂ·b = {dot}&quot;)  # 1*4 + 2*5 + 3*6 = 32

# Alternativamente:
dot_alt = a @ b
dot_sum = np.sum(a * b)
print(f&quot;VerificaciÃ³n: {dot_alt}, {dot_sum}&quot;)
</code></pre>
<h3>1.3 InterpretaciÃ³n GeomÃ©trica del Producto Punto</h3>
<pre><code class="language-python">import numpy as np

def angle_between_vectors(a: np.ndarray, b: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Calcula el Ã¡ngulo entre dos vectores.

    cos(Î¸) = (aÂ·b) / (||a|| ||b||)
    &quot;&quot;&quot;
    cos_theta = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    # Clip para evitar errores numÃ©ricos fuera de [-1, 1]
    cos_theta = np.clip(cos_theta, -1, 1)
    theta_rad = np.arccos(cos_theta)
    theta_deg = np.degrees(theta_rad)
    return theta_deg

# Ejemplos
v1 = np.array([1, 0])
v2 = np.array([0, 1])
v3 = np.array([1, 1])
v4 = np.array([-1, 0])

print(f&quot;Ãngulo entre [1,0] y [0,1]: {angle_between_vectors(v1, v2):.0f}Â°&quot;)  # 90Â°
print(f&quot;Ãngulo entre [1,0] y [1,1]: {angle_between_vectors(v1, v3):.0f}Â°&quot;)  # 45Â°
print(f&quot;Ãngulo entre [1,0] y [-1,0]: {angle_between_vectors(v1, v4):.0f}Â°&quot;) # 180Â°

# InterpretaciÃ³n para ML:
# - Producto punto alto â†’ vectores similares (mismo &quot;sentido&quot;)
# - Producto punto â‰ˆ 0 â†’ vectores ortogonales (independientes)
# - Producto punto negativo â†’ vectores opuestos
</code></pre>
<h3>1.4 ProyecciÃ³n de Vectores</h3>
<pre><code class="language-python">import numpy as np

def project(a: np.ndarray, b: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Proyecta el vector a sobre el vector b.

    proj_b(a) = (aÂ·b / bÂ·b) Â· b

    Ãštil para: PCA, regresiÃ³n, descomposiciÃ³n de seÃ±ales
    &quot;&quot;&quot;
    scalar = np.dot(a, b) / np.dot(b, b)
    return scalar * b

# Ejemplo
a = np.array([3, 4])
b = np.array([1, 0])  # Vector unitario en x

proyeccion = project(a, b)
print(f&quot;ProyecciÃ³n de {a} sobre {b}: {proyeccion}&quot;)  # [3, 0]

# La proyecciÃ³n nos da &quot;cuÃ¡nto&quot; de a estÃ¡ en la direcciÃ³n de b
</code></pre>
<h2>ğŸ’» Parte 2: Normas y Distancias</h2>
<h3>2.1 Norma L2 (Euclidiana)</h3>
<pre><code class="language-python">import numpy as np

def l2_norm(x: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Norma L2 (Euclidiana): longitud del vector.

    ||x||â‚‚ = âˆš(Î£áµ¢ xáµ¢Â²)

    Uso en ML: 
    - RegularizaciÃ³n Ridge
    - NormalizaciÃ³n de vectores
    - Distancia euclidiana
    &quot;&quot;&quot;
    return np.sqrt(np.sum(x ** 2))

# Equivalente en NumPy
x = np.array([3, 4])
print(f&quot;||x||â‚‚ = {l2_norm(x)}&quot;)           # 5.0
print(f&quot;NumPy:  {np.linalg.norm(x)}&quot;)     # 5.0
print(f&quot;NumPy:  {np.linalg.norm(x, 2)}&quot;)  # 5.0 (especificando ord=2)

# Vector unitario (normalizado)
def normalize(x: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;Convierte vector a longitud 1.&quot;&quot;&quot;
    return x / np.linalg.norm(x)

x_unit = normalize(x)
print(f&quot;Unitario: {x_unit}&quot;)  # [0.6, 0.8]
print(f&quot;Norma del unitario: {np.linalg.norm(x_unit)}&quot;)  # 1.0
</code></pre>
<h3>2.2 Norma L1 (Manhattan)</h3>
<pre><code class="language-python">import numpy as np

def l1_norm(x: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Norma L1 (Manhattan): suma de valores absolutos.

    ||x||â‚ = Î£áµ¢ |xáµ¢|

    Uso en ML:
    - RegularizaciÃ³n Lasso (promueve sparsity)
    - Robustez a outliers
    &quot;&quot;&quot;
    return np.sum(np.abs(x))

x = np.array([3, -4, 5])
print(f&quot;||x||â‚ = {l1_norm(x)}&quot;)                  # 12
print(f&quot;NumPy:  {np.linalg.norm(x, 1)}&quot;)         # 12.0

# ComparaciÃ³n L1 vs L2
# L1 penaliza todos los valores igualmente
# L2 penaliza mÃ¡s los valores grandes (cuadrado)
</code></pre>
<h3>2.3 Norma Lâˆ (MÃ¡ximo)</h3>
<pre><code class="language-python">import numpy as np

def linf_norm(x: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Norma Lâˆ: mÃ¡ximo valor absoluto.

    ||x||âˆ = max(|xáµ¢|)
    &quot;&quot;&quot;
    return np.max(np.abs(x))

x = np.array([3, -7, 5])
print(f&quot;||x||âˆ = {linf_norm(x)}&quot;)            # 7
print(f&quot;NumPy:  {np.linalg.norm(x, np.inf)}&quot;) # 7.0
</code></pre>
<h3>2.4 Distancia Euclidiana</h3>
<pre><code class="language-python">import numpy as np

def euclidean_distance(a: np.ndarray, b: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Distancia Euclidiana entre dos puntos.

    d(a, b) = ||a - b||â‚‚ = âˆš(Î£áµ¢ (aáµ¢ - báµ¢)Â²)

    Uso en ML:
    - KNN (k-nearest neighbors)
    - K-Means (asignaciÃ³n a clusters)
    - EvaluaciÃ³n de similaridad
    &quot;&quot;&quot;
    return np.linalg.norm(a - b)

# Ejemplo
p1 = np.array([0, 0])
p2 = np.array([3, 4])
print(f&quot;Distancia: {euclidean_distance(p1, p2)}&quot;)  # 5.0

# Para mÃºltiples puntos (eficiente)
def pairwise_distances(X: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Calcula matriz de distancias entre todos los puntos.
    X: matriz (n_samples, n_features)
    Retorna: matriz (n_samples, n_samples)
    &quot;&quot;&quot;
    # Usando broadcasting
    # ||a - b||Â² = ||a||Â² + ||b||Â² - 2(aÂ·b)
    sq_norms = np.sum(X ** 2, axis=1)
    distances_sq = sq_norms[:, np.newaxis] + sq_norms[np.newaxis, :] - 2 * X @ X.T
    distances_sq = np.maximum(distances_sq, 0)  # Evitar negativos por errores numÃ©ricos
    return np.sqrt(distances_sq)

# Test
X = np.array([[0, 0], [3, 4], [1, 1]])
D = pairwise_distances(X)
print(&quot;Matriz de distancias:&quot;)
print(D)
</code></pre>
<h3>2.5 Similitud Coseno</h3>
<pre><code class="language-python">import numpy as np

def cosine_similarity(a: np.ndarray, b: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Similitud coseno: mide el Ã¡ngulo entre vectores.

    sim(a, b) = (aÂ·b) / (||a|| ||b||)

    Rango: [-1, 1]
    - 1: vectores idÃ©nticos (misma direcciÃ³n)
    - 0: vectores ortogonales
    - -1: vectores opuestos

    Uso en ML:
    - NLP (similitud de documentos)
    - Sistemas de recomendaciÃ³n
    - Embeddings
    &quot;&quot;&quot;
    dot_product = np.dot(a, b)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)

    if norm_a == 0 or norm_b == 0:
        return 0.0

    return dot_product / (norm_a * norm_b)

def cosine_distance(a: np.ndarray, b: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Distancia coseno = 1 - similitud coseno.&quot;&quot;&quot;
    return 1 - cosine_similarity(a, b)

# Ejemplos
v1 = np.array([1, 0, 0])
v2 = np.array([1, 0, 0])
v3 = np.array([0, 1, 0])
v4 = np.array([-1, 0, 0])

print(f&quot;Similitud (idÃ©nticos):  {cosine_similarity(v1, v2)}&quot;)   # 1.0
print(f&quot;Similitud (ortogonales): {cosine_similarity(v1, v3)}&quot;)  # 0.0
print(f&quot;Similitud (opuestos):    {cosine_similarity(v1, v4)}&quot;)  # -1.0
</code></pre>
<h2>ğŸ’» Parte 3: Matrices</h2>
<h3>3.1 Operaciones BÃ¡sicas</h3>
<pre><code class="language-python">import numpy as np

# Crear matrices
A = np.array([
    [1, 2, 3],
    [4, 5, 6]
])  # Shape: (2, 3)

B = np.array([
    [7, 8],
    [9, 10],
    [11, 12]
])  # Shape: (3, 2)

# === SUMA Y RESTA ===
# Solo para matrices del mismo shape
C = np.array([[1, 2, 3], [4, 5, 6]])
print(f&quot;A + C =\n{A + C}&quot;)

# === MULTIPLICACIÃ“N POR ESCALAR ===
print(f&quot;2Â·A =\n{2 * A}&quot;)

# === PRODUCTO MATRICIAL ===
# (mÃ—n) @ (nÃ—p) = (mÃ—p)
# A(2Ã—3) @ B(3Ã—2) = (2Ã—2)
AB = A @ B
print(f&quot;A @ B =\n{AB}&quot;)
# [[58, 64],
#  [139, 154]]

# VerificaciÃ³n manual del elemento [0,0]:
# 1*7 + 2*9 + 3*11 = 7 + 18 + 33 = 58 âœ“

# === TRANSPUESTA ===
print(f&quot;A^T =\n{A.T}&quot;)
# [[1, 4],
#  [2, 5],
#  [3, 6]]
</code></pre>
<h3>3.2 Matriz por Vector (TransformaciÃ³n Lineal)</h3>
<pre><code class="language-python">import numpy as np

# La multiplicaciÃ³n matriz-vector es una TRANSFORMACIÃ“N LINEAL
# y = Ax transforma el vector x al espacio de y

# Ejemplo: RotaciÃ³n 90Â° en RÂ²
theta = np.pi / 2  # 90 grados
R = np.array([
    [np.cos(theta), -np.sin(theta)],
    [np.sin(theta),  np.cos(theta)]
])

x = np.array([1, 0])
y = R @ x
print(f&quot;Rotar [1,0] 90Â°: {y}&quot;)  # [0, 1]

# En ML: y = Wx + b (capa de red neuronal)
W = np.random.randn(10, 784)  # Pesos: 784 entradas â†’ 10 salidas
b = np.random.randn(10)        # Bias
x = np.random.randn(784)       # Input (imagen aplanada)

y = W @ x + b  # Output de la capa
print(f&quot;Shape de y: {y.shape}&quot;)  # (10,)
</code></pre>
<h3>3.3 Matriz Inversa</h3>
<pre><code class="language-python">import numpy as np

def safe_inverse(A: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Calcula la inversa de A si existe.
    A @ Aâ»Â¹ = Aâ»Â¹ @ A = I

    Uso en ML:
    - SoluciÃ³n cerrada de regresiÃ³n lineal: Î¸ = (X^T X)â»Â¹ X^T y
    - Whitening en PCA
    &quot;&quot;&quot;
    try:
        return np.linalg.inv(A)
    except np.linalg.LinAlgError:
        print(&quot;Matriz no invertible (singular)&quot;)
        return None

# Ejemplo
A = np.array([
    [4, 7],
    [2, 6]
])

A_inv = safe_inverse(A)
print(f&quot;Aâ»Â¹ =\n{A_inv}&quot;)

# Verificar: A @ Aâ»Â¹ = I
identity = A @ A_inv
print(f&quot;A @ Aâ»Â¹ â‰ˆ I:\n{np.round(identity, 10)}&quot;)

# NOTA: En ML, evita calcular inversas cuando sea posible
# Usa np.linalg.solve() en su lugar (mÃ¡s estable numÃ©ricamente)
</code></pre>
<h3>3.4 SoluciÃ³n de Sistemas Lineales</h3>
<pre><code class="language-python">import numpy as np

# Sistema: Ax = b
# Encontrar x

A = np.array([
    [3, 1],
    [1, 2]
])
b = np.array([9, 8])

# MÃ©todo 1: Inversa (NO RECOMENDADO)
x_inv = np.linalg.inv(A) @ b

# MÃ©todo 2: solve (RECOMENDADO - mÃ¡s estable)
x_solve = np.linalg.solve(A, b)

print(f&quot;SoluciÃ³n: x = {x_solve}&quot;)  # [2, 3]

# Verificar
print(f&quot;A @ x = {A @ x_solve}&quot;)    # [9, 8] âœ“
</code></pre>
<h2>ğŸ’» Parte 4: Eigenvalues y Eigenvectors</h2>
<h3>4.1 Concepto</h3>
<pre><code class="language-python">import numpy as np

&quot;&quot;&quot;
EIGENVALUES (Autovalores) y EIGENVECTORS (Autovectores)

DefiniciÃ³n: Av = Î»v
- v: eigenvector (vector que solo se escala, no cambia direcciÃ³n)
- Î»: eigenvalue (factor de escala)

InterpretaciÃ³n: 
- Los eigenvectors son las &quot;direcciones principales&quot; de una transformaciÃ³n
- Los eigenvalues indican cuÃ¡nto se estira/comprime en cada direcciÃ³n

Uso en ML:
- PCA: eigenvectors de la matriz de covarianza son las componentes principales
- PageRank: eigenvector dominante de la matriz de transiciÃ³n
- Estabilidad de sistemas dinÃ¡micos
&quot;&quot;&quot;

# Ejemplo simple
A = np.array([
    [2, 1],
    [1, 2]
])

# Calcular eigenvalues y eigenvectors
eigenvalues, eigenvectors = np.linalg.eig(A)

print(f&quot;Eigenvalues: {eigenvalues}&quot;)    # [3, 1]
print(f&quot;Eigenvectors:\n{eigenvectors}&quot;) # columnas son los eigenvectors

# Verificar: Av = Î»v
v1 = eigenvectors[:, 0]  # primer eigenvector
lambda1 = eigenvalues[0]  # primer eigenvalue

Av = A @ v1
lambda_v = lambda1 * v1

print(f&quot;\nVerificaciÃ³n Av = Î»v:&quot;)
print(f&quot;Av     = {Av}&quot;)
print(f&quot;Î»v     = {lambda_v}&quot;)
print(f&quot;Â¿Iguales? {np.allclose(Av, lambda_v)}&quot;)
</code></pre>
<h3>4.2 Eigenvalues para PCA</h3>
<pre><code class="language-python">import numpy as np

def pca_via_eigen(X: np.ndarray, n_components: int) -&gt; tuple:
    &quot;&quot;&quot;
    PCA usando eigendecomposition de la matriz de covarianza.

    Args:
        X: datos (n_samples, n_features)
        n_components: nÃºmero de componentes a retener

    Returns:
        X_transformed: datos proyectados
        components: eigenvectors (componentes principales)
        explained_variance: varianza explicada por cada componente
    &quot;&quot;&quot;
    # 1. Centrar datos (restar media)
    X_centered = X - np.mean(X, axis=0)

    # 2. Calcular matriz de covarianza
    # Cov = (1/n) X^T X
    n_samples = X.shape[0]
    cov_matrix = (X_centered.T @ X_centered) / (n_samples - 1)

    # 3. Calcular eigenvalues y eigenvectors
    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

    # 4. Ordenar por eigenvalue (mayor a menor)
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]

    # 5. Seleccionar top n_components
    components = eigenvectors[:, :n_components].real

    # 6. Proyectar datos
    X_transformed = X_centered @ components

    # 7. Calcular varianza explicada
    total_variance = np.sum(eigenvalues)
    explained_variance = eigenvalues[:n_components].real / total_variance

    return X_transformed, components, explained_variance

# Demo
np.random.seed(42)
X = np.random.randn(100, 5)  # 100 muestras, 5 features

X_pca, components, var_explained = pca_via_eigen(X, n_components=2)

print(f&quot;Shape original: {X.shape}&quot;)
print(f&quot;Shape reducido: {X_pca.shape}&quot;)
print(f&quot;Varianza explicada: {var_explained}&quot;)
print(f&quot;Varianza total explicada: {np.sum(var_explained):.2%}&quot;)
</code></pre>
<h2>ğŸ’» Parte 5: SVD (Singular Value Decomposition)</h2>
<h3>5.1 Concepto</h3>
<pre><code class="language-python">import numpy as np

&quot;&quot;&quot;
SVD: Singular Value Decomposition

A = U Î£ V^T

- U: matriz ortogonal (mÃ—m) - vectores singulares izquierdos
- Î£: matriz diagonal (mÃ—n) - valores singulares (Ïƒâ‚ â‰¥ Ïƒâ‚‚ â‰¥ ... â‰¥ 0)
- V^T: matriz ortogonal (nÃ—n) - vectores singulares derechos

Ventajas sobre Eigendecomposition:
- Funciona para CUALQUIER matriz (no solo cuadradas)
- MÃ¡s estable numÃ©ricamente
- Los valores singulares siempre son no-negativos

Uso en ML:
- PCA (mÃ©todo preferido)
- CompresiÃ³n de imÃ¡genes
- Sistemas de recomendaciÃ³n (matrix factorization)
- RegularizaciÃ³n (truncated SVD)
&quot;&quot;&quot;

# Ejemplo
A = np.array([
    [1, 2],
    [3, 4],
    [5, 6]
])  # 3Ã—2

U, S, Vt = np.linalg.svd(A, full_matrices=False)

print(f&quot;U shape: {U.shape}&quot;)   # (3, 2)
print(f&quot;S shape: {S.shape}&quot;)   # (2,) - valores singulares
print(f&quot;Vt shape: {Vt.shape}&quot;) # (2, 2)

# Reconstruir A
A_reconstructed = U @ np.diag(S) @ Vt
print(f&quot;\nÂ¿A â‰ˆ U Î£ V^T? {np.allclose(A, A_reconstructed)}&quot;)
</code></pre>
<h3>5.2 PCA via SVD (MÃ©todo Preferido)</h3>
<pre><code class="language-python">import numpy as np

def pca_via_svd(X: np.ndarray, n_components: int) -&gt; tuple:
    &quot;&quot;&quot;
    PCA usando SVD (mÃ¡s estable que eigendecomposition).

    La relaciÃ³n: si X = UÎ£V^T, entonces:
    - V contiene las componentes principales
    - Î£Â²/(n-1) son las varianzas (eigenvalues de X^TX)
    &quot;&quot;&quot;
    # 1. Centrar datos
    X_centered = X - np.mean(X, axis=0)

    # 2. SVD
    U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)

    # 3. Componentes principales (filas de Vt)
    components = Vt[:n_components]

    # 4. Proyectar datos
    X_transformed = X_centered @ components.T

    # 5. Varianza explicada
    variance = (S ** 2) / (X.shape[0] - 1)
    explained_variance_ratio = variance[:n_components] / np.sum(variance)

    return X_transformed, components, explained_variance_ratio

# Demo
np.random.seed(42)
X = np.random.randn(100, 10)

X_pca, components, var_ratio = pca_via_svd(X, n_components=3)

print(f&quot;Varianza explicada por componente: {var_ratio}&quot;)
print(f&quot;Varianza total explicada: {np.sum(var_ratio):.2%}&quot;)
</code></pre>
<h3>5.3 CompresiÃ³n de ImÃ¡genes con SVD</h3>
<pre><code class="language-python">import numpy as np

def compress_image_svd(image: np.ndarray, k: int) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Comprime una imagen usando truncated SVD.

    Args:
        image: matriz 2D (grayscale) o 3D (RGB)
        k: nÃºmero de valores singulares a retener

    Returns:
        imagen comprimida
    &quot;&quot;&quot;
    if len(image.shape) == 2:
        # Grayscale
        U, S, Vt = np.linalg.svd(image, full_matrices=False)
        compressed = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]
        return np.clip(compressed, 0, 255).astype(np.uint8)
    else:
        # RGB: comprimir cada canal
        compressed = np.zeros_like(image)
        for i in range(3):
            compressed[:, :, i] = compress_image_svd(image[:, :, i], k)
        return compressed

def compression_ratio(original_shape: tuple, k: int) -&gt; float:
    &quot;&quot;&quot;Calcula ratio de compresiÃ³n.&quot;&quot;&quot;
    m, n = original_shape[:2]
    original_size = m * n
    compressed_size = k * (m + n + 1)  # U[:,:k], S[:k], Vt[:k,:]
    return compressed_size / original_size

# Demo (sin cargar imagen real)
# Simular imagen 100x100
image = np.random.randint(0, 256, (100, 100), dtype=np.uint8)

for k in [5, 10, 20, 50]:
    compressed = compress_image_svd(image, k)
    ratio = compression_ratio(image.shape, k)
    print(f&quot;k={k}: ratio={ratio:.2%}&quot;)
</code></pre>
<h2>ğŸ“¦ Entregable del MÃ³dulo</h2>
<h3>LibrerÃ­a: <code>linear_algebra.py</code></h3>
<pre><code class="language-python">&quot;&quot;&quot;
Linear Algebra Library for Machine Learning

ImplementaciÃ³n desde cero de operaciones fundamentales.
Usando NumPy para eficiencia pero entendiendo las matemÃ¡ticas.

Autor: [Tu nombre]
MÃ³dulo: 02 - Ãlgebra Lineal para ML
&quot;&quot;&quot;

import numpy as np
from typing import Tuple, Optional


# ============================================================
# PARTE 1: OPERACIONES CON VECTORES
# ============================================================

def dot_product(a: np.ndarray, b: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Producto punto de dos vectores.

    aÂ·b = Î£áµ¢ aáµ¢Â·báµ¢
    &quot;&quot;&quot;
    assert a.shape == b.shape, &quot;Vectores deben tener mismo shape&quot;
    return float(np.sum(a * b))


def vector_angle(a: np.ndarray, b: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Ãngulo entre dos vectores en grados.

    cos(Î¸) = (aÂ·b) / (||a|| ||b||)
    &quot;&quot;&quot;
    cos_theta = dot_product(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    cos_theta = np.clip(cos_theta, -1, 1)
    return float(np.degrees(np.arccos(cos_theta)))


def project_vector(a: np.ndarray, b: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    ProyecciÃ³n del vector a sobre el vector b.

    proj_b(a) = (aÂ·b / bÂ·b) Â· b
    &quot;&quot;&quot;
    scalar = dot_product(a, b) / dot_product(b, b)
    return scalar * b


# ============================================================
# PARTE 2: NORMAS
# ============================================================

def l1_norm(x: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Norma L1 (Manhattan): ||x||â‚ = Î£|xáµ¢|&quot;&quot;&quot;
    return float(np.sum(np.abs(x)))


def l2_norm(x: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Norma L2 (Euclidiana): ||x||â‚‚ = âˆš(Î£xáµ¢Â²)&quot;&quot;&quot;
    return float(np.sqrt(np.sum(x ** 2)))


def linf_norm(x: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Norma Lâˆ (MÃ¡ximo): ||x||âˆ = max|xáµ¢|&quot;&quot;&quot;
    return float(np.max(np.abs(x)))


def normalize(x: np.ndarray, ord: int = 2) -&gt; np.ndarray:
    &quot;&quot;&quot;Normaliza vector a norma 1.&quot;&quot;&quot;
    norm = np.linalg.norm(x, ord=ord)
    if norm == 0:
        return x
    return x / norm


# ============================================================
# PARTE 3: DISTANCIAS
# ============================================================

def euclidean_distance(a: np.ndarray, b: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Distancia Euclidiana: d(a,b) = ||a-b||â‚‚&quot;&quot;&quot;
    return l2_norm(a - b)


def manhattan_distance(a: np.ndarray, b: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Distancia Manhattan: d(a,b) = ||a-b||â‚&quot;&quot;&quot;
    return l1_norm(a - b)


def cosine_similarity(a: np.ndarray, b: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Similitud coseno: sim(a,b) = (aÂ·b) / (||a|| ||b||)
    Rango: [-1, 1]
    &quot;&quot;&quot;
    norm_a = l2_norm(a)
    norm_b = l2_norm(b)
    if norm_a == 0 or norm_b == 0:
        return 0.0
    return dot_product(a, b) / (norm_a * norm_b)


def cosine_distance(a: np.ndarray, b: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Distancia coseno: 1 - similitud_coseno&quot;&quot;&quot;
    return 1 - cosine_similarity(a, b)


def pairwise_euclidean(X: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Matriz de distancias euclidianas entre todos los pares.

    Args:
        X: matriz (n_samples, n_features)
    Returns:
        D: matriz (n_samples, n_samples) de distancias
    &quot;&quot;&quot;
    sq_norms = np.sum(X ** 2, axis=1)
    D_sq = sq_norms[:, np.newaxis] + sq_norms[np.newaxis, :] - 2 * X @ X.T
    D_sq = np.maximum(D_sq, 0)  # Evitar negativos por errores numÃ©ricos
    return np.sqrt(D_sq)


# ============================================================
# PARTE 4: EIGENVALUES Y PCA
# ============================================================

def eigendecomposition(A: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:
    &quot;&quot;&quot;
    Calcula eigenvalues y eigenvectors, ordenados por eigenvalue descendente.

    Returns:
        eigenvalues: array de eigenvalues (ordenados)
        eigenvectors: matriz donde columna i es el eigenvector i
    &quot;&quot;&quot;
    eigenvalues, eigenvectors = np.linalg.eig(A)

    # Ordenar por eigenvalue descendente
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[idx].real
    eigenvectors = eigenvectors[:, idx].real

    return eigenvalues, eigenvectors


def pca(X: np.ndarray, n_components: int) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:
    &quot;&quot;&quot;
    Principal Component Analysis via SVD.

    Args:
        X: datos (n_samples, n_features)
        n_components: nÃºmero de componentes

    Returns:
        X_transformed: datos proyectados (n_samples, n_components)
        components: componentes principales (n_components, n_features)
        explained_variance_ratio: proporciÃ³n de varianza explicada
    &quot;&quot;&quot;
    # Centrar datos
    X_centered = X - np.mean(X, axis=0)

    # SVD
    U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)

    # Componentes principales
    components = Vt[:n_components]

    # Proyectar
    X_transformed = X_centered @ components.T

    # Varianza explicada
    variance = (S ** 2) / (X.shape[0] - 1)
    explained_variance_ratio = variance[:n_components] / np.sum(variance)

    return X_transformed, components, explained_variance_ratio


# ============================================================
# TESTS
# ============================================================

def run_tests():
    &quot;&quot;&quot;Ejecuta tests bÃ¡sicos.&quot;&quot;&quot;
    print(&quot;Ejecutando tests...&quot;)

    # Test producto punto
    a = np.array([1, 2, 3])
    b = np.array([4, 5, 6])
    assert abs(dot_product(a, b) - 32) &lt; 1e-10
    print(&quot;âœ“ dot_product&quot;)

    # Test normas
    x = np.array([3, 4])
    assert abs(l2_norm(x) - 5) &lt; 1e-10
    assert abs(l1_norm(x) - 7) &lt; 1e-10
    print(&quot;âœ“ normas&quot;)

    # Test distancias
    p1 = np.array([0, 0])
    p2 = np.array([3, 4])
    assert abs(euclidean_distance(p1, p2) - 5) &lt; 1e-10
    print(&quot;âœ“ distancias&quot;)

    # Test similitud coseno
    v1 = np.array([1, 0])
    v2 = np.array([1, 0])
    v3 = np.array([0, 1])
    assert abs(cosine_similarity(v1, v2) - 1) &lt; 1e-10
    assert abs(cosine_similarity(v1, v3)) &lt; 1e-10
    print(&quot;âœ“ cosine_similarity&quot;)

    # Test PCA
    np.random.seed(42)
    X = np.random.randn(50, 10)
    X_pca, _, var_ratio = pca(X, 3)
    assert X_pca.shape == (50, 3)
    assert np.sum(var_ratio) &lt;= 1.0
    print(&quot;âœ“ PCA&quot;)

    print(&quot;\nÂ¡Todos los tests pasaron!&quot;)


if __name__ == &quot;__main__&quot;:
    run_tests()
</code></pre>
<h2>âœ… Checklist de FinalizaciÃ³n</h2>
<ul>
<li>[ ] Puedo calcular producto punto y explicar su significado geomÃ©trico</li>
<li>[ ] Entiendo las diferencias entre normas L1, L2, Lâˆ</li>
<li>[ ] Puedo calcular distancia euclidiana y similitud coseno</li>
<li>[ ] SÃ© multiplicar matrices y entiendo las dimensiones resultantes</li>
<li>[ ] Puedo explicar quÃ© son eigenvalues/eigenvectors y su uso en PCA</li>
<li>[ ] Entiendo SVD y puedo usarlo para compresiÃ³n/PCA</li>
<li>[ ] ImplementÃ© <code>linear_algebra.py</code> con todos los tests pasando</li>
<li>[ ] Puedo proyectar datos usando PCA y explicar varianza explicada</li>
</ul>
<h2>ğŸ”— NavegaciÃ³n</h2>
<table>
<thead>
<tr>
<th>Anterior</th>
<th>Ãndice</th>
<th>Siguiente</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="#mod_01_PYTHON_CIENTIFICO">01_PYTHON_CIENTIFICO</a></td>
<td><a href="#mod_00_INDICE">00_INDICE</a></td>
<td><a href="#mod_03_CALCULO_MULTIVARIANTE">03_CALCULO_MULTIVARIANTE</a></td>
</tr>
</tbody>
</table>
            </div>
        
            <!-- MÃ“DULO: 03_CALCULO_MULTIVARIANTE.md -->
            <div class="cover-page">
                <div class="cover-content">
                    <div style="font-size:40pt;margin-bottom:20px;">ğŸ’</div>
                    <div id="mod_03_CALCULO_MULTIVARIANTE" class="cover-title">MÃ“DULO 03 - CÃLCULO MULTIVARIANTE</div>
                    <div class="cover-subtitle">MS in AI Pathway - ML Specialist v3.0</div>
                    <div class="cover-badge">MATEMÃTICAS APLICADAS A CÃ“DIGO</div>
                </div>
                <div style="position:absolute;bottom:30px;font-size:10pt;opacity:0.6;">DUQUEOM | 2025</div>
            </div>
            <div class="content">
                <h1>MÃ³dulo 03 - CÃ¡lculo Multivariante para Deep Learning</h1>
<blockquote>
<p><strong>ğŸ¯ Objetivo:</strong> Dominar derivadas, gradientes y la Chain Rule para entender Backpropagation<br />
<strong>Fase:</strong> 1 - Fundamentos MatemÃ¡ticos | <strong>Semanas 6-8</strong><br />
<strong>Prerrequisitos:</strong> MÃ³dulo 02 (Ãlgebra Lineal para ML)</p>
</blockquote>
<h2>ğŸ§  Â¿Por QuÃ© CÃ¡lculo para ML?</h2>
<h3>âš ï¸ CRÃTICO: Sin Chain Rule No Hay Deep Learning</h3>
<pre><code>El algoritmo de Backpropagation ES la Regla de la Cadena aplicada
a funciones compuestas de redes neuronales.

Si no entiendes:
  âˆ‚L/âˆ‚w = âˆ‚L/âˆ‚Å· Â· âˆ‚Å·/âˆ‚z Â· âˆ‚z/âˆ‚w

NO entenderÃ¡s por quÃ© funciona una red neuronal y
probablemente REPROBARÃS el curso de Deep Learning.
</code></pre>
<h3>ConexiÃ³n con el Pathway</h3>
<table>
<thead>
<tr>
<th>Concepto</th>
<th>Uso en ML</th>
<th>Curso del Pathway</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Derivada</strong></td>
<td>Tasa de cambio, pendiente</td>
<td>Todos</td>
</tr>
<tr>
<td><strong>Gradiente</strong></td>
<td>DirecciÃ³n de mÃ¡ximo ascenso</td>
<td>Supervised Learning</td>
</tr>
<tr>
<td><strong>Gradient Descent</strong></td>
<td>OptimizaciÃ³n de parÃ¡metros</td>
<td>Supervised + Deep Learning</td>
</tr>
<tr>
<td><strong>Chain Rule</strong></td>
<td>Backpropagation</td>
<td>Deep Learning</td>
</tr>
</tbody>
</table>
<h2>ğŸ“š Contenido del MÃ³dulo</h2>
<h3>Semana 6: Derivadas y Derivadas Parciales</h3>
<h3>Semana 7: Gradiente y Gradient Descent</h3>
<h3>Semana 8: Chain Rule y PreparaciÃ³n para Backprop</h3>
<h2>ğŸ’» Parte 1: Derivadas</h2>
<h3>1.1 Concepto de Derivada</h3>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

&quot;&quot;&quot;
DERIVADA: Tasa de cambio instantÃ¡nea de una funciÃ³n.

DefiniciÃ³n formal:
    f'(x) = lim[hâ†’0] (f(x+h) - f(x)) / h

InterpretaciÃ³n geomÃ©trica: pendiente de la recta tangente.

Notaciones equivalentes:
    f'(x) = df/dx = d/dx f(x) = Df(x)
&quot;&quot;&quot;

def numerical_derivative(f, x: float, h: float = 1e-7) -&gt; float:
    &quot;&quot;&quot;
    Calcula la derivada numÃ©rica usando diferencias finitas.

    MÃ©todo: diferencia central (mÃ¡s preciso)
    f'(x) â‰ˆ (f(x+h) - f(x-h)) / (2h)
    &quot;&quot;&quot;
    return (f(x + h) - f(x - h)) / (2 * h)


# Ejemplo: f(x) = xÂ²
def f(x):
    return x ** 2

# Derivada analÃ­tica: f'(x) = 2x
def f_prime_analytical(x):
    return 2 * x

# Comparar
x = 3.0
numerical = numerical_derivative(f, x)
analytical = f_prime_analytical(x)

print(f&quot;f(x) = xÂ² en x={x}&quot;)
print(f&quot;Derivada numÃ©rica:  {numerical:.6f}&quot;)
print(f&quot;Derivada analÃ­tica: {analytical:.6f}&quot;)
print(f&quot;Error: {abs(numerical - analytical):.2e}&quot;)
</code></pre>
<h3>1.2 Derivadas Comunes en ML</h3>
<pre><code class="language-python">import numpy as np

&quot;&quot;&quot;
DERIVADAS QUE NECESITAS MEMORIZAR PARA ML:

1. Constante:     d/dx(c) = 0
2. Lineal:        d/dx(x) = 1
3. Potencia:      d/dx(xâ¿) = nÂ·x^(n-1)
4. Exponencial:   d/dx(eË£) = eË£
5. Logaritmo:     d/dx(ln x) = 1/x
6. Suma:          d/dx(f+g) = f' + g'
7. Producto:      d/dx(fÂ·g) = f'g + fg'
8. Cociente:      d/dx(f/g) = (f'g - fg')/gÂ²
9. Cadena:        d/dx(f(g(x))) = f'(g(x))Â·g'(x)
&quot;&quot;&quot;

# Funciones de activaciÃ³n y sus derivadas

def sigmoid(x: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;Ïƒ(x) = 1 / (1 + e^(-x))&quot;&quot;&quot;
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    d/dx Ïƒ(x) = Ïƒ(x) Â· (1 - Ïƒ(x))

    DerivaciÃ³n:
    Ïƒ(x) = (1 + e^(-x))^(-1)
    Ïƒ'(x) = -1Â·(1 + e^(-x))^(-2) Â· (-e^(-x))
          = e^(-x) / (1 + e^(-x))Â²
          = Ïƒ(x) Â· (1 - Ïƒ(x))
    &quot;&quot;&quot;
    s = sigmoid(x)
    return s * (1 - s)


def relu(x: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;ReLU(x) = max(0, x)&quot;&quot;&quot;
    return np.maximum(0, x)

def relu_derivative(x: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    d/dx ReLU(x) = { 1 si x &gt; 0
                  { 0 si x &lt; 0
                  { indefinido si x = 0 (usamos 0)
    &quot;&quot;&quot;
    return (x &gt; 0).astype(float)


def tanh_derivative(x: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    d/dx tanh(x) = 1 - tanhÂ²(x)
    &quot;&quot;&quot;
    return 1 - np.tanh(x) ** 2


# Verificar con derivada numÃ©rica
def verify_derivative(f, f_prime, x, name):
    numerical = (f(x + 1e-7) - f(x - 1e-7)) / (2e-7)
    analytical = f_prime(x)
    error = np.abs(numerical - analytical).max()
    print(f&quot;{name}: error mÃ¡ximo = {error:.2e}&quot;)

x = np.array([-2, -1, 0.5, 1, 2])
verify_derivative(sigmoid, sigmoid_derivative, x, &quot;Sigmoid&quot;)
verify_derivative(np.tanh, tanh_derivative, x, &quot;Tanh&quot;)
</code></pre>
<h3>1.3 Derivadas Parciales</h3>
<pre><code class="language-python">import numpy as np

&quot;&quot;&quot;
DERIVADA PARCIAL: Derivada respecto a UNA variable, 
manteniendo las otras constantes.

Para f(x, y):
    âˆ‚f/âˆ‚x = derivada respecto a x, tratando y como constante
    âˆ‚f/âˆ‚y = derivada respecto a y, tratando x como constante

NotaciÃ³n: âˆ‚ (partial) en lugar de d
&quot;&quot;&quot;

def f(x: float, y: float) -&gt; float:
    &quot;&quot;&quot;f(x, y) = xÂ² + 3xy + yÂ²&quot;&quot;&quot;
    return x**2 + 3*x*y + y**2

# Derivadas parciales analÃ­ticas:
# âˆ‚f/âˆ‚x = 2x + 3y
# âˆ‚f/âˆ‚y = 3x + 2y

def df_dx(x: float, y: float) -&gt; float:
    &quot;&quot;&quot;âˆ‚f/âˆ‚x = 2x + 3y&quot;&quot;&quot;
    return 2*x + 3*y

def df_dy(x: float, y: float) -&gt; float:
    &quot;&quot;&quot;âˆ‚f/âˆ‚y = 3x + 2y&quot;&quot;&quot;
    return 3*x + 2*y


# Derivada parcial numÃ©rica
def partial_derivative(f, var_idx: int, point: list, h: float = 1e-7) -&gt; float:
    &quot;&quot;&quot;
    Calcula âˆ‚f/âˆ‚xáµ¢ en un punto dado.

    Args:
        f: funciÃ³n
        var_idx: Ã­ndice de la variable (0 para x, 1 para y, etc.)
        point: punto donde evaluar [x, y, ...]
        h: paso pequeÃ±o
    &quot;&quot;&quot;
    point_plus = point.copy()
    point_minus = point.copy()
    point_plus[var_idx] += h
    point_minus[var_idx] -= h
    return (f(*point_plus) - f(*point_minus)) / (2 * h)


# Verificar
point = [2.0, 3.0]
print(f&quot;Punto: x={point[0]}, y={point[1]}&quot;)
print(f&quot;f(x,y) = {f(*point)}&quot;)
print(f&quot;\nâˆ‚f/âˆ‚x:&quot;)
print(f&quot;  AnalÃ­tica: {df_dx(*point)}&quot;)
print(f&quot;  NumÃ©rica:  {partial_derivative(f, 0, point):.6f}&quot;)
print(f&quot;\nâˆ‚f/âˆ‚y:&quot;)
print(f&quot;  AnalÃ­tica: {df_dy(*point)}&quot;)
print(f&quot;  NumÃ©rica:  {partial_derivative(f, 1, point):.6f}&quot;)
</code></pre>
<h2>ğŸ’» Parte 2: Gradiente</h2>
<h3>2.1 DefiniciÃ³n del Gradiente</h3>
<pre><code class="language-python">import numpy as np

&quot;&quot;&quot;
GRADIENTE: Vector de todas las derivadas parciales.

Para f: Râ¿ â†’ R (funciÃ³n de n variables que retorna un escalar):

âˆ‡f = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ..., âˆ‚f/âˆ‚xâ‚™]

Propiedades importantes:
1. El gradiente apunta en la direcciÃ³n de MÃXIMO ASCENSO
2. La magnitud indica quÃ© tan rÃ¡pido aumenta f en esa direcciÃ³n
3. -âˆ‡f apunta en la direcciÃ³n de MÃXIMO DESCENSO (usado en optimizaciÃ³n)
&quot;&quot;&quot;

def compute_gradient(f, point: np.ndarray, h: float = 1e-7) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Calcula el gradiente de f en un punto usando diferencias finitas.

    Args:
        f: funciÃ³n f(x) donde x es un array
        point: punto donde calcular el gradiente
        h: paso para diferencias finitas

    Returns:
        gradiente como array
    &quot;&quot;&quot;
    n = len(point)
    gradient = np.zeros(n)

    for i in range(n):
        point_plus = point.copy()
        point_minus = point.copy()
        point_plus[i] += h
        point_minus[i] -= h
        gradient[i] = (f(point_plus) - f(point_minus)) / (2 * h)

    return gradient


# Ejemplo: f(x, y) = xÂ² + yÂ²
def paraboloid(p: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Paraboloide: f(x,y) = xÂ² + yÂ²&quot;&quot;&quot;
    return p[0]**2 + p[1]**2

# Gradiente analÃ­tico: âˆ‡f = [2x, 2y]
def paraboloid_gradient_analytical(p: np.ndarray) -&gt; np.ndarray:
    return np.array([2*p[0], 2*p[1]])


# Verificar
point = np.array([3.0, 4.0])
grad_numerical = compute_gradient(paraboloid, point)
grad_analytical = paraboloid_gradient_analytical(point)

print(f&quot;Punto: {point}&quot;)
print(f&quot;f(punto) = {paraboloid(point)}&quot;)
print(f&quot;Gradiente numÃ©rico:  {grad_numerical}&quot;)
print(f&quot;Gradiente analÃ­tico: {grad_analytical}&quot;)
</code></pre>
<h3>2.2 VisualizaciÃ³n del Gradiente</h3>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def visualize_gradient():
    &quot;&quot;&quot;Visualiza el gradiente como campo vectorial.&quot;&quot;&quot;

    # Crear grid
    x = np.linspace(-3, 3, 15)
    y = np.linspace(-3, 3, 15)
    X, Y = np.meshgrid(x, y)

    # FunciÃ³n: f(x,y) = xÂ² + yÂ²
    Z = X**2 + Y**2

    # Gradiente: âˆ‡f = [2x, 2y]
    U = 2 * X  # âˆ‚f/âˆ‚x
    V = 2 * Y  # âˆ‚f/âˆ‚y

    # Normalizar para visualizaciÃ³n
    magnitude = np.sqrt(U**2 + V**2)
    U_norm = U / (magnitude + 0.1)
    V_norm = V / (magnitude + 0.1)

    plt.figure(figsize=(10, 8))

    # Contornos de nivel
    plt.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.5)
    plt.colorbar(label='f(x,y) = xÂ² + yÂ²')

    # Flechas del gradiente
    plt.quiver(X, Y, U_norm, V_norm, magnitude, cmap='Reds', alpha=0.8)

    # Punto mÃ­nimo
    plt.plot(0, 0, 'g*', markersize=15, label='MÃ­nimo global')

    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('Gradiente de f(x,y) = xÂ² + yÂ²\nLas flechas apuntan hacia ARRIBA (mÃ¡ximo ascenso)')
    plt.legend()
    plt.axis('equal')
    plt.grid(True, alpha=0.3)
    plt.show()

# visualize_gradient()  # Descomentar para ejecutar
</code></pre>
<h2>ğŸ’» Parte 3: Gradient Descent</h2>
<h3>3.1 Algoritmo BÃ¡sico</h3>
<pre><code class="language-python">import numpy as np
from typing import Callable, List, Tuple

&quot;&quot;&quot;
GRADIENT DESCENT: Algoritmo de optimizaciÃ³n iterativo.

Idea: Para minimizar f(x), moverse en direcciÃ³n opuesta al gradiente.

Algoritmo:
    1. Inicializar xâ‚€
    2. Repetir hasta convergencia:
       x_{t+1} = x_t - Î± Â· âˆ‡f(x_t)

Donde Î± (alpha) es el &quot;learning rate&quot; (tasa de aprendizaje).
&quot;&quot;&quot;

def gradient_descent(
    f: Callable,
    grad_f: Callable,
    x0: np.ndarray,
    learning_rate: float = 0.1,
    max_iterations: int = 100,
    tolerance: float = 1e-6
) -&gt; Tuple[np.ndarray, List[np.ndarray], List[float]]:
    &quot;&quot;&quot;
    Gradient Descent para minimizar f.

    Args:
        f: funciÃ³n a minimizar
        grad_f: gradiente de f
        x0: punto inicial
        learning_rate: tasa de aprendizaje (Î±)
        max_iterations: mÃ¡ximo de iteraciones
        tolerance: criterio de parada (norma del gradiente)

    Returns:
        x_final: soluciÃ³n encontrada
        history_x: trayectoria de x
        history_f: valores de f en cada paso
    &quot;&quot;&quot;
    x = x0.copy()
    history_x = [x.copy()]
    history_f = [f(x)]

    for i in range(max_iterations):
        # Calcular gradiente
        grad = grad_f(x)

        # Verificar convergencia
        if np.linalg.norm(grad) &lt; tolerance:
            print(f&quot;ConvergiÃ³ en iteraciÃ³n {i}&quot;)
            break

        # Actualizar x
        x = x - learning_rate * grad

        # Guardar historia
        history_x.append(x.copy())
        history_f.append(f(x))

    return x, history_x, history_f


# Ejemplo: Minimizar f(x,y) = xÂ² + yÂ²
def f(p: np.ndarray) -&gt; float:
    return p[0]**2 + p[1]**2

def grad_f(p: np.ndarray) -&gt; np.ndarray:
    return np.array([2*p[0], 2*p[1]])

# Ejecutar
x0 = np.array([4.0, 3.0])
x_final, history_x, history_f = gradient_descent(f, grad_f, x0, learning_rate=0.1)

print(f&quot;\nPunto inicial: {x0}&quot;)
print(f&quot;MÃ­nimo encontrado: {x_final}&quot;)
print(f&quot;f(mÃ­nimo) = {f(x_final):.6f}&quot;)
print(f&quot;Iteraciones: {len(history_f)}&quot;)
</code></pre>
<h3>3.2 Efecto del Learning Rate</h3>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def compare_learning_rates():
    &quot;&quot;&quot;Compara diferentes learning rates.&quot;&quot;&quot;

    def f(p):
        return p[0]**2 + p[1]**2

    def grad_f(p):
        return np.array([2*p[0], 2*p[1]])

    x0 = np.array([4.0, 3.0])

    learning_rates = [0.01, 0.1, 0.5, 0.9]

    plt.figure(figsize=(12, 4))

    for i, lr in enumerate(learning_rates):
        x_final, history_x, history_f = gradient_descent(
            f, grad_f, x0, learning_rate=lr, max_iterations=50
        )

        plt.subplot(1, 4, i+1)
        plt.plot(history_f, 'b-o', markersize=3)
        plt.xlabel('IteraciÃ³n')
        plt.ylabel('f(x)')
        plt.title(f'Î± = {lr}')
        plt.yscale('log')
        plt.grid(True)

    plt.tight_layout()
    plt.suptitle('Efecto del Learning Rate en Gradient Descent', y=1.02)
    plt.show()

    &quot;&quot;&quot;
    Observaciones:
    - Î± muy pequeÃ±o (0.01): Convergencia muy lenta
    - Î± Ã³ptimo (0.1-0.5): Convergencia rÃ¡pida y estable
    - Î± muy grande (0.9): Oscilaciones, puede diverger
    - Î± &gt; 1: Generalmente diverge para este problema
    &quot;&quot;&quot;

# compare_learning_rates()  # Descomentar para ejecutar
</code></pre>
<h3>3.3 Funciones de PÃ©rdida en ML</h3>
<pre><code class="language-python">import numpy as np

&quot;&quot;&quot;
FUNCIONES DE PÃ‰RDIDA COMUNES Y SUS GRADIENTES

En ML, minimizamos una &quot;funciÃ³n de pÃ©rdida&quot; (loss function)
que mide quÃ© tan mal estÃ¡n nuestras predicciones.
&quot;&quot;&quot;

# 1. MSE (Mean Squared Error) - RegresiÃ³n
def mse_loss(y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    MSE = (1/n) Î£ (y_true - y_pred)Â²
    &quot;&quot;&quot;
    return np.mean((y_true - y_pred) ** 2)

def mse_gradient(y_true: np.ndarray, y_pred: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    âˆ‚MSE/âˆ‚y_pred = (2/n) Î£ (y_pred - y_true)
                 = (2/n) (y_pred - y_true)
    &quot;&quot;&quot;
    n = len(y_true)
    return (2 / n) * (y_pred - y_true)


# 2. Binary Cross-Entropy - ClasificaciÃ³n binaria
def binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-15) -&gt; float:
    &quot;&quot;&quot;
    BCE = -(1/n) Î£ [yÂ·log(Å·) + (1-y)Â·log(1-Å·)]
    &quot;&quot;&quot;
    y_pred = np.clip(y_pred, eps, 1 - eps)  # Evitar log(0)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

def binary_cross_entropy_gradient(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-15) -&gt; np.ndarray:
    &quot;&quot;&quot;
    âˆ‚BCE/âˆ‚y_pred = (1/n) Â· (y_pred - y_true) / (y_pred Â· (1 - y_pred))

    SimplificaciÃ³n cuando y_pred = Ïƒ(z):
    âˆ‚BCE/âˆ‚z = (1/n) Â· (y_pred - y_true)
    &quot;&quot;&quot;
    y_pred = np.clip(y_pred, eps, 1 - eps)
    return (y_pred - y_true) / (y_pred * (1 - y_pred)) / len(y_true)


# Demo
np.random.seed(42)
y_true = np.array([0, 0, 1, 1])
y_pred = np.array([0.1, 0.2, 0.8, 0.9])

print(&quot;MSE Loss:&quot;, mse_loss(y_true, y_pred))
print(&quot;BCE Loss:&quot;, binary_cross_entropy(y_true, y_pred))
</code></pre>
<h2>ğŸ’» Parte 4: Regla de la Cadena (Chain Rule)</h2>
<h3>4.1 Chain Rule en 1D</h3>
<pre><code class="language-python">import numpy as np

&quot;&quot;&quot;
REGLA DE LA CADENA (Chain Rule)

Si y = f(g(x)), entonces:
    dy/dx = df/dg Â· dg/dx

O en notaciÃ³n de composiciÃ³n:
    (f âˆ˜ g)'(x) = f'(g(x)) Â· g'(x)

Esto es FUNDAMENTAL para Backpropagation.
&quot;&quot;&quot;

# Ejemplo: y = (xÂ² + 1)Â³
# 
# Sea g(x) = xÂ² + 1  y  f(u) = uÂ³
# Entonces y = f(g(x))
#
# dy/dx = f'(g(x)) Â· g'(x)
#       = 3(xÂ² + 1)Â² Â· 2x
#       = 6x(xÂ² + 1)Â²

def g(x):
    return x**2 + 1

def f(u):
    return u**3

def y(x):
    return f(g(x))

def dy_dx_analytical(x):
    &quot;&quot;&quot;Derivada usando chain rule.&quot;&quot;&quot;
    return 6 * x * (x**2 + 1)**2

def dy_dx_numerical(x, h=1e-7):
    &quot;&quot;&quot;Derivada numÃ©rica.&quot;&quot;&quot;
    return (y(x + h) - y(x - h)) / (2 * h)


# Verificar
x = 2.0
print(f&quot;y({x}) = {y(x)}&quot;)
print(f&quot;dy/dx analÃ­tica:  {dy_dx_analytical(x)}&quot;)
print(f&quot;dy/dx numÃ©rica:   {dy_dx_numerical(x):.6f}&quot;)
</code></pre>
<h3>4.2 Chain Rule para Funciones Compuestas (Backprop Preview)</h3>
<pre><code class="language-python">import numpy as np

&quot;&quot;&quot;
CHAIN RULE PARA REDES NEURONALES

Una capa de red neuronal:
    z = Wx + b       (transformaciÃ³n lineal)
    a = Ïƒ(z)         (activaciÃ³n)

Si L es la pÃ©rdida, necesitamos:
    âˆ‚L/âˆ‚W, âˆ‚L/âˆ‚b     (para actualizar los pesos)

Usando Chain Rule:
    âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚a Â· âˆ‚a/âˆ‚z Â· âˆ‚z/âˆ‚W
    âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚a Â· âˆ‚a/âˆ‚z Â· âˆ‚z/âˆ‚b
&quot;&quot;&quot;

def simple_forward_backward():
    &quot;&quot;&quot;
    Ejemplo simplificado de forward y backward pass.

    Red: x â†’ [z = wx + b] â†’ [a = sigmoid(z)] â†’ [L = (a - y)Â²]
    &quot;&quot;&quot;
    # Datos
    x = 2.0          # Input
    y_true = 1.0     # Target

    # ParÃ¡metros
    w = 0.5
    b = 0.1

    # ========== FORWARD PASS ==========
    z = w * x + b                    # z = wx + b
    a = 1 / (1 + np.exp(-z))         # a = sigmoid(z)
    L = (a - y_true) ** 2            # L = MSE

    print(&quot;=== FORWARD PASS ===&quot;)
    print(f&quot;z = w*x + b = {w}*{x} + {b} = {z}&quot;)
    print(f&quot;a = sigmoid(z) = {a:.4f}&quot;)
    print(f&quot;L = (a - y)Â² = ({a:.4f} - {y_true})Â² = {L:.4f}&quot;)

    # ========== BACKWARD PASS (Chain Rule) ==========
    # Objetivo: calcular âˆ‚L/âˆ‚w y âˆ‚L/âˆ‚b

    # Paso 1: âˆ‚L/âˆ‚a
    dL_da = 2 * (a - y_true)

    # Paso 2: âˆ‚a/âˆ‚z = sigmoid'(z) = a(1-a)
    da_dz = a * (1 - a)

    # Paso 3: âˆ‚z/âˆ‚w = x,  âˆ‚z/âˆ‚b = 1
    dz_dw = x
    dz_db = 1

    # Aplicar Chain Rule
    dL_dz = dL_da * da_dz           # âˆ‚L/âˆ‚z = âˆ‚L/âˆ‚a Â· âˆ‚a/âˆ‚z
    dL_dw = dL_dz * dz_dw           # âˆ‚L/âˆ‚w = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚w
    dL_db = dL_dz * dz_db           # âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚b

    print(&quot;\n=== BACKWARD PASS (Chain Rule) ===&quot;)
    print(f&quot;âˆ‚L/âˆ‚a = 2(a - y) = {dL_da:.4f}&quot;)
    print(f&quot;âˆ‚a/âˆ‚z = a(1-a) = {da_dz:.4f}&quot;)
    print(f&quot;âˆ‚z/âˆ‚w = x = {dz_dw}&quot;)
    print(f&quot;âˆ‚z/âˆ‚b = 1&quot;)
    print(f&quot;\nâˆ‚L/âˆ‚w = âˆ‚L/âˆ‚a Â· âˆ‚a/âˆ‚z Â· âˆ‚z/âˆ‚w = {dL_dw:.4f}&quot;)
    print(f&quot;âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚a Â· âˆ‚a/âˆ‚z Â· âˆ‚z/âˆ‚b = {dL_db:.4f}&quot;)

    # ========== VERIFICACIÃ“N NUMÃ‰RICA ==========
    h = 1e-7

    # âˆ‚L/âˆ‚w numÃ©rica
    z_plus = (w + h) * x + b
    a_plus = 1 / (1 + np.exp(-z_plus))
    L_plus = (a_plus - y_true) ** 2

    z_minus = (w - h) * x + b
    a_minus = 1 / (1 + np.exp(-z_minus))
    L_minus = (a_minus - y_true) ** 2

    dL_dw_numerical = (L_plus - L_minus) / (2 * h)

    print(f&quot;\n=== VERIFICACIÃ“N ===&quot;)
    print(f&quot;âˆ‚L/âˆ‚w analÃ­tica: {dL_dw:.6f}&quot;)
    print(f&quot;âˆ‚L/âˆ‚w numÃ©rica:  {dL_dw_numerical:.6f}&quot;)
    print(f&quot;Error: {abs(dL_dw - dL_dw_numerical):.2e}&quot;)

    return dL_dw, dL_db

simple_forward_backward()
</code></pre>
<h3>4.3 Backpropagation en una Red de 2 Capas</h3>
<pre><code class="language-python">import numpy as np

&quot;&quot;&quot;
RED NEURONAL DE 2 CAPAS

Arquitectura:
    x (input) 
    â†’ zâ‚ = Wâ‚x + bâ‚ 
    â†’ aâ‚ = sigmoid(zâ‚) 
    â†’ zâ‚‚ = Wâ‚‚aâ‚ + bâ‚‚ 
    â†’ aâ‚‚ = sigmoid(zâ‚‚) 
    â†’ L = MSE(aâ‚‚, y)

Backpropagation usa Chain Rule repetidamente:
    âˆ‚L/âˆ‚Wâ‚‚ = âˆ‚L/âˆ‚aâ‚‚ Â· âˆ‚aâ‚‚/âˆ‚zâ‚‚ Â· âˆ‚zâ‚‚/âˆ‚Wâ‚‚
    âˆ‚L/âˆ‚Wâ‚ = âˆ‚L/âˆ‚aâ‚‚ Â· âˆ‚aâ‚‚/âˆ‚zâ‚‚ Â· âˆ‚zâ‚‚/âˆ‚aâ‚ Â· âˆ‚aâ‚/âˆ‚zâ‚ Â· âˆ‚zâ‚/âˆ‚Wâ‚
&quot;&quot;&quot;

class SimpleNeuralNet:
    &quot;&quot;&quot;Red neuronal de 2 capas para demostrar backprop.&quot;&quot;&quot;

    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        # Inicializar pesos (Xavier initialization)
        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2 / input_size)
        self.b1 = np.zeros(hidden_size)
        self.W2 = np.random.randn(output_size, hidden_size) * np.sqrt(2 / hidden_size)
        self.b2 = np.zeros(output_size)

        # Cache para backprop
        self.cache = {}

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

    def sigmoid_derivative(self, a):
        return a * (1 - a)

    def forward(self, x: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;Forward pass guardando valores intermedios.&quot;&quot;&quot;
        # Capa 1
        z1 = self.W1 @ x + self.b1
        a1 = self.sigmoid(z1)

        # Capa 2
        z2 = self.W2 @ a1 + self.b2
        a2 = self.sigmoid(z2)

        # Guardar para backprop
        self.cache = {'x': x, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2}

        return a2

    def backward(self, y_true: np.ndarray) -&gt; dict:
        &quot;&quot;&quot;
        Backward pass usando Chain Rule.

        Returns:
            Gradientes de todos los parÃ¡metros
        &quot;&quot;&quot;
        x = self.cache['x']
        a1 = self.cache['a1']
        a2 = self.cache['a2']

        # âˆ‚L/âˆ‚aâ‚‚ (MSE)
        dL_da2 = 2 * (a2 - y_true)

        # âˆ‚aâ‚‚/âˆ‚zâ‚‚
        da2_dz2 = self.sigmoid_derivative(a2)

        # âˆ‚L/âˆ‚zâ‚‚ = âˆ‚L/âˆ‚aâ‚‚ Â· âˆ‚aâ‚‚/âˆ‚zâ‚‚
        dL_dz2 = dL_da2 * da2_dz2

        # Gradientes de capa 2
        # âˆ‚zâ‚‚/âˆ‚Wâ‚‚ = aâ‚, âˆ‚zâ‚‚/âˆ‚bâ‚‚ = 1
        dL_dW2 = np.outer(dL_dz2, a1)
        dL_db2 = dL_dz2

        # Propagar hacia atrÃ¡s a capa 1
        # âˆ‚zâ‚‚/âˆ‚aâ‚ = Wâ‚‚
        dL_da1 = self.W2.T @ dL_dz2

        # âˆ‚aâ‚/âˆ‚zâ‚
        da1_dz1 = self.sigmoid_derivative(a1)

        # âˆ‚L/âˆ‚zâ‚
        dL_dz1 = dL_da1 * da1_dz1

        # Gradientes de capa 1
        dL_dW1 = np.outer(dL_dz1, x)
        dL_db1 = dL_dz1

        return {
            'dW1': dL_dW1, 'db1': dL_db1,
            'dW2': dL_dW2, 'db2': dL_db2
        }

    def update(self, gradients: dict, learning_rate: float):
        &quot;&quot;&quot;Actualiza parÃ¡metros usando gradient descent.&quot;&quot;&quot;
        self.W1 -= learning_rate * gradients['dW1']
        self.b1 -= learning_rate * gradients['db1']
        self.W2 -= learning_rate * gradients['dW2']
        self.b2 -= learning_rate * gradients['db2']


# Demo: XOR problem
def demo_xor():
    &quot;&quot;&quot;Entrena la red para resolver XOR.&quot;&quot;&quot;
    # XOR data
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T  # 2x4
    y = np.array([[0], [1], [1], [0]]).T              # 1x4

    # Crear red
    net = SimpleNeuralNet(input_size=2, hidden_size=4, output_size=1)

    # Entrenar
    losses = []
    for epoch in range(10000):
        total_loss = 0
        for i in range(4):
            # Forward
            output = net.forward(X[:, i])
            loss = (output - y[:, i]) ** 2
            total_loss += loss[0]

            # Backward
            gradients = net.backward(y[:, i])

            # Update
            net.update(gradients, learning_rate=0.5)

        losses.append(total_loss / 4)

        if epoch % 2000 == 0:
            print(f&quot;Epoch {epoch}: Loss = {losses[-1]:.4f}&quot;)

    # Test
    print(&quot;\n=== Resultados XOR ===&quot;)
    for i in range(4):
        pred = net.forward(X[:, i])
        print(f&quot;Input: {X[:, i]} â†’ Pred: {pred[0]:.3f} (Target: {y[0, i]})&quot;)

demo_xor()
</code></pre>
<h2>ğŸ“¦ Entregable del MÃ³dulo</h2>
<h3>Script: <code>gradient_descent_demo.py</code></h3>
<pre><code class="language-python">&quot;&quot;&quot;
Gradient Descent Demo - VisualizaciÃ³n de OptimizaciÃ³n

Este script implementa Gradient Descent desde cero y visualiza
la trayectoria de optimizaciÃ³n en diferentes funciones.

Autor: [Tu nombre]
MÃ³dulo: 03 - CÃ¡lculo Multivariante
&quot;&quot;&quot;

import numpy as np
import matplotlib.pyplot as plt
from typing import Callable, Tuple, List


def gradient_descent(
    f: Callable[[np.ndarray], float],
    grad_f: Callable[[np.ndarray], np.ndarray],
    x0: np.ndarray,
    learning_rate: float = 0.1,
    max_iterations: int = 100,
    tolerance: float = 1e-8
) -&gt; Tuple[np.ndarray, List[np.ndarray], List[float]]:
    &quot;&quot;&quot;
    ImplementaciÃ³n de Gradient Descent.

    Args:
        f: funciÃ³n objetivo
        grad_f: gradiente de f
        x0: punto inicial
        learning_rate: Î±
        max_iterations: mÃ¡ximo de iteraciones
        tolerance: criterio de convergencia

    Returns:
        x_final, history_x, history_f
    &quot;&quot;&quot;
    x = x0.copy().astype(float)
    history_x = [x.copy()]
    history_f = [f(x)]

    for i in range(max_iterations):
        grad = grad_f(x)

        if np.linalg.norm(grad) &lt; tolerance:
            break

        x = x - learning_rate * grad
        history_x.append(x.copy())
        history_f.append(f(x))

    return x, history_x, history_f


def visualize_optimization(
    f: Callable,
    grad_f: Callable,
    x0: np.ndarray,
    learning_rate: float,
    title: str,
    xlim: Tuple[float, float] = (-5, 5),
    ylim: Tuple[float, float] = (-5, 5)
):
    &quot;&quot;&quot;Visualiza la trayectoria de optimizaciÃ³n.&quot;&quot;&quot;

    x_final, history_x, history_f = gradient_descent(
        f, grad_f, x0, learning_rate, max_iterations=50
    )

    # Crear grid para contornos
    x = np.linspace(xlim[0], xlim[1], 100)
    y = np.linspace(ylim[0], ylim[1], 100)
    X, Y = np.meshgrid(x, y)
    Z = np.array([[f(np.array([xi, yi])) for xi, yi in zip(row_x, row_y)] 
                  for row_x, row_y in zip(X, Y)])

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Plot 1: Contornos y trayectoria
    ax1 = axes[0]
    contour = ax1.contour(X, Y, Z, levels=30, cmap='viridis')
    ax1.clabel(contour, inline=True, fontsize=8)

    # Trayectoria
    history_x = np.array(history_x)
    ax1.plot(history_x[:, 0], history_x[:, 1], 'r.-', markersize=8, linewidth=1.5)
    ax1.plot(history_x[0, 0], history_x[0, 1], 'go', markersize=12, label='Inicio')
    ax1.plot(history_x[-1, 0], history_x[-1, 1], 'r*', markersize=15, label='Final')

    ax1.set_xlabel('x')
    ax1.set_ylabel('y')
    ax1.set_title(f'{title}\nÎ± = {learning_rate}')
    ax1.legend()
    ax1.set_xlim(xlim)
    ax1.set_ylim(ylim)

    # Plot 2: Convergencia
    ax2 = axes[1]
    ax2.semilogy(history_f, 'b-o', markersize=4)
    ax2.set_xlabel('IteraciÃ³n')
    ax2.set_ylabel('f(x) (escala log)')
    ax2.set_title('Convergencia')
    ax2.grid(True)

    plt.tight_layout()
    plt.savefig(f'gd_{title.lower().replace(&quot; &quot;, &quot;_&quot;)}.png', dpi=150)
    plt.show()

    print(f&quot;\n{title}&quot;)
    print(f&quot;  Punto inicial: {x0}&quot;)
    print(f&quot;  MÃ­nimo encontrado: {x_final}&quot;)
    print(f&quot;  f(mÃ­nimo): {f(x_final):.6f}&quot;)
    print(f&quot;  Iteraciones: {len(history_f)}&quot;)


def main():
    &quot;&quot;&quot;Ejecutar demos.&quot;&quot;&quot;

    # === FunciÃ³n 1: Paraboloide ===
    def paraboloid(p):
        return p[0]**2 + p[1]**2

    def grad_paraboloid(p):
        return np.array([2*p[0], 2*p[1]])

    visualize_optimization(
        paraboloid, grad_paraboloid,
        x0=np.array([4.0, 3.0]),
        learning_rate=0.1,
        title=&quot;Paraboloide f(x,y) = xÂ² + yÂ²&quot;
    )

    # === FunciÃ³n 2: Rosenbrock (mÃ¡s difÃ­cil) ===
    def rosenbrock(p):
        return (1 - p[0])**2 + 100*(p[1] - p[0]**2)**2

    def grad_rosenbrock(p):
        dx = -2*(1 - p[0]) - 400*p[0]*(p[1] - p[0]**2)
        dy = 200*(p[1] - p[0]**2)
        return np.array([dx, dy])

    visualize_optimization(
        rosenbrock, grad_rosenbrock,
        x0=np.array([-1.0, 1.0]),
        learning_rate=0.001,
        title=&quot;Rosenbrock f(x,y) = (1-x)Â² + 100(y-xÂ²)Â²&quot;,
        xlim=(-2, 2),
        ylim=(-1, 3)
    )

    # === FunciÃ³n 3: CuadrÃ¡tica elÃ­ptica ===
    def elliptic(p):
        return p[0]**2 + 10*p[1]**2

    def grad_elliptic(p):
        return np.array([2*p[0], 20*p[1]])

    visualize_optimization(
        elliptic, grad_elliptic,
        x0=np.array([4.0, 2.0]),
        learning_rate=0.05,
        title=&quot;ElÃ­ptica f(x,y) = xÂ² + 10yÂ²&quot;
    )


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<h2>ğŸ”¬ Gradient Checking: ValidaciÃ³n MatemÃ¡tica (v3.3)</h2>
<blockquote>
<p>âš ï¸ <strong>CRÃTICO:</strong> El mayor riesgo en ML es implementar backpropagation incorrectamente. El cÃ³digo puede correr, el loss puede bajar, pero el gradiente estar mal. <strong>Esta tÃ©cnica es estÃ¡ndar en CS231n de Stanford.</strong></p>
</blockquote>
<h3>Concepto: Derivada NumÃ©rica vs AnalÃ­tica</h3>
<pre><code>GRADIENT CHECKING

Tu gradiente analÃ­tico (backprop):
    âˆ‚L/âˆ‚w = [valor calculado con Chain Rule]

Gradiente numÃ©rico (aproximaciÃ³n):
    âˆ‚L/âˆ‚w â‰ˆ [L(w + Îµ) - L(w - Îµ)] / (2Îµ)

Si |analÃ­tico - numÃ©rico| &gt; 10â»â· â†’ TU IMPLEMENTACIÃ“N TIENE UN BUG
</code></pre>
<h3>Script: <code>grad_check.py</code> (Entregable Obligatorio v3.3)</h3>
<pre><code class="language-python">&quot;&quot;&quot;
Gradient Checking - ValidaciÃ³n de Derivadas
TÃ©cnica estÃ¡ndar de CS231n Stanford para debugging de backprop.

Autor: [Tu nombre]
MÃ³dulo: 03 - CÃ¡lculo Multivariante
&quot;&quot;&quot;
import numpy as np
from typing import Callable, Dict, Tuple


def numerical_gradient(
    f: Callable[[np.ndarray], float],
    x: np.ndarray,
    epsilon: float = 1e-5
) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Calcula el gradiente numÃ©rico usando diferencias centrales.

    Args:
        f: FunciÃ³n escalar f(x) -&gt; float
        x: Punto donde calcular el gradiente
        epsilon: TamaÃ±o del paso (default: 1e-5)

    Returns:
        Gradiente numÃ©rico aproximado
    &quot;&quot;&quot;
    grad = np.zeros_like(x)

    # Iterar sobre cada dimensiÃ³n
    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
    while not it.finished:
        idx = it.multi_index
        old_value = x[idx]

        # f(x + epsilon)
        x[idx] = old_value + epsilon
        fx_plus = f(x)

        # f(x - epsilon)
        x[idx] = old_value - epsilon
        fx_minus = f(x)

        # Diferencias centrales: (f(x+Îµ) - f(x-Îµ)) / 2Îµ
        grad[idx] = (fx_plus - fx_minus) / (2 * epsilon)

        # Restaurar valor original
        x[idx] = old_value
        it.iternext()

    return grad


def gradient_check(
    analytic_grad: np.ndarray,
    numerical_grad: np.ndarray,
    threshold: float = 1e-7
) -&gt; Tuple[bool, float]:
    &quot;&quot;&quot;
    Compara gradiente analÃ­tico vs numÃ©rico.

    Args:
        analytic_grad: Gradiente calculado con backprop
        numerical_grad: Gradiente calculado numÃ©ricamente
        threshold: Umbral de error aceptable

    Returns:
        (passed, relative_error)
    &quot;&quot;&quot;
    # Error relativo: ||a - n|| / (||a|| + ||n||)
    diff = np.linalg.norm(analytic_grad - numerical_grad)
    norm_sum = np.linalg.norm(analytic_grad) + np.linalg.norm(numerical_grad)

    if norm_sum == 0:
        relative_error = 0.0
    else:
        relative_error = diff / norm_sum

    passed = relative_error &lt; threshold
    return passed, relative_error


# ============================================================
# EJEMPLO: Validar gradiente de MSE Loss
# ============================================================

def mse_loss(y_pred: np.ndarray, y_true: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Mean Squared Error.&quot;&quot;&quot;
    return float(np.mean((y_pred - y_true) ** 2))


def mse_gradient_analytic(y_pred: np.ndarray, y_true: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;Gradiente analÃ­tico de MSE respecto a y_pred.&quot;&quot;&quot;
    n = len(y_true)
    return 2 * (y_pred - y_true) / n


def test_mse_gradient():
    &quot;&quot;&quot;Test: Validar gradiente de MSE.&quot;&quot;&quot;
    print(&quot;=&quot; * 60)
    print(&quot;GRADIENT CHECK: MSE Loss&quot;)
    print(&quot;=&quot; * 60)

    np.random.seed(42)
    y_pred = np.random.randn(10)
    y_true = np.random.randn(10)

    # Gradiente analÃ­tico
    grad_analytic = mse_gradient_analytic(y_pred, y_true)

    # Gradiente numÃ©rico
    def loss_fn(pred):
        return mse_loss(pred, y_true)

    grad_numerical = numerical_gradient(loss_fn, y_pred.copy())

    # Comparar
    passed, error = gradient_check(grad_analytic, grad_numerical)

    print(f&quot;Gradiente AnalÃ­tico: {grad_analytic[:3]}...&quot;)
    print(f&quot;Gradiente NumÃ©rico:  {grad_numerical[:3]}...&quot;)
    print(f&quot;Error Relativo: {error:.2e}&quot;)
    print(f&quot;Resultado: {'âœ“ PASSED' if passed else 'âœ— FAILED'}&quot;)

    return passed


# ============================================================
# EJEMPLO: Validar gradiente de Sigmoid
# ============================================================

def sigmoid(z: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;Sigmoid activation.&quot;&quot;&quot;
    return 1 / (1 + np.exp(-z))


def sigmoid_derivative_analytic(z: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;Derivada analÃ­tica: Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z))&quot;&quot;&quot;
    s = sigmoid(z)
    return s * (1 - s)


def test_sigmoid_gradient():
    &quot;&quot;&quot;Test: Validar derivada de sigmoid.&quot;&quot;&quot;
    print(&quot;\n&quot; + &quot;=&quot; * 60)
    print(&quot;GRADIENT CHECK: Sigmoid Derivative&quot;)
    print(&quot;=&quot; * 60)

    np.random.seed(42)
    z = np.random.randn(5)

    # Derivada analÃ­tica
    grad_analytic = sigmoid_derivative_analytic(z)

    # Derivada numÃ©rica (para cada elemento)
    def sigmoid_element(z_arr):
        return float(np.sum(sigmoid(z_arr)))  # Suma para tener escalar

    grad_numerical = numerical_gradient(sigmoid_element, z.copy())

    # Comparar
    passed, error = gradient_check(grad_analytic, grad_numerical)

    print(f&quot;Derivada AnalÃ­tica: {grad_analytic}&quot;)
    print(f&quot;Derivada NumÃ©rica:  {grad_numerical}&quot;)
    print(f&quot;Error Relativo: {error:.2e}&quot;)
    print(f&quot;Resultado: {'âœ“ PASSED' if passed else 'âœ— FAILED'}&quot;)

    return passed


# ============================================================
# EJEMPLO: Validar gradiente de una capa lineal
# ============================================================

def test_linear_layer_gradient():
    &quot;&quot;&quot;Test: Validar gradiente de capa lineal y = Wx + b.&quot;&quot;&quot;
    print(&quot;\n&quot; + &quot;=&quot; * 60)
    print(&quot;GRADIENT CHECK: Linear Layer (y = Wx + b)&quot;)
    print(&quot;=&quot; * 60)

    np.random.seed(42)

    # Dimensiones
    n_in, n_out = 4, 3

    # ParÃ¡metros
    W = np.random.randn(n_out, n_in)
    b = np.random.randn(n_out)
    x = np.random.randn(n_in)
    y_true = np.random.randn(n_out)

    # Forward + Loss
    def forward_and_loss(W_flat):
        W_reshaped = W_flat.reshape(n_out, n_in)
        y_pred = W_reshaped @ x + b
        return mse_loss(y_pred, y_true)

    # Gradiente analÃ­tico de W
    y_pred = W @ x + b
    dL_dy = 2 * (y_pred - y_true) / n_out  # Gradiente de MSE
    dL_dW_analytic = np.outer(dL_dy, x)    # âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚y Â· x^T

    # Gradiente numÃ©rico de W
    dL_dW_numerical = numerical_gradient(forward_and_loss, W.flatten().copy())
    dL_dW_numerical = dL_dW_numerical.reshape(n_out, n_in)

    # Comparar
    passed, error = gradient_check(
        dL_dW_analytic.flatten(), 
        dL_dW_numerical.flatten()
    )

    print(f&quot;Error Relativo: {error:.2e}&quot;)
    print(f&quot;Resultado: {'âœ“ PASSED' if passed else 'âœ— FAILED'}&quot;)

    return passed


def main():
    &quot;&quot;&quot;Ejecutar todos los gradient checks.&quot;&quot;&quot;
    print(&quot;\n&quot; + &quot;=&quot; * 60)
    print(&quot;       GRADIENT CHECKING SUITE&quot;)
    print(&quot;       ValidaciÃ³n MatemÃ¡tica v3.3&quot;)
    print(&quot;=&quot; * 60)

    results = []
    results.append((&quot;MSE Loss&quot;, test_mse_gradient()))
    results.append((&quot;Sigmoid&quot;, test_sigmoid_gradient()))
    results.append((&quot;Linear Layer&quot;, test_linear_layer_gradient()))

    print(&quot;\n&quot; + &quot;=&quot; * 60)
    print(&quot;RESUMEN&quot;)
    print(&quot;=&quot; * 60)

    all_passed = True
    for name, passed in results:
        status = &quot;âœ“ PASSED&quot; if passed else &quot;âœ— FAILED&quot;
        print(f&quot;  {name}: {status}&quot;)
        all_passed = all_passed and passed

    print(&quot;-&quot; * 60)
    if all_passed:
        print(&quot;âœ“ TODOS LOS GRADIENT CHECKS PASARON&quot;)
        print(&quot;  Tu implementaciÃ³n de derivadas es correcta.&quot;)
    else:
        print(&quot;âœ— ALGUNOS GRADIENT CHECKS FALLARON&quot;)
        print(&quot;  Revisa tu implementaciÃ³n de backprop.&quot;)

    return all_passed


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<h3>CÃ³mo Usar Gradient Checking</h3>
<pre><code class="language-python"># En tu cÃ³digo de backprop:

# 1. Calcula el gradiente analÃ­tico (tu implementaciÃ³n)
grad_analytic = my_backward_pass(...)

# 2. Calcula el gradiente numÃ©rico
def loss_wrapper(params):
    return forward_pass(params, ...)

grad_numerical = numerical_gradient(loss_wrapper, params)

# 3. Compara
passed, error = gradient_check(grad_analytic, grad_numerical)
if not passed:
    raise ValueError(f&quot;Gradient check failed! Error: {error:.2e}&quot;)
</code></pre>
<h3>Reglas del Gradient Checking</h3>
<table>
<thead>
<tr>
<th>Error Relativo</th>
<th>InterpretaciÃ³n</th>
</tr>
</thead>
<tbody>
<tr>
<td>&lt; 10â»â·</td>
<td>âœ“ Excelente - tu gradiente es correcto</td>
</tr>
<tr>
<td>10â»â· a 10â»âµ</td>
<td>âš ï¸ Sospechoso - revisa tu cÃ³digo</td>
</tr>
<tr>
<td>&gt; 10â»âµ</td>
<td>âœ— Bug - tu backprop estÃ¡ mal</td>
</tr>
</tbody>
</table>
<blockquote>
<p>âš ï¸ <strong>Nota:</strong> Desactiva gradient checking durante el entrenamiento real (es lento). Solo Ãºsalo para validar tu implementaciÃ³n.</p>
</blockquote>
<h2>âœ… Checklist de FinalizaciÃ³n (v3.3)</h2>
<h3>Conocimiento</h3>
<ul>
<li>[ ] Puedo calcular derivadas de funciones comunes (polinomios, exp, log)</li>
<li>[ ] Entiendo derivadas parciales y puedo calcularlas</li>
<li>[ ] Puedo calcular el gradiente de una funciÃ³n multivariable</li>
<li>[ ] ImplementÃ© Gradient Descent desde cero</li>
<li>[ ] Entiendo el efecto del learning rate</li>
<li>[ ] Puedo aplicar la Chain Rule a funciones compuestas</li>
<li>[ ] Entiendo cÃ³mo la Chain Rule se aplica en Backpropagation</li>
<li>[ ] Puedo derivar âˆ‚L/âˆ‚w para una neurona simple</li>
</ul>
<h3>Entregables v3.3</h3>
<ul>
<li>[ ] <code>gradient_descent_demo.py</code> funcional</li>
<li>[ ] <strong><code>grad_check.py</code> implementado y todos los tests pasan</strong></li>
<li>[ ] ValidÃ© mis derivadas de sigmoid, MSE y capa lineal</li>
</ul>
<h3>MetodologÃ­a Feynman</h3>
<ul>
<li>[ ] Puedo explicar Chain Rule en 5 lÃ­neas sin jerga</li>
<li>[ ] Puedo explicar por quÃ© gradient checking funciona</li>
</ul>
<h2>ğŸ”— NavegaciÃ³n</h2>
<table>
<thead>
<tr>
<th>Anterior</th>
<th>Ãndice</th>
<th>Siguiente</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="#mod_02_ALGEBRA_LINEAL_ML">02_ALGEBRA_LINEAL_ML</a></td>
<td><a href="#mod_00_INDICE">00_INDICE</a></td>
<td><a href="#mod_04_PROBABILIDAD_ML">04_PROBABILIDAD_ML</a></td>
</tr>
</tbody>
</table>
            </div>
        
            <!-- MÃ“DULO: 04_PROBABILIDAD_ML.md -->
            <div class="cover-page">
                <div class="cover-content">
                    <div style="font-size:40pt;margin-bottom:20px;">ğŸ’</div>
                    <div id="mod_04_PROBABILIDAD_ML" class="cover-title">MÃ“DULO 04 - PROBABILIDAD PARA ML</div>
                    <div class="cover-subtitle">MS in AI Pathway - ML Specialist v3.0</div>
                    <div class="cover-badge">MATEMÃTICAS APLICADAS A CÃ“DIGO</div>
                </div>
                <div style="position:absolute;bottom:30px;font-size:10pt;opacity:0.6;">DUQUEOM | 2025</div>
            </div>
            <div class="content">
                <h1>MÃ³dulo 04: Probabilidad Esencial para Machine Learning</h1>
<blockquote>
<p><strong>Semana 8 | Prerequisito para entender Loss Functions y GMM</strong><br />
<strong>FilosofÃ­a: Solo la probabilidad que necesitas para la LÃ­nea 1</strong></p>
</blockquote>
<h2>ğŸ¯ Objetivo del MÃ³dulo</h2>
<p>Dominar los <strong>conceptos mÃ­nimos de probabilidad</strong> necesarios para:</p>
<ol>
<li>Entender <strong>Logistic Regression</strong> como modelo probabilÃ­stico</li>
<li>Comprender <strong>Cross-Entropy Loss</strong> y por quÃ© funciona</li>
<li>Prepararte para <strong>Gaussian Mixture Models (GMM)</strong> en Unsupervised</li>
<li>Entender <strong>Softmax</strong> como distribuciÃ³n de probabilidad</li>
</ol>
<blockquote>
<p>âš ï¸ <strong>Nota:</strong> Este NO es el curso completo de Probabilidad (LÃ­nea 2). Es solo lo esencial para ML.</p>
</blockquote>
<h2>ğŸ“š Contenido</h2>
<h3>DÃ­a 1-2: Fundamentos de Probabilidad</h3>
<h4>1.1 Probabilidad BÃ¡sica</h4>
<pre><code>P(A) = casos favorables / casos totales

Propiedades:
- 0 â‰¤ P(A) â‰¤ 1
- P(Î©) = 1 (espacio muestral)
- P(âˆ…) = 0 (evento imposible)
</code></pre>
<h4>1.2 Probabilidad Condicional</h4>
<pre><code>P(A|B) = P(A âˆ© B) / P(B)

&quot;Probabilidad de A dado que B ocurriÃ³&quot;
</code></pre>
<p><strong>Ejemplo en ML:</strong><br />
- P(spam | contiene "gratis") = Â¿QuÃ© tan probable es spam si el email dice "gratis"?</p>
<h4>1.3 Independencia</h4>
<pre><code>A y B son independientes si:
P(A âˆ© B) = P(A) Â· P(B)

Equivalente a:
P(A|B) = P(A)
</code></pre>
<h3>DÃ­a 3-4: Teorema de Bayes (CrÃ­tico para ML)</h3>
<h4>2.1 La FÃ³rmula</h4>
<pre><code>            P(B|A) Â· P(A)
P(A|B) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
               P(B)

Donde:
- P(A|B) = Posterior (lo que queremos calcular)
- P(B|A) = Likelihood (verosimilitud)
- P(A)   = Prior (conocimiento previo)
- P(B)   = Evidence (normalizador)
</code></pre>
<h4>2.2 InterpretaciÃ³n para ML</h4>
<pre><code>              P(datos|clase) Â· P(clase)
P(clase|datos) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                      P(datos)

Ejemplo: ClasificaciÃ³n de spam
- P(spam|palabras) = P(palabras|spam) Â· P(spam) / P(palabras)
</code></pre>
<h4>2.3 ImplementaciÃ³n en Python</h4>
<pre><code class="language-python">import numpy as np

def bayes_classifier(x: np.ndarray, 
                     likelihood_spam: float,
                     likelihood_ham: float,
                     prior_spam: float = 0.3) -&gt; str:
    &quot;&quot;&quot;
    Clasificador Bayesiano simple.

    Args:
        x: CaracterÃ­sticas del email (simplificado)
        likelihood_spam: P(x|spam)
        likelihood_ham: P(x|ham)
        prior_spam: P(spam) - conocimiento previo

    Returns:
        'spam' o 'ham'
    &quot;&quot;&quot;
    prior_ham = 1 - prior_spam

    # Posterior (sin normalizar, solo comparamos)
    posterior_spam = likelihood_spam * prior_spam
    posterior_ham = likelihood_ham * prior_ham

    return 'spam' if posterior_spam &gt; posterior_ham else 'ham'


# Ejemplo: Email con palabra &quot;gratis&quot;
# P(&quot;gratis&quot;|spam) = 0.8, P(&quot;gratis&quot;|ham) = 0.1
result = bayes_classifier(
    x=None,  # simplificado
    likelihood_spam=0.8,
    likelihood_ham=0.1,
    prior_spam=0.3
)
print(f&quot;ClasificaciÃ³n: {result}&quot;)  # spam
</code></pre>
<h4>2.4 Naive Bayes (ConexiÃ³n con Supervised Learning)</h4>
<pre><code class="language-python">def naive_bayes_predict(X: np.ndarray, 
                        class_priors: np.ndarray,
                        feature_probs: dict) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Naive Bayes asume independencia entre features:
    P(x1, x2, ..., xn | clase) = P(x1|clase) Â· P(x2|clase) Â· ... Â· P(xn|clase)

    Esta &quot;ingenuidad&quot; simplifica mucho el cÃ¡lculo.
    &quot;&quot;&quot;
    n_samples = X.shape[0]
    n_classes = len(class_priors)

    log_posteriors = np.zeros((n_samples, n_classes))

    for c in range(n_classes):
        # Log para evitar underflow con muchas features
        log_prior = np.log(class_priors[c])
        log_likelihood = np.sum(np.log(feature_probs[c][X]), axis=1)
        log_posteriors[:, c] = log_prior + log_likelihood

    return np.argmax(log_posteriors, axis=1)
</code></pre>
<h3>DÃ­a 5: DistribuciÃ³n Gaussiana (Normal)</h3>
<h4>3.1 La DistribuciÃ³n MÃ¡s Importante en ML</h4>
<pre><code>                    1              (x - Î¼)Â²
f(x) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Â· exp(- â”€â”€â”€â”€â”€â”€â”€â”€â”€)
       Ïƒ Â· âˆš(2Ï€)                   2ÏƒÂ²

ParÃ¡metros:
- Î¼ (mu): Media (centro de la campana)
- Ïƒ (sigma): DesviaciÃ³n estÃ¡ndar (ancho)
- ÏƒÂ² (sigmaÂ²): Varianza
</code></pre>
<h4>3.2 Por QuÃ© es Importante</h4>
<ol>
<li><strong>Muchos fenÃ³menos naturales</strong> siguen esta distribuciÃ³n</li>
<li><strong>Teorema del LÃ­mite Central:</strong> promedios de cualquier distribuciÃ³n â†’ Normal</li>
<li><strong>GMM usa Gaussianas</strong> para modelar clusters</li>
<li><strong>InicializaciÃ³n de pesos</strong> en redes neuronales</li>
</ol>
<h4>3.3 ImplementaciÃ³n</h4>
<pre><code class="language-python">import numpy as np

def gaussian_pdf(x: np.ndarray, mu: float, sigma: float) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Probability Density Function de la Gaussiana.

    Args:
        x: Puntos donde evaluar
        mu: Media
        sigma: DesviaciÃ³n estÃ¡ndar

    Returns:
        Densidad de probabilidad en cada punto
    &quot;&quot;&quot;
    coefficient = 1 / (sigma * np.sqrt(2 * np.pi))
    exponent = -((x - mu) ** 2) / (2 * sigma ** 2)
    return coefficient * np.exp(exponent)


# VisualizaciÃ³n
import matplotlib.pyplot as plt

x = np.linspace(-5, 5, 1000)

# Diferentes Gaussianas
plt.figure(figsize=(10, 6))
plt.plot(x, gaussian_pdf(x, mu=0, sigma=1), label='Î¼=0, Ïƒ=1 (estÃ¡ndar)')
plt.plot(x, gaussian_pdf(x, mu=0, sigma=2), label='Î¼=0, Ïƒ=2 (mÃ¡s ancha)')
plt.plot(x, gaussian_pdf(x, mu=2, sigma=1), label='Î¼=2, Ïƒ=1 (desplazada)')
plt.legend()
plt.title('Distribuciones Gaussianas')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.grid(True)
plt.savefig('gaussian_distributions.png')
</code></pre>
<h4>3.4 Gaussiana Multivariada (Para GMM)</h4>
<pre><code class="language-python">def multivariate_gaussian_pdf(x: np.ndarray, 
                               mu: np.ndarray, 
                               cov: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Gaussiana multivariada para vectores.

    Args:
        x: Vector de caracterÃ­sticas (d,)
        mu: Vector de medias (d,)
        cov: Matriz de covarianza (d, d)

    Returns:
        Densidad de probabilidad
    &quot;&quot;&quot;
    d = len(mu)
    diff = x - mu

    # Determinante e inversa de la covarianza
    det_cov = np.linalg.det(cov)
    inv_cov = np.linalg.inv(cov)

    # Coeficiente de normalizaciÃ³n
    coefficient = 1 / (np.sqrt((2 * np.pi) ** d * det_cov))

    # Exponente (forma cuadrÃ¡tica)
    exponent = -0.5 * diff.T @ inv_cov @ diff

    return coefficient * np.exp(exponent)


# Ejemplo 2D
mu = np.array([0, 0])
cov = np.array([[1, 0.5], 
                [0.5, 1]])  # CorrelaciÃ³n positiva

x = np.array([0.5, 0.5])
prob = multivariate_gaussian_pdf(x, mu, cov)
print(f&quot;P(x=[0.5, 0.5]) = {prob:.4f}&quot;)
</code></pre>
<h3>DÃ­a 6: Maximum Likelihood Estimation (MLE)</h3>
<h4>4.1 La Idea Central</h4>
<pre><code>MLE: Encontrar los parÃ¡metros Î¸ que maximizan la probabilidad 
     de observar los datos que tenemos.

Î¸_MLE = argmax P(datos | Î¸)
            Î¸
</code></pre>
<h4>4.2 Por QuÃ© es Fundamental</h4>
<ul>
<li><strong>Logistic Regression</strong> usa MLE para encontrar los pesos</li>
<li><strong>Cross-Entropy Loss</strong> viene de maximizar likelihood</li>
<li><strong>GMM</strong> usa MLE (via EM algorithm)</li>
</ul>
<h4>4.3 MLE para Gaussiana</h4>
<pre><code class="language-python">def mle_gaussian(data: np.ndarray) -&gt; tuple[float, float]:
    &quot;&quot;&quot;
    Estimar parÃ¡metros de Gaussiana con MLE.

    Para una Gaussiana, los estimadores MLE son:
    - Î¼_MLE = media muestral
    - ÏƒÂ²_MLE = varianza muestral (con n, no n-1)

    Args:
        data: Muestras observadas

    Returns:
        (mu_mle, sigma_mle)
    &quot;&quot;&quot;
    n = len(data)

    # MLE de la media
    mu_mle = np.mean(data)

    # MLE de la varianza (dividir por n, no n-1)
    sigma_squared_mle = np.sum((data - mu_mle) ** 2) / n
    sigma_mle = np.sqrt(sigma_squared_mle)

    return mu_mle, sigma_mle


# Ejemplo: Generar datos y estimar
np.random.seed(42)
true_mu, true_sigma = 5.0, 2.0
samples = np.random.normal(true_mu, true_sigma, size=1000)

estimated_mu, estimated_sigma = mle_gaussian(samples)
print(f&quot;ParÃ¡metros reales: Î¼={true_mu}, Ïƒ={true_sigma}&quot;)
print(f&quot;MLE estimados:     Î¼={estimated_mu:.3f}, Ïƒ={estimated_sigma:.3f}&quot;)
</code></pre>
<h4>4.4 ConexiÃ³n con Cross-Entropy Loss</h4>
<pre><code class="language-python">def cross_entropy_from_mle():
    &quot;&quot;&quot;
    DemostraciÃ³n de que Cross-Entropy viene de MLE.

    Para clasificaciÃ³n binaria con Bernoulli:
    P(y|x, Î¸) = p^y Â· (1-p)^(1-y)

    Donde p = Ïƒ(Î¸áµ€x) (predicciÃ³n del modelo)

    Log-likelihood:
    log P(y|x, Î¸) = yÂ·log(p) + (1-y)Â·log(1-p)

    Maximizar likelihood = Minimizar negative log-likelihood
    = Minimizar Cross-Entropy!
    &quot;&quot;&quot;
    # Ejemplo numÃ©rico
    y_true = np.array([1, 0, 1, 1, 0])
    y_pred = np.array([0.9, 0.1, 0.8, 0.7, 0.2])  # Probabilidades

    # Cross-Entropy (negative log-likelihood promedio)
    epsilon = 1e-15  # Para evitar log(0)
    ce = -np.mean(
        y_true * np.log(y_pred + epsilon) + 
        (1 - y_true) * np.log(1 - y_pred + epsilon)
    )

    print(f&quot;Cross-Entropy Loss: {ce:.4f}&quot;)
    return ce

cross_entropy_from_mle()
</code></pre>
<h3>DÃ­a 7: Softmax como DistribuciÃ³n de Probabilidad</h3>
<h4>5.1 De Logits a Probabilidades</h4>
<pre><code>                    exp(záµ¢)
softmax(z)áµ¢ = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              Î£â±¼ exp(zâ±¼)

Propiedades:
- Cada salida âˆˆ (0, 1)
- Suma de salidas = 1 (distribuciÃ³n vÃ¡lida)
- Preserva el orden (mayor logit â†’ mayor probabilidad)
</code></pre>
<h4>5.2 El Problema de Estabilidad NumÃ©rica (v3.3)</h4>
<pre><code>âš ï¸ PROBLEMA: exp() puede causar overflow/underflow

Ejemplo peligroso:
    z = [1000, 1001, 1002]
    exp(z) = [inf, inf, inf]  â†’ NaN en softmax!

Ejemplo underflow:
    z = [-1000, -1001, -1002]
    exp(z) = [0, 0, 0]  â†’ 0/0 = NaN!
</code></pre>
<h4>5.3 Log-Sum-Exp Trick (Estabilidad NumÃ©rica)</h4>
<pre><code>TRUCO: softmax(z) = softmax(z - max(z))

DemostraciÃ³n:
    softmax(z - c)áµ¢ = exp(záµ¢ - c) / Î£â±¼ exp(zâ±¼ - c)
                    = exp(záµ¢)Â·exp(-c) / Î£â±¼ exp(zâ±¼)Â·exp(-c)
                    = exp(záµ¢) / Î£â±¼ exp(zâ±¼)
                    = softmax(z)áµ¢

Al restar max(z), todos los exponentes son â‰¤ 0, evitando overflow.
</code></pre>
<h4>5.4 ImplementaciÃ³n NumÃ©ricamente Estable</h4>
<pre><code class="language-python">def softmax(z: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Softmax numÃ©ricamente estable usando Log-Sum-Exp trick.

    Truco: Restar el mÃ¡ximo para evitar overflow en exp()
    softmax(z) = softmax(z - max(z))

    Args:
        z: Logits (scores antes de activaciÃ³n)

    Returns:
        Probabilidades que suman 1
    &quot;&quot;&quot;
    # Log-Sum-Exp trick: restar el mÃ¡ximo
    z_stable = z - np.max(z, axis=-1, keepdims=True)

    exp_z = np.exp(z_stable)
    return exp_z / np.sum(exp_z, axis=-1, keepdims=True)


def log_softmax(z: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Log-Softmax estable (Ãºtil para Cross-Entropy).

    log(softmax(z)) calculado de forma estable.
    Evita calcular softmax primero y luego log (pierde precisiÃ³n).
    &quot;&quot;&quot;
    z_stable = z - np.max(z, axis=-1, keepdims=True)
    log_sum_exp = np.log(np.sum(np.exp(z_stable), axis=-1, keepdims=True))
    return z_stable - log_sum_exp


# ============================================================
# DEMOSTRACIÃ“N: Por quÃ© el trick es necesario
# ============================================================

def demo_numerical_stability():
    &quot;&quot;&quot;Muestra por quÃ© necesitamos el Log-Sum-Exp trick.&quot;&quot;&quot;

    # Caso peligroso: logits muy grandes
    z_dangerous = np.array([1000.0, 1001.0, 1002.0])

    # Sin el trick (INCORRECTO)
    def softmax_naive(z):
        exp_z = np.exp(z)  # Â¡Overflow!
        return exp_z / np.sum(exp_z)

    # Con el trick (CORRECTO)
    def softmax_stable(z):
        z_stable = z - np.max(z)
        exp_z = np.exp(z_stable)
        return exp_z / np.sum(exp_z)

    print(&quot;Logits peligrosos:&quot;, z_dangerous)
    print()

    # Naive (falla)
    import warnings
    with warnings.catch_warnings():
        warnings.simplefilter(&quot;ignore&quot;)
        result_naive = softmax_naive(z_dangerous)
        print(f&quot;Softmax NAIVE: {result_naive}&quot;)
        print(f&quot;  â†’ Suma: {np.sum(result_naive)} (deberÃ­a ser 1.0)&quot;)

    # Estable (funciona)
    result_stable = softmax_stable(z_dangerous)
    print(f&quot;\nSoftmax ESTABLE: {result_stable}&quot;)
    print(f&quot;  â†’ Suma: {np.sum(result_stable):.6f} âœ“&quot;)

demo_numerical_stability()


# Ejemplo: ClasificaciÃ³n multiclase (dÃ­gitos 0-9)
logits = np.array([2.0, 1.0, 0.1, -1.0, 3.0, 0.5, -0.5, 1.5, 0.0, -2.0])
probs = softmax(logits)

print(&quot;\nLogits â†’ Probabilidades:&quot;)
for i, (l, p) in enumerate(zip(logits, probs)):
    print(f&quot;  Clase {i}: logit={l:+.1f} â†’ prob={p:.3f}&quot;)
print(f&quot;\nSuma de probabilidades: {np.sum(probs):.6f}&quot;)
print(f&quot;Clase predicha: {np.argmax(probs)}&quot;)
</code></pre>
<h4>5.3 Categorical Cross-Entropy (Multiclase)</h4>
<pre><code class="language-python">def categorical_cross_entropy(y_true: np.ndarray, 
                               y_pred: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Loss para clasificaciÃ³n multiclase.

    Args:
        y_true: One-hot encoded labels (n_samples, n_classes)
        y_pred: Probabilidades softmax (n_samples, n_classes)

    Returns:
        Loss promedio
    &quot;&quot;&quot;
    epsilon = 1e-15
    # Solo cuenta la clase correcta (donde y_true=1)
    return -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=1))


# Ejemplo
y_true = np.array([
    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],  # Clase 4
    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # Clase 0
])

y_pred = np.array([
    softmax(np.array([0, 0, 0, 0, 5, 0, 0, 0, 0, 0])),  # Confiado en 4
    softmax(np.array([3, 1, 0, 0, 0, 0, 0, 0, 0, 0])),  # Confiado en 0
])

loss = categorical_cross_entropy(y_true, y_pred)
print(f&quot;Categorical Cross-Entropy: {loss:.4f}&quot;)
</code></pre>
<h2>ğŸ”¨ Entregables del MÃ³dulo</h2>
<h3>E1: <code>probability.py</code></h3>
<pre><code class="language-python">&quot;&quot;&quot;
MÃ³dulo de probabilidad esencial para ML.
Implementaciones desde cero con NumPy.
&quot;&quot;&quot;

import numpy as np
from typing import Tuple

def gaussian_pdf(x: np.ndarray, mu: float, sigma: float) -&gt; np.ndarray:
    &quot;&quot;&quot;Densidad de probabilidad Gaussiana univariada.&quot;&quot;&quot;
    pass

def multivariate_gaussian_pdf(x: np.ndarray, 
                               mu: np.ndarray, 
                               cov: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Densidad de probabilidad Gaussiana multivariada.&quot;&quot;&quot;
    pass

def mle_gaussian(data: np.ndarray) -&gt; Tuple[float, float]:
    &quot;&quot;&quot;EstimaciÃ³n MLE de parÃ¡metros de Gaussiana.&quot;&quot;&quot;
    pass

def softmax(z: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;FunciÃ³n softmax numÃ©ricamente estable.&quot;&quot;&quot;
    pass

def cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Binary cross-entropy loss.&quot;&quot;&quot;
    pass

def categorical_cross_entropy(y_true: np.ndarray, 
                               y_pred: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Categorical cross-entropy loss para multiclase.&quot;&quot;&quot;
    pass
</code></pre>
<h3>E2: Tests</h3>
<pre><code class="language-python"># tests/test_probability.py
import numpy as np
import pytest
from src.probability import (
    gaussian_pdf, mle_gaussian, softmax, 
    cross_entropy, categorical_cross_entropy
)

def test_gaussian_pdf_standard():
    &quot;&quot;&quot;PDF de Gaussiana estÃ¡ndar en x=0 debe ser ~0.3989.&quot;&quot;&quot;
    result = gaussian_pdf(np.array([0.0]), mu=0, sigma=1)
    expected = 1 / np.sqrt(2 * np.pi)  # ~0.3989
    assert np.isclose(result[0], expected, rtol=1e-5)

def test_softmax_sums_to_one():
    &quot;&quot;&quot;Softmax debe sumar 1.&quot;&quot;&quot;
    z = np.random.randn(10)
    probs = softmax(z)
    assert np.isclose(np.sum(probs), 1.0)

def test_softmax_preserves_order():
    &quot;&quot;&quot;Mayor logit â†’ mayor probabilidad.&quot;&quot;&quot;
    z = np.array([1.0, 2.0, 3.0])
    probs = softmax(z)
    assert probs[2] &gt; probs[1] &gt; probs[0]

def test_mle_gaussian_accuracy():
    &quot;&quot;&quot;MLE debe recuperar parÃ¡metros con suficientes datos.&quot;&quot;&quot;
    np.random.seed(42)
    true_mu, true_sigma = 10.0, 3.0
    data = np.random.normal(true_mu, true_sigma, size=10000)

    est_mu, est_sigma = mle_gaussian(data)

    assert np.isclose(est_mu, true_mu, rtol=0.05)
    assert np.isclose(est_sigma, true_sigma, rtol=0.05)

def test_cross_entropy_perfect_prediction():
    &quot;&quot;&quot;CE debe ser ~0 para predicciones perfectas.&quot;&quot;&quot;
    y_true = np.array([1, 0, 1])
    y_pred = np.array([0.999, 0.001, 0.999])

    loss = cross_entropy(y_true, y_pred)
    assert loss &lt; 0.01
</code></pre>
<h2>ğŸ“Š Resumen Visual</h2>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PROBABILIDAD PARA ML - MAPA CONCEPTUAL                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  TEOREMA DE BAYES                                               â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€â–º Naive Bayes Classifier (MÃ³dulo 05)                   â”‚
â”‚       â””â”€â”€â–º IntuiciÃ³n de posterior vs prior                      â”‚
â”‚                                                                 â”‚
â”‚  DISTRIBUCIÃ“N GAUSSIANA                                         â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€â–º GMM en Unsupervised (MÃ³dulo 06)                      â”‚
â”‚       â”œâ”€â”€â–º InicializaciÃ³n de pesos en DL (MÃ³dulo 07)            â”‚
â”‚       â””â”€â”€â–º NormalizaciÃ³n de datos                               â”‚
â”‚                                                                 â”‚
â”‚  MAXIMUM LIKELIHOOD (MLE)                                       â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€â–º Cross-Entropy Loss (Logistic Regression)             â”‚
â”‚       â”œâ”€â”€â–º Categorical CE (Softmax + Multiclase)                â”‚
â”‚       â””â”€â”€â–º EM Algorithm en GMM                                  â”‚
â”‚                                                                 â”‚
â”‚  SOFTMAX                                                        â”‚
â”‚       â”‚                                                         â”‚
â”‚       â””â”€â”€â–º Capa de salida en clasificaciÃ³n multiclase           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h2>ğŸ”— Conexiones con Otros MÃ³dulos</h2>
<table>
<thead>
<tr>
<th>Concepto</th>
<th>DÃ³nde se usa</th>
</tr>
</thead>
<tbody>
<tr>
<td>Teorema de Bayes</td>
<td>Naive Bayes en MÃ³dulo 05</td>
</tr>
<tr>
<td>Gaussiana</td>
<td>GMM en MÃ³dulo 06, inicializaciÃ³n en MÃ³dulo 07</td>
</tr>
<tr>
<td>MLE</td>
<td>DerivaciÃ³n de Cross-Entropy en MÃ³dulo 05</td>
</tr>
<tr>
<td>Softmax</td>
<td>Capa de salida en MÃ³dulo 07</td>
</tr>
<tr>
<td>Cross-Entropy</td>
<td>Loss function principal en MÃ³dulo 05 y 07</td>
</tr>
</tbody>
</table>
<h2>âœ… Checklist del MÃ³dulo</h2>
<ul>
<li>[ ] Puedo explicar el Teorema de Bayes con un ejemplo</li>
<li>[ ] SÃ© calcular la PDF de una Gaussiana a mano</li>
<li>[ ] Entiendo por quÃ© MLE da Cross-Entropy como loss</li>
<li>[ ] ImplementÃ© softmax numÃ©ricamente estable</li>
<li>[ ] Los tests de <code>probability.py</code> pasan</li>
</ul>
<h2>ğŸ“– Recursos Adicionales</h2>
<h3>Videos</h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=HZGCoVF3YvM">3Blue1Brown - Bayes Theorem</a></li>
<li><a href="https://www.youtube.com/watch?v=XepXtl9YKwc">StatQuest - Maximum Likelihood</a></li>
<li><a href="https://www.youtube.com/watch?v=rzFX5NWojp0">StatQuest - Gaussian Distribution</a></li>
</ul>
<h3>Lecturas</h3>
<ul>
<li>Mathematics for ML, Cap. 6 (Probability)</li>
<li>Pattern Recognition and ML (Bishop), Cap. 1-2</li>
</ul>
<blockquote>
<p>ğŸ’¡ <strong>Nota Final:</strong> Este mÃ³dulo es deliberadamente corto (1 semana). No necesitas ser experto en probabilidad para la LÃ­nea 1, pero estos conceptos son el "pegamento" que conecta las matemÃ¡ticas con las funciones de pÃ©rdida que usarÃ¡s en los siguientes mÃ³dulos.</p>
</blockquote>
            </div>
        
            <!-- MÃ“DULO: 05_SUPERVISED_LEARNING.md -->
            <div class="cover-page">
                <div class="cover-content">
                    <div style="font-size:40pt;margin-bottom:20px;">ğŸ’</div>
                    <div id="mod_05_SUPERVISED_LEARNING" class="cover-title">MÃ“DULO 05 - SUPERVISED LEARNING</div>
                    <div class="cover-subtitle">MS in AI Pathway - ML Specialist v3.0</div>
                    <div class="cover-badge">MATEMÃTICAS APLICADAS A CÃ“DIGO</div>
                </div>
                <div style="position:absolute;bottom:30px;font-size:10pt;opacity:0.6;">DUQUEOM | 2025</div>
            </div>
            <div class="content">
                <h1>MÃ³dulo 05 - Supervised Learning</h1>
<blockquote>
<p><strong>ğŸ¯ Objetivo:</strong> Dominar regresiÃ³n lineal, logÃ­stica y mÃ©tricas de evaluaciÃ³n<br />
<strong>Fase:</strong> 2 - NÃºcleo de ML | <strong>Semanas 9-12</strong><br />
<strong>Curso del Pathway:</strong> Introduction to Machine Learning: Supervised Learning</p>
</blockquote>
<h2>ğŸ§  Â¿QuÃ© es Supervised Learning?</h2>
<pre><code>APRENDIZAJE SUPERVISADO

Tenemos:
- Datos de entrada X (features)
- Etiquetas Y (targets/labels)

Objetivo: Aprender una funciÃ³n f tal que f(X) â‰ˆ Y

Tipos principales:
â”œâ”€â”€ REGRESIÃ“N: Y es continuo (precio, temperatura)
â”‚   â””â”€â”€ Output: nÃºmero real
â””â”€â”€ CLASIFICACIÃ“N: Y es discreto (spam/no spam, dÃ­gito 0-9)
    â””â”€â”€ Output: clase o probabilidad
</code></pre>
<h2>ğŸ“š Contenido del MÃ³dulo</h2>
<table>
<thead>
<tr>
<th>Semana</th>
<th>Tema</th>
<th>Entregable</th>
</tr>
</thead>
<tbody>
<tr>
<td>9</td>
<td>RegresiÃ³n Lineal</td>
<td><code>linear_regression.py</code></td>
</tr>
<tr>
<td>10</td>
<td>RegresiÃ³n LogÃ­stica</td>
<td><code>logistic_regression.py</code></td>
</tr>
<tr>
<td>11</td>
<td>MÃ©tricas de EvaluaciÃ³n</td>
<td><code>metrics.py</code></td>
</tr>
<tr>
<td>12</td>
<td>ValidaciÃ³n y RegularizaciÃ³n</td>
<td>Cross-validation, L1/L2</td>
</tr>
</tbody>
</table>
<h2>ğŸ’» Parte 1: RegresiÃ³n Lineal</h2>
<h3>1.1 Modelo</h3>
<pre><code class="language-python">import numpy as np

&quot;&quot;&quot;
REGRESIÃ“N LINEAL

HipÃ³tesis: h(x) = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚™xâ‚™
         = Î¸áµ€x (forma matricial)

Donde:
- Î¸ (theta): parÃ¡metros/pesos del modelo
- x: vector de features (con xâ‚€ = 1 para el bias)

En forma matricial para mÃºltiples muestras:
    Å· = XÎ¸

Donde:
- X: matriz (m Ã— n+1) con m muestras y n features + columna de 1s
- Î¸: vector (n+1 Ã— 1) de parÃ¡metros
- Å·: vector (m Ã— 1) de predicciones
&quot;&quot;&quot;

def add_bias_term(X: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;AÃ±ade columna de 1s para el tÃ©rmino de bias.&quot;&quot;&quot;
    m = X.shape[0]
    return np.column_stack([np.ones(m), X])

def predict_linear(X: np.ndarray, theta: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;PredicciÃ³n lineal: Å· = XÎ¸&quot;&quot;&quot;
    return X @ theta
</code></pre>
<h3>1.2 FunciÃ³n de Costo (MSE)</h3>
<pre><code class="language-python">import numpy as np

def mse_cost(X: np.ndarray, y: np.ndarray, theta: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Mean Squared Error Cost Function.

    J(Î¸) = (1/2m) Î£áµ¢ (h(xáµ¢) - yáµ¢)Â²
         = (1/2m) ||XÎ¸ - y||Â²

    El factor 1/2 es por conveniencia (cancela con la derivada).
    &quot;&quot;&quot;
    m = len(y)
    predictions = X @ theta
    errors = predictions - y
    return (1 / (2 * m)) * np.sum(errors ** 2)

def mse_gradient(X: np.ndarray, y: np.ndarray, theta: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Gradiente del MSE respecto a Î¸.

    âˆ‚J/âˆ‚Î¸ = (1/m) Xáµ€(XÎ¸ - y)
    &quot;&quot;&quot;
    m = len(y)
    predictions = X @ theta
    errors = predictions - y
    return (1 / m) * X.T @ errors
</code></pre>
<h3>1.3 SoluciÃ³n Cerrada (Normal Equation)</h3>
<pre><code class="language-python">import numpy as np

def normal_equation(X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    SoluciÃ³n cerrada para regresiÃ³n lineal.

    Î¸ = (Xáµ€X)â»Â¹ Xáµ€y

    Ventajas:
    - No requiere iteraciones
    - No hay hiperparÃ¡metros (learning rate)

    Desventajas:
    - O(nÂ³) por la inversiÃ³n de matriz
    - No funciona si Xáµ€X es singular
    - No escala bien para n grande (&gt;10,000 features)
    &quot;&quot;&quot;
    XtX = X.T @ X
    Xty = X.T @ y

    # Usar solve en lugar de inv para estabilidad numÃ©rica
    theta = np.linalg.solve(XtX, Xty)
    return theta
</code></pre>
<h3>1.4 Gradient Descent para RegresiÃ³n</h3>
<pre><code class="language-python">import numpy as np
from typing import List, Tuple

class LinearRegression:
    &quot;&quot;&quot;RegresiÃ³n Lineal implementada desde cero.&quot;&quot;&quot;

    def __init__(self):
        self.theta = None
        self.cost_history = []

    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        method: str = 'gradient_descent',
        learning_rate: float = 0.01,
        n_iterations: int = 1000
    ) -&gt; 'LinearRegression':
        &quot;&quot;&quot;
        Entrena el modelo.

        Args:
            X: features (m, n)
            y: targets (m,)
            method: 'gradient_descent' o 'normal_equation'
            learning_rate: tasa de aprendizaje (solo para GD)
            n_iterations: nÃºmero de iteraciones (solo para GD)
        &quot;&quot;&quot;
        # AÃ±adir bias
        X_b = add_bias_term(X)
        m, n = X_b.shape

        if method == 'normal_equation':
            self.theta = normal_equation(X_b, y)
        else:
            # Inicializar theta con ceros o valores pequeÃ±os
            self.theta = np.zeros(n)

            for i in range(n_iterations):
                # Calcular gradiente
                gradient = mse_gradient(X_b, y, self.theta)

                # Actualizar theta
                self.theta = self.theta - learning_rate * gradient

                # Guardar costo para monitoreo
                cost = mse_cost(X_b, y, self.theta)
                self.cost_history.append(cost)

        return self

    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;Predice valores.&quot;&quot;&quot;
        X_b = add_bias_term(X)
        return X_b @ self.theta

    def score(self, X: np.ndarray, y: np.ndarray) -&gt; float:
        &quot;&quot;&quot;RÂ² score.&quot;&quot;&quot;
        y_pred = self.predict(X)
        ss_res = np.sum((y - y_pred) ** 2)
        ss_tot = np.sum((y - np.mean(y)) ** 2)
        return 1 - (ss_res / ss_tot)


# Demo
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X.flatten() + np.random.randn(100) * 0.5  # y = 4 + 3x + ruido

model = LinearRegression()
model.fit(X, y, method='gradient_descent', learning_rate=0.1, n_iterations=1000)

print(f&quot;ParÃ¡metros aprendidos: {model.theta}&quot;)
print(f&quot;Esperados: [4, 3]&quot;)
print(f&quot;RÂ² score: {model.score(X, y):.4f}&quot;)
</code></pre>
<h2>ğŸ’» Parte 2: RegresiÃ³n LogÃ­stica</h2>
<h3>2.1 FunciÃ³n Sigmoid</h3>
<pre><code class="language-python">import numpy as np

def sigmoid(z: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    FunciÃ³n sigmoid/logÃ­stica.

    Ïƒ(z) = 1 / (1 + e^(-z))

    Propiedades:
    - Rango: (0, 1) - perfecto para probabilidades
    - Ïƒ(0) = 0.5
    - Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z))
    &quot;&quot;&quot;
    # Clip para evitar overflow
    z = np.clip(z, -500, 500)
    return 1 / (1 + np.exp(-z))

# Visualizar
import matplotlib.pyplot as plt

z = np.linspace(-10, 10, 100)
plt.figure(figsize=(8, 4))
plt.plot(z, sigmoid(z))
plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5)
plt.axvline(x=0, color='r', linestyle='--', alpha=0.5)
plt.xlabel('z')
plt.ylabel('Ïƒ(z)')
plt.title('FunciÃ³n Sigmoid')
plt.grid(True)
# plt.show()
</code></pre>
<h3>2.2 HipÃ³tesis LogÃ­stica</h3>
<pre><code class="language-python">&quot;&quot;&quot;
REGRESIÃ“N LOGÃSTICA

No predice un valor continuo, sino la PROBABILIDAD de pertenecer a la clase 1.

h(x) = P(y=1|x; Î¸) = Ïƒ(Î¸áµ€x)

DecisiÃ³n:
- Si h(x) â‰¥ 0.5 â†’ predicir clase 1
- Si h(x) &lt; 0.5 â†’ predicir clase 0

Equivalente a:
- Si Î¸áµ€x â‰¥ 0 â†’ clase 1
- Si Î¸áµ€x &lt; 0 â†’ clase 0

El &quot;decision boundary&quot; estÃ¡ en Î¸áµ€x = 0
&quot;&quot;&quot;

def predict_proba(X: np.ndarray, theta: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;Predice probabilidad de clase 1.&quot;&quot;&quot;
    return sigmoid(X @ theta)

def predict_class(X: np.ndarray, theta: np.ndarray, threshold: float = 0.5) -&gt; np.ndarray:
    &quot;&quot;&quot;Predice clase (0 o 1).&quot;&quot;&quot;
    return (predict_proba(X, theta) &gt;= threshold).astype(int)
</code></pre>
<h3>2.3 Binary Cross-Entropy Loss</h3>
<pre><code class="language-python">import numpy as np

def binary_cross_entropy(
    X: np.ndarray,
    y: np.ndarray,
    theta: np.ndarray,
    eps: float = 1e-15
) -&gt; float:
    &quot;&quot;&quot;
    Binary Cross-Entropy (Log Loss).

    J(Î¸) = -(1/m) Î£áµ¢ [yáµ¢ log(háµ¢) + (1-yáµ¢) log(1-háµ¢)]

    Donde háµ¢ = Ïƒ(Î¸áµ€xáµ¢)

    Por quÃ© esta funciÃ³n de costo:
    - Es convexa (tiene un Ãºnico mÃ­nimo global)
    - Penaliza mucho las predicciones muy incorrectas
    - Es la derivaciÃ³n de Maximum Likelihood Estimation
    &quot;&quot;&quot;
    m = len(y)
    h = sigmoid(X @ theta)

    # Clip para evitar log(0)
    h = np.clip(h, eps, 1 - eps)

    cost = -(1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
    return cost

def bce_gradient(X: np.ndarray, y: np.ndarray, theta: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Gradiente de Binary Cross-Entropy.

    âˆ‚J/âˆ‚Î¸ = (1/m) Xáµ€(h - y)

    Â¡Tiene la misma forma que el gradiente del MSE!
    Esto es porque derivamos Ïƒ(z) y la derivada Ïƒ'(z) = Ïƒ(z)(1-Ïƒ(z))
    cancela parte de la expresiÃ³n.
    &quot;&quot;&quot;
    m = len(y)
    h = sigmoid(X @ theta)
    return (1/m) * X.T @ (h - y)
</code></pre>
<h3>2.4 ImplementaciÃ³n Completa</h3>
<pre><code class="language-python">import numpy as np
from typing import List

class LogisticRegression:
    &quot;&quot;&quot;RegresiÃ³n LogÃ­stica implementada desde cero.&quot;&quot;&quot;

    def __init__(self):
        self.theta = None
        self.cost_history = []

    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        learning_rate: float = 0.1,
        n_iterations: int = 1000
    ) -&gt; 'LogisticRegression':
        &quot;&quot;&quot;Entrena con gradient descent.&quot;&quot;&quot;
        # AÃ±adir bias
        X_b = np.column_stack([np.ones(len(X)), X])
        m, n = X_b.shape

        # Inicializar
        self.theta = np.zeros(n)

        for i in range(n_iterations):
            # Gradiente
            gradient = bce_gradient(X_b, y, self.theta)

            # Actualizar
            self.theta = self.theta - learning_rate * gradient

            # Guardar costo
            cost = binary_cross_entropy(X_b, y, self.theta)
            self.cost_history.append(cost)

        return self

    def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;Predice probabilidades.&quot;&quot;&quot;
        X_b = np.column_stack([np.ones(len(X)), X])
        return sigmoid(X_b @ self.theta)

    def predict(self, X: np.ndarray, threshold: float = 0.5) -&gt; np.ndarray:
        &quot;&quot;&quot;Predice clases.&quot;&quot;&quot;
        return (self.predict_proba(X) &gt;= threshold).astype(int)

    def score(self, X: np.ndarray, y: np.ndarray) -&gt; float:
        &quot;&quot;&quot;Accuracy.&quot;&quot;&quot;
        return np.mean(self.predict(X) == y)


# Demo con datos sintÃ©ticos
np.random.seed(42)

# Generar datos de dos clases
n_samples = 200
X_class0 = np.random.randn(n_samples // 2, 2) + np.array([-2, -2])
X_class1 = np.random.randn(n_samples // 2, 2) + np.array([2, 2])
X = np.vstack([X_class0, X_class1])
y = np.array([0] * (n_samples // 2) + [1] * (n_samples // 2))

# Entrenar
model = LogisticRegression()
model.fit(X, y, learning_rate=0.1, n_iterations=1000)

print(f&quot;Accuracy: {model.score(X, y):.2%}&quot;)
print(f&quot;ParÃ¡metros: {model.theta}&quot;)
</code></pre>
<h2>ğŸ’» Parte 3: MÃ©tricas de EvaluaciÃ³n</h2>
<h3>3.1 Matriz de ConfusiÃ³n</h3>
<pre><code class="language-python">import numpy as np

def confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Calcula la matriz de confusiÃ³n.

    Para clasificaciÃ³n binaria:

                    Predicho
                    0       1
    Real    0      TN      FP
            1      FN      TP

    - TP (True Positive): Predijo 1, era 1
    - TN (True Negative): Predijo 0, era 0
    - FP (False Positive): Predijo 1, era 0 (Error Tipo I)
    - FN (False Negative): Predijo 0, era 1 (Error Tipo II)
    &quot;&quot;&quot;
    classes = np.unique(np.concatenate([y_true, y_pred]))
    n_classes = len(classes)
    cm = np.zeros((n_classes, n_classes), dtype=int)

    for i, true_class in enumerate(classes):
        for j, pred_class in enumerate(classes):
            cm[i, j] = np.sum((y_true == true_class) &amp; (y_pred == pred_class))

    return cm

def extract_tp_tn_fp_fn(y_true: np.ndarray, y_pred: np.ndarray):
    &quot;&quot;&quot;Extrae TP, TN, FP, FN para clasificaciÃ³n binaria.&quot;&quot;&quot;
    tp = np.sum((y_true == 1) &amp; (y_pred == 1))
    tn = np.sum((y_true == 0) &amp; (y_pred == 0))
    fp = np.sum((y_true == 0) &amp; (y_pred == 1))
    fn = np.sum((y_true == 1) &amp; (y_pred == 0))
    return tp, tn, fp, fn
</code></pre>
<h3>3.2 Accuracy, Precision, Recall, F1</h3>
<pre><code class="language-python">import numpy as np

def accuracy(y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Accuracy = (TP + TN) / (TP + TN + FP + FN)

    ProporciÃ³n de predicciones correctas.

    Problema: Puede ser engaÃ±oso con clases desbalanceadas.
    Si 99% son clase 0, predecir siempre 0 da 99% accuracy.
    &quot;&quot;&quot;
    return np.mean(y_true == y_pred)

def precision(y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Precision = TP / (TP + FP)

    De todos los que predije como positivos, Â¿cuÃ¡ntos realmente lo son?

    Alta precisiÃ³n = pocos falsos positivos.
    Importante cuando el costo de FP es alto (ej: spam â†’ inbox).
    &quot;&quot;&quot;
    tp, tn, fp, fn = extract_tp_tn_fp_fn(y_true, y_pred)
    if tp + fp == 0:
        return 0.0
    return tp / (tp + fp)

def recall(y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Recall (Sensitivity, True Positive Rate) = TP / (TP + FN)

    De todos los positivos reales, Â¿cuÃ¡ntos capturÃ©?

    Alto recall = pocos falsos negativos.
    Importante cuando el costo de FN es alto (ej: detecciÃ³n de cÃ¡ncer).
    &quot;&quot;&quot;
    tp, tn, fp, fn = extract_tp_tn_fp_fn(y_true, y_pred)
    if tp + fn == 0:
        return 0.0
    return tp / (tp + fn)

def f1_score(y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    F1 = 2 * (precision * recall) / (precision + recall)

    Media armÃ³nica de precision y recall.

    Ãštil cuando quieres un balance entre ambas mÃ©tricas.
    F1 alto solo si AMBAS precision y recall son altas.
    &quot;&quot;&quot;
    p = precision(y_true, y_pred)
    r = recall(y_true, y_pred)
    if p + r == 0:
        return 0.0
    return 2 * (p * r) / (p + r)

def specificity(y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Specificity (True Negative Rate) = TN / (TN + FP)

    De todos los negativos reales, Â¿cuÃ¡ntos identifiquÃ©?
    &quot;&quot;&quot;
    tp, tn, fp, fn = extract_tp_tn_fp_fn(y_true, y_pred)
    if tn + fp == 0:
        return 0.0
    return tn / (tn + fp)
</code></pre>
<h3>3.3 Clase Metrics Completa</h3>
<pre><code class="language-python">import numpy as np
from dataclasses import dataclass

@dataclass
class ClassificationReport:
    &quot;&quot;&quot;Reporte de mÃ©tricas de clasificaciÃ³n.&quot;&quot;&quot;
    accuracy: float
    precision: float
    recall: float
    f1: float
    specificity: float
    confusion_matrix: np.ndarray

    def __str__(self) -&gt; str:
        cm = self.confusion_matrix
        return f&quot;&quot;&quot;
Classification Report
=====================
Accuracy:    {self.accuracy:.4f}
Precision:   {self.precision:.4f}
Recall:      {self.recall:.4f}
F1 Score:    {self.f1:.4f}
Specificity: {self.specificity:.4f}

Confusion Matrix:
           Pred 0  Pred 1
Actual 0   {cm[0,0]:5d}   {cm[0,1]:5d}
Actual 1   {cm[1,0]:5d}   {cm[1,1]:5d}
&quot;&quot;&quot;

def classification_report(y_true: np.ndarray, y_pred: np.ndarray) -&gt; ClassificationReport:
    &quot;&quot;&quot;Genera reporte completo de mÃ©tricas.&quot;&quot;&quot;
    return ClassificationReport(
        accuracy=accuracy(y_true, y_pred),
        precision=precision(y_true, y_pred),
        recall=recall(y_true, y_pred),
        f1=f1_score(y_true, y_pred),
        specificity=specificity(y_true, y_pred),
        confusion_matrix=confusion_matrix(y_true, y_pred)
    )

# Demo
y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])
y_pred = np.array([0, 0, 1, 0, 1, 1, 0, 1, 1, 1])

report = classification_report(y_true, y_pred)
print(report)
</code></pre>
<h2>ğŸ’» Parte 4: ValidaciÃ³n y RegularizaciÃ³n</h2>
<h3>4.1 Train/Test Split</h3>
<pre><code class="language-python">import numpy as np

def train_test_split(
    X: np.ndarray,
    y: np.ndarray,
    test_size: float = 0.2,
    random_state: int = None
) -&gt; tuple:
    &quot;&quot;&quot;
    Divide datos en conjuntos de entrenamiento y prueba.

    Args:
        X: features
        y: targets
        test_size: proporciÃ³n para test (0-1)
        random_state: semilla para reproducibilidad
    &quot;&quot;&quot;
    if random_state is not None:
        np.random.seed(random_state)

    n = len(y)
    indices = np.random.permutation(n)

    test_size_n = int(n * test_size)
    test_indices = indices[:test_size_n]
    train_indices = indices[test_size_n:]

    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]
</code></pre>
<h3>4.2 K-Fold Cross Validation</h3>
<pre><code class="language-python">import numpy as np
from typing import List, Tuple

def k_fold_split(n: int, k: int) -&gt; List[Tuple[np.ndarray, np.ndarray]]:
    &quot;&quot;&quot;
    Genera Ã­ndices para K-Fold Cross Validation.

    Returns:
        Lista de (train_indices, val_indices) para cada fold
    &quot;&quot;&quot;
    indices = np.arange(n)
    np.random.shuffle(indices)

    fold_size = n // k
    folds = []

    for i in range(k):
        start = i * fold_size
        end = start + fold_size if i &lt; k - 1 else n

        val_indices = indices[start:end]
        train_indices = np.concatenate([indices[:start], indices[end:]])

        folds.append((train_indices, val_indices))

    return folds

def cross_validate(
    model_class,
    X: np.ndarray,
    y: np.ndarray,
    k: int = 5,
    **model_params
) -&gt; dict:
    &quot;&quot;&quot;
    Realiza K-Fold Cross Validation.

    Returns:
        Dict con scores de cada fold y promedio
    &quot;&quot;&quot;
    folds = k_fold_split(len(y), k)
    scores = []

    for i, (train_idx, val_idx) in enumerate(folds):
        # Split
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        # Train
        model = model_class()
        model.fit(X_train, y_train, **model_params)

        # Evaluate
        score = model.score(X_val, y_val)
        scores.append(score)

    return {
        'scores': scores,
        'mean': np.mean(scores),
        'std': np.std(scores)
    }

# Demo
# cv_results = cross_validate(LogisticRegression, X, y, k=5, learning_rate=0.1, n_iterations=500)
# print(f&quot;CV Accuracy: {cv_results['mean']:.4f} Â± {cv_results['std']:.4f}&quot;)
</code></pre>
<h3>4.3 RegularizaciÃ³n</h3>
<pre><code class="language-python">import numpy as np

class LogisticRegressionRegularized:
    &quot;&quot;&quot;Logistic Regression con regularizaciÃ³n L1/L2.&quot;&quot;&quot;

    def __init__(self, regularization: str = 'l2', lambda_: float = 0.01):
        &quot;&quot;&quot;
        Args:
            regularization: 'l1', 'l2', o None
            lambda_: fuerza de regularizaciÃ³n
        &quot;&quot;&quot;
        self.regularization = regularization
        self.lambda_ = lambda_
        self.theta = None
        self.cost_history = []

    def _cost(self, X: np.ndarray, y: np.ndarray) -&gt; float:
        &quot;&quot;&quot;Costo con regularizaciÃ³n.&quot;&quot;&quot;
        m = len(y)
        h = sigmoid(X @ self.theta)
        h = np.clip(h, 1e-15, 1 - 1e-15)

        # Cross-entropy base
        bce = -(1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))

        # RegularizaciÃ³n (excluir bias theta[0])
        if self.regularization == 'l2':
            # Ridge: Î»/2m * Î£Î¸â±¼Â²
            reg = (self.lambda_ / (2 * m)) * np.sum(self.theta[1:] ** 2)
        elif self.regularization == 'l1':
            # Lasso: Î»/m * Î£|Î¸â±¼|
            reg = (self.lambda_ / m) * np.sum(np.abs(self.theta[1:]))
        else:
            reg = 0

        return bce + reg

    def _gradient(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;Gradiente con regularizaciÃ³n.&quot;&quot;&quot;
        m = len(y)
        h = sigmoid(X @ self.theta)

        # Gradiente base
        grad = (1/m) * X.T @ (h - y)

        # RegularizaciÃ³n (excluir bias)
        if self.regularization == 'l2':
            reg_grad = np.concatenate([[0], (self.lambda_ / m) * self.theta[1:]])
        elif self.regularization == 'l1':
            reg_grad = np.concatenate([[0], (self.lambda_ / m) * np.sign(self.theta[1:])])
        else:
            reg_grad = 0

        return grad + reg_grad

    def fit(self, X: np.ndarray, y: np.ndarray, 
            learning_rate: float = 0.1, n_iterations: int = 1000):
        X_b = np.column_stack([np.ones(len(X)), X])
        self.theta = np.zeros(X_b.shape[1])

        for _ in range(n_iterations):
            gradient = self._gradient(X_b, y)
            self.theta -= learning_rate * gradient
            self.cost_history.append(self._cost(X_b, y))

        return self

    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        X_b = np.column_stack([np.ones(len(X)), X])
        return (sigmoid(X_b @ self.theta) &gt;= 0.5).astype(int)

    def score(self, X: np.ndarray, y: np.ndarray) -&gt; float:
        return np.mean(self.predict(X) == y)
</code></pre>
<h2>ğŸ“¦ Entregable del MÃ³dulo</h2>
<h3><code>supervised_learning.py</code></h3>
<pre><code class="language-python">&quot;&quot;&quot;
Supervised Learning Module

ImplementaciÃ³n desde cero de:
- Linear Regression (con Normal Equation y Gradient Descent)
- Logistic Regression (con regularizaciÃ³n L1/L2)
- MÃ©tricas de evaluaciÃ³n
- Cross Validation

Autor: [Tu nombre]
MÃ³dulo: 04 - Supervised Learning
&quot;&quot;&quot;

import numpy as np
from typing import Tuple, List, Optional
from dataclasses import dataclass


# ============================================================
# FUNCIONES AUXILIARES
# ============================================================

def sigmoid(z: np.ndarray) -&gt; np.ndarray:
    z = np.clip(z, -500, 500)
    return 1 / (1 + np.exp(-z))

def add_bias(X: np.ndarray) -&gt; np.ndarray:
    return np.column_stack([np.ones(len(X)), X])


# ============================================================
# REGRESIÃ“N LINEAL
# ============================================================

class LinearRegression:
    def __init__(self):
        self.theta = None
        self.cost_history = []

    def fit(self, X: np.ndarray, y: np.ndarray,
            method: str = 'normal', lr: float = 0.01, n_iter: int = 1000):
        X_b = add_bias(X)

        if method == 'normal':
            self.theta = np.linalg.solve(X_b.T @ X_b, X_b.T @ y)
        else:
            m, n = X_b.shape
            self.theta = np.zeros(n)
            for _ in range(n_iter):
                grad = (1/m) * X_b.T @ (X_b @ self.theta - y)
                self.theta -= lr * grad
                self.cost_history.append(np.mean((X_b @ self.theta - y)**2))
        return self

    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        return add_bias(X) @ self.theta

    def score(self, X: np.ndarray, y: np.ndarray) -&gt; float:
        y_pred = self.predict(X)
        ss_res = np.sum((y - y_pred)**2)
        ss_tot = np.sum((y - np.mean(y))**2)
        return 1 - ss_res / ss_tot


# ============================================================
# REGRESIÃ“N LOGÃSTICA
# ============================================================

class LogisticRegression:
    def __init__(self, reg: str = None, lambda_: float = 0.01):
        self.reg = reg
        self.lambda_ = lambda_
        self.theta = None
        self.cost_history = []

    def fit(self, X: np.ndarray, y: np.ndarray,
            lr: float = 0.1, n_iter: int = 1000):
        X_b = add_bias(X)
        m, n = X_b.shape
        self.theta = np.zeros(n)

        for _ in range(n_iter):
            h = sigmoid(X_b @ self.theta)
            grad = (1/m) * X_b.T @ (h - y)

            if self.reg == 'l2':
                grad[1:] += (self.lambda_/m) * self.theta[1:]
            elif self.reg == 'l1':
                grad[1:] += (self.lambda_/m) * np.sign(self.theta[1:])

            self.theta -= lr * grad
        return self

    def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:
        return sigmoid(add_bias(X) @ self.theta)

    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        return (self.predict_proba(X) &gt;= 0.5).astype(int)

    def score(self, X: np.ndarray, y: np.ndarray) -&gt; float:
        return np.mean(self.predict(X) == y)


# ============================================================
# MÃ‰TRICAS
# ============================================================

def accuracy(y_true, y_pred): 
    return np.mean(y_true == y_pred)

def precision(y_true, y_pred):
    tp = np.sum((y_true == 1) &amp; (y_pred == 1))
    fp = np.sum((y_true == 0) &amp; (y_pred == 1))
    return tp / (tp + fp) if (tp + fp) &gt; 0 else 0

def recall(y_true, y_pred):
    tp = np.sum((y_true == 1) &amp; (y_pred == 1))
    fn = np.sum((y_true == 1) &amp; (y_pred == 0))
    return tp / (tp + fn) if (tp + fn) &gt; 0 else 0

def f1_score(y_true, y_pred):
    p, r = precision(y_true, y_pred), recall(y_true, y_pred)
    return 2*p*r/(p+r) if (p+r) &gt; 0 else 0

def confusion_matrix(y_true, y_pred):
    cm = np.zeros((2, 2), dtype=int)
    cm[0, 0] = np.sum((y_true == 0) &amp; (y_pred == 0))
    cm[0, 1] = np.sum((y_true == 0) &amp; (y_pred == 1))
    cm[1, 0] = np.sum((y_true == 1) &amp; (y_pred == 0))
    cm[1, 1] = np.sum((y_true == 1) &amp; (y_pred == 1))
    return cm


# ============================================================
# VALIDACIÃ“N
# ============================================================

def train_test_split(X, y, test_size=0.2, seed=None):
    if seed: np.random.seed(seed)
    n = len(y)
    idx = np.random.permutation(n)
    split = int(n * test_size)
    return X[idx[split:]], X[idx[:split]], y[idx[split:]], y[idx[:split]]

def cross_validate(model_class, X, y, k=5, **params):
    n = len(y)
    idx = np.random.permutation(n)
    fold_size = n // k
    scores = []

    for i in range(k):
        val_idx = idx[i*fold_size:(i+1)*fold_size]
        train_idx = np.concatenate([idx[:i*fold_size], idx[(i+1)*fold_size:]])

        model = model_class()
        model.fit(X[train_idx], y[train_idx], **params)
        scores.append(model.score(X[val_idx], y[val_idx]))

    return {'scores': scores, 'mean': np.mean(scores), 'std': np.std(scores)}


# ============================================================
# TESTS
# ============================================================

if __name__ == &quot;__main__&quot;:
    np.random.seed(42)

    # Test Linear Regression
    X = 2 * np.random.rand(100, 1)
    y = 4 + 3 * X.flatten() + np.random.randn(100) * 0.5

    lr = LinearRegression()
    lr.fit(X, y)
    print(f&quot;Linear Regression RÂ²: {lr.score(X, y):.4f}&quot;)

    # Test Logistic Regression
    X_c0 = np.random.randn(50, 2) + [-2, -2]
    X_c1 = np.random.randn(50, 2) + [2, 2]
    X_clf = np.vstack([X_c0, X_c1])
    y_clf = np.array([0]*50 + [1]*50)

    log_reg = LogisticRegression()
    log_reg.fit(X_clf, y_clf)
    print(f&quot;Logistic Regression Accuracy: {log_reg.score(X_clf, y_clf):.4f}&quot;)

    # Test metrics
    y_true = np.array([0,0,0,1,1,1,1,1])
    y_pred = np.array([0,0,1,1,1,0,1,1])
    print(f&quot;Precision: {precision(y_true, y_pred):.4f}&quot;)
    print(f&quot;Recall: {recall(y_true, y_pred):.4f}&quot;)
    print(f&quot;F1: {f1_score(y_true, y_pred):.4f}&quot;)

    # Test CV
    cv = cross_validate(LogisticRegression, X_clf, y_clf, k=5, lr=0.1, n_iter=500)
    print(f&quot;CV Score: {cv['mean']:.4f} Â± {cv['std']:.4f}&quot;)

    print(&quot;\nâœ“ Todos los tests pasaron!&quot;)
</code></pre>
<h2>ğŸ“ DerivaciÃ³n AnalÃ­tica: El Entregable de LÃ¡piz y Papel (v3.2)</h2>
<blockquote>
<p>ğŸ“ <strong>SimulaciÃ³n de Examen:</strong> En la maestrÃ­a te pedirÃ¡n: <em>"Derive la regla de actualizaciÃ³n de pesos para Logistic Regression"</em>. Debes poder hacerlo a mano.</p>
</blockquote>
<h3>DerivaciÃ³n del Gradiente de Logistic Regression</h3>
<p><strong>Objetivo:</strong> Derivar $\frac{\partial L}{\partial w}$ para la funciÃ³n de costo Cross-Entropy.</p>
<h4>Paso 1: Definir la FunciÃ³n de Costo</h4>
<p>$$L(w) = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]$$</p>
<p>Donde:<br />
- $\hat{y}_i = \sigma(w^T x_i) = \frac{1}{1 + e^{-w^T x_i}}$<br />
- $\sigma(z)$ es la funciÃ³n sigmoid</p>
<h4>Paso 2: Derivar la Sigmoid</h4>
<p>$$\frac{d\sigma}{dz} = \sigma(z)(1 - \sigma(z))$$</p>
<p><strong>DemostraciÃ³n:</strong><br />
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$<br />
$$\frac{d\sigma}{dz} = \frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1}{1 + e^{-z}} \cdot \frac{e^{-z}}{1 + e^{-z}} = \sigma(z)(1 - \sigma(z))$$</p>
<h4>Paso 3: Aplicar la Regla de la Cadena</h4>
<p>Para un solo ejemplo $(x_i, y_i)$:</p>
<p>$$\frac{\partial L_i}{\partial w} = \frac{\partial L_i}{\partial \hat{y}_i} \cdot \frac{\partial \hat{y}_i}{\partial z_i} \cdot \frac{\partial z_i}{\partial w}$$</p>
<p>Donde $z_i = w^T x_i$</p>
<p><strong>Calcular cada tÃ©rmino:</strong></p>
<ol>
<li>
<p>$\frac{\partial L_i}{\partial \hat{y}_i} = -\frac{y_i}{\hat{y}_i} + \frac{1 - y_i}{1 - \hat{y}_i}$</p>
</li>
<li>
<p>$\frac{\partial \hat{y}_i}{\partial z_i} = \hat{y}_i(1 - \hat{y}_i)$</p>
</li>
<li>
<p>$\frac{\partial z_i}{\partial w} = x_i$</p>
</li>
</ol>
<h4>Paso 4: Simplificar</h4>
<p>$$\frac{\partial L_i}{\partial w} = \left( -\frac{y_i}{\hat{y}_i} + \frac{1 - y_i}{1 - \hat{y}_i} \right) \cdot \hat{y}_i(1 - \hat{y}_i) \cdot x_i$$</p>
<p>Simplificando el tÃ©rmino entre parÃ©ntesis:<br />
$$= \frac{-y_i(1 - \hat{y}_i) + (1-y_i)\hat{y}_i}{\hat{y}_i(1 - \hat{y}_i)} \cdot \hat{y}_i(1 - \hat{y}_i) \cdot x_i$$<br />
$$= (-y_i + y_i\hat{y}_i + \hat{y}_i - y_i\hat{y}_i) \cdot x_i$$<br />
$$= (\hat{y}_i - y_i) \cdot x_i$$</p>
<h4>Resultado Final</h4>
<p>$$\boxed{\frac{\partial L}{\partial w} = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i) x_i = \frac{1}{n} X^T (\hat{y} - y)}$$</p>
<p><strong>Forma vectorizada (para cÃ³digo):</strong></p>
<pre><code class="language-python">gradient = (1/n) * X.T @ (y_pred - y_true)
</code></pre>
<h3>Tu Entregable</h3>
<p>Escribe en un documento (Markdown o LaTeX):<br />
1. La derivaciÃ³n completa del gradiente de Cross-Entropy<br />
2. La derivaciÃ³n de la regla de actualizaciÃ³n: $w \leftarrow w - \alpha \nabla L$<br />
3. Por quÃ© el gradiente tiene la forma $(\hat{y} - y)$ (interpretaciÃ³n geomÃ©trica)</p>
<h2>ğŸ¯ El Reto del Tablero Blanco (MetodologÃ­a Feynman)</h2>
<p>Explica en <strong>mÃ¡ximo 5 lÃ­neas</strong> sin jerga tÃ©cnica:</p>
<ol>
<li>
<p><strong>Â¿Por quÃ© usamos sigmoid en clasificaciÃ³n?</strong></p>
<blockquote>
<p>Pista: Piensa en probabilidades entre 0 y 1.</p>
</blockquote>
</li>
<li>
<p><strong>Â¿Por quÃ© Cross-Entropy y no MSE para clasificaciÃ³n?</strong></p>
<blockquote>
<p>Pista: Piensa en quÃ© pasa cuando $\hat{y} \approx 0$ pero $y = 1$.</p>
</blockquote>
</li>
<li>
<p><strong>Â¿QuÃ© significa "One-vs-All"?</strong></p>
<blockquote>
<p>Pista: Piensa en cÃ³mo clasificar 10 dÃ­gitos con clasificadores binarios.</p>
</blockquote>
</li>
</ol>
<h2>ğŸ” Shadow Mode: ValidaciÃ³n con sklearn (v3.3)</h2>
<blockquote>
<p>âš ï¸ <strong>Regla:</strong> sklearn estÃ¡ <strong>prohibido para aprender</strong>, pero es <strong>necesario para validar</strong>. Si tu implementaciÃ³n difiere significativamente de sklearn, tienes un bug.</p>
</blockquote>
<h3>Protocolo de ValidaciÃ³n (Viernes de Fase 2)</h3>
<pre><code class="language-python">&quot;&quot;&quot;
Shadow Mode - ValidaciÃ³n de Implementaciones
Compara tu cÃ³digo desde cero vs sklearn para detectar bugs.

Regla: Si la diferencia de accuracy es &gt;5%, revisar matemÃ¡ticas.
&quot;&quot;&quot;
import numpy as np
from sklearn.linear_model import LogisticRegression as SklearnLR
from sklearn.linear_model import LinearRegression as SklearnLinReg
from sklearn.metrics import accuracy_score, mean_squared_error

# Importar tu implementaciÃ³n
# from src.logistic_regression import LogisticRegression as MyLR
# from src.linear_regression import LinearRegression as MyLinReg


def shadow_mode_logistic_regression(X_train, y_train, X_test, y_test):
    &quot;&quot;&quot;
    Compara tu Logistic Regression vs sklearn.

    Los coeficientes y accuracy deben ser casi idÃ©nticos.
    &quot;&quot;&quot;
    print(&quot;=&quot; * 60)
    print(&quot;SHADOW MODE: Logistic Regression&quot;)
    print(&quot;=&quot; * 60)

    # ========== TU IMPLEMENTACIÃ“N ==========
    # my_model = MyLR()
    # my_model.fit(X_train, y_train, lr=0.1, n_iter=1000)
    # my_pred = my_model.predict(X_test)
    # my_acc = accuracy_score(y_test, my_pred)
    # my_weights = my_model.weights

    # Placeholder (reemplazar con tu cÃ³digo)
    my_acc = 0.85
    my_weights = np.zeros(X_train.shape[1])

    # ========== SKLEARN (GROUND TRUTH) ==========
    sklearn_model = SklearnLR(max_iter=1000, solver='lbfgs')
    sklearn_model.fit(X_train, y_train)
    sklearn_pred = sklearn_model.predict(X_test)
    sklearn_acc = accuracy_score(y_test, sklearn_pred)
    sklearn_weights = sklearn_model.coef_.flatten()

    # ========== COMPARACIÃ“N ==========
    acc_diff = abs(my_acc - sklearn_acc)
    weight_diff = np.linalg.norm(my_weights - sklearn_weights[:len(my_weights)])

    print(f&quot;\nğŸ“Š RESULTADOS:&quot;)
    print(f&quot;  Tu Accuracy:     {my_acc:.4f}&quot;)
    print(f&quot;  sklearn Accuracy: {sklearn_acc:.4f}&quot;)
    print(f&quot;  Diferencia:       {acc_diff:.4f}&quot;)

    print(f&quot;\nğŸ“ PESOS:&quot;)
    print(f&quot;  Diferencia L2 de pesos: {weight_diff:.4f}&quot;)

    # Veredicto
    print(&quot;\n&quot; + &quot;-&quot; * 60)
    if acc_diff &lt; 0.05:
        print(&quot;âœ“ PASSED: Tu implementaciÃ³n es correcta&quot;)
        return True
    else:
        print(&quot;âœ— FAILED: Diferencia significativa - revisa tu matemÃ¡tica&quot;)
        print(&quot;  Posibles causas:&quot;)
        print(&quot;  - Gradiente mal calculado&quot;)
        print(&quot;  - Learning rate muy alto/bajo&quot;)
        print(&quot;  - Falta de normalizaciÃ³n de datos&quot;)
        return False


def shadow_mode_linear_regression(X_train, y_train, X_test, y_test):
    &quot;&quot;&quot;
    Compara tu Linear Regression vs sklearn.
    &quot;&quot;&quot;
    print(&quot;=&quot; * 60)
    print(&quot;SHADOW MODE: Linear Regression&quot;)
    print(&quot;=&quot; * 60)

    # ========== TU IMPLEMENTACIÃ“N ==========
    # my_model = MyLinReg()
    # my_model.fit(X_train, y_train)
    # my_pred = my_model.predict(X_test)
    # my_mse = mean_squared_error(y_test, my_pred)

    # Placeholder
    my_mse = 0.5

    # ========== SKLEARN ==========
    sklearn_model = SklearnLinReg()
    sklearn_model.fit(X_train, y_train)
    sklearn_pred = sklearn_model.predict(X_test)
    sklearn_mse = mean_squared_error(y_test, sklearn_pred)

    # ========== COMPARACIÃ“N ==========
    mse_ratio = my_mse / sklearn_mse if sklearn_mse &gt; 0 else float('inf')

    print(f&quot;\nğŸ“Š RESULTADOS:&quot;)
    print(f&quot;  Tu MSE:     {my_mse:.4f}&quot;)
    print(f&quot;  sklearn MSE: {sklearn_mse:.4f}&quot;)
    print(f&quot;  Ratio:       {mse_ratio:.2f}x&quot;)

    print(&quot;\n&quot; + &quot;-&quot; * 60)
    if mse_ratio &lt; 1.1:  # Dentro del 10%
        print(&quot;âœ“ PASSED: Tu implementaciÃ³n es correcta&quot;)
        return True
    else:
        print(&quot;âœ— FAILED: Tu MSE es significativamente mayor&quot;)
        return False


# ============================================================
# EJEMPLO DE USO
# ============================================================

if __name__ == &quot;__main__&quot;:
    from sklearn.datasets import make_classification, make_regression
    from sklearn.model_selection import train_test_split

    # Dataset de clasificaciÃ³n
    X_clf, y_clf = make_classification(
        n_samples=1000, n_features=10, n_classes=2, random_state=42
    )
    X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
        X_clf, y_clf, test_size=0.2, random_state=42
    )

    # Dataset de regresiÃ³n
    X_reg, y_reg = make_regression(
        n_samples=1000, n_features=10, noise=10, random_state=42
    )
    X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(
        X_reg, y_reg, test_size=0.2, random_state=42
    )

    # Ejecutar Shadow Mode
    shadow_mode_logistic_regression(X_train_c, y_train_c, X_test_c, y_test_c)
    print(&quot;\n&quot;)
    shadow_mode_linear_regression(X_train_r, y_train_r, X_test_r, y_test_r)
</code></pre>
<h3>Checklist Shadow Mode</h3>
<table>
<thead>
<tr>
<th>DÃ­a</th>
<th>Algoritmo</th>
<th>Validar</th>
</tr>
</thead>
<tbody>
<tr>
<td>Viernes Sem 10</td>
<td>Linear Regression</td>
<td>MSE â‰ˆ sklearn</td>
</tr>
<tr>
<td>Viernes Sem 11</td>
<td>Logistic Regression</td>
<td>Accuracy â‰ˆ sklearn</td>
</tr>
<tr>
<td>Viernes Sem 12</td>
<td>MÃ©tricas</td>
<td>Precision/Recall = sklearn</td>
</tr>
</tbody>
</table>
<h2>âœ… Checklist de FinalizaciÃ³n (v3.3)</h2>
<h3>Conocimiento</h3>
<ul>
<li>[ ] ImplementÃ© regresiÃ³n lineal con Normal Equation y GD</li>
<li>[ ] Entiendo MSE y su gradiente</li>
<li>[ ] ImplementÃ© regresiÃ³n logÃ­stica desde cero</li>
<li>[ ] Entiendo sigmoid y binary cross-entropy</li>
<li>[ ] Puedo calcular TP, TN, FP, FN de una matriz de confusiÃ³n</li>
<li>[ ] ImplementÃ© accuracy, precision, recall, F1</li>
<li>[ ] ImplementÃ© train/test split</li>
<li>[ ] ImplementÃ© K-fold cross validation</li>
<li>[ ] Entiendo regularizaciÃ³n L1 vs L2</li>
</ul>
<h3>Shadow Mode (v3.3 - Obligatorio)</h3>
<ul>
<li>[ ] <strong>Linear Regression</strong>: Mi MSE â‰ˆ sklearn (ratio &lt; 1.1)</li>
<li>[ ] <strong>Logistic Regression</strong>: Mi Accuracy â‰ˆ sklearn (diff &lt; 5%)</li>
</ul>
<h3>Entregables de CÃ³digo</h3>
<ul>
<li>[ ] <code>logistic_regression.py</code> con tests pasando</li>
<li>[ ] <code>mypy src/</code> pasa sin errores</li>
<li>[ ] <code>pytest tests/</code> pasa sin errores</li>
</ul>
<h3>DerivaciÃ³n AnalÃ­tica (Obligatorio)</h3>
<ul>
<li>[ ] DerivÃ© el gradiente de Cross-Entropy a mano</li>
<li>[ ] Documento con derivaciÃ³n completa (Markdown o LaTeX)</li>
<li>[ ] Puedo explicar por quÃ© $\nabla L = X^T(\hat{y} - y)$</li>
</ul>
<h3>MetodologÃ­a Feynman</h3>
<ul>
<li>[ ] Puedo explicar sigmoid en 5 lÃ­neas sin jerga</li>
<li>[ ] Puedo explicar Cross-Entropy vs MSE en 5 lÃ­neas</li>
<li>[ ] Puedo explicar One-vs-All en 5 lÃ­neas</li>
</ul>
<h2>ğŸ”— NavegaciÃ³n</h2>
<table>
<thead>
<tr>
<th>Anterior</th>
<th>Ãndice</th>
<th>Siguiente</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="#mod_04_PROBABILIDAD_ML">04_PROBABILIDAD_ML</a></td>
<td><a href="#mod_00_INDICE">00_INDICE</a></td>
<td><a href="#mod_06_UNSUPERVISED_LEARNING">06_UNSUPERVISED_LEARNING</a></td>
</tr>
</tbody>
</table>
            </div>
        
            <!-- MÃ“DULO: 06_UNSUPERVISED_LEARNING.md -->
            <div class="cover-page">
                <div class="cover-content">
                    <div style="font-size:40pt;margin-bottom:20px;">ğŸ’</div>
                    <div id="mod_06_UNSUPERVISED_LEARNING" class="cover-title">MÃ“DULO 06 - UNSUPERVISED LEARNING</div>
                    <div class="cover-subtitle">MS in AI Pathway - ML Specialist v3.0</div>
                    <div class="cover-badge">MATEMÃTICAS APLICADAS A CÃ“DIGO</div>
                </div>
                <div style="position:absolute;bottom:30px;font-size:10pt;opacity:0.6;">DUQUEOM | 2025</div>
            </div>
            <div class="content">
                <h1>MÃ³dulo 06 - Unsupervised Learning</h1>
<blockquote>
<p><strong>ğŸ¯ Objetivo:</strong> Dominar K-Means clustering y PCA para reducciÃ³n dimensional<br />
<strong>Fase:</strong> 2 - NÃºcleo de ML | <strong>Semanas 13-16</strong><br />
<strong>Curso del Pathway:</strong> Unsupervised Algorithms in Machine Learning</p>
</blockquote>
<h2>ğŸ§  Â¿QuÃ© es Unsupervised Learning?</h2>
<pre><code>APRENDIZAJE NO SUPERVISADO

Tenemos:
- Datos de entrada X (features)
- NO tenemos etiquetas Y

Objetivo: Encontrar estructura oculta en los datos

Tipos principales:
â”œâ”€â”€ CLUSTERING: Agrupar puntos similares
â”‚   â””â”€â”€ K-Means, DBSCAN, Hierarchical
â”œâ”€â”€ REDUCCIÃ“N DIMENSIONAL: Comprimir features
â”‚   â””â”€â”€ PCA, t-SNE, UMAP
â””â”€â”€ DETECCIÃ“N DE ANOMALÃAS: Encontrar outliers
    â””â”€â”€ Isolation Forest, GMM
</code></pre>
<h2>ğŸ“š Contenido del MÃ³dulo</h2>
<table>
<thead>
<tr>
<th>Semana</th>
<th>Tema</th>
<th>Entregable</th>
</tr>
</thead>
<tbody>
<tr>
<td>13</td>
<td>K-Means Clustering</td>
<td><code>kmeans.py</code></td>
</tr>
<tr>
<td>14</td>
<td>EvaluaciÃ³n de Clusters</td>
<td>MÃ©tricas de clustering</td>
</tr>
<tr>
<td>15</td>
<td>PCA</td>
<td><code>pca.py</code></td>
</tr>
<tr>
<td>16</td>
<td>PCA Aplicado + GMM</td>
<td>CompresiÃ³n de imÃ¡genes</td>
</tr>
</tbody>
</table>
<h2>ğŸ’» Parte 1: K-Means Clustering</h2>
<h3>1.1 Algoritmo de Lloyd</h3>
<pre><code class="language-python">import numpy as np

&quot;&quot;&quot;
K-MEANS CLUSTERING (Algoritmo de Lloyd)

Objetivo: Particionar n puntos en k clusters, minimizando la
varianza intra-cluster (inercia).

Algoritmo:
1. Inicializar k centroides (aleatorio o k-means++)
2. Repetir hasta convergencia:
   a. ASIGNAR: cada punto al centroide mÃ¡s cercano
   b. ACTUALIZAR: mover cada centroide al promedio de sus puntos
3. Retornar centroides y asignaciones

FunciÃ³n objetivo (minimizar):
    J = Î£áµ¢ Î£â±¼ ||xâ±¼ - Î¼áµ¢||Â²

Donde xâ±¼ pertenece al cluster i con centroide Î¼áµ¢
&quot;&quot;&quot;

def euclidean_distance(a: np.ndarray, b: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Distancia euclidiana entre dos puntos.&quot;&quot;&quot;
    return np.sqrt(np.sum((a - b) ** 2))

def assign_clusters(X: np.ndarray, centroids: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Asigna cada punto al centroide mÃ¡s cercano.

    Args:
        X: datos (n_samples, n_features)
        centroids: centroides actuales (k, n_features)

    Returns:
        labels: Ã­ndice del cluster para cada punto (n_samples,)
    &quot;&quot;&quot;
    n_samples = X.shape[0]
    k = centroids.shape[0]

    # Calcular distancia de cada punto a cada centroide
    distances = np.zeros((n_samples, k))
    for i in range(k):
        distances[:, i] = np.sqrt(np.sum((X - centroids[i]) ** 2, axis=1))

    # Asignar al mÃ¡s cercano
    return np.argmin(distances, axis=1)

def update_centroids(X: np.ndarray, labels: np.ndarray, k: int) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Actualiza centroides como el promedio de los puntos asignados.

    Args:
        X: datos
        labels: asignaciones actuales
        k: nÃºmero de clusters

    Returns:
        nuevos centroides
    &quot;&quot;&quot;
    n_features = X.shape[1]
    centroids = np.zeros((k, n_features))

    for i in range(k):
        points_in_cluster = X[labels == i]
        if len(points_in_cluster) &gt; 0:
            centroids[i] = np.mean(points_in_cluster, axis=0)

    return centroids
</code></pre>
<h3>1.2 K-Means++ Initialization</h3>
<pre><code class="language-python">import numpy as np

def kmeans_plus_plus_init(X: np.ndarray, k: int, random_state: int = None) -&gt; np.ndarray:
    &quot;&quot;&quot;
    InicializaciÃ³n K-Means++.

    Mejor que inicializaciÃ³n aleatoria porque:
    - Elige centroides que estÃ¡n lejos entre sÃ­
    - Reduce la probabilidad de mala convergencia
    - Garantiza O(log k) de la soluciÃ³n Ã³ptima

    Algoritmo:
    1. Elegir primer centroide aleatoriamente
    2. Para cada centroide restante:
       a. Calcular distancia de cada punto al centroide mÃ¡s cercano
       b. Elegir nuevo centroide con probabilidad proporcional a dÂ²
    &quot;&quot;&quot;
    if random_state is not None:
        np.random.seed(random_state)

    n_samples, n_features = X.shape
    centroids = np.zeros((k, n_features))

    # Primer centroide aleatorio
    first_idx = np.random.randint(n_samples)
    centroids[0] = X[first_idx]

    # Centroides restantes
    for c in range(1, k):
        # Calcular distancia al centroide mÃ¡s cercano para cada punto
        distances = np.zeros(n_samples)
        for i in range(n_samples):
            min_dist = float('inf')
            for j in range(c):
                dist = np.sum((X[i] - centroids[j]) ** 2)
                min_dist = min(min_dist, dist)
            distances[i] = min_dist

        # Probabilidad proporcional a dÂ²
        probabilities = distances / np.sum(distances)

        # Elegir nuevo centroide
        new_idx = np.random.choice(n_samples, p=probabilities)
        centroids[c] = X[new_idx]

    return centroids
</code></pre>
<h3>1.3 ImplementaciÃ³n Completa</h3>
<pre><code class="language-python">import numpy as np
from typing import Tuple

class KMeans:
    &quot;&quot;&quot;K-Means Clustering implementado desde cero.&quot;&quot;&quot;

    def __init__(
        self,
        n_clusters: int = 3,
        max_iter: int = 300,
        tol: float = 1e-4,
        init: str = 'kmeans++',
        random_state: int = None
    ):
        &quot;&quot;&quot;
        Args:
            n_clusters: nÃºmero de clusters (k)
            max_iter: mÃ¡ximo de iteraciones
            tol: tolerancia para convergencia
            init: 'kmeans++' o 'random'
            random_state: semilla para reproducibilidad
        &quot;&quot;&quot;
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.tol = tol
        self.init = init
        self.random_state = random_state

        self.centroids = None
        self.labels_ = None
        self.inertia_ = None
        self.n_iter_ = 0

    def _init_centroids(self, X: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;Inicializa centroides.&quot;&quot;&quot;
        if self.random_state is not None:
            np.random.seed(self.random_state)

        if self.init == 'kmeans++':
            return kmeans_plus_plus_init(X, self.n_clusters, self.random_state)
        else:
            # InicializaciÃ³n aleatoria
            indices = np.random.choice(len(X), self.n_clusters, replace=False)
            return X[indices].copy()

    def _compute_inertia(self, X: np.ndarray) -&gt; float:
        &quot;&quot;&quot;
        Calcula inercia (within-cluster sum of squares).

        Inercia = Î£áµ¢ Î£â±¼ ||xâ±¼ - Î¼áµ¢||Â²
        &quot;&quot;&quot;
        inertia = 0
        for i in range(self.n_clusters):
            cluster_points = X[self.labels_ == i]
            if len(cluster_points) &gt; 0:
                inertia += np.sum((cluster_points - self.centroids[i]) ** 2)
        return inertia

    def fit(self, X: np.ndarray) -&gt; 'KMeans':
        &quot;&quot;&quot;Entrena el modelo.&quot;&quot;&quot;
        # Inicializar centroides
        self.centroids = self._init_centroids(X)

        for iteration in range(self.max_iter):
            # Guardar centroides anteriores
            old_centroids = self.centroids.copy()

            # Paso 1: Asignar puntos a clusters
            self.labels_ = assign_clusters(X, self.centroids)

            # Paso 2: Actualizar centroides
            self.centroids = update_centroids(X, self.labels_, self.n_clusters)

            # Verificar convergencia
            centroid_shift = np.sum((self.centroids - old_centroids) ** 2)
            if centroid_shift &lt; self.tol:
                break

        self.n_iter_ = iteration + 1
        self.inertia_ = self._compute_inertia(X)

        return self

    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;Predice clusters para nuevos datos.&quot;&quot;&quot;
        return assign_clusters(X, self.centroids)

    def fit_predict(self, X: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;Entrena y predice.&quot;&quot;&quot;
        self.fit(X)
        return self.labels_


# Demo
np.random.seed(42)

# Generar datos sintÃ©ticos (3 clusters)
cluster1 = np.random.randn(100, 2) + [0, 0]
cluster2 = np.random.randn(100, 2) + [5, 5]
cluster3 = np.random.randn(100, 2) + [10, 0]
X = np.vstack([cluster1, cluster2, cluster3])

# Entrenar
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)

print(f&quot;Iteraciones: {kmeans.n_iter_}&quot;)
print(f&quot;Inercia: {kmeans.inertia_:.2f}&quot;)
print(f&quot;Centroides:\n{kmeans.centroids}&quot;)
</code></pre>
<h2>ğŸ’» Parte 2: EvaluaciÃ³n de Clusters</h2>
<h3>2.1 Inercia (Within-Cluster Sum of Squares)</h3>
<pre><code class="language-python">def compute_inertia(X: np.ndarray, labels: np.ndarray, centroids: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Inercia: suma de distancias cuadradas al centroide.

    Menor inercia = clusters mÃ¡s compactos.

    Problema: siempre disminuye al aumentar k.
    SoluciÃ³n: usar mÃ©todo del codo.
    &quot;&quot;&quot;
    inertia = 0
    for i, centroid in enumerate(centroids):
        cluster_points = X[labels == i]
        inertia += np.sum((cluster_points - centroid) ** 2)
    return inertia
</code></pre>
<h3>2.2 MÃ©todo del Codo (Elbow Method)</h3>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def elbow_method(X: np.ndarray, k_range: range) -&gt; list:
    &quot;&quot;&quot;
    MÃ©todo del codo para elegir k Ã³ptimo.

    Busca el punto donde aÃ±adir mÃ¡s clusters
    no reduce significativamente la inercia.
    &quot;&quot;&quot;
    inertias = []

    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42)
        kmeans.fit(X)
        inertias.append(kmeans.inertia_)

    return inertias

def plot_elbow(k_range: range, inertias: list):
    &quot;&quot;&quot;Visualiza el mÃ©todo del codo.&quot;&quot;&quot;
    plt.figure(figsize=(8, 5))
    plt.plot(list(k_range), inertias, 'bo-')
    plt.xlabel('NÃºmero de clusters (k)')
    plt.ylabel('Inercia')
    plt.title('MÃ©todo del Codo')
    plt.grid(True)
    plt.show()

# Demo
# inertias = elbow_method(X, range(1, 11))
# plot_elbow(range(1, 11), inertias)
</code></pre>
<h3>2.3 Silhouette Score</h3>
<pre><code class="language-python">import numpy as np

def silhouette_sample(X: np.ndarray, labels: np.ndarray, idx: int) -&gt; float:
    &quot;&quot;&quot;
    Calcula silhouette para un solo punto.

    s(i) = (b(i) - a(i)) / max(a(i), b(i))

    Donde:
    - a(i): distancia promedio a puntos del mismo cluster
    - b(i): distancia promedio mÃ­nima a puntos de otro cluster

    Rango: [-1, 1]
    - 1: punto bien asignado
    - 0: punto en frontera entre clusters
    - -1: punto mal asignado
    &quot;&quot;&quot;
    point = X[idx]
    label = labels[idx]

    # a(i): distancia promedio intra-cluster
    same_cluster = X[labels == label]
    if len(same_cluster) &gt; 1:
        a = np.mean([np.sqrt(np.sum((point - p) ** 2)) 
                     for p in same_cluster if not np.array_equal(p, point)])
    else:
        a = 0

    # b(i): distancia promedio al cluster mÃ¡s cercano
    unique_labels = np.unique(labels)
    b = float('inf')
    for other_label in unique_labels:
        if other_label != label:
            other_cluster = X[labels == other_label]
            if len(other_cluster) &gt; 0:
                avg_dist = np.mean([np.sqrt(np.sum((point - p) ** 2)) 
                                   for p in other_cluster])
                b = min(b, avg_dist)

    if b == float('inf'):
        return 0

    return (b - a) / max(a, b)

def silhouette_score(X: np.ndarray, labels: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Silhouette Score promedio para todos los puntos.

    Mayor es mejor (max = 1).
    &quot;&quot;&quot;
    scores = [silhouette_sample(X, labels, i) for i in range(len(X))]
    return np.mean(scores)


# Demo
# score = silhouette_score(X, labels)
# print(f&quot;Silhouette Score: {score:.4f}&quot;)
</code></pre>
<h2>ğŸ’» Parte 3: PCA (Principal Component Analysis)</h2>
<h3>3.1 Concepto</h3>
<pre><code class="language-python">&quot;&quot;&quot;
PCA - ANÃLISIS DE COMPONENTES PRINCIPALES

Objetivo: Reducir dimensionalidad preservando la mÃ¡xima varianza.

Idea:
1. Centrar los datos (restar media)
2. Encontrar direcciones de mÃ¡xima varianza (eigenvectors)
3. Proyectar datos en las top-k direcciones

MatemÃ¡ticamente:
- Las componentes principales son los eigenvectors de la matriz de covarianza
- Los eigenvalues indican cuÃ¡nta varianza captura cada componente

Aplicaciones:
- VisualizaciÃ³n (reducir a 2D/3D)
- Preprocesamiento (eliminar ruido, reducir features)
- CompresiÃ³n de datos/imÃ¡genes
&quot;&quot;&quot;
</code></pre>
<h3>3.2 PCA via Eigendecomposition</h3>
<pre><code class="language-python">import numpy as np

def pca_eigen(X: np.ndarray, n_components: int) -&gt; tuple:
    &quot;&quot;&quot;
    PCA usando eigendecomposition de la matriz de covarianza.

    Pasos:
    1. Centrar datos: X_centered = X - mean(X)
    2. Calcular matriz de covarianza: Î£ = (1/(n-1)) X^T X
    3. Eigendecomposition: Î£v = Î»v
    4. Ordenar eigenvectors por eigenvalue descendente
    5. Proyectar: X_pca = X_centered @ V[:, :k]

    Returns:
        X_pca: datos transformados
        components: eigenvectors (componentes principales)
        explained_variance_ratio: proporciÃ³n de varianza por componente
    &quot;&quot;&quot;
    # 1. Centrar
    mean = np.mean(X, axis=0)
    X_centered = X - mean

    # 2. Matriz de covarianza
    n_samples = X.shape[0]
    cov_matrix = (X_centered.T @ X_centered) / (n_samples - 1)

    # 3. Eigendecomposition
    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

    # Convertir a reales (puede haber componentes imaginarias pequeÃ±as)
    eigenvalues = eigenvalues.real
    eigenvectors = eigenvectors.real

    # 4. Ordenar por eigenvalue descendente
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]

    # 5. Seleccionar top k componentes
    components = eigenvectors[:, :n_components]

    # 6. Proyectar
    X_pca = X_centered @ components

    # 7. Varianza explicada
    total_variance = np.sum(eigenvalues)
    explained_variance_ratio = eigenvalues[:n_components] / total_variance

    return X_pca, components, explained_variance_ratio, mean
</code></pre>
<h3>3.3 PCA via SVD (MÃ¡s Estable)</h3>
<pre><code class="language-python">import numpy as np

def pca_svd(X: np.ndarray, n_components: int) -&gt; tuple:
    &quot;&quot;&quot;
    PCA usando SVD (Singular Value Decomposition).

    MÃ¡s estable numÃ©ricamente que eigendecomposition.

    Si X = UÎ£V^T, entonces:
    - V contiene las componentes principales
    - Î£Â²/(n-1) son los eigenvalues (varianzas)
    &quot;&quot;&quot;
    # 1. Centrar
    mean = np.mean(X, axis=0)
    X_centered = X - mean

    # 2. SVD
    U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)

    # 3. Componentes principales (filas de Vt, o columnas de V)
    components = Vt[:n_components].T

    # 4. Proyectar
    X_pca = X_centered @ components

    # 5. Varianza explicada
    n_samples = X.shape[0]
    variance = (S ** 2) / (n_samples - 1)
    explained_variance_ratio = variance[:n_components] / np.sum(variance)

    return X_pca, components, explained_variance_ratio, mean
</code></pre>
<h3>3.4 ImplementaciÃ³n Completa</h3>
<pre><code class="language-python">import numpy as np

class PCA:
    &quot;&quot;&quot;Principal Component Analysis implementado desde cero.&quot;&quot;&quot;

    def __init__(self, n_components: int = 2):
        &quot;&quot;&quot;
        Args:
            n_components: nÃºmero de componentes a retener
        &quot;&quot;&quot;
        self.n_components = n_components
        self.components_ = None  # (n_features, n_components)
        self.explained_variance_ratio_ = None
        self.mean_ = None

    def fit(self, X: np.ndarray) -&gt; 'PCA':
        &quot;&quot;&quot;Calcula componentes principales.&quot;&quot;&quot;
        # Centrar
        self.mean_ = np.mean(X, axis=0)
        X_centered = X - self.mean_

        # SVD
        U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)

        # Componentes principales
        self.components_ = Vt[:self.n_components].T

        # Varianza explicada
        n_samples = X.shape[0]
        variance = (S ** 2) / (n_samples - 1)
        self.explained_variance_ratio_ = variance[:self.n_components] / np.sum(variance)
        self.singular_values_ = S[:self.n_components]

        return self

    def transform(self, X: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;Proyecta datos a espacio de componentes principales.&quot;&quot;&quot;
        X_centered = X - self.mean_
        return X_centered @ self.components_

    def fit_transform(self, X: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;Fit y transform en un paso.&quot;&quot;&quot;
        self.fit(X)
        return self.transform(X)

    def inverse_transform(self, X_pca: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;
        Reconstruye datos desde el espacio PCA.

        X_reconstructed = X_pca @ components.T + mean

        Nota: hay pÃ©rdida de informaciÃ³n si n_components &lt; n_features
        &quot;&quot;&quot;
        return X_pca @ self.components_.T + self.mean_

    def get_covariance(self) -&gt; np.ndarray:
        &quot;&quot;&quot;Retorna matriz de covarianza aproximada.&quot;&quot;&quot;
        return self.components_ @ np.diag(self.singular_values_ ** 2) @ self.components_.T


# Demo
np.random.seed(42)

# Datos correlacionados en 3D
n_samples = 200
X = np.random.randn(n_samples, 3)
X[:, 1] = X[:, 0] * 2 + np.random.randn(n_samples) * 0.1  # y correlacionado con x
X[:, 2] = X[:, 0] + X[:, 1] + np.random.randn(n_samples) * 0.1

# PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print(f&quot;Shape original: {X.shape}&quot;)
print(f&quot;Shape reducido: {X_pca.shape}&quot;)
print(f&quot;Varianza explicada: {pca.explained_variance_ratio_}&quot;)
print(f&quot;Varianza total: {np.sum(pca.explained_variance_ratio_):.2%}&quot;)
</code></pre>
<h3>3.5 ReconstrucciÃ³n y Error</h3>
<pre><code class="language-python">import numpy as np

def reconstruction_error(X: np.ndarray, pca: PCA) -&gt; float:
    &quot;&quot;&quot;
    Calcula el error de reconstrucciÃ³n.

    Error = ||X - X_reconstructed||Â² / ||X||Â²
    &quot;&quot;&quot;
    X_pca = pca.transform(X)
    X_reconstructed = pca.inverse_transform(X_pca)

    error = np.sum((X - X_reconstructed) ** 2)
    total = np.sum((X - np.mean(X, axis=0)) ** 2)

    return error / total

def choose_n_components(X: np.ndarray, variance_threshold: float = 0.95) -&gt; int:
    &quot;&quot;&quot;
    Elige nÃºmero de componentes para retener cierta varianza.

    Args:
        variance_threshold: proporciÃ³n de varianza a retener (ej: 0.95 = 95%)
    &quot;&quot;&quot;
    # PCA con todos los componentes
    pca = PCA(n_components=min(X.shape))
    pca.fit(X)

    # Varianza acumulada
    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)

    # Encontrar n_components
    n_components = np.argmax(cumulative_variance &gt;= variance_threshold) + 1

    return n_components, cumulative_variance
</code></pre>
<h2>ğŸ’» Parte 4: Aplicaciones de PCA</h2>
<h3>4.1 CompresiÃ³n de ImÃ¡genes</h3>
<pre><code class="language-python">import numpy as np

def compress_image_pca(image: np.ndarray, n_components: int) -&gt; tuple:
    &quot;&quot;&quot;
    Comprime una imagen usando PCA.

    Args:
        image: imagen grayscale (height, width)
        n_components: nÃºmero de componentes a retener

    Returns:
        imagen comprimida, pca model
    &quot;&quot;&quot;
    # Tratar filas como muestras
    pca = PCA(n_components=n_components)
    image_pca = pca.fit_transform(image)

    # Reconstruir
    image_reconstructed = pca.inverse_transform(image_pca)

    return image_reconstructed, pca

def compression_ratio_pca(original_shape: tuple, n_components: int) -&gt; float:
    &quot;&quot;&quot;Calcula ratio de compresiÃ³n.&quot;&quot;&quot;
    height, width = original_shape
    original_size = height * width
    # Almacenamos: componentes + proyecciones + media
    compressed_size = n_components * width + height * n_components + width
    return compressed_size / original_size
</code></pre>
<h3>4.2 VisualizaciÃ³n en 2D</h3>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def visualize_pca_2d(X: np.ndarray, labels: np.ndarray = None, title: str = &quot;PCA&quot;):
    &quot;&quot;&quot;Reduce a 2D y visualiza.&quot;&quot;&quot;
    pca = PCA(n_components=2)
    X_2d = pca.fit_transform(X)

    plt.figure(figsize=(10, 6))

    if labels is not None:
        for label in np.unique(labels):
            mask = labels == label
            plt.scatter(X_2d[mask, 0], X_2d[mask, 1], 
                       label=f'Clase {label}', alpha=0.7)
        plt.legend()
    else:
        plt.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.7)

    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} var)')
    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} var)')
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.show()
</code></pre>
<h2>ğŸ“¦ Entregable del MÃ³dulo</h2>
<h3><code>unsupervised_learning.py</code></h3>
<pre><code class="language-python">&quot;&quot;&quot;
Unsupervised Learning Module

ImplementaciÃ³n desde cero de:
- K-Means Clustering (con K-Means++ initialization)
- PCA (Principal Component Analysis)
- MÃ©tricas de evaluaciÃ³n de clusters

Autor: [Tu nombre]
MÃ³dulo: 05 - Unsupervised Learning
&quot;&quot;&quot;

import numpy as np
from typing import Tuple, List


# ============================================================
# K-MEANS CLUSTERING
# ============================================================

def kmeans_plus_plus(X: np.ndarray, k: int, seed: int = None) -&gt; np.ndarray:
    &quot;&quot;&quot;InicializaciÃ³n K-Means++.&quot;&quot;&quot;
    if seed: np.random.seed(seed)
    n = len(X)
    centroids = [X[np.random.randint(n)]]

    for _ in range(1, k):
        distances = np.array([min(np.sum((x - c)**2) for c in centroids) for x in X])
        probs = distances / distances.sum()
        centroids.append(X[np.random.choice(n, p=probs)])

    return np.array(centroids)


class KMeans:
    def __init__(self, n_clusters=3, max_iter=300, tol=1e-4, seed=None):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.tol = tol
        self.seed = seed
        self.centroids = None
        self.labels_ = None
        self.inertia_ = None
        self.n_iter_ = 0

    def fit(self, X: np.ndarray) -&gt; 'KMeans':
        self.centroids = kmeans_plus_plus(X, self.n_clusters, self.seed)

        for i in range(self.max_iter):
            old_centroids = self.centroids.copy()

            # Asignar
            distances = np.array([[np.sum((x - c)**2) for c in self.centroids] for x in X])
            self.labels_ = np.argmin(distances, axis=1)

            # Actualizar
            for j in range(self.n_clusters):
                points = X[self.labels_ == j]
                if len(points) &gt; 0:
                    self.centroids[j] = points.mean(axis=0)

            if np.sum((self.centroids - old_centroids)**2) &lt; self.tol:
                break

        self.n_iter_ = i + 1
        self.inertia_ = sum(np.sum((X[self.labels_ == j] - self.centroids[j])**2) 
                           for j in range(self.n_clusters))
        return self

    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        distances = np.array([[np.sum((x - c)**2) for c in self.centroids] for x in X])
        return np.argmin(distances, axis=1)

    def fit_predict(self, X: np.ndarray) -&gt; np.ndarray:
        self.fit(X)
        return self.labels_


# ============================================================
# PCA
# ============================================================

class PCA:
    def __init__(self, n_components: int = 2):
        self.n_components = n_components
        self.components_ = None
        self.explained_variance_ratio_ = None
        self.mean_ = None

    def fit(self, X: np.ndarray) -&gt; 'PCA':
        self.mean_ = X.mean(axis=0)
        X_centered = X - self.mean_

        U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)

        self.components_ = Vt[:self.n_components].T
        variance = (S**2) / (len(X) - 1)
        self.explained_variance_ratio_ = variance[:self.n_components] / variance.sum()

        return self

    def transform(self, X: np.ndarray) -&gt; np.ndarray:
        return (X - self.mean_) @ self.components_

    def fit_transform(self, X: np.ndarray) -&gt; np.ndarray:
        self.fit(X)
        return self.transform(X)

    def inverse_transform(self, X_pca: np.ndarray) -&gt; np.ndarray:
        return X_pca @ self.components_.T + self.mean_


# ============================================================
# MÃ‰TRICAS
# ============================================================

def inertia(X: np.ndarray, labels: np.ndarray, centroids: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Within-cluster sum of squares.&quot;&quot;&quot;
    return sum(np.sum((X[labels == i] - centroids[i])**2) 
               for i in range(len(centroids)))

def silhouette_score(X: np.ndarray, labels: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Silhouette score promedio.&quot;&quot;&quot;
    n = len(X)
    scores = []

    for i in range(n):
        # a: distancia promedio intra-cluster
        same = X[labels == labels[i]]
        a = np.mean([np.sqrt(np.sum((X[i] - x)**2)) for x in same if not np.array_equal(x, X[i])])

        # b: distancia promedio al cluster mÃ¡s cercano
        b = float('inf')
        for label in np.unique(labels):
            if label != labels[i]:
                other = X[labels == label]
                if len(other) &gt; 0:
                    b = min(b, np.mean([np.sqrt(np.sum((X[i] - x)**2)) for x in other]))

        if b == float('inf'):
            scores.append(0)
        else:
            scores.append((b - a) / max(a, b))

    return np.mean(scores)


# ============================================================
# TESTS
# ============================================================

if __name__ == &quot;__main__&quot;:
    np.random.seed(42)

    # Test K-Means
    c1 = np.random.randn(50, 2) + [0, 0]
    c2 = np.random.randn(50, 2) + [5, 5]
    c3 = np.random.randn(50, 2) + [10, 0]
    X = np.vstack([c1, c2, c3])

    kmeans = KMeans(n_clusters=3, seed=42)
    labels = kmeans.fit_predict(X)

    print(f&quot;K-Means Inertia: {kmeans.inertia_:.2f}&quot;)
    print(f&quot;Silhouette Score: {silhouette_score(X, labels):.4f}&quot;)

    # Test PCA
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)
    X_reconstructed = pca.inverse_transform(X_pca)

    print(f&quot;\nPCA Varianza explicada: {pca.explained_variance_ratio_}&quot;)
    print(f&quot;Error reconstrucciÃ³n: {np.mean((X - X_reconstructed)**2):.6f}&quot;)

    print(&quot;\nâœ“ Todos los tests pasaron!&quot;)
</code></pre>
<h2>âœ… Checklist de FinalizaciÃ³n</h2>
<ul>
<li>[ ] ImplementÃ© K-Means con inicializaciÃ³n K-Means++</li>
<li>[ ] Entiendo el algoritmo de Lloyd (asignar-actualizar)</li>
<li>[ ] Puedo calcular inercia y usarla para el mÃ©todo del codo</li>
<li>[ ] ImplementÃ© silhouette score</li>
<li>[ ] ImplementÃ© PCA usando SVD</li>
<li>[ ] Entiendo varianza explicada y puedo elegir n_components</li>
<li>[ ] Puedo reconstruir datos desde PCA</li>
<li>[ ] ApliquÃ© PCA para visualizaciÃ³n 2D</li>
<li>[ ] Todos los tests del mÃ³dulo pasan</li>
</ul>
<h2>ğŸ”— NavegaciÃ³n</h2>
<table>
<thead>
<tr>
<th>Anterior</th>
<th>Ãndice</th>
<th>Siguiente</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="#mod_04_SUPERVISED_LEARNING">04_SUPERVISED_LEARNING</a></td>
<td><a href="#mod_00_INDICE">00_INDICE</a></td>
<td><a href="#mod_06_DEEP_LEARNING">06_DEEP_LEARNING</a></td>
</tr>
</tbody>
</table>
            </div>
        
            <!-- MÃ“DULO: 07_DEEP_LEARNING.md -->
            <div class="cover-page">
                <div class="cover-content">
                    <div style="font-size:40pt;margin-bottom:20px;">ğŸ’</div>
                    <div id="mod_07_DEEP_LEARNING" class="cover-title">MÃ“DULO 07 - DEEP LEARNING + CNNs</div>
                    <div class="cover-subtitle">MS in AI Pathway - ML Specialist v3.0</div>
                    <div class="cover-badge">MATEMÃTICAS APLICADAS A CÃ“DIGO</div>
                </div>
                <div style="position:absolute;bottom:30px;font-size:10pt;opacity:0.6;">DUQUEOM | 2025</div>
            </div>
            <div class="content">
                <h1>MÃ³dulo 07 - Deep Learning</h1>
<blockquote>
<p><strong>ğŸ¯ Objetivo:</strong> Implementar MLP con backprop + entender fundamentos de CNNs<br />
<strong>Fase:</strong> 2 - NÃºcleo de ML | <strong>Semanas 17-20</strong><br />
<strong>Curso del Pathway:</strong> Introduction to Deep Learning</p>
</blockquote>
<h2>ğŸ§  Â¿Por QuÃ© Deep Learning?</h2>
<pre><code>DEEP LEARNING = Redes Neuronales Multicapa + Arquitecturas Especializadas

Ventajas sobre ML clÃ¡sico:
â”œâ”€â”€ Aprende features automÃ¡ticamente (no feature engineering manual)
â”œâ”€â”€ Puede modelar relaciones NO LINEALES complejas
â”œâ”€â”€ Escala con mÃ¡s datos y mÃ¡s compute
â””â”€â”€ Estado del arte en visiÃ³n (CNNs), NLP (Transformers), etc.

Desventajas:
â”œâ”€â”€ Requiere mÃ¡s datos
â”œâ”€â”€ &quot;Caja negra&quot; - menos interpretable
â””â”€â”€ Costoso computacionalmente
</code></pre>
<h2>ğŸ“š Contenido del MÃ³dulo</h2>
<table>
<thead>
<tr>
<th>Semana</th>
<th>Tema</th>
<th>Entregable</th>
</tr>
</thead>
<tbody>
<tr>
<td>17</td>
<td>PerceptrÃ³n y MLP</td>
<td><code>activations.py</code> + forward pass</td>
</tr>
<tr>
<td>18</td>
<td>Backpropagation</td>
<td><code>backward()</code> con Chain Rule</td>
</tr>
<tr>
<td>19</td>
<td><strong>CNNs: TeorÃ­a</strong></td>
<td>Entender convoluciÃ³n, pooling, stride</td>
</tr>
<tr>
<td>20</td>
<td>Optimizadores y Entrenamiento</td>
<td><code>neural_network.py</code> completo</td>
</tr>
</tbody>
</table>
<h2>ğŸ’» Parte 1: PerceptrÃ³n y Activaciones</h2>
<h3>1.1 La Neurona Artificial</h3>
<pre><code class="language-python">import numpy as np

&quot;&quot;&quot;
NEURONA ARTIFICIAL (PerceptrÃ³n)

InspiraciÃ³n biolÃ³gica:
- Recibe seÃ±ales de entrada (dendrites)
- Procesa y decide si &quot;dispara&quot; (soma)
- EnvÃ­a seÃ±al de salida (axon)

Modelo matemÃ¡tico:
    z = Î£ wáµ¢xáµ¢ + b = wÂ·x + b  (combinaciÃ³n lineal)
    a = Ïƒ(z)                    (activaciÃ³n)

Donde:
- x: vector de entradas
- w: vector de pesos (learnable)
- b: bias (learnable)
- Ïƒ: funciÃ³n de activaciÃ³n (introduce no-linealidad)
&quot;&quot;&quot;

def perceptron(x: np.ndarray, w: np.ndarray, b: float) -&gt; float:
    &quot;&quot;&quot;
    Un perceptrÃ³n simple.

    Args:
        x: entrada (n_features,)
        w: pesos (n_features,)
        b: bias

    Returns:
        salida activada
    &quot;&quot;&quot;
    z = np.dot(w, x) + b
    return 1 if z &gt; 0 else 0  # FunciÃ³n escalÃ³n
</code></pre>
<h3>1.2 Funciones de ActivaciÃ³n</h3>
<pre><code class="language-python">import numpy as np

class Activations:
    &quot;&quot;&quot;Funciones de activaciÃ³n y sus derivadas.&quot;&quot;&quot;

    @staticmethod
    def sigmoid(z: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;
        Sigmoid: Ïƒ(z) = 1 / (1 + e^(-z))

        Rango: (0, 1)
        Uso: Capa de salida para clasificaciÃ³n binaria
        Problema: Vanishing gradient para |z| grande
        &quot;&quot;&quot;
        z = np.clip(z, -500, 500)
        return 1 / (1 + np.exp(-z))

    @staticmethod
    def sigmoid_derivative(a: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;Ïƒ'(z) = Ïƒ(z) Â· (1 - Ïƒ(z)) = a Â· (1 - a)&quot;&quot;&quot;
        return a * (1 - a)

    @staticmethod
    def relu(z: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;
        ReLU: f(z) = max(0, z)

        Rango: [0, âˆ)
        Uso: Capas ocultas (default moderno)
        Ventaja: No vanishing gradient para z &gt; 0
        Problema: &quot;Dying ReLU&quot; si z &lt; 0 siempre
        &quot;&quot;&quot;
        return np.maximum(0, z)

    @staticmethod
    def relu_derivative(z: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;ReLU'(z) = 1 si z &gt; 0, 0 si z â‰¤ 0&quot;&quot;&quot;
        return (z &gt; 0).astype(float)

    @staticmethod
    def tanh(z: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;
        Tanh: f(z) = (e^z - e^(-z)) / (e^z + e^(-z))

        Rango: (-1, 1)
        Uso: Alternativa a sigmoid (centrado en 0)
        &quot;&quot;&quot;
        return np.tanh(z)

    @staticmethod
    def tanh_derivative(a: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;tanh'(z) = 1 - tanhÂ²(z) = 1 - aÂ²&quot;&quot;&quot;
        return 1 - a ** 2

    @staticmethod
    def softmax(z: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;
        Softmax: softmax(z)áµ¢ = e^(záµ¢) / Î£â±¼ e^(zâ±¼)

        Rango: (0, 1), suma = 1
        Uso: Capa de salida para clasificaciÃ³n multiclase
        Output: probabilidades de cada clase
        &quot;&quot;&quot;
        # Restar mÃ¡ximo para estabilidad numÃ©rica
        z_shifted = z - np.max(z, axis=-1, keepdims=True)
        exp_z = np.exp(z_shifted)
        return exp_z / np.sum(exp_z, axis=-1, keepdims=True)


# Demo
z = np.array([-2, -1, 0, 1, 2])
act = Activations()

print(&quot;z:&quot;, z)
print(&quot;sigmoid:&quot;, act.sigmoid(z))
print(&quot;relu:&quot;, act.relu(z))
print(&quot;tanh:&quot;, act.tanh(z))
print(&quot;softmax:&quot;, act.softmax(z))
</code></pre>
<h3>1.3 El Problema XOR</h3>
<pre><code class="language-python">&quot;&quot;&quot;
XOR: La limitaciÃ³n del PerceptrÃ³n Simple

XOR truth table:
    x1  x2  |  y
    0   0   |  0
    0   1   |  1
    1   0   |  1
    1   1   |  0

Un perceptrÃ³n simple NO puede resolver XOR porque:
- XOR no es linealmente separable
- No existe una lÃ­nea que separe las clases

SoluciÃ³n: Red multicapa (MLP)
- Una capa oculta puede aprender features intermedias
- CombinaciÃ³n de features no lineales resuelve XOR
&quot;&quot;&quot;

# Datos XOR
X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_xor = np.array([0, 1, 1, 0])

# Un perceptrÃ³n simple no puede aprender esto
# Necesitamos una red con al menos una capa oculta
</code></pre>
<h2>ğŸ’» Parte 2: Forward Propagation</h2>
<h3>2.1 Arquitectura MLP</h3>
<pre><code class="language-python">&quot;&quot;&quot;
MLP - Multilayer Perceptron

Arquitectura tÃ­pica:
    Input Layer â†’ Hidden Layer(s) â†’ Output Layer

Ejemplo para clasificaciÃ³n binaria:
    x (n_features) â†’ h (n_hidden) â†’ y (1)

Forward Pass:
    zâ‚ = Wâ‚x + bâ‚        (capa 1: lineal)
    aâ‚ = Ïƒ(zâ‚)           (capa 1: activaciÃ³n)
    zâ‚‚ = Wâ‚‚aâ‚ + bâ‚‚       (capa 2: lineal)
    aâ‚‚ = Ïƒ(zâ‚‚)           (capa 2: activaciÃ³n = output)

Dimensiones:
    x: (n_features,)
    Wâ‚: (n_hidden, n_features)
    bâ‚: (n_hidden,)
    zâ‚, aâ‚: (n_hidden,)
    Wâ‚‚: (n_output, n_hidden)
    bâ‚‚: (n_output,)
    zâ‚‚, aâ‚‚: (n_output,)
&quot;&quot;&quot;
</code></pre>
<h3>2.2 ImplementaciÃ³n Forward Pass</h3>
<pre><code class="language-python">import numpy as np
from typing import List, Dict

class Layer:
    &quot;&quot;&quot;Una capa de la red neuronal.&quot;&quot;&quot;

    def __init__(self, input_size: int, output_size: int, activation: str = 'relu'):
        &quot;&quot;&quot;
        Args:
            input_size: nÃºmero de entradas
            output_size: nÃºmero de neuronas
            activation: 'relu', 'sigmoid', 'tanh', 'softmax', 'linear'
        &quot;&quot;&quot;
        self.input_size = input_size
        self.output_size = output_size
        self.activation = activation

        # InicializaciÃ³n Xavier/He
        if activation == 'relu':
            # He initialization para ReLU
            std = np.sqrt(2.0 / input_size)
        else:
            # Xavier initialization
            std = np.sqrt(1.0 / input_size)

        self.W = np.random.randn(output_size, input_size) * std
        self.b = np.zeros(output_size)

        # Cache para backprop
        self.cache = {}

    def forward(self, x: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;
        Forward pass de una capa.

        z = Wx + b
        a = activation(z)
        &quot;&quot;&quot;
        self.cache['x'] = x

        # TransformaciÃ³n lineal
        z = self.W @ x + self.b
        self.cache['z'] = z

        # ActivaciÃ³n
        if self.activation == 'relu':
            a = np.maximum(0, z)
        elif self.activation == 'sigmoid':
            a = 1 / (1 + np.exp(-np.clip(z, -500, 500)))
        elif self.activation == 'tanh':
            a = np.tanh(z)
        elif self.activation == 'softmax':
            z_shifted = z - np.max(z)
            exp_z = np.exp(z_shifted)
            a = exp_z / np.sum(exp_z)
        else:  # linear
            a = z

        self.cache['a'] = a
        return a


class NeuralNetwork:
    &quot;&quot;&quot;Red Neuronal Multicapa.&quot;&quot;&quot;

    def __init__(self, layer_sizes: List[int], activations: List[str]):
        &quot;&quot;&quot;
        Args:
            layer_sizes: [input_size, hidden1, hidden2, ..., output_size]
            activations: ['relu', 'relu', ..., 'sigmoid'] para cada capa
        &quot;&quot;&quot;
        assert len(activations) == len(layer_sizes) - 1

        self.layers = []
        for i in range(len(layer_sizes) - 1):
            layer = Layer(layer_sizes[i], layer_sizes[i+1], activations[i])
            self.layers.append(layer)

    def forward(self, x: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;Forward pass a travÃ©s de todas las capas.&quot;&quot;&quot;
        a = x
        for layer in self.layers:
            a = layer.forward(a)
        return a

    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;PredicciÃ³n para mÃºltiples muestras.&quot;&quot;&quot;
        predictions = []
        for x in X:
            output = self.forward(x)
            if len(output) == 1:
                predictions.append(1 if output[0] &gt; 0.5 else 0)
            else:
                predictions.append(np.argmax(output))
        return np.array(predictions)


# Demo
net = NeuralNetwork(
    layer_sizes=[2, 4, 1],  # 2 inputs â†’ 4 hidden â†’ 1 output
    activations=['relu', 'sigmoid']
)

# Forward pass
x = np.array([0.5, 0.3])
output = net.forward(x)
print(f&quot;Input: {x}&quot;)
print(f&quot;Output: {output}&quot;)
</code></pre>
<h2>ğŸ’» Parte 3: Backpropagation</h2>
<h3>3.1 Funciones de PÃ©rdida</h3>
<pre><code class="language-python">import numpy as np

def binary_cross_entropy(y_true: float, y_pred: float, eps: float = 1e-15) -&gt; float:
    &quot;&quot;&quot;
    Binary Cross-Entropy Loss.

    L = -[yÂ·log(Å·) + (1-y)Â·log(1-Å·)]

    Args:
        y_true: etiqueta real (0 o 1)
        y_pred: predicciÃ³n (probabilidad)
    &quot;&quot;&quot;
    y_pred = np.clip(y_pred, eps, 1 - eps)
    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

def bce_derivative(y_true: float, y_pred: float, eps: float = 1e-15) -&gt; float:
    &quot;&quot;&quot;
    Derivada de BCE respecto a y_pred.

    âˆ‚L/âˆ‚Å· = -y/Å· + (1-y)/(1-Å·)
    &quot;&quot;&quot;
    y_pred = np.clip(y_pred, eps, 1 - eps)
    return -y_true / y_pred + (1 - y_true) / (1 - y_pred)

def categorical_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-15) -&gt; float:
    &quot;&quot;&quot;
    Categorical Cross-Entropy para multiclase.

    L = -Î£áµ¢ yáµ¢Â·log(Å·áµ¢)

    Args:
        y_true: one-hot encoded (k,)
        y_pred: probabilidades softmax (k,)
    &quot;&quot;&quot;
    y_pred = np.clip(y_pred, eps, 1 - eps)
    return -np.sum(y_true * np.log(y_pred))

def mse_loss(y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Mean Squared Error.&quot;&quot;&quot;
    return np.mean((y_true - y_pred) ** 2)
</code></pre>
<h3>3.2 Backpropagation: La Chain Rule en AcciÃ³n</h3>
<pre><code class="language-python">&quot;&quot;&quot;
BACKPROPAGATION

Objetivo: Calcular âˆ‚L/âˆ‚W y âˆ‚L/âˆ‚b para cada capa.

Usando Chain Rule:
    âˆ‚L/âˆ‚Wâ‚‚ = âˆ‚L/âˆ‚aâ‚‚ Â· âˆ‚aâ‚‚/âˆ‚zâ‚‚ Â· âˆ‚zâ‚‚/âˆ‚Wâ‚‚
    âˆ‚L/âˆ‚Wâ‚ = âˆ‚L/âˆ‚aâ‚‚ Â· âˆ‚aâ‚‚/âˆ‚zâ‚‚ Â· âˆ‚zâ‚‚/âˆ‚aâ‚ Â· âˆ‚aâ‚/âˆ‚zâ‚ Â· âˆ‚zâ‚/âˆ‚Wâ‚

PatrÃ³n:
1. Calcular âˆ‚L/âˆ‚a_output (derivada de la loss)
2. Para cada capa, de atrÃ¡s hacia adelante:
   a. Î´ = âˆ‚L/âˆ‚z = âˆ‚L/âˆ‚a Â· âˆ‚a/âˆ‚z (error de la capa)
   b. âˆ‚L/âˆ‚W = Î´ Â· x^T
   c. âˆ‚L/âˆ‚b = Î´
   d. Propagar: âˆ‚L/âˆ‚a_prev = W^T Â· Î´
&quot;&quot;&quot;

def backward_layer(layer, dL_da: np.ndarray) -&gt; tuple:
    &quot;&quot;&quot;
    Backward pass de una capa.

    Args:
        layer: capa con cache del forward pass
        dL_da: gradiente de la loss respecto a la activaciÃ³n

    Returns:
        dL_dx: gradiente respecto a la entrada
        dL_dW: gradiente respecto a los pesos
        dL_db: gradiente respecto al bias
    &quot;&quot;&quot;
    z = layer.cache['z']
    x = layer.cache['x']
    a = layer.cache['a']

    # Derivada de la activaciÃ³n: âˆ‚a/âˆ‚z
    if layer.activation == 'sigmoid':
        da_dz = a * (1 - a)
    elif layer.activation == 'relu':
        da_dz = (z &gt; 0).astype(float)
    elif layer.activation == 'tanh':
        da_dz = 1 - a ** 2
    elif layer.activation == 'softmax':
        # Para softmax + cross-entropy, usamos el gradiente simplificado
        da_dz = np.ones_like(z)  # se maneja especialmente
    else:  # linear
        da_dz = np.ones_like(z)

    # Î´ = âˆ‚L/âˆ‚z = âˆ‚L/âˆ‚a Â· âˆ‚a/âˆ‚z
    delta = dL_da * da_dz

    # Gradientes
    dL_dW = np.outer(delta, x)
    dL_db = delta
    dL_dx = layer.W.T @ delta

    return dL_dx, dL_dW, dL_db
</code></pre>
<h3>3.3 Red Neuronal Completa con Backprop</h3>
<pre><code class="language-python">import numpy as np
from typing import List, Tuple

class NeuralNetworkFull:
    &quot;&quot;&quot;Red Neuronal con Backpropagation completo.&quot;&quot;&quot;

    def __init__(self, layer_sizes: List[int], activations: List[str]):
        self.layers = []
        for i in range(len(layer_sizes) - 1):
            layer = Layer(layer_sizes[i], layer_sizes[i+1], activations[i])
            self.layers.append(layer)

        self.loss_history = []

    def forward(self, x: np.ndarray) -&gt; np.ndarray:
        a = x
        for layer in self.layers:
            a = layer.forward(a)
        return a

    def backward(self, y_true: np.ndarray) -&gt; List[Tuple[np.ndarray, np.ndarray]]:
        &quot;&quot;&quot;
        Backward pass: calcula gradientes para todas las capas.

        Returns:
            Lista de (dW, db) para cada capa
        &quot;&quot;&quot;
        gradients = []

        # Obtener predicciÃ³n (Ãºltima activaciÃ³n)
        y_pred = self.layers[-1].cache['a']

        # Gradiente inicial: âˆ‚L/âˆ‚a_output
        # Para sigmoid + BCE: simplificado a (y_pred - y_true)
        # Para softmax + CCE: tambiÃ©n (y_pred - y_true)
        if self.layers[-1].activation in ['sigmoid', 'softmax']:
            dL_da = y_pred - y_true
        else:
            # MSE: 2(y_pred - y_true)
            dL_da = 2 * (y_pred - y_true)

        # Propagar hacia atrÃ¡s
        for layer in reversed(self.layers):
            dL_dx, dL_dW, dL_db = backward_layer(layer, dL_da)
            gradients.insert(0, (dL_dW, dL_db))
            dL_da = dL_dx

        return gradients

    def update_weights(self, gradients: List[Tuple], learning_rate: float):
        &quot;&quot;&quot;Actualiza pesos usando gradient descent.&quot;&quot;&quot;
        for layer, (dW, db) in zip(self.layers, gradients):
            layer.W -= learning_rate * dW
            layer.b -= learning_rate * db

    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        epochs: int = 1000,
        learning_rate: float = 0.1,
        verbose: bool = True
    ):
        &quot;&quot;&quot;Entrena la red.&quot;&quot;&quot;
        for epoch in range(epochs):
            total_loss = 0

            for xi, yi in zip(X, y):
                # Forward
                output = self.forward(xi)

                # Loss
                if isinstance(yi, (int, float)):
                    yi_arr = np.array([yi])
                else:
                    yi_arr = yi
                loss = binary_cross_entropy(yi_arr[0], output[0])
                total_loss += loss

                # Backward
                gradients = self.backward(yi_arr)

                # Update
                self.update_weights(gradients, learning_rate)

            avg_loss = total_loss / len(X)
            self.loss_history.append(avg_loss)

            if verbose and epoch % 100 == 0:
                print(f&quot;Epoch {epoch}: Loss = {avg_loss:.4f}&quot;)

    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        predictions = []
        for x in X:
            output = self.forward(x)
            predictions.append(1 if output[0] &gt; 0.5 else 0)
        return np.array(predictions)


# Demo: Resolver XOR
print(&quot;=== Entrenando para XOR ===&quot;)
X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_xor = np.array([0, 1, 1, 0])

net = NeuralNetworkFull(
    layer_sizes=[2, 4, 1],
    activations=['tanh', 'sigmoid']
)

net.fit(X_xor, y_xor, epochs=5000, learning_rate=0.5, verbose=True)

print(&quot;\n=== Predicciones XOR ===&quot;)
for x, y in zip(X_xor, y_xor):
    pred = net.forward(x)[0]
    print(f&quot;{x} -&gt; {pred:.4f} (target: {y})&quot;)
</code></pre>
<h2>ğŸ’» Parte 4: Optimizadores</h2>
<h3>4.1 SGD (Stochastic Gradient Descent)</h3>
<pre><code class="language-python">class SGD:
    &quot;&quot;&quot;Vanilla Stochastic Gradient Descent.&quot;&quot;&quot;

    def __init__(self, learning_rate: float = 0.01):
        self.lr = learning_rate

    def update(self, layer, dW: np.ndarray, db: np.ndarray):
        layer.W -= self.lr * dW
        layer.b -= self.lr * db
</code></pre>
<h3>4.2 SGD con Momentum</h3>
<pre><code class="language-python">class SGDMomentum:
    &quot;&quot;&quot;
    SGD con Momentum.

    v_t = Î²Â·v_{t-1} + (1-Î²)Â·âˆ‡L
    Î¸ = Î¸ - lrÂ·v_t

    Momentum ayuda a:
    - Acelerar convergencia
    - Escapar de mÃ­nimos locales
    - Reducir oscilaciones
    &quot;&quot;&quot;

    def __init__(self, learning_rate: float = 0.01, momentum: float = 0.9):
        self.lr = learning_rate
        self.momentum = momentum
        self.velocities = {}

    def update(self, layer, dW: np.ndarray, db: np.ndarray, layer_id: int):
        if layer_id not in self.velocities:
            self.velocities[layer_id] = {
                'W': np.zeros_like(dW),
                'b': np.zeros_like(db)
            }

        v = self.velocities[layer_id]

        # Actualizar velocidad
        v['W'] = self.momentum * v['W'] + (1 - self.momentum) * dW
        v['b'] = self.momentum * v['b'] + (1 - self.momentum) * db

        # Actualizar parÃ¡metros
        layer.W -= self.lr * v['W']
        layer.b -= self.lr * v['b']
</code></pre>
<h3>4.3 Adam Optimizer</h3>
<pre><code class="language-python">class Adam:
    &quot;&quot;&quot;
    Adam: Adaptive Moment Estimation.

    Combina:
    - Momentum (primer momento)
    - RMSprop (segundo momento)

    m_t = Î²â‚Â·m_{t-1} + (1-Î²â‚)Â·g_t       (momentum)
    v_t = Î²â‚‚Â·v_{t-1} + (1-Î²â‚‚)Â·g_tÂ²      (velocidad adaptativa)
    mÌ‚_t = m_t / (1 - Î²â‚^t)              (correcciÃ³n de bias)
    vÌ‚_t = v_t / (1 - Î²â‚‚^t)
    Î¸ = Î¸ - lr Â· mÌ‚_t / (âˆšvÌ‚_t + Îµ)
    &quot;&quot;&quot;

    def __init__(
        self,
        learning_rate: float = 0.001,
        beta1: float = 0.9,
        beta2: float = 0.999,
        epsilon: float = 1e-8
    ):
        self.lr = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = {}
        self.v = {}
        self.t = 0

    def update(self, layer, dW: np.ndarray, db: np.ndarray, layer_id: int):
        if layer_id not in self.m:
            self.m[layer_id] = {'W': np.zeros_like(dW), 'b': np.zeros_like(db)}
            self.v[layer_id] = {'W': np.zeros_like(dW), 'b': np.zeros_like(db)}

        self.t += 1
        m, v = self.m[layer_id], self.v[layer_id]

        # Actualizar momentos
        m['W'] = self.beta1 * m['W'] + (1 - self.beta1) * dW
        m['b'] = self.beta1 * m['b'] + (1 - self.beta1) * db
        v['W'] = self.beta2 * v['W'] + (1 - self.beta2) * dW**2
        v['b'] = self.beta2 * v['b'] + (1 - self.beta2) * db**2

        # CorrecciÃ³n de bias
        m_hat_W = m['W'] / (1 - self.beta1**self.t)
        m_hat_b = m['b'] / (1 - self.beta1**self.t)
        v_hat_W = v['W'] / (1 - self.beta2**self.t)
        v_hat_b = v['b'] / (1 - self.beta2**self.t)

        # Actualizar parÃ¡metros
        layer.W -= self.lr * m_hat_W / (np.sqrt(v_hat_W) + self.epsilon)
        layer.b -= self.lr * m_hat_b / (np.sqrt(v_hat_b) + self.epsilon)
</code></pre>
<h2>ğŸ“¦ Entregable del MÃ³dulo</h2>
<h3><code>neural_network.py</code></h3>
<pre><code class="language-python">&quot;&quot;&quot;
Neural Network Module

ImplementaciÃ³n desde cero de:
- MLP (Multilayer Perceptron)
- Backpropagation
- Optimizadores (SGD, Momentum, Adam)
- Funciones de activaciÃ³n

Autor: [Tu nombre]
MÃ³dulo: 06 - Deep Learning
&quot;&quot;&quot;

import numpy as np
from typing import List, Tuple, Optional


# ============================================================
# ACTIVACIONES
# ============================================================

def sigmoid(z): 
    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

def sigmoid_deriv(a): 
    return a * (1 - a)

def relu(z): 
    return np.maximum(0, z)

def relu_deriv(z): 
    return (z &gt; 0).astype(float)

def tanh_deriv(a): 
    return 1 - a**2

def softmax(z):
    exp_z = np.exp(z - np.max(z))
    return exp_z / np.sum(exp_z)


# ============================================================
# CAPA
# ============================================================

class Layer:
    def __init__(self, input_size: int, output_size: int, activation: str = 'relu'):
        self.activation = activation
        scale = np.sqrt(2.0 / input_size) if activation == 'relu' else np.sqrt(1.0 / input_size)
        self.W = np.random.randn(output_size, input_size) * scale
        self.b = np.zeros(output_size)
        self.cache = {}

    def forward(self, x: np.ndarray) -&gt; np.ndarray:
        self.cache['x'] = x
        z = self.W @ x + self.b
        self.cache['z'] = z

        if self.activation == 'relu':
            a = relu(z)
        elif self.activation == 'sigmoid':
            a = sigmoid(z)
        elif self.activation == 'tanh':
            a = np.tanh(z)
        elif self.activation == 'softmax':
            a = softmax(z)
        else:
            a = z

        self.cache['a'] = a
        return a

    def backward(self, dL_da: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:
        z, x, a = self.cache['z'], self.cache['x'], self.cache['a']

        if self.activation == 'sigmoid':
            da_dz = sigmoid_deriv(a)
        elif self.activation == 'relu':
            da_dz = relu_deriv(z)
        elif self.activation == 'tanh':
            da_dz = tanh_deriv(a)
        else:
            da_dz = np.ones_like(z)

        delta = dL_da * da_dz
        dL_dW = np.outer(delta, x)
        dL_db = delta
        dL_dx = self.W.T @ delta

        return dL_dx, dL_dW, dL_db


# ============================================================
# OPTIMIZADORES
# ============================================================

class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr

    def step(self, layers, gradients):
        for layer, (dW, db) in zip(layers, gradients):
            layer.W -= self.lr * dW
            layer.b -= self.lr * db


class Adam:
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        self.lr, self.beta1, self.beta2, self.eps = lr, beta1, beta2, eps
        self.m, self.v, self.t = {}, {}, 0

    def step(self, layers, gradients):
        self.t += 1
        for i, (layer, (dW, db)) in enumerate(zip(layers, gradients)):
            if i not in self.m:
                self.m[i] = {'W': np.zeros_like(dW), 'b': np.zeros_like(db)}
                self.v[i] = {'W': np.zeros_like(dW), 'b': np.zeros_like(db)}

            self.m[i]['W'] = self.beta1 * self.m[i]['W'] + (1 - self.beta1) * dW
            self.m[i]['b'] = self.beta1 * self.m[i]['b'] + (1 - self.beta1) * db
            self.v[i]['W'] = self.beta2 * self.v[i]['W'] + (1 - self.beta2) * dW**2
            self.v[i]['b'] = self.beta2 * self.v[i]['b'] + (1 - self.beta2) * db**2

            m_hat_W = self.m[i]['W'] / (1 - self.beta1**self.t)
            m_hat_b = self.m[i]['b'] / (1 - self.beta1**self.t)
            v_hat_W = self.v[i]['W'] / (1 - self.beta2**self.t)
            v_hat_b = self.v[i]['b'] / (1 - self.beta2**self.t)

            layer.W -= self.lr * m_hat_W / (np.sqrt(v_hat_W) + self.eps)
            layer.b -= self.lr * m_hat_b / (np.sqrt(v_hat_b) + self.eps)


# ============================================================
# RED NEURONAL
# ============================================================

class NeuralNetwork:
    def __init__(self, layer_sizes: List[int], activations: List[str]):
        self.layers = [Layer(layer_sizes[i], layer_sizes[i+1], activations[i]) 
                       for i in range(len(layer_sizes)-1)]
        self.loss_history = []

    def forward(self, x: np.ndarray) -&gt; np.ndarray:
        for layer in self.layers:
            x = layer.forward(x)
        return x

    def backward(self, y_true: np.ndarray) -&gt; List[Tuple]:
        y_pred = self.layers[-1].cache['a']
        dL_da = y_pred - y_true

        gradients = []
        for layer in reversed(self.layers):
            dL_da, dW, db = layer.backward(dL_da)
            gradients.insert(0, (dW, db))
        return gradients

    def fit(self, X, y, epochs=1000, lr=0.1, optimizer='sgd', verbose=True):
        opt = Adam(lr) if optimizer == 'adam' else SGD(lr)

        for epoch in range(epochs):
            total_loss = 0
            for xi, yi in zip(X, y):
                yi_arr = np.atleast_1d(yi)
                output = self.forward(xi)

                # BCE loss
                output_clip = np.clip(output, 1e-15, 1-1e-15)
                loss = -np.sum(yi_arr * np.log(output_clip) + (1-yi_arr) * np.log(1-output_clip))
                total_loss += loss

                gradients = self.backward(yi_arr)
                opt.step(self.layers, gradients)

            self.loss_history.append(total_loss / len(X))
            if verbose and epoch % (epochs//10) == 0:
                print(f&quot;Epoch {epoch}: Loss = {self.loss_history[-1]:.4f}&quot;)

    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        return np.array([1 if self.forward(x)[0] &gt; 0.5 else 0 for x in X])

    def score(self, X: np.ndarray, y: np.ndarray) -&gt; float:
        return np.mean(self.predict(X) == y)


# ============================================================
# TESTS
# ============================================================

if __name__ == &quot;__main__&quot;:
    print(&quot;=== Test: XOR Problem ===&quot;)
    X = np.array([[0,0], [0,1], [1,0], [1,1]])
    y = np.array([0, 1, 1, 0])

    net = NeuralNetwork([2, 4, 1], ['tanh', 'sigmoid'])
    net.fit(X, y, epochs=5000, lr=0.5, verbose=True)

    print(&quot;\nPredicciones:&quot;)
    for xi, yi in zip(X, y):
        pred = net.forward(xi)[0]
        print(f&quot;{xi} -&gt; {pred:.4f} (target: {yi})&quot;)

    print(f&quot;\nAccuracy: {net.score(X, y):.2%}&quot;)
    print(&quot;\nâœ“ Test XOR completado!&quot;)
</code></pre>
<h2>ğŸ’» Parte 5: CNNs - Redes Convolucionales (Semana 19)</h2>
<blockquote>
<p>âš ï¸ <strong>Nota:</strong> En este mÃ³dulo NO implementamos CNNs desde cero (es complejo). El objetivo es <strong>entender la teorÃ­a</strong> para el curso de Deep Learning de CU Boulder.</p>
</blockquote>
<h3>5.1 Â¿Por QuÃ© CNNs para ImÃ¡genes?</h3>
<pre><code>PROBLEMA CON MLP PARA IMÃGENES:

Imagen MNIST: 28x28 = 784 pÃ­xeles
MLP fully connected a capa de 256 neuronas:
  â†’ 784 Ã— 256 = 200,704 parÃ¡metros (Â¡solo primera capa!)

Imagen HD: 1920x1080x3 = 6,220,800 pÃ­xeles
  â†’ Imposible conectar todo con todo

SOLUCIÃ“N: CONVOLUCIÃ“N
- Procesar regiones locales (no toda la imagen)
- Compartir pesos (el mismo filtro en toda la imagen)
- Detectar patrones sin importar su posiciÃ³n
</code></pre>
<h3>5.2 La OperaciÃ³n de ConvoluciÃ³n</h3>
<pre><code class="language-python">import numpy as np

def convolve2d_simple(image: np.ndarray, kernel: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    ConvoluciÃ³n 2D simplificada (para entender el concepto).

    La convoluciÃ³n desliza un kernel (filtro) sobre la imagen
    y calcula el producto punto en cada posiciÃ³n.

    Args:
        image: Imagen de entrada (H, W)
        kernel: Filtro (kH, kW), tÃ­picamente 3x3 o 5x5

    Returns:
        Feature map (H-kH+1, W-kW+1)
    &quot;&quot;&quot;
    H, W = image.shape
    kH, kW = kernel.shape

    # TamaÃ±o del output (sin padding)
    out_H = H - kH + 1
    out_W = W - kW + 1

    output = np.zeros((out_H, out_W))

    for i in range(out_H):
        for j in range(out_W):
            # Extraer regiÃ³n de la imagen
            region = image[i:i+kH, j:j+kW]
            # Producto punto con el kernel
            output[i, j] = np.sum(region * kernel)

    return output


# Ejemplo: DetecciÃ³n de bordes verticales
image = np.array([
    [0, 0, 0, 1, 1, 1],
    [0, 0, 0, 1, 1, 1],
    [0, 0, 0, 1, 1, 1],
    [0, 0, 0, 1, 1, 1],
])

# Kernel Sobel para bordes verticales
sobel_vertical = np.array([
    [-1, 0, 1],
    [-2, 0, 2],
    [-1, 0, 1]
])

edges = convolve2d_simple(image, sobel_vertical)
print(&quot;Feature map (bordes verticales):&quot;)
print(edges)
</code></pre>
<h3>5.3 Conceptos Clave de CNNs</h3>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VOCABULARIO CNN                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  KERNEL (FILTRO)                                                â”‚
â”‚  â”œâ”€â”€ Matriz pequeÃ±a (3x3, 5x5) que detecta patrones             â”‚
â”‚  â”œâ”€â”€ Los valores del kernel son APRENDIDOS (backprop)           â”‚
â”‚  â””â”€â”€ Diferentes kernels detectan diferentes features            â”‚
â”‚                                                                 â”‚
â”‚  STRIDE                                                         â”‚
â”‚  â”œâ”€â”€ CuÃ¡ntos pÃ­xeles se mueve el kernel en cada paso            â”‚
â”‚  â”œâ”€â”€ stride=1: mueve 1 pÃ­xel (output grande)                    â”‚
â”‚  â””â”€â”€ stride=2: mueve 2 pÃ­xeles (output mÃ¡s pequeÃ±o)             â”‚
â”‚                                                                 â”‚
â”‚  PADDING                                                        â”‚
â”‚  â”œâ”€â”€ AÃ±adir ceros alrededor de la imagen                        â”‚
â”‚  â”œâ”€â”€ 'valid': sin padding (output mÃ¡s pequeÃ±o)                  â”‚
â”‚  â””â”€â”€ 'same': padding para mantener tamaÃ±o                       â”‚
â”‚                                                                 â”‚
â”‚  POOLING                                                        â”‚
â”‚  â”œâ”€â”€ Reduce dimensiones (downsampling)                          â”‚
â”‚  â”œâ”€â”€ Max Pooling: toma el mÃ¡ximo de cada regiÃ³n                 â”‚
â”‚  â””â”€â”€ Average Pooling: toma el promedio                          â”‚
â”‚                                                                 â”‚
â”‚  FEATURE MAP                                                    â”‚
â”‚  â””â”€â”€ Output de aplicar un filtro (lo que &quot;ve&quot; el filtro)        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h3>5.4 CÃ¡lculo de Dimensiones (Importante para ExÃ¡menes)</h3>
<pre><code class="language-python">def output_size(input_size: int, kernel_size: int, 
                stride: int = 1, padding: int = 0) -&gt; int:
    &quot;&quot;&quot;
    FÃ³rmula para calcular tamaÃ±o del output de convoluciÃ³n.

    output_size = floor((input + 2*padding - kernel) / stride) + 1
    &quot;&quot;&quot;
    return (input_size + 2 * padding - kernel_size) // stride + 1


# Ejemplos tÃ­picos de examen:
print(&quot;=== Ejercicios de dimensiones ===&quot;)

# Ejemplo 1: MNIST sin padding
# Input: 28x28, Kernel: 5x5, Stride: 1, Padding: 0
out = output_size(28, 5, stride=1, padding=0)
print(f&quot;MNIST 28x28, kernel 5x5, stride 1: output = {out}x{out}&quot;)  # 24x24

# Ejemplo 2: Con padding 'same'
# Para mantener tamaÃ±o con kernel 3x3, necesitas padding=1
out = output_size(28, 3, stride=1, padding=1)
print(f&quot;MNIST 28x28, kernel 3x3, padding 1: output = {out}x{out}&quot;)  # 28x28

# Ejemplo 3: Max Pooling 2x2 stride 2
out = output_size(24, 2, stride=2, padding=0)
print(f&quot;24x24, pooling 2x2 stride 2: output = {out}x{out}&quot;)  # 12x12
</code></pre>
<h3>5.5 Arquitectura TÃ­pica de CNN</h3>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ARQUITECTURA LENET-5 (ClÃ¡sica para MNIST)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Input: 28x28x1 (imagen grayscale)                              â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [CONV 5x5, 6 filtros] â†’ 24x24x6                                â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [ReLU]                                                         â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [MaxPool 2x2] â†’ 12x12x6                                        â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [CONV 5x5, 16 filtros] â†’ 8x8x16                                â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [ReLU]                                                         â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [MaxPool 2x2] â†’ 4x4x16 = 256 neuronas                          â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [Flatten] â†’ 256                                                â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [FC 120] â†’ 120                                                 â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [FC 84] â†’ 84                                                   â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  [FC 10 + Softmax] â†’ 10 clases (dÃ­gitos 0-9)                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h3>5.6 Max Pooling</h3>
<pre><code class="language-python">def max_pool2d(x: np.ndarray, pool_size: int = 2) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Max Pooling 2D.

    Reduce dimensiones tomando el mÃ¡ximo de cada regiÃ³n.
    Hace la red mÃ¡s robusta a pequeÃ±as traslaciones.

    Args:
        x: Feature map (H, W)
        pool_size: TamaÃ±o de la ventana (tÃ­picamente 2)

    Returns:
        Pooled output (H//pool_size, W//pool_size)
    &quot;&quot;&quot;
    H, W = x.shape
    out_H, out_W = H // pool_size, W // pool_size

    output = np.zeros((out_H, out_W))

    for i in range(out_H):
        for j in range(out_W):
            region = x[i*pool_size:(i+1)*pool_size, 
                      j*pool_size:(j+1)*pool_size]
            output[i, j] = np.max(region)

    return output


# Ejemplo
feature_map = np.array([
    [1, 3, 2, 4],
    [5, 6, 1, 2],
    [3, 2, 1, 0],
    [1, 2, 3, 4]
])

pooled = max_pool2d(feature_map, pool_size=2)
print(&quot;Original 4x4:&quot;)
print(feature_map)
print(&quot;\nMax Pooled 2x2:&quot;)
print(pooled)  # [[6, 4], [3, 4]]
</code></pre>
<h3>5.7 Por QuÃ© Funcionan las CNNs</h3>
<pre><code>INTUICIÃ“N:

1. CAPAS INICIALES: Detectan features simples
   - Bordes horizontales, verticales, diagonales
   - Cambios de color, texturas

2. CAPAS MEDIAS: Combinan features simples
   - Esquinas, curvas, patrones

3. CAPAS PROFUNDAS: Features de alto nivel
   - Partes de objetos (ojos, ruedas, letras)

4. CAPAS FINALES: Objetos completos
   - &quot;Esto es un 7&quot;, &quot;Esto es un gato&quot;

VENTAJAS CLAVE:
â”œâ”€â”€ Parameter sharing: mismo filtro en toda la imagen
â”œâ”€â”€ Sparse connectivity: cada output depende de regiÃ³n local
â”œâ”€â”€ Translation invariance: detecta patrones sin importar posiciÃ³n
â””â”€â”€ Hierarchical features: de simple a complejo
</code></pre>
<h3>5.8 Recursos para Profundizar en CNNs</h3>
<table>
<thead>
<tr>
<th>Recurso</th>
<th>DescripciÃ³n</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://www.youtube.com/watch?v=KuXjwB4LzSA">3B1B - But what is a convolution?</a></td>
<td>IntuiciÃ³n visual</td>
</tr>
<tr>
<td><a href="http://cs231n.stanford.edu/">CS231n Stanford</a></td>
<td>Curso completo de CNNs</td>
</tr>
<tr>
<td>Deep Learning Book, Cap. 9</td>
<td>TeorÃ­a formal</td>
</tr>
</tbody>
</table>
<h2>ğŸ“ DerivaciÃ³n AnalÃ­tica: Backpropagation a Mano (v3.2)</h2>
<blockquote>
<p>ğŸ“ <strong>SimulaciÃ³n de Examen:</strong> <em>"Derive las ecuaciones de backpropagation para una red de 2 capas"</em>. Este es un clÃ¡sico de exÃ¡menes de posgrado.</p>
</blockquote>
<h3>Red de 2 Capas: DerivaciÃ³n Completa</h3>
<p><strong>Arquitectura:</strong><br />
- Input: $x$ (vector de features)<br />
- Capa 1: $z_1 = W_1 x + b_1$, $a_1 = \sigma(z_1)$<br />
- Capa 2: $z_2 = W_2 a_1 + b_2$, $\hat{y} = \sigma(z_2)$<br />
- Loss: $L = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$</p>
<h4>Paso 1: Gradiente de la Capa de Salida</h4>
<p>$$\frac{\partial L}{\partial z_2} = \hat{y} - y = \delta_2$$</p>
<p>(Resultado elegante gracias a la combinaciÃ³n sigmoid + cross-entropy)</p>
<p>$$\frac{\partial L}{\partial W_2} = \delta_2 \cdot a_1^T$$</p>
<p>$$\frac{\partial L}{\partial b_2} = \delta_2$$</p>
<h4>Paso 2: Propagar el Error Hacia AtrÃ¡s (Capa Oculta)</h4>
<p>$$\frac{\partial L}{\partial a_1} = W_2^T \delta_2$$</p>
<p>$$\frac{\partial L}{\partial z_1} = \frac{\partial L}{\partial a_1} \odot \sigma'(z_1) = W_2^T \delta_2 \odot a_1 \odot (1 - a_1) = \delta_1$$</p>
<p>$$\frac{\partial L}{\partial W_1} = \delta_1 \cdot x^T$$</p>
<p>$$\frac{\partial L}{\partial b_1} = \delta_1$$</p>
<h4>Resumen: Las 4 Ecuaciones de Backprop</h4>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ECUACIONES DE BACKPROPAGATION                               â”‚
â”‚                                                             â”‚
â”‚ 1. Î´_L = âˆ‡_a L âŠ™ Ïƒ'(z_L)     Error en capa final           â”‚
â”‚                                                             â”‚
â”‚ 2. Î´_l = (W_{l+1}^T Î´_{l+1}) âŠ™ Ïƒ'(z_l)   Propagar atrÃ¡s    â”‚
â”‚                                                             â”‚
â”‚ 3. âˆ‚L/âˆ‚W_l = Î´_l Â· a_{l-1}^T   Gradiente de pesos           â”‚
â”‚                                                             â”‚
â”‚ 4. âˆ‚L/âˆ‚b_l = Î´_l              Gradiente de bias             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h3>Tu Entregable</h3>
<p>Escribe en un documento (Markdown o LaTeX):<br />
1. DerivaciÃ³n completa de backprop para red de 2 capas<br />
2. Por quÃ© $\delta_L = \hat{y} - y$ cuando usamos sigmoid + cross-entropy<br />
3. Diagrama de grafo computacional mostrando el flujo de gradientes</p>
<h2>ğŸ§ª Overfit on Small Batch: Debugging de Redes Neuronales (v3.3)</h2>
<blockquote>
<p>âš ï¸ <strong>CRÃTICO:</strong> Esta es la tÃ©cnica #1 de debugging en Deep Learning. Si tu red no puede hacer overfitting en 10 ejemplos, tiene un bug.</p>
</blockquote>
<h3>El Principio</h3>
<pre><code>REGLA DE ORO DEL DEBUGGING EN DL:

Una red neuronal DEBE poder memorizar un dataset pequeÃ±o.

Si entrenas con:
- 10 ejemplos
- Muchas Ã©pocas (1000+)
- Sin regularizaciÃ³n

El loss DEBE llegar a ~0.00 (o muy cercano).

Si NO llega a 0 â†’ TU IMPLEMENTACIÃ“N TIENE UN BUG
</code></pre>
<h3>Por QuÃ© Funciona</h3>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OVERFIT TEST                                                â”‚
â”‚                                                             â”‚
â”‚ Dataset pequeÃ±o (10 ejemplos):                              â”‚
â”‚ - Capacidad de la red &gt;&gt; complejidad del dataset            â”‚
â”‚ - La red puede &quot;memorizar&quot; cada ejemplo perfectamente       â”‚
â”‚ - Loss debe â†’ 0 si backprop funciona                        â”‚
â”‚                                                             â”‚
â”‚ Si loss NO baja:                                            â”‚
â”‚ - Gradiente mal calculado                                   â”‚
â”‚ - Learning rate incorrecto                                  â”‚
â”‚ - Arquitectura rota (dimensiones)                           â”‚
â”‚ - Bug en forward o backward pass                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h3>Script: <code>overfit_test.py</code> (Entregable Obligatorio v3.3)</h3>
<pre><code class="language-python">&quot;&quot;&quot;
Overfit Test - ValidaciÃ³n de Redes Neuronales
Si tu red no puede hacer overfit en 10 ejemplos, estÃ¡ rota.

Autor: [Tu nombre]
MÃ³dulo: 07 - Deep Learning
&quot;&quot;&quot;
import numpy as np
from typing import List, Tuple


def overfit_test(
    model,
    X_small: np.ndarray,
    y_small: np.ndarray,
    epochs: int = 2000,
    target_loss: float = 0.01,
    verbose: bool = True
) -&gt; Tuple[bool, List[float]]:
    &quot;&quot;&quot;
    Test de overfitting: la red debe memorizar un dataset pequeÃ±o.

    Args:
        model: Tu red neuronal (debe tener .fit() y .forward())
        X_small: Dataset pequeÃ±o (10-20 ejemplos)
        y_small: Labels del dataset
        epochs: Ã‰pocas de entrenamiento
        target_loss: Loss objetivo (default: 0.01)
        verbose: Mostrar progreso

    Returns:
        (passed, loss_history)
    &quot;&quot;&quot;
    if verbose:
        print(&quot;=&quot; * 60)
        print(&quot;OVERFIT TEST: Â¿Puede tu red memorizar 10 ejemplos?&quot;)
        print(&quot;=&quot; * 60)
        print(f&quot;Dataset size: {len(y_small)}&quot;)
        print(f&quot;Epochs: {epochs}&quot;)
        print(f&quot;Target loss: {target_loss}&quot;)
        print(&quot;-&quot; * 60)

    # Entrenar
    loss_history = []
    for epoch in range(epochs):
        # Forward pass para todos los ejemplos
        total_loss = 0.0
        for i in range(len(y_small)):
            output = model.forward(X_small[i])
            loss = np.mean((output - y_small[i]) ** 2)  # MSE
            total_loss += loss

            # Backward y update (asumiendo que model tiene estos mÃ©todos)
            model.backward(y_small[i])
            model.update(learning_rate=0.1)

        avg_loss = total_loss / len(y_small)
        loss_history.append(avg_loss)

        if verbose and epoch % 500 == 0:
            print(f&quot;Epoch {epoch:4d}: Loss = {avg_loss:.6f}&quot;)

    final_loss = loss_history[-1]
    passed = final_loss &lt; target_loss

    if verbose:
        print(&quot;-&quot; * 60)
        print(f&quot;Final Loss: {final_loss:.6f}&quot;)
        if passed:
            print(&quot;âœ“ PASSED: Tu red puede hacer overfitting&quot;)
            print(&quot;  â†’ El forward y backward pass funcionan correctamente&quot;)
        else:
            print(&quot;âœ— FAILED: Tu red NO puede hacer overfitting&quot;)
            print(&quot;  â†’ Revisa tu implementaciÃ³n de backprop&quot;)
            print(&quot;  Posibles causas:&quot;)
            print(&quot;  - Gradiente mal calculado&quot;)
            print(&quot;  - Learning rate muy bajo&quot;)
            print(&quot;  - Bug en forward pass&quot;)
            print(&quot;  - Dimensiones incorrectas&quot;)

    return passed, loss_history


# ============================================================
# EJEMPLO: Test con XOR (debe pasar)
# ============================================================

def test_xor_overfit():
    &quot;&quot;&quot;Test: Una red pequeÃ±a debe resolver XOR perfectamente.&quot;&quot;&quot;
    print(&quot;\n&quot; + &quot;=&quot; * 60)
    print(&quot;TEST: Overfit on XOR Problem&quot;)
    print(&quot;=&quot; * 60)

    # XOR dataset (4 ejemplos)
    X = np.array([
        [0, 0],
        [0, 1],
        [1, 0],
        [1, 1]
    ], dtype=np.float64)

    y = np.array([
        [0],
        [1],
        [1],
        [0]
    ], dtype=np.float64)

    # Crear red simple (2 -&gt; 8 -&gt; 1)
    # NOTA: Reemplaza esto con tu clase NeuralNetwork
    class SimpleNet:
        def __init__(self):
            np.random.seed(42)
            self.W1 = np.random.randn(8, 2) * 0.5
            self.b1 = np.zeros((8, 1))
            self.W2 = np.random.randn(1, 8) * 0.5
            self.b2 = np.zeros((1, 1))

            # Cache para backprop
            self.cache = {}

        def sigmoid(self, z):
            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

        def forward(self, x):
            x = x.reshape(-1, 1)
            z1 = self.W1 @ x + self.b1
            a1 = self.sigmoid(z1)
            z2 = self.W2 @ a1 + self.b2
            a2 = self.sigmoid(z2)

            self.cache = {'x': x, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2}
            return a2.flatten()

        def backward(self, y_true):
            y_true = np.array(y_true).reshape(-1, 1)
            a2 = self.cache['a2']
            a1 = self.cache['a1']
            x = self.cache['x']

            # Gradientes
            dz2 = a2 - y_true
            self.dW2 = dz2 @ a1.T
            self.db2 = dz2

            da1 = self.W2.T @ dz2
            dz1 = da1 * a1 * (1 - a1)
            self.dW1 = dz1 @ x.T
            self.db1 = dz1

        def update(self, learning_rate):
            self.W1 -= learning_rate * self.dW1
            self.b1 -= learning_rate * self.db1
            self.W2 -= learning_rate * self.dW2
            self.b2 -= learning_rate * self.db2

    # Ejecutar test
    model = SimpleNet()
    passed, history = overfit_test(model, X, y, epochs=2000, target_loss=0.01)

    # Verificar predicciones finales
    print(&quot;\nPredicciones finales:&quot;)
    for i in range(len(X)):
        pred = model.forward(X[i])
        print(f&quot;  Input: {X[i]} â†’ Pred: {pred[0]:.3f} (Target: {y[i][0]})&quot;)

    return passed


if __name__ == &quot;__main__&quot;:
    test_xor_overfit()
</code></pre>
<h3>Checklist de Debugging con Overfit Test</h3>
<table>
<thead>
<tr>
<th>SÃ­ntoma</th>
<th>DiagnÃ³stico</th>
<th>SoluciÃ³n</th>
</tr>
</thead>
<tbody>
<tr>
<td>Loss no baja</td>
<td>Gradiente = 0 o NaN</td>
<td>Verificar derivadas con grad_check</td>
</tr>
<tr>
<td>Loss baja muy lento</td>
<td>Learning rate muy bajo</td>
<td>Aumentar LR (probar 0.1, 0.5, 1.0)</td>
</tr>
<tr>
<td>Loss oscila mucho</td>
<td>Learning rate muy alto</td>
<td>Reducir LR</td>
</tr>
<tr>
<td>Loss sube</td>
<td>Signos invertidos en gradiente</td>
<td>Revisar forward/backward</td>
</tr>
<tr>
<td>Loss = NaN</td>
<td>Overflow en exp/softmax</td>
<td>Usar versiones numÃ©ricamente estables</td>
</tr>
</tbody>
</table>
<h2>ğŸ¯ El Reto del Tablero Blanco (MetodologÃ­a Feynman)</h2>
<p>Explica en <strong>mÃ¡ximo 5 lÃ­neas</strong> sin jerga tÃ©cnica:</p>
<ol>
<li>
<p><strong>Â¿QuÃ© es backpropagation?</strong></p>
<blockquote>
<p>Pista: Piensa en "culpar" a cada peso por el error.</p>
</blockquote>
</li>
<li>
<p><strong>Â¿Por quÃ© ReLU es mejor que sigmoid en capas ocultas?</strong></p>
<blockquote>
<p>Pista: Piensa en quÃ© pasa con el gradiente de sigmoid cuando z es muy grande o muy pequeÃ±o.</p>
</blockquote>
</li>
<li>
<p><strong>Â¿QuÃ© hace una convoluciÃ³n en una imagen?</strong></p>
<blockquote>
<p>Pista: Piensa en "deslizar una lupa" buscando un patrÃ³n especÃ­fico.</p>
</blockquote>
</li>
<li>
<p><strong>Â¿Por quÃ© usamos pooling?</strong></p>
<blockquote>
<p>Pista: Piensa en "resumir" una regiÃ³n y hacerla mÃ¡s pequeÃ±a.</p>
</blockquote>
</li>
</ol>
<h2>âœ… Checklist de FinalizaciÃ³n (v3.3)</h2>
<h3>Conocimiento</h3>
<ul>
<li>[ ] Entiendo la analogÃ­a neurona biolÃ³gica â†’ neurona artificial</li>
<li>[ ] ImplementÃ© sigmoid, ReLU, tanh, softmax y sus derivadas</li>
<li>[ ] Entiendo por quÃ© XOR no es linealmente separable</li>
<li>[ ] ImplementÃ© forward pass para MLP</li>
<li>[ ] Entiendo la Chain Rule aplicada a backpropagation</li>
<li>[ ] ImplementÃ© backward pass calculando gradientes</li>
<li>[ ] ImplementÃ© SGD, SGD+Momentum y Adam</li>
<li>[ ] Mi red resuelve el problema XOR</li>
</ul>
<h3>CNNs (TeorÃ­a)</h3>
<ul>
<li>[ ] Entiendo quÃ© es convoluciÃ³n, stride, padding y pooling</li>
<li>[ ] Puedo calcular dimensiones de output de una CNN</li>
<li>[ ] Conozco la arquitectura LeNet-5</li>
</ul>
<h3>Entregables de CÃ³digo</h3>
<ul>
<li>[ ] <code>neural_network.py</code> con tests pasando</li>
<li>[ ] <code>mypy src/</code> pasa sin errores</li>
<li>[ ] <code>pytest tests/</code> pasa sin errores</li>
</ul>
<h3>Overfit Test (v3.3 - Obligatorio)</h3>
<ul>
<li>[ ] <strong><code>overfit_test.py</code> implementado</strong></li>
<li>[ ] <strong>Mi red hace overfit en XOR (loss &lt; 0.01)</strong></li>
<li>[ ] Si el test falla, debuggeÃ© con grad_check</li>
</ul>
<h3>DerivaciÃ³n AnalÃ­tica (Obligatorio)</h3>
<ul>
<li>[ ] DerivÃ© las ecuaciones de backprop a mano</li>
<li>[ ] Documento con derivaciÃ³n completa (Markdown o LaTeX)</li>
<li>[ ] Diagrama de grafo computacional</li>
</ul>
<h3>MetodologÃ­a Feynman</h3>
<ul>
<li>[ ] Puedo explicar backpropagation en 5 lÃ­neas sin jerga</li>
<li>[ ] Puedo explicar ReLU vs sigmoid en 5 lÃ­neas</li>
<li>[ ] Puedo explicar convoluciÃ³n en 5 lÃ­neas</li>
<li>[ ] Puedo explicar pooling en 5 lÃ­neas</li>
</ul>
<h2>ğŸ”— NavegaciÃ³n</h2>
<table>
<thead>
<tr>
<th>Anterior</th>
<th>Ãndice</th>
<th>Siguiente</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="#mod_06_UNSUPERVISED_LEARNING">06_UNSUPERVISED_LEARNING</a></td>
<td><a href="#mod_00_INDICE">00_INDICE</a></td>
<td><a href="#mod_08_PROYECTO_MNIST">08_PROYECTO_MNIST</a></td>
</tr>
</tbody>
</table>
            </div>
        
            <!-- MÃ“DULO: 08_PROYECTO_MNIST.md -->
            <div class="cover-page">
                <div class="cover-content">
                    <div style="font-size:40pt;margin-bottom:20px;">ğŸ’</div>
                    <div id="mod_08_PROYECTO_MNIST" class="cover-title">MÃ“DULO 08 - PROYECTO MNIST ANALYST</div>
                    <div class="cover-subtitle">MS in AI Pathway - ML Specialist v3.0</div>
                    <div class="cover-badge">MATEMÃTICAS APLICADAS A CÃ“DIGO</div>
                </div>
                <div style="position:absolute;bottom:30px;font-size:10pt;opacity:0.6;">DUQUEOM | 2025</div>
            </div>
            <div class="content">
                <h1>MÃ³dulo 08 - Proyecto Final: MNIST Analyst</h1>
<blockquote>
<p><strong>ğŸ¯ Objetivo:</strong> Pipeline end-to-end que demuestra competencia en las 3 Ã¡reas del Pathway<br />
<strong>Fase:</strong> 3 - Proyecto Integrador | <strong>Semanas 21-24</strong> (4 semanas)<br />
<strong>Dataset:</strong> MNIST (dÃ­gitos escritos a mano, 28Ã—28 pÃ­xeles)</p>
</blockquote>
<h2>ğŸ§  Â¿QuÃ© Estamos Construyendo?</h2>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                             â”‚
â”‚   PROYECTO: END-TO-END HANDWRITTEN DIGIT ANALYSIS PIPELINE                  â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚                                                                             â”‚
â”‚   LÃNEA 1: MACHINE LEARNING (3 crÃ©ditos) - DEMOSTRADO EN 4 SEMANAS          â”‚
â”‚   â”œâ”€â”€ Semana 21: EDA + PCA + K-Means                                        â”‚
â”‚   â”œâ”€â”€ Semana 22: Logistic Regression One-vs-All                             â”‚
â”‚   â”œâ”€â”€ Semana 23: MLP con Backprop                                           â”‚
â”‚   â””â”€â”€ Semana 24: ComparaciÃ³n de Modelos + Informe                           â”‚
â”‚                                                                             â”‚
â”‚   RESULTADO:                                                                â”‚
â”‚   Un pipeline que analiza, agrupa y clasifica dÃ­gitos MNIST                 â”‚
â”‚   usando algoritmos implementados 100% desde cero.                          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<blockquote>
<p>ğŸ’¡ <strong>Nota v3.1:</strong> MNIST es un dataset simple. 4 semanas son suficientes para un proyecto bien estructurado.</p>
</blockquote>
<h2>ğŸ“š Estructura del Proyecto</h2>
<h3>Cronograma (4 Semanas)</h3>
<table>
<thead>
<tr>
<th>Semana</th>
<th>Fase</th>
<th>Materia Demostrada</th>
<th>Entregable</th>
</tr>
</thead>
<tbody>
<tr>
<td>21</td>
<td>EDA + No Supervisado</td>
<td>Unsupervised Algorithms</td>
<td>PCA + K-Means funcionando</td>
</tr>
<tr>
<td>22</td>
<td>ClasificaciÃ³n ClÃ¡sica</td>
<td>Supervised Learning</td>
<td>Logistic Regression OvA</td>
</tr>
<tr>
<td>23</td>
<td>Deep Learning</td>
<td>Introduction to Deep Learning</td>
<td>MLP con backprop</td>
</tr>
<tr>
<td>24</td>
<td>Benchmark + Informe</td>
<td>IntegraciÃ³n</td>
<td>MODEL_COMPARISON.md</td>
</tr>
</tbody>
</table>
<h3>Estructura de Archivos</h3>
<pre><code>mnist-analyst/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py         # Cargar y preprocesar MNIST (MÃ³dulo 01)
â”‚   â”œâ”€â”€ linear_algebra.py      # Operaciones vectoriales (MÃ³dulo 02)
â”‚   â”œâ”€â”€ pca.py                 # PCA desde cero (MÃ³dulo 06)
â”‚   â”œâ”€â”€ kmeans.py              # K-Means desde cero (MÃ³dulo 06)
â”‚   â”œâ”€â”€ logistic_regression.py # Logistic multiclase (MÃ³dulo 05)
â”‚   â”œâ”€â”€ neural_network.py      # MLP con backprop (MÃ³dulo 07)
â”‚   â”œâ”€â”€ metrics.py             # MÃ©tricas de evaluaciÃ³n (MÃ³dulo 05)
â”‚   â””â”€â”€ pipeline.py            # Pipeline integrado
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_pca_visualization.ipynb
â”‚   â”œâ”€â”€ 03_kmeans_clustering.ipynb
â”‚   â”œâ”€â”€ 04_logistic_classification.ipynb
â”‚   â””â”€â”€ 05_neural_network_benchmark.ipynb
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_*.py
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ MODEL_COMPARISON.md
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
</code></pre>
<h2>ğŸ’» Parte 1: Cargar MNIST</h2>
<h3>1.1 Data Loader</h3>
<pre><code class="language-python">&quot;&quot;&quot;
MNIST Dataset Loader

MNIST contiene:
- 60,000 imÃ¡genes de entrenamiento
- 10,000 imÃ¡genes de test
- Cada imagen: 28x28 pÃ­xeles grayscale (0-255)
- 10 clases: dÃ­gitos 0-9

Formato aplanado: cada imagen es un vector de 784 dimensiones
&quot;&quot;&quot;

import numpy as np
import struct
import gzip
from pathlib import Path
from typing import Tuple


def load_mnist_images(filepath: str) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Carga imÃ¡genes MNIST desde archivo IDX.

    Formato IDX:
    - 4 bytes: magic number
    - 4 bytes: nÃºmero de imÃ¡genes
    - 4 bytes: nÃºmero de filas
    - 4 bytes: nÃºmero de columnas
    - resto: pÃ­xeles (unsigned bytes)
    &quot;&quot;&quot;
    with gzip.open(filepath, 'rb') as f:
        magic, num_images, rows, cols = struct.unpack('&gt;IIII', f.read(16))
        images = np.frombuffer(f.read(), dtype=np.uint8)
        images = images.reshape(num_images, rows * cols)
    return images


def load_mnist_labels(filepath: str) -&gt; np.ndarray:
    &quot;&quot;&quot;Carga etiquetas MNIST.&quot;&quot;&quot;
    with gzip.open(filepath, 'rb') as f:
        magic, num_labels = struct.unpack('&gt;II', f.read(8))
        labels = np.frombuffer(f.read(), dtype=np.uint8)
    return labels


def load_mnist(data_dir: str = 'data/mnist') -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    &quot;&quot;&quot;
    Carga dataset MNIST completo.

    Returns:
        X_train: (60000, 784)
        y_train: (60000,)
        X_test: (10000, 784)
        y_test: (10000,)
    &quot;&quot;&quot;
    data_dir = Path(data_dir)

    X_train = load_mnist_images(data_dir / 'train-images-idx3-ubyte.gz')
    y_train = load_mnist_labels(data_dir / 'train-labels-idx1-ubyte.gz')
    X_test = load_mnist_images(data_dir / 't10k-images-idx3-ubyte.gz')
    y_test = load_mnist_labels(data_dir / 't10k-labels-idx1-ubyte.gz')

    return X_train, y_train, X_test, y_test


def normalize_data(X: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;Normaliza pÃ­xeles a rango [0, 1].&quot;&quot;&quot;
    return X.astype(np.float64) / 255.0


def one_hot_encode(y: np.ndarray, num_classes: int = 10) -&gt; np.ndarray:
    &quot;&quot;&quot;Convierte etiquetas a one-hot encoding.&quot;&quot;&quot;
    one_hot = np.zeros((len(y), num_classes))
    one_hot[np.arange(len(y)), y] = 1
    return one_hot


# Alternativa: generar datos sintÃ©ticos si no tienes MNIST
def generate_synthetic_mnist(n_samples: int = 1000, seed: int = 42) -&gt; Tuple:
    &quot;&quot;&quot;
    Genera datos sintÃ©ticos similares a MNIST para pruebas.
    &quot;&quot;&quot;
    np.random.seed(seed)

    X = np.random.rand(n_samples, 784)  # ImÃ¡genes aleatorias
    y = np.random.randint(0, 10, n_samples)  # Etiquetas aleatorias

    # Split 80/20
    split = int(0.8 * n_samples)
    return X[:split], y[:split], X[split:], y[split:]
</code></pre>
<h3>1.2 VisualizaciÃ³n</h3>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def visualize_digits(X: np.ndarray, y: np.ndarray, n_samples: int = 25):
    &quot;&quot;&quot;Visualiza una cuadrÃ­cula de dÃ­gitos.&quot;&quot;&quot;
    n_cols = 5
    n_rows = (n_samples + n_cols - 1) // n_cols

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 2*n_rows))
    axes = axes.flatten()

    for i, ax in enumerate(axes):
        if i &lt; n_samples:
            img = X[i].reshape(28, 28)
            ax.imshow(img, cmap='gray')
            ax.set_title(f'Label: {y[i]}')
        ax.axis('off')

    plt.tight_layout()
    plt.show()


def visualize_digit_single(x: np.ndarray, title: str = ''):
    &quot;&quot;&quot;Visualiza un solo dÃ­gito.&quot;&quot;&quot;
    plt.figure(figsize=(4, 4))
    plt.imshow(x.reshape(28, 28), cmap='gray')
    plt.title(title)
    plt.axis('off')
    plt.show()
</code></pre>
<h2>ğŸ’» Parte 2: ExploraciÃ³n No Supervisada (Semanas 21-22)</h2>
<h3>2.1 PCA para VisualizaciÃ³n</h3>
<pre><code class="language-python">&quot;&quot;&quot;
SEMANA 21: PCA en MNIST

Objetivo: Reducir de 784 dimensiones a 2-3 para visualizaciÃ³n.

Preguntas a responder:
1. Â¿CuÃ¡nta varianza se retiene con pocos componentes?
2. Â¿Se separan visualmente las clases en 2D?
3. Â¿QuÃ© &quot;aprenden&quot; las componentes principales?
&quot;&quot;&quot;

import numpy as np
from typing import Tuple

class PCA:
    &quot;&quot;&quot;PCA implementado desde cero (del MÃ³dulo 05).&quot;&quot;&quot;

    def __init__(self, n_components: int):
        self.n_components = n_components
        self.components_ = None
        self.mean_ = None
        self.explained_variance_ratio_ = None

    def fit(self, X: np.ndarray) -&gt; 'PCA':
        self.mean_ = np.mean(X, axis=0)
        X_centered = X - self.mean_

        # SVD (mÃ¡s estable que eigendecomposition)
        U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)

        self.components_ = Vt[:self.n_components].T
        variance = (S ** 2) / (len(X) - 1)
        self.explained_variance_ratio_ = variance[:self.n_components] / np.sum(variance)

        return self

    def transform(self, X: np.ndarray) -&gt; np.ndarray:
        return (X - self.mean_) @ self.components_

    def fit_transform(self, X: np.ndarray) -&gt; np.ndarray:
        self.fit(X)
        return self.transform(X)

    def inverse_transform(self, X_pca: np.ndarray) -&gt; np.ndarray:
        return X_pca @ self.components_.T + self.mean_


def analyze_pca_mnist(X: np.ndarray, y: np.ndarray):
    &quot;&quot;&quot;AnÃ¡lisis PCA completo de MNIST.&quot;&quot;&quot;

    # 1. PCA con diferentes nÃºmeros de componentes
    print(&quot;=== AnÃ¡lisis de Varianza Explicada ===&quot;)
    pca_full = PCA(n_components=min(50, X.shape[1]))
    pca_full.fit(X)

    cumulative_var = np.cumsum(pca_full.explained_variance_ratio_)

    for n in [2, 10, 50]:
        if n &lt;= len(cumulative_var):
            print(f&quot;  {n} componentes: {cumulative_var[n-1]:.2%} varianza&quot;)

    # 2. VisualizaciÃ³n 2D
    print(&quot;\n=== ProyecciÃ³n 2D ===&quot;)
    pca_2d = PCA(n_components=2)
    X_2d = pca_2d.fit_transform(X)

    plt.figure(figsize=(10, 8))
    for digit in range(10):
        mask = y == digit
        plt.scatter(X_2d[mask, 0], X_2d[mask, 1], 
                   alpha=0.5, label=str(digit), s=10)
    plt.legend()
    plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%})')
    plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%})')
    plt.title('MNIST en 2D (PCA)')
    plt.show()

    # 3. Visualizar componentes principales
    print(&quot;\n=== Componentes Principales como ImÃ¡genes ===&quot;)
    fig, axes = plt.subplots(2, 5, figsize=(12, 5))
    pca_10 = PCA(n_components=10)
    pca_10.fit(X)

    for i, ax in enumerate(axes.flatten()):
        component = pca_10.components_[:, i].reshape(28, 28)
        ax.imshow(component, cmap='RdBu')
        ax.set_title(f'PC{i+1}')
        ax.axis('off')
    plt.suptitle('Top 10 Componentes Principales')
    plt.tight_layout()
    plt.show()

    return pca_2d, X_2d
</code></pre>
<h3>2.2 K-Means Clustering</h3>
<pre><code class="language-python">&quot;&quot;&quot;
SEMANA 22: K-Means en MNIST

Objetivo: Agrupar dÃ­gitos SIN usar etiquetas.

Preguntas a responder:
1. Â¿K-Means encuentra los 10 dÃ­gitos?
2. Â¿QuÃ© tan puros son los clusters?
3. Â¿CÃ³mo se ven los centroides?
&quot;&quot;&quot;

import numpy as np

class KMeans:
    &quot;&quot;&quot;K-Means implementado desde cero (del MÃ³dulo 05).&quot;&quot;&quot;

    def __init__(self, n_clusters: int = 10, max_iter: int = 100, seed: int = None):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.seed = seed
        self.centroids = None
        self.labels_ = None
        self.inertia_ = None

    def _init_centroids_plusplus(self, X: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;K-Means++ initialization.&quot;&quot;&quot;
        if self.seed:
            np.random.seed(self.seed)

        n_samples = len(X)
        centroids = [X[np.random.randint(n_samples)]]

        for _ in range(1, self.n_clusters):
            distances = np.array([min(np.sum((x - c)**2) for c in centroids) for x in X])
            probs = distances / distances.sum()
            centroids.append(X[np.random.choice(n_samples, p=probs)])

        return np.array(centroids)

    def fit(self, X: np.ndarray) -&gt; 'KMeans':
        self.centroids = self._init_centroids_plusplus(X)

        for _ in range(self.max_iter):
            # Asignar
            distances = np.array([[np.sum((x - c)**2) for c in self.centroids] for x in X])
            self.labels_ = np.argmin(distances, axis=1)

            # Actualizar
            new_centroids = np.array([X[self.labels_ == k].mean(axis=0) 
                                      if np.sum(self.labels_ == k) &gt; 0 
                                      else self.centroids[k]
                                      for k in range(self.n_clusters)])

            if np.allclose(self.centroids, new_centroids):
                break
            self.centroids = new_centroids

        self.inertia_ = sum(np.sum((X[self.labels_ == k] - self.centroids[k])**2) 
                           for k in range(self.n_clusters))
        return self

    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        distances = np.array([[np.sum((x - c)**2) for c in self.centroids] for x in X])
        return np.argmin(distances, axis=1)


def analyze_kmeans_mnist(X: np.ndarray, y: np.ndarray):
    &quot;&quot;&quot;AnÃ¡lisis K-Means de MNIST.&quot;&quot;&quot;

    print(&quot;=== K-Means Clustering ===&quot;)
    kmeans = KMeans(n_clusters=10, seed=42)
    kmeans.fit(X)

    # 1. Visualizar centroides
    print(&quot;\n=== Centroides (promedio de cada cluster) ===&quot;)
    fig, axes = plt.subplots(2, 5, figsize=(12, 5))
    for i, ax in enumerate(axes.flatten()):
        centroid = kmeans.centroids[i].reshape(28, 28)
        ax.imshow(centroid, cmap='gray')
        ax.set_title(f'Cluster {i}')
        ax.axis('off')
    plt.suptitle('Centroides K-Means')
    plt.tight_layout()
    plt.show()

    # 2. Analizar pureza de clusters
    print(&quot;\n=== Pureza de Clusters ===&quot;)
    print(&quot;Cluster | DÃ­gito Dominante | Pureza&quot;)
    print(&quot;-&quot; * 40)

    total_correct = 0
    for cluster in range(10):
        cluster_mask = kmeans.labels_ == cluster
        cluster_labels = y[cluster_mask]

        if len(cluster_labels) &gt; 0:
            dominant_digit = np.bincount(cluster_labels).argmax()
            purity = np.sum(cluster_labels == dominant_digit) / len(cluster_labels)
            total_correct += np.sum(cluster_labels == dominant_digit)
            print(f&quot;   {cluster}    |        {dominant_digit}         | {purity:.2%}&quot;)

    overall_purity = total_correct / len(y)
    print(f&quot;\nPureza Global: {overall_purity:.2%}&quot;)

    return kmeans
</code></pre>
<h2>ğŸ’» Parte 3: ClasificaciÃ³n Supervisada (Semanas 23-24)</h2>
<h3>3.1 Logistic Regression One-vs-All</h3>
<pre><code class="language-python">&quot;&quot;&quot;
SEMANAS 23-24: Logistic Regression Multiclase

Estrategia One-vs-All (OvA):
- Entrenar 10 clasificadores binarios
- Cada uno: &quot;Â¿Es este dÃ­gito X o no?&quot;
- PredicciÃ³n: elegir la clase con mayor probabilidad
&quot;&quot;&quot;

import numpy as np
from typing import List

def sigmoid(z):
    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))


class LogisticRegressionBinary:
    &quot;&quot;&quot;Logistic Regression binario.&quot;&quot;&quot;

    def __init__(self, lr: float = 0.1, n_iter: int = 100, reg: float = 0.01):
        self.lr = lr
        self.n_iter = n_iter
        self.reg = reg  # L2 regularization
        self.theta = None

    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; 'LogisticRegressionBinary':
        n_samples, n_features = X.shape
        self.theta = np.zeros(n_features)

        for _ in range(self.n_iter):
            h = sigmoid(X @ self.theta)
            grad = (1/n_samples) * X.T @ (h - y) + (self.reg/n_samples) * self.theta
            self.theta -= self.lr * grad

        return self

    def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:
        return sigmoid(X @ self.theta)


class LogisticRegressionOvA:
    &quot;&quot;&quot;Logistic Regression One-vs-All para clasificaciÃ³n multiclase.&quot;&quot;&quot;

    def __init__(self, n_classes: int = 10, lr: float = 0.1, n_iter: int = 100):
        self.n_classes = n_classes
        self.lr = lr
        self.n_iter = n_iter
        self.classifiers: List[LogisticRegressionBinary] = []

    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; 'LogisticRegressionOvA':
        &quot;&quot;&quot;Entrena un clasificador por clase.&quot;&quot;&quot;
        # AÃ±adir bias
        X_b = np.column_stack([np.ones(len(X)), X])

        self.classifiers = []
        for c in range(self.n_classes):
            print(f&quot;  Entrenando clasificador para clase {c}...&quot;, end='\r')
            y_binary = (y == c).astype(int)
            clf = LogisticRegressionBinary(self.lr, self.n_iter)
            clf.fit(X_b, y_binary)
            self.classifiers.append(clf)

        print(&quot;  Entrenamiento completado.                &quot;)
        return self

    def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;Retorna probabilidades para cada clase.&quot;&quot;&quot;
        X_b = np.column_stack([np.ones(len(X)), X])
        probs = np.column_stack([clf.predict_proba(X_b) for clf in self.classifiers])
        return probs

    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;Predice la clase con mayor probabilidad.&quot;&quot;&quot;
        probs = self.predict_proba(X)
        return np.argmax(probs, axis=1)

    def score(self, X: np.ndarray, y: np.ndarray) -&gt; float:
        &quot;&quot;&quot;Accuracy.&quot;&quot;&quot;
        return np.mean(self.predict(X) == y)


def train_logistic_mnist(X_train, y_train, X_test, y_test):
    &quot;&quot;&quot;Entrena y evalÃºa Logistic Regression en MNIST.&quot;&quot;&quot;

    print(&quot;=== Logistic Regression One-vs-All ===&quot;)

    # Entrenar
    lr_model = LogisticRegressionOvA(n_classes=10, lr=0.1, n_iter=200)
    lr_model.fit(X_train, y_train)

    # Evaluar
    train_acc = lr_model.score(X_train, y_train)
    test_acc = lr_model.score(X_test, y_test)

    print(f&quot;\nTrain Accuracy: {train_acc:.2%}&quot;)
    print(f&quot;Test Accuracy:  {test_acc:.2%}&quot;)

    # MÃ©tricas detalladas
    y_pred = lr_model.predict(X_test)

    print(&quot;\n=== MÃ©tricas por Clase ===&quot;)
    print(&quot;DÃ­gito | Precision | Recall | F1-Score&quot;)
    print(&quot;-&quot; * 45)

    for digit in range(10):
        tp = np.sum((y_test == digit) &amp; (y_pred == digit))
        fp = np.sum((y_test != digit) &amp; (y_pred == digit))
        fn = np.sum((y_test == digit) &amp; (y_pred != digit))

        precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0
        recall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) &gt; 0 else 0

        print(f&quot;   {digit}   |   {precision:.3f}   |  {recall:.3f}  |   {f1:.3f}&quot;)

    # Matriz de confusiÃ³n
    print(&quot;\n=== Matriz de ConfusiÃ³n ===&quot;)
    cm = np.zeros((10, 10), dtype=int)
    for true, pred in zip(y_test, y_pred):
        cm[true, pred] += 1

    print(&quot;    &quot; + &quot;  &quot;.join(str(i) for i in range(10)))
    for i in range(10):
        print(f&quot;{i}: &quot; + &quot; &quot;.join(f&quot;{cm[i,j]:3d}&quot; for j in range(10)))

    return lr_model
</code></pre>
<h2>ğŸ’» Parte 4: Deep Learning (Semanas 25-26)</h2>
<h3>4.1 MLP para MNIST</h3>
<pre><code class="language-python">&quot;&quot;&quot;
SEMANAS 25-26: Neural Network para MNIST

Arquitectura:
- Input: 784 (28x28 pÃ­xeles aplanados)
- Hidden 1: 128 neuronas, ReLU
- Hidden 2: 64 neuronas, ReLU
- Output: 10 neuronas, Softmax

Objetivo: Superar a Logistic Regression
&quot;&quot;&quot;

import numpy as np
from typing import List, Tuple

# Funciones de activaciÃ³n
def relu(z): 
    return np.maximum(0, z)

def relu_deriv(z): 
    return (z &gt; 0).astype(float)

def softmax(z):
    exp_z = np.exp(z - np.max(z))
    return exp_z / np.sum(exp_z)


class NeuralNetworkMNIST:
    &quot;&quot;&quot;Red Neuronal optimizada para MNIST.&quot;&quot;&quot;

    def __init__(self, layer_sizes: List[int] = [784, 128, 64, 10], seed: int = 42):
        &quot;&quot;&quot;
        Args:
            layer_sizes: [input, hidden1, hidden2, ..., output]
        &quot;&quot;&quot;
        np.random.seed(seed)

        self.layer_sizes = layer_sizes
        self.n_layers = len(layer_sizes)

        # Inicializar pesos (He initialization para ReLU)
        self.weights = []
        self.biases = []

        for i in range(self.n_layers - 1):
            w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(2.0 / layer_sizes[i])
            b = np.zeros(layer_sizes[i+1])
            self.weights.append(w)
            self.biases.append(b)

        self.cache = {}
        self.loss_history = []

    def forward(self, x: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;Forward pass.&quot;&quot;&quot;
        self.cache['a0'] = x
        a = x

        for i in range(self.n_layers - 2):
            z = self.weights[i] @ a + self.biases[i]
            a = relu(z)
            self.cache[f'z{i+1}'] = z
            self.cache[f'a{i+1}'] = a

        # Ãšltima capa: softmax
        z = self.weights[-1] @ a + self.biases[-1]
        a = softmax(z)
        self.cache[f'z{self.n_layers-1}'] = z
        self.cache[f'a{self.n_layers-1}'] = a

        return a

    def backward(self, y_true: np.ndarray) -&gt; Tuple[List, List]:
        &quot;&quot;&quot;Backward pass.&quot;&quot;&quot;
        y_pred = self.cache[f'a{self.n_layers-1}']

        # Gradiente de softmax + cross-entropy
        dz = y_pred - y_true

        dW_list = []
        db_list = []

        for i in range(self.n_layers - 2, -1, -1):
            a_prev = self.cache[f'a{i}']

            dW = np.outer(dz, a_prev)
            db = dz

            dW_list.insert(0, dW)
            db_list.insert(0, db)

            if i &gt; 0:
                da_prev = self.weights[i].T @ dz
                z_prev = self.cache[f'z{i}']
                dz = da_prev * relu_deriv(z_prev)

        return dW_list, db_list

    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        epochs: int = 10,
        batch_size: int = 32,
        learning_rate: float = 0.01,
        verbose: bool = True
    ):
        &quot;&quot;&quot;Entrena la red con mini-batch SGD.&quot;&quot;&quot;
        n_samples = len(X)

        for epoch in range(epochs):
            # Shuffle
            indices = np.random.permutation(n_samples)
            X_shuffled = X[indices]
            y_shuffled = y[indices]

            total_loss = 0

            for i in range(0, n_samples, batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                y_batch = y_shuffled[i:i+batch_size]

                # Acumular gradientes del batch
                dW_accum = [np.zeros_like(w) for w in self.weights]
                db_accum = [np.zeros_like(b) for b in self.biases]

                for x, y_true_label in zip(X_batch, y_batch):
                    # One-hot encode
                    y_one_hot = np.zeros(10)
                    y_one_hot[y_true_label] = 1

                    # Forward
                    y_pred = self.forward(x)

                    # Loss
                    loss = -np.sum(y_one_hot * np.log(np.clip(y_pred, 1e-15, 1)))
                    total_loss += loss

                    # Backward
                    dW_list, db_list = self.backward(y_one_hot)

                    for j in range(len(self.weights)):
                        dW_accum[j] += dW_list[j]
                        db_accum[j] += db_list[j]

                # Update
                batch_len = len(X_batch)
                for j in range(len(self.weights)):
                    self.weights[j] -= learning_rate * dW_accum[j] / batch_len
                    self.biases[j] -= learning_rate * db_accum[j] / batch_len

            avg_loss = total_loss / n_samples
            self.loss_history.append(avg_loss)

            if verbose:
                train_acc = self.score(X[:1000], y[:1000])
                print(f&quot;Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Acc: {train_acc:.2%}&quot;)

    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;Predice clases.&quot;&quot;&quot;
        return np.array([np.argmax(self.forward(x)) for x in X])

    def score(self, X: np.ndarray, y: np.ndarray) -&gt; float:
        &quot;&quot;&quot;Accuracy.&quot;&quot;&quot;
        return np.mean(self.predict(X) == y)


def train_neural_network_mnist(X_train, y_train, X_test, y_test):
    &quot;&quot;&quot;Entrena y evalÃºa Neural Network en MNIST.&quot;&quot;&quot;

    print(&quot;=== Neural Network (MLP) ===&quot;)
    print(&quot;Arquitectura: 784 â†’ 128 â†’ 64 â†’ 10&quot;)

    nn = NeuralNetworkMNIST([784, 128, 64, 10])
    nn.fit(X_train, y_train, epochs=10, batch_size=32, learning_rate=0.01)

    train_acc = nn.score(X_train, y_train)
    test_acc = nn.score(X_test, y_test)

    print(f&quot;\nTrain Accuracy: {train_acc:.2%}&quot;)
    print(f&quot;Test Accuracy:  {test_acc:.2%}&quot;)

    return nn
</code></pre>
<h2>ğŸ’» Parte 5: Benchmark y ComparaciÃ³n</h2>
<h3>5.1 Pipeline Completo</h3>
<pre><code class="language-python">&quot;&quot;&quot;
Pipeline completo que ejecuta todos los anÃ¡lisis
y compara los modelos.
&quot;&quot;&quot;

import numpy as np
import time

def run_mnist_pipeline(X_train, y_train, X_test, y_test, use_subset: bool = True):
    &quot;&quot;&quot;
    Ejecuta el pipeline completo de MNIST.

    Args:
        use_subset: Si True, usa solo 10k samples para rapidez
    &quot;&quot;&quot;
    if use_subset:
        X_train = X_train[:10000]
        y_train = y_train[:10000]
        X_test = X_test[:2000]
        y_test = y_test[:2000]

    # Normalizar
    X_train = X_train / 255.0
    X_test = X_test / 255.0

    print(&quot;=&quot; * 60)
    print(&quot;MNIST ANALYST PIPELINE&quot;)
    print(&quot;=&quot; * 60)
    print(f&quot;Train samples: {len(X_train)}&quot;)
    print(f&quot;Test samples: {len(X_test)}&quot;)
    print(f&quot;Features: {X_train.shape[1]}&quot;)
    print(&quot;=&quot; * 60)

    results = {}

    # === FASE 1: Unsupervised ===
    print(&quot;\n&quot; + &quot;=&quot; * 60)
    print(&quot;FASE 1: EXPLORACIÃ“N NO SUPERVISADA&quot;)
    print(&quot;=&quot; * 60)

    # PCA
    print(&quot;\n[PCA]&quot;)
    pca = PCA(n_components=50)
    pca.fit(X_train)
    print(f&quot;Varianza explicada (50 PCs): {sum(pca.explained_variance_ratio_):.2%}&quot;)

    # K-Means
    print(&quot;\n[K-Means]&quot;)
    start = time.time()
    kmeans = KMeans(n_clusters=10, seed=42)
    kmeans.fit(X_train)
    kmeans_time = time.time() - start
    print(f&quot;Inercia: {kmeans.inertia_:.2f}&quot;)
    print(f&quot;Tiempo: {kmeans_time:.2f}s&quot;)

    # === FASE 2: Supervised ===
    print(&quot;\n&quot; + &quot;=&quot; * 60)
    print(&quot;FASE 2: CLASIFICACIÃ“N SUPERVISADA&quot;)
    print(&quot;=&quot; * 60)

    # Logistic Regression
    print(&quot;\n[Logistic Regression One-vs-All]&quot;)
    start = time.time()
    lr_model = LogisticRegressionOvA(n_classes=10, lr=0.1, n_iter=100)
    lr_model.fit(X_train, y_train)
    lr_time = time.time() - start
    lr_acc = lr_model.score(X_test, y_test)
    print(f&quot;Test Accuracy: {lr_acc:.2%}&quot;)
    print(f&quot;Tiempo: {lr_time:.2f}s&quot;)
    results['Logistic Regression'] = lr_acc

    # === FASE 3: Deep Learning ===
    print(&quot;\n&quot; + &quot;=&quot; * 60)
    print(&quot;FASE 3: DEEP LEARNING&quot;)
    print(&quot;=&quot; * 60)

    # Neural Network
    print(&quot;\n[Neural Network MLP]&quot;)
    start = time.time()
    nn = NeuralNetworkMNIST([784, 128, 64, 10])
    nn.fit(X_train, y_train, epochs=5, batch_size=32, learning_rate=0.01, verbose=False)
    nn_time = time.time() - start
    nn_acc = nn.score(X_test, y_test)
    print(f&quot;Test Accuracy: {nn_acc:.2%}&quot;)
    print(f&quot;Tiempo: {nn_time:.2f}s&quot;)
    results['Neural Network'] = nn_acc

    # === COMPARACIÃ“N ===
    print(&quot;\n&quot; + &quot;=&quot; * 60)
    print(&quot;COMPARACIÃ“N DE MODELOS&quot;)
    print(&quot;=&quot; * 60)

    print(&quot;\nModelo               | Accuracy | Mejora vs LR&quot;)
    print(&quot;-&quot; * 50)
    baseline = results['Logistic Regression']
    for name, acc in results.items():
        improvement = ((acc - baseline) / baseline) * 100 if name != 'Logistic Regression' else 0
        print(f&quot;{name:&lt;20} | {acc:.2%}    | {improvement:+.1f}%&quot;)

    # === ANÃLISIS ===
    print(&quot;\n&quot; + &quot;=&quot; * 60)
    print(&quot;ANÃLISIS: Â¿Por quÃ© NN es mejor?&quot;)
    print(&quot;=&quot; * 60)
    print(&quot;&quot;&quot;
1. NO-LINEALIDAD: ReLU permite aprender fronteras no lineales.
   Logistic Regression solo puede aprender fronteras lineales.

2. REPRESENTACIÃ“N JERÃRQUICA: Las capas ocultas aprenden features
   de complejidad creciente (bordes â†’ formas â†’ dÃ­gitos).

3. CAPACIDAD: MÃ¡s parÃ¡metros = puede memorizar patrones mÃ¡s complejos.
   Pero cuidado con overfitting si hay pocos datos.

4. COMPOSICIÃ“N: La red compone funciones simples (lineales + activaciones)
   para aproximar funciones complejas.
&quot;&quot;&quot;)

    return results
</code></pre>
<h2>ğŸ“¦ Entregable Final</h2>
<h3><code>MODEL_COMPARISON.md</code></h3>
<pre><code class="language-markdown"># Model Comparison Report - MNIST Analyst

## Executive Summary

This project demonstrates competency in all three courses of the Machine Learning
Pathway (Line 1) through a complete analysis of the MNIST dataset.

## Results

| Model | Test Accuracy | Training Time |
|-------|---------------|---------------|
| K-Means (Unsupervised) | N/A (clustering) | ~5s |
| Logistic Regression | ~85-90% | ~30s |
| Neural Network (MLP) | ~95%+ | ~60s |

## Analysis

### Why does the Neural Network outperform Logistic Regression?

1. **Non-linearity**: ReLU activations allow learning non-linear decision boundaries
2. **Hierarchical features**: Hidden layers learn increasingly abstract representations
3. **Capacity**: More parameters enable capturing complex patterns

### Mathematical Explanation

Logistic Regression:
</code></pre>
<p>Å· = Ïƒ(Wx + b)</p>
<pre><code>- Single linear transformation + sigmoid
- Can only learn linear decision boundaries

Neural Network:
</code></pre>
<p>Å· = softmax(Wâ‚ƒ Â· ReLU(Wâ‚‚ Â· ReLU(Wâ‚x + bâ‚) + bâ‚‚) + bâ‚ƒ)</p>
<pre><code>- Multiple non-linear transformations
- Universal function approximator

### PCA Insights

- 50 components retain ~85% of variance
- First 2 components show partial class separation
- Principal components capture stroke patterns

### K-Means Insights

- Cluster centroids resemble average digit shapes
- Some digits (1, 7) cluster well; others (4, 9) overlap
- Unsupervised clustering achieves ~60% purity

## Conclusion

The Neural Network achieves the highest accuracy by learning hierarchical,
non-linear representations of the input images. This project demonstrates
practical implementation of Supervised Learning, Unsupervised Learning,
and Deep Learning algorithms from scratch.
</code></pre>
<h2>ğŸ“Š AnÃ¡lisis Bias-Variance (v3.2)</h2>
<blockquote>
<p>ğŸ“ <strong>Concepto Central de la MaestrÃ­a:</strong> Entender el tradeoff Bias-Variance es fundamental para diseÃ±ar modelos de ML.</p>
</blockquote>
<h3>El Tradeoff Bias-Variance</h3>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚   ERROR TOTAL = BIASÂ² + VARIANCE + RUIDO IRREDUCIBLE            â”‚
â”‚                                                                 â”‚
â”‚   BIAS (Sesgo): Error por suposiciones simplificadoras          â”‚
â”‚   - Modelo muy simple â†’ NO captura patrones â†’ UNDERFITTING      â”‚
â”‚                                                                 â”‚
â”‚   VARIANCE (Varianza): Error por sensibilidad a datos           â”‚
â”‚   - Modelo muy complejo â†’ Memoriza ruido â†’ OVERFITTING          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h3>AnÃ¡lisis de los Modelos del Proyecto</h3>
<table>
<thead>
<tr>
<th>Modelo</th>
<th>Bias</th>
<th>Variance</th>
<th>Comportamiento Esperado</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Logistic Regression</strong></td>
<td>Alto</td>
<td>Baja</td>
<td>Underfitting (solo lÃ­mites lineales)</td>
</tr>
<tr>
<td><strong>MLP pequeÃ±o (128-64)</strong></td>
<td>Medio</td>
<td>Media</td>
<td>Balance Ã³ptimo</td>
</tr>
<tr>
<td><strong>MLP grande (512-256-128)</strong></td>
<td>Bajo</td>
<td>Alta</td>
<td>Riesgo de overfitting</td>
</tr>
</tbody>
</table>
<h3>Tu Entregable: SecciÃ³n en MODEL_COMPARISON.md</h3>
<p>AÃ±ade una secciÃ³n que responda:</p>
<ol>
<li><strong>Â¿Por quÃ© Logistic Regression tiene alto bias?</strong></li>
<li>Solo puede aprender fronteras de decisiÃ³n lineales</li>
<li>
<p>MNIST tiene patrones no lineales (curvas, esquinas)</p>
</li>
<li>
<p><strong>Â¿Por quÃ© MLP puede tener alta variance?</strong></p>
</li>
<li>Muchos parÃ¡metros pueden memorizar ejemplos de entrenamiento</li>
<li>
<p>SoluciÃ³n: RegularizaciÃ³n L2, Dropout, Early Stopping</p>
</li>
<li>
<p><strong>Experimento prÃ¡ctico:</strong></p>
</li>
<li>Entrenar MLP con diferentes tamaÃ±os</li>
<li>Graficar train_accuracy vs test_accuracy</li>
<li>Identificar punto de overfitting</li>
</ol>
<pre><code class="language-python"># CÃ³digo para anÃ¡lisis Bias-Variance
def train_and_evaluate(hidden_sizes: list, X_train, y_train, X_test, y_test):
    &quot;&quot;&quot;Entrenar modelos de diferentes tamaÃ±os y comparar.&quot;&quot;&quot;
    results = []

    for sizes in hidden_sizes:
        model = NeuralNetwork(layers=[784] + list(sizes) + [10])
        model.fit(X_train, y_train, epochs=100)

        train_acc = model.score(X_train, y_train)
        test_acc = model.score(X_test, y_test)
        gap = train_acc - test_acc  # Gap grande = overfitting

        results.append({
            'hidden_sizes': sizes,
            'train_acc': train_acc,
            'test_acc': test_acc,
            'gap': gap
        })

    return results

# Experimento
sizes_to_test = [
    (32,),           # Muy pequeÃ±o (alto bias)
    (128, 64),       # Medio (balanceado)
    (512, 256, 128)  # Grande (alta variance)
]
</code></pre>
<h2>ğŸ“ Formato de Informe: Paper CientÃ­fico (v3.2)</h2>
<blockquote>
<p>ğŸ’ <strong>Profesionalismo:</strong> El notebook final debe tener el formato de un paper acadÃ©mico.</p>
</blockquote>
<h3>Estructura del Jupyter Notebook Final</h3>
<pre><code class="language-markdown"># MNIST Digit Classification: A From-Scratch Implementation

## Abstract
[3-4 oraciones resumiendo objetivo, mÃ©todos y resultados principales]

## 1. Introduction
- Problema: clasificaciÃ³n de dÃ­gitos escritos a mano
- MotivaciÃ³n: demostrar competencia en ML
- ContribuciÃ³n: implementaciÃ³n 100% desde cero

## 2. Dataset
- DescripciÃ³n de MNIST (60K train, 10K test, 28x28 pixels)
- Preprocesamiento aplicado

## 3. Methodology
### 3.1 Unsupervised Learning
- PCA: reducciÃ³n dimensional
- K-Means: clustering

### 3.2 Supervised Learning
- Logistic Regression One-vs-All

### 3.3 Deep Learning
- MLP architecture: 784â†’128â†’64â†’10
- Training: SGD with momentum

## 4. Results
### 4.1 PCA Analysis
[GrÃ¡ficos de varianza explicada, visualizaciÃ³n 2D]

### 4.2 K-Means Clustering
[Centroides, pureza de clusters]

### 4.3 Model Comparison
[Tabla comparativa, matriz de confusiÃ³n]

### 4.4 Bias-Variance Analysis
[Gap train-test para diferentes modelos]

## 5. Discussion
- Â¿Por quÃ© MLP supera a Logistic Regression?
- Limitaciones del estudio
- Trabajo futuro

## 6. Conclusion
[2-3 oraciones de cierre]

## References
- LeCun, Y., et al. &quot;Gradient-based learning applied to document recognition.&quot;
- Deep Learning Book (Goodfellow et al.)
</code></pre>
<h2>ğŸ” AnÃ¡lisis de Errores: Nivel Senior (v3.3)</h2>
<blockquote>
<p>ğŸ’ <strong>Profesionalismo:</strong> No solo muestres el accuracy. Muestra las imÃ¡genes que la red fallÃ³ y explica por quÃ©.</p>
</blockquote>
<h3>Por QuÃ© Es Importante</h3>
<pre><code>ANÃLISIS DE ERRORES = LO QUE SEPARA JUNIOR DE SENIOR

Junior: &quot;Mi modelo tiene 92% accuracy&quot;

Senior: &quot;Mi modelo tiene 92% accuracy. Los errores se concentran en:
        - ConfusiÃ³n 4â†”9 (formas similares)
        - ConfusiÃ³n 3â†”8 (curvas similares)
        - DÃ­gitos mal escritos o cortados
        Esto sugiere que el modelo necesita mÃ¡s ejemplos
        de estos casos difÃ­ciles o data augmentation.&quot;
</code></pre>
<h3>Script: AnÃ¡lisis de Errores</h3>
<pre><code class="language-python">&quot;&quot;&quot;
Error Analysis - VisualizaciÃ³n de Fallos del Modelo
Nivel Senior: No solo accuracy, tambiÃ©n entender los errores.
&quot;&quot;&quot;
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List


def analyze_errors(
    model,
    X_test: np.ndarray,
    y_test: np.ndarray,
    n_errors: int = 20
) -&gt; dict:
    &quot;&quot;&quot;
    Analiza y visualiza los errores del modelo.

    Args:
        model: Modelo entrenado (con .predict())
        X_test: Datos de test
        y_test: Labels de test
        n_errors: NÃºmero de errores a visualizar

    Returns:
        Diccionario con anÃ¡lisis completo
    &quot;&quot;&quot;
    # Predicciones
    y_pred = model.predict(X_test)

    # Identificar errores
    errors_mask = y_pred != y_test
    error_indices = np.where(errors_mask)[0]

    print(&quot;=&quot; * 60)
    print(&quot;ANÃLISIS DE ERRORES&quot;)
    print(&quot;=&quot; * 60)
    print(f&quot;Total errores: {len(error_indices)} / {len(y_test)}&quot;)
    print(f&quot;Error rate: {100 * len(error_indices) / len(y_test):.2f}%&quot;)

    # Matriz de confusiÃ³n de errores
    confusion_pairs = {}
    for idx in error_indices:
        pair = (y_test[idx], y_pred[idx])
        confusion_pairs[pair] = confusion_pairs.get(pair, 0) + 1

    # Top confusiones
    sorted_pairs = sorted(confusion_pairs.items(), key=lambda x: -x[1])

    print(&quot;\nğŸ“Š TOP CONFUSIONES:&quot;)
    for (true, pred), count in sorted_pairs[:10]:
        print(f&quot;  {true} â†’ {pred}: {count} errores&quot;)

    # Visualizar errores
    fig, axes = plt.subplots(4, 5, figsize=(12, 10))
    fig.suptitle(&quot;Ejemplos de Errores del Modelo&quot;, fontsize=14)

    for i, ax in enumerate(axes.flat):
        if i &lt; min(n_errors, len(error_indices)):
            idx = error_indices[i]
            img = X_test[idx].reshape(28, 28)
            ax.imshow(img, cmap='gray')
            ax.set_title(f&quot;True: {y_test[idx]}, Pred: {y_pred[idx]}&quot;, 
                        color='red', fontsize=10)
            ax.axis('off')
        else:
            ax.axis('off')

    plt.tight_layout()
    plt.savefig('error_analysis.png', dpi=150)
    plt.show()

    return {
        'n_errors': len(error_indices),
        'error_rate': len(error_indices) / len(y_test),
        'confusion_pairs': sorted_pairs,
        'error_indices': error_indices
    }


def plot_learning_curves(
    train_losses: List[float],
    val_losses: List[float],
    train_accs: List[float],
    val_accs: List[float]
) -&gt; None:
    &quot;&quot;&quot;
    Visualiza curvas de aprendizaje para diagnÃ³stico Bias-Variance.

    - Train alto, Val alto â†’ Underfitting (High Bias)
    - Train bajo, Val alto â†’ Overfitting (High Variance)
    - Train bajo, Val bajo â†’ Buen modelo
    &quot;&quot;&quot;
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    epochs = range(1, len(train_losses) + 1)

    # Loss curves
    ax1.plot(epochs, train_losses, 'b-', label='Train Loss')
    ax1.plot(epochs, val_losses, 'r-', label='Validation Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Learning Curves: Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Accuracy curves
    ax2.plot(epochs, train_accs, 'b-', label='Train Accuracy')
    ax2.plot(epochs, val_accs, 'r-', label='Validation Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.set_title('Learning Curves: Accuracy')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # DiagnÃ³stico
    final_gap = train_accs[-1] - val_accs[-1]

    if val_accs[-1] &lt; 0.7:
        diagnosis = &quot;âš ï¸ UNDERFITTING: Modelo muy simple o poco entrenamiento&quot;
    elif final_gap &gt; 0.1:
        diagnosis = &quot;âš ï¸ OVERFITTING: Gap train-val &gt; 10%&quot;
    else:
        diagnosis = &quot;âœ“ BUEN AJUSTE: Modelo generaliza bien&quot;

    fig.suptitle(f&quot;DiagnÃ³stico: {diagnosis}&quot;, fontsize=12, y=1.02)

    plt.tight_layout()
    plt.savefig('learning_curves.png', dpi=150)
    plt.show()

    print(&quot;\nğŸ“ˆ DIAGNÃ“STICO BIAS-VARIANCE:&quot;)
    print(f&quot;  Train Accuracy Final: {train_accs[-1]:.4f}&quot;)
    print(f&quot;  Val Accuracy Final:   {val_accs[-1]:.4f}&quot;)
    print(f&quot;  Gap:                  {final_gap:.4f}&quot;)
    print(f&quot;  â†’ {diagnosis}&quot;)
</code></pre>
<h3>SecciÃ³n Obligatoria en MODEL_COMPARISON.md</h3>
<pre><code class="language-markdown">## Error Analysis

### Top Confusiones del Modelo

| True â†’ Pred | Count | ExplicaciÃ³n |
|-------------|-------|-------------|
| 4 â†’ 9 | 23 | Formas similares (bucle arriba) |
| 9 â†’ 4 | 18 | Formas similares |
| 3 â†’ 8 | 15 | Curvas similares |
| 7 â†’ 1 | 12 | Trazo vertical dominante |

### VisualizaciÃ³n de Errores

![Errores del modelo](error_analysis.png)

### InterpretaciÃ³n

Los errores se concentran principalmente en dÃ­gitos con formas similares.
Esto sugiere que:
1. El modelo captura bien las features principales
2. Features mÃ¡s finas (bucles, cruces) necesitan mÃ¡s ejemplos
3. Data augmentation podrÃ­a ayudar
</code></pre>
<h2>âœ… Checklist de FinalizaciÃ³n (v3.3)</h2>
<h3>Semana 21: EDA + No Supervisado</h3>
<ul>
<li>[ ] PCA reduce MNIST a 2D/50D con visualizaciÃ³n</li>
<li>[ ] AnalicÃ© varianza explicada por componente</li>
<li>[ ] K-Means agrupa dÃ­gitos sin etiquetas</li>
<li>[ ] VisualicÃ© centroides como imÃ¡genes 28x28</li>
</ul>
<h3>Semana 22: ClasificaciÃ³n Supervisada</h3>
<ul>
<li>[ ] Logistic Regression One-vs-All funcional</li>
<li>[ ] Accuracy &gt;85% en test set</li>
<li>[ ] CalculÃ© Precision, Recall, F1 por clase</li>
<li>[ ] AnalicÃ© matriz de confusiÃ³n</li>
</ul>
<h3>Semana 23: Deep Learning</h3>
<ul>
<li>[ ] MLP con arquitectura 784â†’128â†’64â†’10</li>
<li>[ ] Forward y backward pass implementados</li>
<li>[ ] Mini-batch SGD funcionando</li>
<li>[ ] Accuracy &gt;90% en test set</li>
</ul>
<h3>Semana 24: Benchmark + Informe</h3>
<ul>
<li>[ ] MODEL_COMPARISON.md completo</li>
<li>[ ] README.md profesional en inglÃ©s</li>
</ul>
<h3>Requisitos v3.3</h3>
<ul>
<li>[ ] <strong>AnÃ¡lisis Bias-Variance</strong> con experimento prÃ¡ctico</li>
<li>[ ] <strong>Notebook en formato Paper</strong> (Abstract, Methods, Results, Discussion)</li>
<li>[ ] <strong>AnÃ¡lisis de Errores</strong> con visualizaciÃ³n de fallos</li>
<li>[ ] <strong>Curvas de Aprendizaje</strong> con diagnÃ³stico Bias-Variance</li>
<li>[ ] SecciÃ³n "Error Analysis" en MODEL_COMPARISON.md</li>
<li>[ ] <code>mypy src/</code> pasa sin errores</li>
<li>[ ] <code>pytest tests/</code> pasa sin errores</li>
</ul>
<h3>MetodologÃ­a Feynman</h3>
<ul>
<li>[ ] Puedo explicar por quÃ© MLP supera a Logistic en 5 lÃ­neas</li>
<li>[ ] Puedo explicar Bias vs Variance en 5 lÃ­neas</li>
<li>[ ] Puedo explicar por quÃ© 4â†”9 se confunden frecuentemente</li>
</ul>
<h2>ğŸ”— NavegaciÃ³n</h2>
<table>
<thead>
<tr>
<th>Anterior</th>
<th>Ãndice</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="#mod_07_DEEP_LEARNING">07_DEEP_LEARNING</a></td>
<td><a href="#mod_00_INDICE">00_INDICE</a></td>
</tr>
</tbody>
</table>
            </div>
        
            <!-- MÃ“DULO: CHECKLIST.md -->
            <div class="cover-page">
                <div class="cover-content">
                    <div style="font-size:40pt;margin-bottom:20px;">ğŸ’</div>
                    <div id="mod_CHECKLIST" class="cover-title">CHECKLIST FINAL</div>
                    <div class="cover-subtitle">MS in AI Pathway - ML Specialist v3.0</div>
                    <div class="cover-badge">MATEMÃTICAS APLICADAS A CÃ“DIGO</div>
                </div>
                <div style="position:absolute;bottom:30px;font-size:10pt;opacity:0.6;">DUQUEOM | 2025</div>
            </div>
            <div class="content">
                <h1>â˜‘ï¸ Checklist Final - ML Specialist v3.3</h1>
<blockquote>
<p>VerificaciÃ³n completa del programa de <strong>24 semanas</strong> con validaciÃ³n matemÃ¡tica rigurosa.</p>
</blockquote>
<h2>ğŸ“š Fase 1: Fundamentos (Semanas 1-8)</h2>
<h3>MÃ³dulo 01: Python + Pandas + NumPy (Semanas 1-2)</h3>
<h4>Conocimiento</h4>
<ul>
<li>[ ] Pandas: cargar CSV con <code>read_csv()</code></li>
<li>[ ] Pandas: limpiar datos con <code>dropna()</code>, <code>fillna()</code></li>
<li>[ ] Pandas: selecciÃ³n con <code>.loc[]</code>, <code>.iloc[]</code></li>
<li>[ ] Pandas â†’ NumPy: <code>.to_numpy()</code></li>
<li>[ ] NumPy: creaciÃ³n de arrays (1D, 2D, 3D)</li>
<li>[ ] NumPy: indexing, slicing, broadcasting</li>
<li>[ ] NumPy: agregaciones por eje (axis=0, axis=1)</li>
<li>[ ] <strong>Conozco los 5 errores comunes de NumPy y sus soluciones</strong></li>
</ul>
<h4>EstÃ¡ndares Profesionales (v3.2)</h4>
<ul>
<li>[ ] <code>mypy src/</code> pasa sin errores</li>
<li>[ ] <code>ruff check src/</code> pasa sin errores</li>
<li>[ ] Al menos 3 tests con <code>pytest</code> pasando</li>
</ul>
<h4>MetodologÃ­a Feynman</h4>
<ul>
<li>[ ] Puedo explicar broadcasting en 5 lÃ­neas sin jerga</li>
</ul>
<h3>MÃ³dulo 02: Ãlgebra Lineal para ML (Semanas 3-5)</h3>
<ul>
<li>[ ] Producto punto y significado geomÃ©trico</li>
<li>[ ] Normas L1, L2, Lâˆ implementadas</li>
<li>[ ] Distancia euclidiana y similitud coseno</li>
<li>[ ] MultiplicaciÃ³n de matrices con <code>@</code></li>
<li>[ ] Eigenvalues/eigenvectors con <code>np.linalg.eig()</code></li>
<li>[ ] SVD con <code>np.linalg.svd()</code></li>
<li>[ ] <code>linear_algebra.py</code> con tests pasando</li>
</ul>
<h3>MÃ³dulo 03: CÃ¡lculo Multivariante (Semanas 6-7)</h3>
<ul>
<li>[ ] Derivadas parciales calculadas</li>
<li>[ ] Gradiente de funciones multivariables</li>
<li>[ ] Gradient Descent implementado desde cero</li>
<li>[ ] Efecto del learning rate entendido</li>
<li>[ ] Chain Rule aplicada a funciones compuestas</li>
<li>[ ] <code>calculus.py</code> con Gradient Descent funcional</li>
</ul>
<h4>Gradient Checking (v3.3 - Obligatorio)</h4>
<ul>
<li>[ ] <strong><code>grad_check.py</code> implementado</strong></li>
<li>[ ] <strong>ValidÃ© derivadas de MSE, sigmoid y capa lineal</strong></li>
<li>[ ] Error relativo &lt; 10â»â· en todos los tests</li>
</ul>
<h3>MÃ³dulo 04: Probabilidad para ML (Semana 8)</h3>
<ul>
<li>[ ] Teorema de Bayes explicado con ejemplo</li>
<li>[ ] Gaussiana univariada: PDF implementada</li>
<li>[ ] Gaussiana multivariada: concepto entendido</li>
<li>[ ] MLE: conexiÃ³n con Cross-Entropy explicada</li>
<li>[ ] <strong>Softmax con Log-Sum-Exp trick implementado (v3.3)</strong></li>
<li>[ ] <code>probability.py</code> con tests pasando</li>
</ul>
<h2>ğŸ¤– Fase 2: NÃºcleo de ML (Semanas 9-20) â­ PATHWAY</h2>
<h3>MÃ³dulo 05: Supervised Learning (Semanas 9-12)</h3>
<h4>Conocimiento</h4>
<ul>
<li>[ ] RegresiÃ³n lineal (Normal Equation + GD)</li>
<li>[ ] MSE y su gradiente derivado</li>
<li>[ ] RegresiÃ³n logÃ­stica desde cero</li>
<li>[ ] Sigmoid y binary cross-entropy</li>
<li>[ ] Matriz de confusiÃ³n (TP, TN, FP, FN)</li>
<li>[ ] Accuracy, Precision, Recall, F1 implementados</li>
<li>[ ] Train/test split manual</li>
<li>[ ] K-fold cross validation</li>
<li>[ ] RegularizaciÃ³n L2 (Ridge)</li>
</ul>
<h4>DerivaciÃ³n AnalÃ­tica (v3.2 - Obligatorio)</h4>
<ul>
<li>[ ] <strong>DerivÃ© el gradiente de Cross-Entropy a mano</strong></li>
<li>[ ] <strong>Documento con derivaciÃ³n completa (Markdown o LaTeX)</strong></li>
</ul>
<h4>MetodologÃ­a Feynman</h4>
<ul>
<li>[ ] Puedo explicar sigmoid vs softmax en 5 lÃ­neas</li>
</ul>
<h3>MÃ³dulo 06: Unsupervised Learning (Semanas 13-16)</h3>
<ul>
<li>[ ] K-Means con K-Means++ initialization</li>
<li>[ ] Algoritmo de Lloyd (asignar-actualizar-repetir)</li>
<li>[ ] Inercia y mÃ©todo del codo</li>
<li>[ ] PCA usando SVD (<code>np.linalg.svd()</code>)</li>
<li>[ ] Varianza explicada y elecciÃ³n de n_components</li>
<li>[ ] ReconstrucciÃ³n desde componentes principales</li>
<li>[ ] <code>kmeans.py</code> y <code>pca.py</code> con tests pasando</li>
</ul>
<h3>MÃ³dulo 07: Deep Learning + CNNs (Semanas 17-20)</h3>
<h4>Conocimiento</h4>
<ul>
<li>[ ] Neurona artificial y perceptrÃ³n</li>
<li>[ ] Sigmoid, ReLU, tanh, softmax + derivadas</li>
<li>[ ] Problema XOR y su no-linealidad</li>
<li>[ ] Forward pass para MLP</li>
<li>[ ] Backpropagation con Chain Rule</li>
<li>[ ] SGD, Momentum, Adam implementados</li>
<li>[ ] Red resuelve problema XOR</li>
<li>[ ] <strong>CNNs (teorÃ­a):</strong> convoluciÃ³n, stride, padding, pooling</li>
</ul>
<h4>DerivaciÃ³n AnalÃ­tica (v3.2 - Obligatorio)</h4>
<ul>
<li>[ ] <strong>DerivÃ© las ecuaciones de backprop para red de 2 capas</strong></li>
<li>[ ] <strong>Diagrama de grafo computacional</strong></li>
</ul>
<h4>MetodologÃ­a Feynman</h4>
<ul>
<li>[ ] Puedo explicar backpropagation en 5 lÃ­neas sin jerga</li>
</ul>
<h2>ğŸ¯ Fase 3: Proyecto MNIST Analyst (Semanas 21-24)</h2>
<h3>Semana 21: EDA + No Supervisado</h3>
<ul>
<li>[ ] MNIST cargado y normalizado</li>
<li>[ ] PCA reduce a 2D con visualizaciÃ³n</li>
<li>[ ] Varianza explicada analizada</li>
<li>[ ] K-Means agrupa dÃ­gitos sin etiquetas</li>
<li>[ ] Centroides visualizados como imÃ¡genes 28x28</li>
</ul>
<h3>Semana 22: ClasificaciÃ³n Supervisada</h3>
<ul>
<li>[ ] Logistic Regression One-vs-All implementado</li>
<li>[ ] Accuracy &gt; 85% en test set</li>
<li>[ ] Precision, Recall, F1 por clase</li>
<li>[ ] Matriz de confusiÃ³n analizada</li>
<li>[ ] Errores visualizados (imÃ¡genes mal clasificadas)</li>
</ul>
<h3>Semana 23: Deep Learning</h3>
<ul>
<li>[ ] MLP 784â†’128â†’64â†’10 implementado</li>
<li>[ ] Forward y backward pass funcionales</li>
<li>[ ] Mini-batch SGD funcionando</li>
<li>[ ] Accuracy &gt; 90% en test set</li>
</ul>
<h3>Semana 24: Benchmark + Informe</h3>
<ul>
<li>[ ] ComparaciÃ³n MLP vs Logistic Regression</li>
<li>[ ] <code>MODEL_COMPARISON.md</code> explicando diferencias</li>
<li>[ ] <code>README.md</code> profesional en inglÃ©s</li>
<li>[ ] Demo notebook completo</li>
</ul>
<h3>Requisitos v3.2 (Obligatorios)</h3>
<ul>
<li>[ ] <strong>AnÃ¡lisis Bias-Variance</strong> con experimento prÃ¡ctico (3 tamaÃ±os de MLP)</li>
<li>[ ] <strong>Notebook en formato Paper</strong> (Abstract, Methods, Results, Discussion)</li>
<li>[ ] <code>mypy src/</code> pasa sin errores en todo el proyecto</li>
<li>[ ] <code>pytest tests/</code> con cobertura significativa</li>
</ul>
<h3>MetodologÃ­a Feynman</h3>
<ul>
<li>[ ] Puedo explicar Bias vs Variance en 5 lÃ­neas</li>
<li>[ ] Puedo explicar por quÃ© MLP supera a Logistic en 5 lÃ­neas</li>
</ul>
<h2>ğŸ’» CÃ³digo</h2>
<h3>Estructura del Proyecto MNIST</h3>
<pre><code>mnist-analyst/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ linear_algebra.py
â”‚   â”œâ”€â”€ probability.py
â”‚   â”œâ”€â”€ pca.py
â”‚   â”œâ”€â”€ kmeans.py
â”‚   â”œâ”€â”€ logistic_regression.py
â”‚   â”œâ”€â”€ neural_network.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ pipeline.py
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda_pca_kmeans.ipynb
â”‚   â”œâ”€â”€ 02_logistic_classification.ipynb
â”‚   â””â”€â”€ 03_neural_network_benchmark.ipynb
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_*.py
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ MODEL_COMPARISON.md
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
</code></pre>
<h3>Calidad de CÃ³digo</h3>
<ul>
<li>[ ] Type hints en todas las funciones</li>
<li>[ ] Docstrings con Args, Returns</li>
<li>[ ] <code>mypy src/</code> pasa sin errores</li>
<li>[ ] CÃ³digo vectorizado (sin loops innecesarios)</li>
</ul>
<h3>Tests</h3>
<ul>
<li>[ ] Tests unitarios para cada mÃ³dulo</li>
<li>[ ] Tests para edge cases</li>
<li>[ ] Todos los tests pasan</li>
</ul>
<h2>ğŸ“ DocumentaciÃ³n</h2>
<h3>README.md del Proyecto</h3>
<ul>
<li>[ ] DescripciÃ³n del proyecto</li>
<li>[ ] Instrucciones de instalaciÃ³n</li>
<li>[ ] Ejemplo de uso</li>
<li>[ ] Resultados y mÃ©tricas</li>
<li>[ ] Escrito en inglÃ©s</li>
</ul>
<h3>MODEL_COMPARISON.md</h3>
<ul>
<li>[ ] Tabla comparativa de modelos</li>
<li>[ ] ExplicaciÃ³n matemÃ¡tica de diferencias</li>
<li>[ ] AnÃ¡lisis de PCA</li>
<li>[ ] AnÃ¡lisis de K-Means</li>
<li>[ ] Conclusiones</li>
</ul>
<h2>ğŸš€ VerificaciÃ³n Final</h2>
<pre><code class="language-bash"># 1. Tests
python -m pytest tests/ -v

# 2. Pipeline completo
python -c &quot;
from src.pipeline import run_mnist_pipeline
# Ejecutar pipeline demo
&quot;

# 3. Verificar accuracy
# Logistic Regression: &gt; 85%
# Neural Network: &gt; 90%
</code></pre>
<h2>âœ… DeclaraciÃ³n de Completitud</h2>
<h3>Por Fase</h3>
<ul>
<li>[ ] <strong>Fase 1:</strong> Fundamentos matemÃ¡ticos dominados</li>
<li>[ ] <strong>Fase 2:</strong> Algoritmos ML implementados desde cero</li>
<li>[ ] <strong>Fase 3:</strong> Proyecto MNIST completo</li>
</ul>
<h3>Por Curso del Pathway</h3>
<ul>
<li>[ ] <strong>Supervised Learning:</strong> RegresiÃ³n + ClasificaciÃ³n</li>
<li>[ ] <strong>Unsupervised Learning:</strong> K-Means + PCA</li>
<li>[ ] <strong>Deep Learning:</strong> MLP con Backpropagation</li>
</ul>
<h3>MÃ©tricas Finales</h3>
<table>
<thead>
<tr>
<th>MÃ©trica</th>
<th>Objetivo</th>
<th>Logrado</th>
</tr>
</thead>
<tbody>
<tr>
<td>Logistic Regression Accuracy</td>
<td>&gt;85%</td>
<td>___%</td>
</tr>
<tr>
<td>Neural Network Accuracy</td>
<td>&gt;90%</td>
<td>___%</td>
</tr>
<tr>
<td>MÃ³dulos completados</td>
<td>8/8</td>
<td>___/8</td>
</tr>
<tr>
<td>Tests pasando</td>
<td>100%</td>
<td>___%</td>
</tr>
</tbody>
</table>
<p><strong>Fecha de completitud:</strong> <strong><em>_</em></strong><strong><em>_</em></strong>_</p>
<p><strong>Listo para el MS in AI Pathway - LÃ­nea 1:</strong> â˜ SÃ­ â˜ No</p>
            </div>
        
            <!-- MÃ“DULO: RECURSOS.md -->
            <div class="cover-page">
                <div class="cover-content">
                    <div style="font-size:40pt;margin-bottom:20px;">ğŸ’</div>
                    <div id="mod_RECURSOS" class="cover-title">RECURSOS DE APRENDIZAJE</div>
                    <div class="cover-subtitle">MS in AI Pathway - ML Specialist v3.0</div>
                    <div class="cover-badge">MATEMÃTICAS APLICADAS A CÃ“DIGO</div>
                </div>
                <div style="position:absolute;bottom:30px;font-size:10pt;opacity:0.6;">DUQUEOM | 2025</div>
            </div>
            <div class="content">
                <h1>ğŸ“š Recursos de Aprendizaje - ML Specialist v3.0</h1>
<blockquote>
<p>Recursos organizados para dominar la <strong>LÃ­nea 1: Machine Learning</strong> del MS in AI Pathway.</p>
</blockquote>
<h2>ğŸ¯ CURSOS DEL PATHWAY (LÃNEA 1 - FOCO)</h2>
<p>Estos son los <strong>3 cursos</strong> de la LÃ­nea de Machine Learning:</p>
<table>
<thead>
<tr>
<th>Curso</th>
<th>Enlace</th>
<th>MÃ³dulo de PreparaciÃ³n</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Introduction to ML: Supervised Learning</strong></td>
<td><a href="https://www.coursera.org/learn/introduction-to-machine-learning-supervised-learning">Coursera</a></td>
<td>MÃ³dulo 04</td>
</tr>
<tr>
<td><strong>Unsupervised Algorithms in ML</strong></td>
<td><a href="https://www.coursera.org/learn/unsupervised-algorithms-in-machine-learning">Coursera</a></td>
<td>MÃ³dulo 05</td>
</tr>
<tr>
<td><strong>Introduction to Deep Learning</strong></td>
<td><a href="https://www.coursera.org/learn/introduction-to-deep-learning-boulder">Coursera</a></td>
<td>MÃ³dulo 06</td>
</tr>
</tbody>
</table>
<blockquote>
<p>ğŸ’¡ Puedes auditar estos cursos GRATIS en Coursera para ver el contenido.</p>
</blockquote>
<h3>LÃ­nea 2: Probabilidad y EstadÃ­stica (Lectura Opcional)</h3>
<table>
<thead>
<tr>
<th>Curso</th>
<th>Enlace</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probability Theory: Foundation for Data Science</td>
<td><a href="https://www.coursera.org/learn/probability-theory-foundation-for-data-science">Coursera</a></td>
</tr>
<tr>
<td>Statistical Inference for Estimation</td>
<td><a href="https://www.coursera.org/learn/statistical-inference-for-estimation-in-data-science">Coursera</a></td>
</tr>
<tr>
<td>Discrete-Time Markov Chains</td>
<td><a href="https://www.coursera.org/learn/discrete-time-markov-chains">Coursera</a></td>
</tr>
</tbody>
</table>
<h2>ğŸ”¥ CURSOS DE PREPARACIÃ“N RECOMENDADOS</h2>
<h3>Machine Learning (Prioridad Alta)</h3>
<table>
<thead>
<tr>
<th>Curso</th>
<th>Plataforma</th>
<th>Por QuÃ©</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://www.coursera.org/specializations/machine-learning-introduction">Machine Learning Specialization</a></td>
<td>Coursera (Andrew Ng)</td>
<td>El mejor curso de ML, usa Python</td>
</tr>
<tr>
<td><a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a></td>
<td>Coursera (Andrew Ng)</td>
<td>Profundiza en redes neuronales</td>
</tr>
</tbody>
</table>
<h3>MatemÃ¡ticas para ML (Esencial)</h3>
<table>
<thead>
<tr>
<th>Curso</th>
<th>Plataforma</th>
<th>Por QuÃ©</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://www.coursera.org/learn/linear-algebra-machine-learning">Mathematics for ML: Linear Algebra</a></td>
<td>Coursera (Imperial)</td>
<td>Vectores, matrices, eigenvalues</td>
</tr>
<tr>
<td><a href="https://www.coursera.org/learn/multivariate-calculus-machine-learning">Mathematics for ML: Multivariate Calculus</a></td>
<td>Coursera (Imperial)</td>
<td>Gradientes, Chain Rule, optimizaciÃ³n</td>
</tr>
</tbody>
</table>
<h3>NumPy y Python CientÃ­fico</h3>
<table>
<thead>
<tr>
<th>Curso</th>
<th>Plataforma</th>
<th>Por QuÃ©</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://numpy.org/doc/stable/user/absolute_beginners.html">NumPy Documentation</a></td>
<td>numpy.org</td>
<td>Referencia oficial</td>
</tr>
<tr>
<td><a href="https://realpython.com/numpy-tutorial/">Real Python - NumPy</a></td>
<td>Real Python</td>
<td>Tutorial prÃ¡ctico</td>
</tr>
</tbody>
</table>
<h2>ğŸ“– Libros</h2>
<h3>Machine Learning (Prioridad Alta)</h3>
<table>
<thead>
<tr>
<th>Libro</th>
<th>Autor</th>
<th>Por QuÃ©</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Hands-On Machine Learning</strong></td>
<td>AurÃ©lien GÃ©ron</td>
<td>PrÃ¡ctico, cÃ³digo completo</td>
</tr>
<tr>
<td><strong>Pattern Recognition and ML</strong></td>
<td>Christopher Bishop</td>
<td>Fundamentos teÃ³ricos sÃ³lidos</td>
</tr>
<tr>
<td><strong>The Hundred-Page ML Book</strong></td>
<td>Andriy Burkov</td>
<td>Resumen conciso</td>
</tr>
</tbody>
</table>
<h3>Deep Learning</h3>
<table>
<thead>
<tr>
<th>Libro</th>
<th>Autor</th>
<th>Por QuÃ©</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Deep Learning</strong></td>
<td>Goodfellow et al.</td>
<td><a href="https://www.deeplearningbook.org/">Gratis online</a> - La biblia</td>
</tr>
<tr>
<td><strong>Neural Networks and Deep Learning</strong></td>
<td>Michael Nielsen</td>
<td><a href="http://neuralnetworksanddeeplearning.com/">Gratis online</a> - Muy didÃ¡ctico</td>
</tr>
</tbody>
</table>
<h3>MatemÃ¡ticas para ML</h3>
<table>
<thead>
<tr>
<th>Libro</th>
<th>Autor</th>
<th>Por QuÃ©</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Mathematics for ML</strong></td>
<td>Deisenroth et al.</td>
<td><a href="https://mml-book.github.io/">Gratis online</a> - Fundamental</td>
</tr>
<tr>
<td><strong>Linear Algebra Done Right</strong></td>
<td>Axler</td>
<td>Ãlgebra lineal rigurosa</td>
</tr>
</tbody>
</table>
<h2>ğŸ¥ Videos Esenciales</h2>
<h3>Canales de YouTube</h3>
<table>
<thead>
<tr>
<th>Canal</th>
<th>Tema</th>
<th>Por QuÃ©</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://www.youtube.com/c/3blue1brown">3Blue1Brown</a></td>
<td>MatemÃ¡ticas + DL</td>
<td>VisualizaciÃ³n excepcional</td>
</tr>
<tr>
<td><a href="https://www.youtube.com/user/joshstarmer">StatQuest</a></td>
<td>ML + EstadÃ­stica</td>
<td>Explicaciones claras</td>
</tr>
<tr>
<td><a href="https://www.youtube.com/c/sentdex">Sentdex</a></td>
<td>ML prÃ¡ctico</td>
<td>Implementaciones desde cero</td>
</tr>
</tbody>
</table>
<h3>Playlists Esenciales</h3>
<table>
<thead>
<tr>
<th>Playlist</th>
<th>Tema</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Neural Networks</strong></td>
<td>Deep Learning</td>
<td><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">3B1B Neural Networks</a></td>
</tr>
<tr>
<td><strong>Linear Algebra</strong></td>
<td>MatemÃ¡ticas</td>
<td><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">3B1B Linear Algebra</a></td>
</tr>
<tr>
<td><strong>Calculus</strong></td>
<td>MatemÃ¡ticas</td>
<td><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">3B1B Calculus</a></td>
</tr>
<tr>
<td><strong>Machine Learning</strong></td>
<td>ML</td>
<td><a href="https://www.youtube.com/playlist?list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF">StatQuest ML</a></td>
</tr>
</tbody>
</table>
<h2>ğŸ› ï¸ Herramientas</h2>
<h3>Desarrollo</h3>
<table>
<thead>
<tr>
<th>Herramienta</th>
<th>PropÃ³sito</th>
<th>InstalaciÃ³n</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://www.python.org/">Python 3.11+</a></td>
<td>Lenguaje base</td>
<td><code>brew install python</code> / <code>apt install python3</code></td>
</tr>
<tr>
<td><a href="https://code.visualstudio.com/">VS Code</a></td>
<td>Editor de cÃ³digo</td>
<td>Descargar de web</td>
</tr>
<tr>
<td><a href="https://git-scm.com/">Git</a></td>
<td>Control de versiones</td>
<td><code>brew install git</code> / <code>apt install git</code></td>
</tr>
</tbody>
</table>
<h3>Python ML Stack</h3>
<table>
<thead>
<tr>
<th>Paquete</th>
<th>PropÃ³sito</th>
<th>Comando</th>
</tr>
</thead>
<tbody>
<tr>
<td>numpy</td>
<td>Operaciones vectoriales</td>
<td><code>pip install numpy</code></td>
</tr>
<tr>
<td>matplotlib</td>
<td>VisualizaciÃ³n</td>
<td><code>pip install matplotlib</code></td>
</tr>
<tr>
<td>mypy</td>
<td>Type checking</td>
<td><code>pip install mypy</code></td>
</tr>
<tr>
<td>pytest</td>
<td>Testing</td>
<td><code>pip install pytest</code></td>
</tr>
</tbody>
</table>
<h3>Datasets</h3>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>DescripciÃ³n</th>
<th>Acceso</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MNIST</strong></td>
<td>DÃ­gitos escritos a mano (28x28)</td>
<td><code>keras.datasets.mnist</code> o descarga directa</td>
</tr>
<tr>
<td><strong>Iris</strong></td>
<td>ClasificaciÃ³n clÃ¡sica (4 features)</td>
<td>CSV disponible en UCI</td>
</tr>
<tr>
<td><strong>Boston Housing</strong></td>
<td>RegresiÃ³n (precios de casas)</td>
<td>CSV disponible en Kaggle</td>
</tr>
</tbody>
</table>
<h2>ğŸ“… Ruta de Aprendizaje (26 Semanas)</h2>
<h3>Semanas 1-8: Fundamentos MatemÃ¡ticos</h3>
<table>
<thead>
<tr>
<th>Semana</th>
<th>Tema</th>
<th>Recursos</th>
</tr>
</thead>
<tbody>
<tr>
<td>1-2</td>
<td>Python CientÃ­fico + NumPy</td>
<td>MÃ³dulo 01 + NumPy docs</td>
</tr>
<tr>
<td>3-5</td>
<td>Ãlgebra Lineal</td>
<td>MÃ³dulo 02 + 3B1B Linear Algebra</td>
</tr>
<tr>
<td>6-8</td>
<td>CÃ¡lculo Multivariante</td>
<td>MÃ³dulo 03 + 3B1B Calculus</td>
</tr>
</tbody>
</table>
<h3>Semanas 9-20: NÃºcleo ML</h3>
<table>
<thead>
<tr>
<th>Semana</th>
<th>Tema</th>
<th>Recursos</th>
</tr>
</thead>
<tbody>
<tr>
<td>9-12</td>
<td>Supervised Learning</td>
<td>MÃ³dulo 04 + StatQuest ML</td>
</tr>
<tr>
<td>13-16</td>
<td>Unsupervised Learning</td>
<td>MÃ³dulo 05 + Andrew Ng videos</td>
</tr>
<tr>
<td>17-20</td>
<td>Deep Learning</td>
<td>MÃ³dulo 06 + 3B1B Neural Networks</td>
</tr>
</tbody>
</table>
<h3>Semanas 21-26: Proyecto MNIST</h3>
<table>
<thead>
<tr>
<th>Semana</th>
<th>Fase</th>
<th>Entregable</th>
</tr>
</thead>
<tbody>
<tr>
<td>21-22</td>
<td>PCA + K-Means</td>
<td>ExploraciÃ³n no supervisada</td>
</tr>
<tr>
<td>23-24</td>
<td>Logistic Regression</td>
<td>ClasificaciÃ³n supervisada</td>
</tr>
<tr>
<td>25-26</td>
<td>MLP + Benchmark</td>
<td>ComparaciÃ³n de modelos</td>
</tr>
</tbody>
</table>
<h2>ğŸ”— Links Directos</h2>
<h3>Programa</h3>
<ul>
<li><a href="https://www.coursera.org/degrees/ms-artificial-intelligence-boulder">MS in AI - CU Boulder</a></li>
<li><a href="https://www.coursera.org/degrees/ms-artificial-intelligence-boulder/admissions">Pathway Admissions</a></li>
</ul>
<h3>Libros Gratis</h3>
<ul>
<li><a href="https://mml-book.github.io/">Mathematics for ML</a></li>
<li><a href="https://www.deeplearningbook.org/">Deep Learning Book</a></li>
<li><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and DL</a></li>
</ul>
<h3>Datasets</h3>
<ul>
<li><a href="http://yann.lecun.com/exdb/mnist/">MNIST</a></li>
<li><a href="https://archive.ics.uci.edu/ml/index.php">UCI ML Repository</a></li>
<li><a href="https://www.kaggle.com/datasets">Kaggle Datasets</a></li>
</ul>
            </div>
        
            <!-- MÃ“DULO: GLOSARIO.md -->
            <div class="cover-page">
                <div class="cover-content">
                    <div style="font-size:40pt;margin-bottom:20px;">ğŸ’</div>
                    <div id="mod_GLOSARIO" class="cover-title">GLOSARIO TÃ‰CNICO</div>
                    <div class="cover-subtitle">MS in AI Pathway - ML Specialist v3.0</div>
                    <div class="cover-badge">MATEMÃTICAS APLICADAS A CÃ“DIGO</div>
                </div>
                <div style="position:absolute;bottom:30px;font-size:10pt;opacity:0.6;">DUQUEOM | 2025</div>
            </div>
            <div class="content">
                <h1>ğŸ“– Glosario TÃ©cnico - ML Specialist v3.0</h1>
<blockquote>
<p>Definiciones A-Z de tÃ©rminos de Machine Learning usados en la guÃ­a.</p>
</blockquote>
<h2>A</h2>
<h3>Activation Function</h3>
<p><strong>DefiniciÃ³n:</strong> FunciÃ³n no lineal aplicada a la salida de una neurona.<br />
<strong>Ejemplos:</strong> ReLU, Sigmoid, Tanh, Softmax.<br />
<strong>Por quÃ©:</strong> Sin activaciones, una red serÃ­a solo transformaciones lineales.</p>
<h3>Adam</h3>
<p><strong>DefiniciÃ³n:</strong> Adaptive Moment Estimation - optimizador que combina Momentum y RMSprop.<br />
<strong>ParÃ¡metros:</strong> lr=0.001, Î²â‚=0.9, Î²â‚‚=0.999, Îµ=1e-8<br />
<strong>Uso:</strong> Default moderno para entrenar redes neuronales.</p>
<h3>Accuracy</h3>
<p><strong>DefiniciÃ³n:</strong> ProporciÃ³n de predicciones correctas.<br />
<strong>FÃ³rmula:</strong> (TP + TN) / (TP + TN + FP + FN)<br />
<strong>LimitaciÃ³n:</strong> EngaÃ±oso con clases desbalanceadas.</p>
<h2>B</h2>
<h3>Backpropagation</h3>
<p><strong>DefiniciÃ³n:</strong> Algoritmo para calcular gradientes en redes neuronales usando la Chain Rule.<br />
<strong>Proceso:</strong> Forward pass â†’ calcular loss â†’ backward pass â†’ actualizar pesos.<br />
<strong>Base matemÃ¡tica:</strong> âˆ‚L/âˆ‚w = âˆ‚L/âˆ‚a Â· âˆ‚a/âˆ‚z Â· âˆ‚z/âˆ‚w</p>
<h3>Batch Size</h3>
<p><strong>DefiniciÃ³n:</strong> NÃºmero de muestras procesadas antes de actualizar pesos.<br />
<strong>Trade-off:</strong> Grande = estable pero lento; pequeÃ±o = ruidoso pero rÃ¡pido.<br />
<strong>ComÃºn:</strong> 32, 64, 128, 256.</p>
<h3>Bias (parÃ¡metro)</h3>
<p><strong>DefiniciÃ³n:</strong> TÃ©rmino constante en z = Wx + b que permite desplazar la funciÃ³n.<br />
<strong>AnalogÃ­a:</strong> El intercepto en una recta y = mx + b.</p>
<h3>Binary Cross-Entropy</h3>
<p><strong>DefiniciÃ³n:</strong> FunciÃ³n de pÃ©rdida para clasificaciÃ³n binaria.<br />
<strong>FÃ³rmula:</strong> L = -[yÂ·log(Å·) + (1-y)Â·log(1-Å·)]<br />
<strong>Uso:</strong> Salida sigmoid, predicciÃ³n de probabilidad.</p>
<h3>Broadcasting</h3>
<p><strong>DefiniciÃ³n:</strong> ExpansiÃ³n automÃ¡tica de arrays para operaciones elemento a elemento.<br />
<strong>Ejemplo:</strong> array(3,1) + array(1,4) â†’ array(3,4)<br />
<strong>Regla:</strong> Dimensiones deben ser iguales o una debe ser 1.</p>
<h2>C</h2>
<h3>Centroid</h3>
<p><strong>DefiniciÃ³n:</strong> Punto central de un cluster (promedio de sus puntos).<br />
<strong>En K-Means:</strong> Se actualiza iterativamente hasta convergencia.</p>
<h3>Chain Rule</h3>
<p><strong>DefiniciÃ³n:</strong> Regla para derivar funciones compuestas.<br />
<strong>FÃ³rmula:</strong> d/dx f(g(x)) = f'(g(x)) Â· g'(x)<br />
<strong>Importancia:</strong> Base matemÃ¡tica de Backpropagation.</p>
<h3>Classification</h3>
<p><strong>DefiniciÃ³n:</strong> Tarea de predecir una categorÃ­a discreta.<br />
<strong>Binaria:</strong> 2 clases (spam/no spam).<br />
<strong>Multiclase:</strong> &gt;2 clases (dÃ­gitos 0-9).</p>
<h3>Clustering</h3>
<p><strong>DefiniciÃ³n:</strong> Agrupar puntos similares sin etiquetas supervisadas.<br />
<strong>Algoritmos:</strong> K-Means, DBSCAN, Hierarchical.</p>
<h3>Confusion Matrix</h3>
<p><strong>DefiniciÃ³n:</strong> Tabla que muestra predicciones vs valores reales.<br />
<strong>Componentes:</strong> TP, TN, FP, FN.</p>
<h3>Convergence</h3>
<p><strong>DefiniciÃ³n:</strong> Cuando el algoritmo deja de mejorar significativamente.<br />
<strong>Criterio:</strong> Cambio en loss &lt; tolerancia, o gradiente â‰ˆ 0.</p>
<h3>Cosine Similarity</h3>
<p><strong>DefiniciÃ³n:</strong> Similitud basada en el Ã¡ngulo entre vectores.<br />
<strong>FÃ³rmula:</strong> cos(Î¸) = (aÂ·b) / (||a|| ||b||)<br />
<strong>Rango:</strong> [-1, 1], donde 1 = idÃ©nticos.</p>
<h3>Cross-Validation</h3>
<p><strong>DefiniciÃ³n:</strong> TÃ©cnica para evaluar modelo dividiendo datos en K folds.<br />
<strong>K-Fold:</strong> Entrenar K veces, cada vez con diferente fold como validaciÃ³n.<br />
<strong>Uso:</strong> Estimar rendimiento real, evitar overfitting.</p>
<h2>D</h2>
<h3>Deep Learning</h3>
<p><strong>DefiniciÃ³n:</strong> ML con redes neuronales de mÃºltiples capas ocultas.<br />
<strong>Ventaja:</strong> Aprende features automÃ¡ticamente.<br />
<strong>Requisito:</strong> Muchos datos y compute.</p>
<h3>Derivative</h3>
<p><strong>DefiniciÃ³n:</strong> Tasa de cambio instantÃ¡nea de una funciÃ³n.<br />
<strong>NotaciÃ³n:</strong> f'(x), df/dx, âˆ‚f/âˆ‚x (parcial).</p>
<h3>Dimensionality Reduction</h3>
<p><strong>DefiniciÃ³n:</strong> Reducir nÃºmero de features preservando informaciÃ³n.<br />
<strong>MÃ©todos:</strong> PCA, t-SNE, UMAP.<br />
<strong>Uso:</strong> VisualizaciÃ³n, eliminar ruido, acelerar entrenamiento.</p>
<h3>Dot Product</h3>
<p><strong>DefiniciÃ³n:</strong> Suma de productos elemento a elemento.<br />
<strong>FÃ³rmula:</strong> aÂ·b = Î£ aáµ¢báµ¢<br />
<strong>Uso:</strong> Similitud, proyecciones, capas de red neuronal.</p>
<h2>E</h2>
<h3>Eigenvalue / Eigenvector</h3>
<p><strong>DefiniciÃ³n:</strong> Para matriz A, Av = Î»v donde v es eigenvector y Î» es eigenvalue.<br />
<strong>InterpretaciÃ³n:</strong> Direcciones principales de la transformaciÃ³n.<br />
<strong>Uso en ML:</strong> PCA usa eigenvectors de la matriz de covarianza.</p>
<h3>Epoch</h3>
<p><strong>DefiniciÃ³n:</strong> Una pasada completa por todo el dataset de entrenamiento.<br />
<strong>TÃ­pico:</strong> 10-100 epochs dependiendo del problema.</p>
<h3>Euclidean Distance</h3>
<p><strong>DefiniciÃ³n:</strong> Distancia en lÃ­nea recta entre dos puntos.<br />
<strong>FÃ³rmula:</strong> d(a,b) = âˆšÎ£(aáµ¢ - báµ¢)Â²<br />
<strong>Uso:</strong> K-Means, KNN.</p>
<h2>F</h2>
<h3>F1 Score</h3>
<p><strong>DefiniciÃ³n:</strong> Media armÃ³nica de Precision y Recall.<br />
<strong>FÃ³rmula:</strong> F1 = 2 Â· (P Â· R) / (P + R)<br />
<strong>Uso:</strong> Balance entre precision y recall.</p>
<h3>Feature</h3>
<p><strong>DefiniciÃ³n:</strong> Variable de entrada (columna) en un dataset.<br />
<strong>Ejemplo:</strong> En MNIST, cada pÃ­xel es un feature (784 total).</p>
<h3>Forward Pass</h3>
<p><strong>DefiniciÃ³n:</strong> PropagaciÃ³n de input a travÃ©s de la red para obtener output.<br />
<strong>CÃ¡lculo:</strong> z = Wx + b, a = activation(z), repetir por capa.</p>
<h2>G</h2>
<h3>Gradient</h3>
<p><strong>DefiniciÃ³n:</strong> Vector de derivadas parciales.<br />
<strong>NotaciÃ³n:</strong> âˆ‡f = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ...]<br />
<strong>Propiedad:</strong> Apunta en direcciÃ³n de mÃ¡ximo ascenso.</p>
<h3>Gradient Descent</h3>
<p><strong>DefiniciÃ³n:</strong> Algoritmo de optimizaciÃ³n que sigue el gradiente negativo.<br />
<strong>Update:</strong> Î¸ = Î¸ - Î± Â· âˆ‡L(Î¸)<br />
<strong>Variantes:</strong> Batch, Mini-batch, Stochastic (SGD).</p>
<h2>H</h2>
<h3>Hidden Layer</h3>
<p><strong>DefiniciÃ³n:</strong> Capa entre input y output en una red neuronal.<br />
<strong>FunciÃ³n:</strong> Aprende representaciones intermedias.</p>
<h3>Hyperparameter</h3>
<p><strong>DefiniciÃ³n:</strong> ParÃ¡metro configurado antes del entrenamiento (no aprendido).<br />
<strong>Ejemplos:</strong> Learning rate, nÃºmero de capas, batch size.</p>
<h2>I</h2>
<h3>Inertia</h3>
<p><strong>DefiniciÃ³n:</strong> Suma de distancias cuadradas de puntos a sus centroides.<br />
<strong>En K-Means:</strong> MÃ©trica a minimizar.<br />
<strong>Uso:</strong> MÃ©todo del codo para elegir K.</p>
<h2>K</h2>
<h3>K-Means</h3>
<p><strong>DefiniciÃ³n:</strong> Algoritmo de clustering que particiona en K grupos.<br />
<strong>Pasos:</strong> 1) Inicializar centroides 2) Asignar puntos 3) Actualizar centroides 4) Repetir.<br />
<strong>Complejidad:</strong> O(n Â· k Â· i Â· d) donde i=iteraciones, d=dimensiones.</p>
<h3>K-Means++</h3>
<p><strong>DefiniciÃ³n:</strong> InicializaciÃ³n inteligente para K-Means.<br />
<strong>MÃ©todo:</strong> Elegir centroides iniciales lejos entre sÃ­.<br />
<strong>Ventaja:</strong> Mejor convergencia, evita mÃ­nimos locales.</p>
<h2>L</h2>
<h3>L1 Norm (Manhattan)</h3>
<p><strong>DefiniciÃ³n:</strong> Suma de valores absolutos.<br />
<strong>FÃ³rmula:</strong> ||x||â‚ = Î£|xáµ¢|<br />
<strong>Uso:</strong> RegularizaciÃ³n Lasso, promueve sparsity.</p>
<h3>L2 Norm (Euclidean)</h3>
<p><strong>DefiniciÃ³n:</strong> RaÃ­z de suma de cuadrados (longitud del vector).<br />
<strong>FÃ³rmula:</strong> ||x||â‚‚ = âˆšÎ£xáµ¢Â²<br />
<strong>Uso:</strong> RegularizaciÃ³n Ridge, normalizaciÃ³n.</p>
<h3>Learning Rate</h3>
<p><strong>DefiniciÃ³n:</strong> TamaÃ±o del paso en Gradient Descent.<br />
<strong>SÃ­mbolo:</strong> Î± (alpha) o lr.<br />
<strong>Trade-off:</strong> Grande = rÃ¡pido pero inestable; pequeÃ±o = estable pero lento.</p>
<h3>Linear Regression</h3>
<p><strong>DefiniciÃ³n:</strong> Modelo que predice valor continuo con combinaciÃ³n lineal.<br />
<strong>FÃ³rmula:</strong> Å· = XÎ¸<br />
<strong>Loss:</strong> MSE (Mean Squared Error).</p>
<h3>Logistic Regression</h3>
<p><strong>DefiniciÃ³n:</strong> Modelo de clasificaciÃ³n binaria usando sigmoid.<br />
<strong>FÃ³rmula:</strong> P(y=1) = Ïƒ(XÎ¸)<br />
<strong>Loss:</strong> Binary Cross-Entropy.</p>
<h3>Loss Function</h3>
<p><strong>DefiniciÃ³n:</strong> FunciÃ³n que mide error entre predicciÃ³n y valor real.<br />
<strong>Ejemplos:</strong> MSE (regresiÃ³n), Cross-Entropy (clasificaciÃ³n).<br />
<strong>Objetivo:</strong> Minimizar durante entrenamiento.</p>
<h2>M</h2>
<h3>Matrix Multiplication</h3>
<p><strong>DefiniciÃ³n:</strong> OperaciÃ³n (mÃ—n) @ (nÃ—p) â†’ (mÃ—p).<br />
<strong>Elemento:</strong> C[i,j] = Î£â‚– A[i,k] Â· B[k,j]<br />
<strong>Uso:</strong> Transformaciones lineales, capas de red.</p>
<h3>Mini-batch</h3>
<p><strong>DefiniciÃ³n:</strong> Subconjunto de datos usado en una iteraciÃ³n de SGD.<br />
<strong>Ventaja:</strong> Balance entre eficiencia y estabilidad.</p>
<h3>MLP (Multilayer Perceptron)</h3>
<p><strong>DefiniciÃ³n:</strong> Red neuronal fully-connected con capas ocultas.<br />
<strong>Arquitectura:</strong> Input â†’ Hidden(s) â†’ Output.</p>
<h3>MNIST</h3>
<p><strong>DefiniciÃ³n:</strong> Dataset de dÃ­gitos escritos a mano (28Ã—28 pÃ­xeles).<br />
<strong>TamaÃ±o:</strong> 60k train, 10k test.<br />
<strong>Uso:</strong> Benchmark clÃ¡sico de clasificaciÃ³n de imÃ¡genes.</p>
<h3>MSE (Mean Squared Error)</h3>
<p><strong>DefiniciÃ³n:</strong> Promedio de errores al cuadrado.<br />
<strong>FÃ³rmula:</strong> MSE = (1/n) Î£(y - Å·)Â²<br />
<strong>Uso:</strong> Loss para regresiÃ³n.</p>
<h3>Momentum</h3>
<p><strong>DefiniciÃ³n:</strong> TÃ©cnica que acelera SGD acumulando gradientes pasados.<br />
<strong>FÃ³rmula:</strong> v = Î²Â·v + (1-Î²)Â·âˆ‡L; Î¸ = Î¸ - Î±Â·v<br />
<strong>Ventaja:</strong> Escapa mÃ­nimos locales, reduce oscilaciones.</p>
<h2>N</h2>
<h3>Normalization</h3>
<p><strong>DefiniciÃ³n:</strong> Escalar datos a un rango estÃ¡ndar.<br />
<strong>Min-Max:</strong> x' = (x - min) / (max - min) â†’ [0, 1]<br />
<strong>Z-score:</strong> x' = (x - Î¼) / Ïƒ â†’ media 0, std 1.</p>
<h3>NumPy</h3>
<p><strong>DefiniciÃ³n:</strong> LibrerÃ­a de Python para computaciÃ³n numÃ©rica eficiente.<br />
<strong>Ventaja:</strong> Operaciones vectorizadas (evita loops).<br />
<strong>Objeto principal:</strong> ndarray (n-dimensional array).</p>
<h2>O</h2>
<h3>One-Hot Encoding</h3>
<p><strong>DefiniciÃ³n:</strong> Representar categorÃ­a como vector binario.<br />
<strong>Ejemplo:</strong> clase 3 de 5 â†’ [0, 0, 0, 1, 0]<br />
<strong>Uso:</strong> Labels para clasificaciÃ³n multiclase.</p>
<h3>Overfitting</h3>
<p><strong>DefiniciÃ³n:</strong> Modelo que memoriza training data pero no generaliza.<br />
<strong>SÃ­ntoma:</strong> Train loss bajo, test loss alto.<br />
<strong>Soluciones:</strong> MÃ¡s datos, regularizaciÃ³n, dropout, early stopping.</p>
<h2>P</h2>
<h3>Partial Derivative</h3>
<p><strong>DefiniciÃ³n:</strong> Derivada respecto a una variable, tratando otras como constantes.<br />
<strong>NotaciÃ³n:</strong> âˆ‚f/âˆ‚x<br />
<strong>Uso:</strong> Calcular gradientes en funciones multivariable.</p>
<h3>PCA (Principal Component Analysis)</h3>
<p><strong>DefiniciÃ³n:</strong> ReducciÃ³n dimensional que preserva mÃ¡xima varianza.<br />
<strong>MÃ©todo:</strong> Proyectar datos en eigenvectors principales.<br />
<strong>Output:</strong> Componentes principales ordenados por varianza explicada.</p>
<h3>Precision</h3>
<p><strong>DefiniciÃ³n:</strong> De los predichos positivos, Â¿cuÃ¡ntos son correctos?<br />
<strong>FÃ³rmula:</strong> TP / (TP + FP)<br />
<strong>Importancia:</strong> Cuando FP es costoso.</p>
<h3>Projection</h3>
<p><strong>DefiniciÃ³n:</strong> Mapear un punto a un subespacio (lÃ­nea, plano).<br />
<strong>En PCA:</strong> Proyectar datos al espacio de componentes principales.</p>
<h2>R</h2>
<h3>Recall</h3>
<p><strong>DefiniciÃ³n:</strong> De los positivos reales, Â¿cuÃ¡ntos capturÃ©?<br />
<strong>FÃ³rmula:</strong> TP / (TP + FN)<br />
<strong>Importancia:</strong> Cuando FN es costoso.</p>
<h3>Regression</h3>
<p><strong>DefiniciÃ³n:</strong> Predecir un valor continuo.<br />
<strong>Ejemplos:</strong> Precio de casa, temperatura.</p>
<h3>Regularization</h3>
<p><strong>DefiniciÃ³n:</strong> TÃ©cnica para prevenir overfitting penalizando complejidad.<br />
<strong>L1 (Lasso):</strong> AÃ±ade Î»Â·||Î¸||â‚ al loss.<br />
<strong>L2 (Ridge):</strong> AÃ±ade Î»Â·||Î¸||â‚‚Â² al loss.</p>
<h3>ReLU (Rectified Linear Unit)</h3>
<p><strong>DefiniciÃ³n:</strong> f(x) = max(0, x)<br />
<strong>Derivada:</strong> 1 si x &gt; 0, 0 si x â‰¤ 0.<br />
<strong>Ventaja:</strong> Simple, evita vanishing gradient.</p>
<h2>S</h2>
<h3>SGD (Stochastic Gradient Descent)</h3>
<p><strong>DefiniciÃ³n:</strong> Gradient descent con una muestra (o mini-batch) por update.<br />
<strong>Ventaja:</strong> MÃ¡s rÃ¡pido, escapa mÃ­nimos locales.<br />
<strong>Desventaja:</strong> Updates ruidosos.</p>
<h3>Sigmoid</h3>
<p><strong>DefiniciÃ³n:</strong> Ïƒ(x) = 1 / (1 + eâ»Ë£)<br />
<strong>Rango:</strong> (0, 1)<br />
<strong>Uso:</strong> ClasificaciÃ³n binaria, probabilidades.<br />
<strong>Derivada:</strong> Ïƒ(x) Â· (1 - Ïƒ(x))</p>
<h3>Silhouette Score</h3>
<p><strong>DefiniciÃ³n:</strong> MÃ©trica de calidad de clustering.<br />
<strong>Rango:</strong> [-1, 1], mayor es mejor.<br />
<strong>CÃ¡lculo:</strong> Basado en cohesiÃ³n intra-cluster y separaciÃ³n inter-cluster.</p>
<h3>Softmax</h3>
<p><strong>DefiniciÃ³n:</strong> Convierte vector a distribuciÃ³n de probabilidad.<br />
<strong>FÃ³rmula:</strong> softmax(z)áµ¢ = eá¶»â± / Î£â±¼ eá¶»Ê²<br />
<strong>Uso:</strong> Capa de salida para clasificaciÃ³n multiclase.</p>
<h3>Supervised Learning</h3>
<p><strong>DefiniciÃ³n:</strong> Aprender de datos con etiquetas (X, y).<br />
<strong>Tareas:</strong> ClasificaciÃ³n, RegresiÃ³n.</p>
<h3>SVD (Singular Value Decomposition)</h3>
<p><strong>DefiniciÃ³n:</strong> FactorizaciÃ³n A = UÎ£Váµ€.<br />
<strong>Uso:</strong> PCA (mÃ¡s estable), compresiÃ³n, sistemas de recomendaciÃ³n.</p>
<h2>T</h2>
<h3>Tanh</h3>
<p><strong>DefiniciÃ³n:</strong> Tangente hiperbÃ³lica, similar a sigmoid pero centrada en 0.<br />
<strong>Rango:</strong> (-1, 1)<br />
<strong>Derivada:</strong> 1 - tanhÂ²(x)</p>
<h3>Test Set</h3>
<p><strong>DefiniciÃ³n:</strong> Datos reservados para evaluaciÃ³n final del modelo.<br />
<strong>Regla:</strong> NUNCA usar para entrenar o seleccionar hiperparÃ¡metros.</p>
<h3>Training Set</h3>
<p><strong>DefiniciÃ³n:</strong> Datos usados para entrenar el modelo.<br />
<strong>TÃ­pico:</strong> 70-80% del dataset total.</p>
<h3>Transpose</h3>
<p><strong>DefiniciÃ³n:</strong> Intercambiar filas y columnas de una matriz.<br />
<strong>NotaciÃ³n:</strong> Aáµ€<br />
<strong>Propiedad:</strong> (AB)áµ€ = Báµ€Aáµ€</p>
<h2>U</h2>
<h3>Underfitting</h3>
<p><strong>DefiniciÃ³n:</strong> Modelo demasiado simple que no captura patrones.<br />
<strong>SÃ­ntoma:</strong> Train loss alto, test loss alto.<br />
<strong>Soluciones:</strong> Modelo mÃ¡s complejo, mÃ¡s features, mÃ¡s entrenamiento.</p>
<h3>Unsupervised Learning</h3>
<p><strong>DefiniciÃ³n:</strong> Aprender de datos sin etiquetas.<br />
<strong>Tareas:</strong> Clustering, reducciÃ³n dimensional, detecciÃ³n de anomalÃ­as.</p>
<h2>V</h2>
<h3>Validation Set</h3>
<p><strong>DefiniciÃ³n:</strong> Datos para ajustar hiperparÃ¡metros y detectar overfitting.<br />
<strong>TÃ­pico:</strong> 10-20% del training data.</p>
<h3>Variance (estadÃ­stica)</h3>
<p><strong>DefiniciÃ³n:</strong> Medida de dispersiÃ³n de los datos.<br />
<strong>FÃ³rmula:</strong> Var(X) = E[(X - Î¼)Â²]</p>
<h3>Variance (ML)</h3>
<p><strong>DefiniciÃ³n:</strong> Error por sensibilidad a fluctuaciones en training data.<br />
<strong>Alta varianza:</strong> Overfitting.</p>
<h3>Vectorization</h3>
<p><strong>DefiniciÃ³n:</strong> Reemplazar loops por operaciones de arrays.<br />
<strong>Ventaja:</strong> 10-100x mÃ¡s rÃ¡pido con NumPy.<br />
<strong>Ejemplo:</strong> <code>np.dot(a, b)</code> en lugar de <code>sum(a[i]*b[i] for i in range(n))</code></p>
<h2>W</h2>
<h3>Weight</h3>
<p><strong>DefiniciÃ³n:</strong> ParÃ¡metro aprendido que determina importancia de input.<br />
<strong>En redes:</strong> Matriz W en z = Wx + b.</p>
<h2>X</h2>
<h3>Xavier Initialization</h3>
<p><strong>DefiniciÃ³n:</strong> Inicializar pesos con varianza 1/n_inputs.<br />
<strong>FÃ³rmula:</strong> W ~ N(0, 1/n_in) o U(-âˆš(1/n_in), âˆš(1/n_in))<br />
<strong>Uso:</strong> Capas con tanh/sigmoid.</p>
<h3>XOR Problem</h3>
<p><strong>DefiniciÃ³n:</strong> Problema no linealmente separable clÃ¡sico.<br />
<strong>Importancia:</strong> Demuestra necesidad de capas ocultas en redes neuronales.<br />
<strong>SoluciÃ³n:</strong> MLP con al menos una capa oculta.</p>
            </div>
        
    </body>
    </html>
    