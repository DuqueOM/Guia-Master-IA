#!/usr/bin/env python3
"""
Simulacro de Examen: CSCA 5622 - Supervised Learning
=====================================================

MÃ³dulo: M05 - Aprendizaje Supervisado
Tiempo Estimado: 90 minutos
PuntuaciÃ³n Total: 100 puntos

Estructura:
- Parte A: Preguntas TeÃ³ricas (30 puntos)
- Parte B: Ejercicios de CÃ³digo (70 puntos)

Criterio para aprobar con B: >= 80 puntos

Instrucciones:
1. Ejecutar todas las celdas en orden
2. Completar las funciones marcadas con # TODO
3. Ejecutar los tests al final para validar
4. No modificar las funciones de test

Ejecutar tests: pytest tests/test_simulacro_csca5622.py -v
"""
from __future__ import annotations

import numpy as np
from numpy.typing import NDArray

rng = np.random.default_rng(42)

# =============================================================================
# PARTE A: PREGUNTAS TEÃ“RICAS (30 puntos)
# =============================================================================
# Responde en las variables indicadas (string)

print("=" * 70)
print("PARTE A: PREGUNTAS TEÃ“RICAS (30 puntos)")
print("=" * 70)

# -----------------------------------------------------------------------------
# Pregunta A1 (8 puntos): Bias-Variance Tradeoff
# -----------------------------------------------------------------------------
# Un modelo de regresiÃ³n tiene MSE = 25 en el conjunto de test.
# Sabes que BiasÂ² = 9 y la varianza irreducible (ruido) = 4.
#
# a) Â¿CuÃ¡l es la Variance del modelo? (2 pts)
# b) Â¿El modelo sufre mÃ¡s de underfitting o overfitting? Justifica. (3 pts)
# c) Â¿QuÃ© acciÃ³n tomarÃ­as para mejorar el modelo? (3 pts)

respuesta_A1_a: float = 0.0  # TODO: Reemplazar con el valor correcto
respuesta_A1_b: str = ""  # TODO: "underfitting" o "overfitting" + justificaciÃ³n
respuesta_A1_c: str = ""  # TODO: AcciÃ³n especÃ­fica


# -----------------------------------------------------------------------------
# Pregunta A2 (7 puntos): RegularizaciÃ³n
# -----------------------------------------------------------------------------
# En regresiÃ³n Ridge, la funciÃ³n de costo es:
# J(w) = MSE + Î»||w||â‚‚Â²
#
# a) Â¿QuÃ© sucede con los pesos w cuando Î» â†’ âˆ? (2 pts)
# b) Â¿QuÃ© sucede cuando Î» = 0? (2 pts)
# c) Â¿Por quÃ© Ridge NO produce pesos exactamente 0 pero Lasso sÃ­? (3 pts)

respuesta_A2_a: str = ""  # TODO
respuesta_A2_b: str = ""  # TODO
respuesta_A2_c: str = ""  # TODO


# -----------------------------------------------------------------------------
# Pregunta A3 (8 puntos): MÃ©tricas de ClasificaciÃ³n
# -----------------------------------------------------------------------------
# Un clasificador de spam tiene la siguiente matriz de confusiÃ³n:
#
#                    Predicho
#                  Spam    No-Spam
# Real  Spam       80        20
#       No-Spam    10       890
#
# a) Calcula Precision para la clase "Spam" (2 pts)
# b) Calcula Recall para la clase "Spam" (2 pts)
# c) Si el costo de un False Negative (spam no detectado) es 10x mayor
#    que un False Positive, Â¿quÃ© mÃ©trica priorizarÃ­as? (4 pts)

respuesta_A3_precision: float = 0.0  # TODO
respuesta_A3_recall: float = 0.0  # TODO
respuesta_A3_c: str = ""  # TODO


# -----------------------------------------------------------------------------
# Pregunta A4 (7 puntos): Ãrboles de DecisiÃ³n
# -----------------------------------------------------------------------------
# a) Â¿QuÃ© mide el Gini Impurity? Escribe la fÃ³rmula. (3 pts)
# b) Â¿Por quÃ© Random Forest reduce la varianza comparado con un solo Ã¡rbol? (4 pts)

respuesta_A4_a: str = ""  # TODO
respuesta_A4_b: str = ""  # TODO


# =============================================================================
# PARTE B: EJERCICIOS DE CÃ“DIGO (70 puntos)
# =============================================================================

print("\n" + "=" * 70)
print("PARTE B: EJERCICIOS DE CÃ“DIGO (70 puntos)")
print("=" * 70)


# -----------------------------------------------------------------------------
# Ejercicio B1 (20 puntos): RegresiÃ³n Lineal desde Cero
# -----------------------------------------------------------------------------
def linear_regression_gradient_descent(
    X: NDArray[np.float64],
    y: NDArray[np.float64],
    learning_rate: float = 0.01,
    n_iterations: int = 1000,
) -> NDArray[np.float64]:
    """
    Implementa RegresiÃ³n Lineal usando Gradiente Descendente.

    ParÃ¡metros
    ----------
    X : NDArray[np.float64]
        Matriz de caracterÃ­sticas (n_samples, n_features).
        Ya incluye columna de 1s para el bias.
    y : NDArray[np.float64]
        Vector objetivo (n_samples,).
    learning_rate : float
        Tasa de aprendizaje Î±.
    n_iterations : int
        NÃºmero de iteraciones.

    Retorna
    -------
    NDArray[np.float64]
        Vector de pesos optimizados (n_features,).

    FÃ³rmulas:
    ---------
    - PredicciÃ³n: Å· = Xw
    - Gradiente MSE: âˆ‡w = (2/n) * X^T * (Xw - y)
    - ActualizaciÃ³n: w = w - Î± * âˆ‡w
    """
    n_samples, n_features = X.shape

    # TODO: Inicializar pesos con ceros
    weights = np.zeros(n_features)  # Placeholder

    # TODO: Implementar gradiente descendente
    for _ in range(n_iterations):
        # TODO: Calcular predicciones
        # TODO: Calcular gradiente
        # TODO: Actualizar pesos
        pass

    return weights


# Test B1
print("\n--- Test B1: RegresiÃ³n Lineal ---")
X_test = np.column_stack([np.ones(100), rng.standard_normal((100, 2))])
true_weights = np.array([2.0, 1.5, -0.5])
y_test = X_test @ true_weights + rng.standard_normal(100) * 0.1

weights_pred = linear_regression_gradient_descent(X_test, y_test, learning_rate=0.1)
print(f"Pesos verdaderos: {true_weights}")
print(f"Pesos estimados:  {weights_pred}")


# -----------------------------------------------------------------------------
# Ejercicio B2 (25 puntos): RegresiÃ³n LogÃ­stica desde Cero
# -----------------------------------------------------------------------------
def sigmoid(z: NDArray[np.float64]) -> NDArray[np.float64]:
    """FunciÃ³n sigmoide: Ïƒ(z) = 1 / (1 + exp(-z))."""
    # TODO: Implementar sigmoid (manejar overflow)
    return np.zeros_like(z)  # Placeholder


def logistic_regression_gradient_descent(
    X: NDArray[np.float64],
    y: NDArray[np.float64],
    learning_rate: float = 0.1,
    n_iterations: int = 1000,
) -> NDArray[np.float64]:
    """
    Implementa RegresiÃ³n LogÃ­stica usando Gradiente Descendente.

    ParÃ¡metros
    ----------
    X : NDArray[np.float64]
        Matriz de caracterÃ­sticas (n_samples, n_features).
    y : NDArray[np.float64]
        Vector de labels binarios (n_samples,) con valores 0 o 1.
    learning_rate : float
        Tasa de aprendizaje Î±.
    n_iterations : int
        NÃºmero de iteraciones.

    Retorna
    -------
    NDArray[np.float64]
        Vector de pesos optimizados (n_features,).

    FÃ³rmulas:
    ---------
    - PredicciÃ³n: p = Ïƒ(Xw)
    - Gradiente BCE: âˆ‡w = (1/n) * X^T * (p - y)
    - ActualizaciÃ³n: w = w - Î± * âˆ‡w
    """
    n_samples, n_features = X.shape

    # TODO: Inicializar pesos
    weights = np.zeros(n_features)

    # TODO: Implementar gradiente descendente
    for _ in range(n_iterations):
        # TODO: Calcular probabilidades con sigmoid
        # TODO: Calcular gradiente
        # TODO: Actualizar pesos
        pass

    return weights


def predict_proba(
    X: NDArray[np.float64], weights: NDArray[np.float64]
) -> NDArray[np.float64]:
    """Predice probabilidades P(y=1|X)."""
    # TODO: Implementar
    return np.zeros(X.shape[0])  # Placeholder


def predict(
    X: NDArray[np.float64], weights: NDArray[np.float64], threshold: float = 0.5
) -> NDArray[np.int64]:
    """Predice clases binarias."""
    # TODO: Implementar
    return np.zeros(X.shape[0], dtype=np.int64)  # Placeholder


# Test B2
print("\n--- Test B2: RegresiÃ³n LogÃ­stica ---")
X_log = np.column_stack([np.ones(200), rng.standard_normal((200, 2))])
true_w_log = np.array([0.0, 2.0, -1.5])
y_log = (sigmoid(X_log @ true_w_log) > 0.5).astype(int)

weights_log = logistic_regression_gradient_descent(X_log, y_log, learning_rate=0.5)
preds = predict(X_log, weights_log)
accuracy = np.mean(preds == y_log)
print(f"Accuracy: {accuracy:.2%}")


# -----------------------------------------------------------------------------
# Ejercicio B3 (25 puntos): Ãrbol de DecisiÃ³n - Gini Impurity
# -----------------------------------------------------------------------------
def gini_impurity(y: NDArray[np.int64]) -> float:
    """
    Calcula el Gini Impurity de un conjunto de labels.

    Gini = 1 - Î£ p_iÂ²

    donde p_i es la proporciÃ³n de la clase i.

    ParÃ¡metros
    ----------
    y : NDArray[np.int64]
        Vector de labels (n_samples,).

    Retorna
    -------
    float
        Gini impurity entre 0 (puro) y 0.5 (mÃ¡xima impureza para binario).
    """
    if len(y) == 0:
        return 0.0

    # TODO: Implementar Gini impurity
    return 0.0  # Placeholder


def information_gain(
    y_parent: NDArray[np.int64],
    y_left: NDArray[np.int64],
    y_right: NDArray[np.int64],
) -> float:
    """
    Calcula el Information Gain de un split.

    IG = Gini(parent) - [n_left/n * Gini(left) + n_right/n * Gini(right)]

    ParÃ¡metros
    ----------
    y_parent : NDArray[np.int64]
        Labels del nodo padre.
    y_left : NDArray[np.int64]
        Labels del hijo izquierdo.
    y_right : NDArray[np.int64]
        Labels del hijo derecho.

    Retorna
    -------
    float
        Information gain (siempre >= 0).
    """
    n = len(y_parent)
    if n == 0:
        return 0.0

    # TODO: Implementar information gain
    return 0.0  # Placeholder


def find_best_split(
    X: NDArray[np.float64],
    y: NDArray[np.int64],
) -> tuple[int, float, float]:
    """
    Encuentra el mejor split para un nodo.

    ParÃ¡metros
    ----------
    X : NDArray[np.float64]
        Matriz de caracterÃ­sticas (n_samples, n_features).
    y : NDArray[np.int64]
        Vector de labels (n_samples,).

    Retorna
    -------
    tuple[int, float, float]
        - best_feature: Ã­ndice de la mejor caracterÃ­stica
        - best_threshold: valor del umbral
        - best_gain: information gain del split
    """
    best_feature = 0
    best_threshold = 0.0
    best_gain = 0.0

    n_samples, n_features = X.shape

    # TODO: Iterar sobre features y thresholds
    # TODO: Encontrar el split con mayor information gain

    return best_feature, best_threshold, best_gain


# Test B3
print("\n--- Test B3: Gini Impurity ---")
y_pure = np.array([1, 1, 1, 1])
y_impure = np.array([0, 0, 1, 1])
y_mixed = np.array([0, 0, 0, 1, 1, 1, 1, 1])

print(f"Gini puro (esperado ~0.0): {gini_impurity(y_pure):.4f}")
print(f"Gini 50/50 (esperado ~0.5): {gini_impurity(y_impure):.4f}")
print(f"Gini 3/5 (esperado ~0.469): {gini_impurity(y_mixed):.4f}")


# =============================================================================
# VALIDACIÃ“N FINAL
# =============================================================================

print("\n" + "=" * 70)
print("VALIDACIÃ“N FINAL")
print("=" * 70)


def validar_simulacro() -> dict[str, bool]:
    """Valida todas las respuestas del simulacro."""
    resultados: dict[str, bool] = {}

    # Validar A1
    resultados["A1_variance"] = (
        abs(respuesta_A1_a - 12.0) < 0.1
    )  # MSE = BiasÂ² + Var + Noise
    resultados["A1_diagnostico"] = "overfitting" in respuesta_A1_b.lower()

    # Validar A3 (mÃ©tricas)
    resultados["A3_precision"] = abs(respuesta_A3_precision - 80 / 90) < 0.01
    resultados["A3_recall"] = abs(respuesta_A3_recall - 80 / 100) < 0.01

    # Validar B1 (regresiÃ³n lineal)
    X_val = np.column_stack([np.ones(50), rng.standard_normal((50, 2))])
    true_w = np.array([1.0, 2.0, -1.0])
    y_val = X_val @ true_w
    w_pred = linear_regression_gradient_descent(
        X_val, y_val, learning_rate=0.1, n_iterations=1000
    )
    resultados["B1_linear_reg"] = np.allclose(w_pred, true_w, atol=0.1)

    # Validar B2 (sigmoid)
    resultados["B2_sigmoid"] = np.allclose(
        sigmoid(np.array([0.0])), np.array([0.5]), atol=0.01
    )

    # Validar B3 (gini)
    resultados["B3_gini_pure"] = abs(gini_impurity(np.array([1, 1, 1])) - 0.0) < 0.01
    resultados["B3_gini_impure"] = abs(gini_impurity(np.array([0, 1])) - 0.5) < 0.01

    return resultados


# Ejecutar validaciÃ³n
print("\nğŸ” Validando respuestas...")
resultados = validar_simulacro()

puntos = 0
for test, passed in resultados.items():
    status = "âœ…" if passed else "âŒ"
    pts = 10 if passed else 0
    puntos += pts
    print(f"  {status} {test}: {pts} pts")

print(f"\nğŸ“Š PUNTUACIÃ“N ESTIMADA: {puntos}/70 (solo cÃ³digo)")
print("   + Parte TeÃ³rica: /30 (requiere revisiÃ³n manual)")

if puntos >= 56:  # 80% de 70
    print("\nğŸ‰ Â¡Vas bien! El cÃ³digo cumple el criterio para B.")
else:
    print("\nâš ï¸ Necesitas revisar las implementaciones antes del examen real.")

print("\n" + "=" * 70)
print("FIN DEL SIMULACRO")
print("=" * 70)
