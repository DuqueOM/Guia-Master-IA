# M√≥dulo 06 - Unsupervised Learning

> **üéØ Objetivo:** Dominar K-Means clustering y PCA para reducci√≥n dimensional
> **Fase:** 2 - N√∫cleo de ML | **Semanas 13-16**
> **Curso del Pathway:** Unsupervised Algorithms in Machine Learning

---

<a id="m06-0"></a>

## üß≠ C√≥mo usar este m√≥dulo (modo 0‚Üí100)

**Prop√≥sito:** que puedas:

- encontrar estructura sin etiquetas (clustering)
- reducir dimensionalidad con rigor (PCA)
- decidir cu√°ndo NO usar estos m√©todos

### Objetivos de aprendizaje (medibles)

Al terminar este m√≥dulo podr√°s:

- **Implementar** K-Means (Lloyd + K-Means++).
- **Evaluar** clustering con inercia/codo y silhouette (entendiendo limitaciones).
- **Implementar** PCA con SVD y usar varianza explicada para elegir `n_components`.
- **Diagnosticar** cu√°ndo K-Means/PCA fallan y proponer alternativas.

Enlaces r√°pidos:

- [GLOSARIO.md](GLOSARIO.md)
- [RECURSOS.md](RECURSOS.md)
- [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)
- [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md)

### Recursos (cu√°ndo usarlos)

| Prioridad | Recurso | Cu√°ndo usarlo en este m√≥dulo | Para qu√© |
|----------|---------|------------------------------|----------|
| **Obligatorio** | `study_tools/DRILL_DIMENSIONES_NUMPY.md` | Semana 13‚Äì16, cada vez que implementes distancias/proyecciones y se rompan shapes | Evitar errores silenciosos en broadcasting/`axis` |
| **Obligatorio** | `study_tools/DIARIO_ERRORES.md` | Cuando K-Means produzca clusters vac√≠os, `NaN` o PCA devuelva resultados inestables | Registrar el caso y dejarlo ‚Äúdebuggeable‚Äù |
| **Complementario** | [3Blue1Brown: Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) | Semana 15 (PCA), al ver varianza/proyecciones/autovectores | Intuici√≥n visual para PCA |
| **Complementario** | [VisuAlgo](https://visualgo.net/en) | Semana 13‚Äì14, al estudiar el comportamiento iterativo de K-Means y su sensibilidad a inicializaci√≥n | Visualizar algoritmos paso a paso para construir intuici√≥n |
| **Complementario** | [Mathematics for ML (book)](https://mml-book.github.io/) | Semana 15‚Äì16, al formalizar covarianza, eigen/SVD | Notaci√≥n y derivaciones m√°s rigurosas |
| **Opcional** | [RECURSOS.md](RECURSOS.md) | Al terminar el m√≥dulo (para profundizar en clustering/reducci√≥n dimensional) | Elegir material extra sin dispersarte |

---

## üß† ¬øQu√© es Unsupervised Learning?

```text
APRENDIZAJE NO SUPERVISADO

Tenemos:
- Datos de entrada X (features)
- NO tenemos etiquetas Y

Objetivo: Encontrar estructura oculta en los datos

Tipos principales:
‚îú‚îÄ‚îÄ CLUSTERING: Agrupar puntos similares
‚îÇ   ‚îî‚îÄ‚îÄ K-Means, DBSCAN, Hierarchical
‚îú‚îÄ‚îÄ REDUCCI√ìN DIMENSIONAL: Comprimir features
‚îÇ   ‚îî‚îÄ‚îÄ PCA, t-SNE, UMAP
‚îî‚îÄ‚îÄ DETECCI√ìN DE ANOMAL√çAS: Encontrar outliers
    ‚îî‚îÄ‚îÄ Isolation Forest, GMM
```

---

## üìö Contenido del M√≥dulo

| Semana | Tema | Entregable |
|--------|------|------------|
| 13 | K-Means Clustering | `kmeans.py` |
| 14 | Evaluaci√≥n de Clusters | M√©tricas de clustering |
| 15 | PCA | `pca.py` |
| 16 | PCA Aplicado + GMM | Compresi√≥n de im√°genes |

---

## üß© Micro-Cap√≠tulo Maestro: PCA (Principal Component Analysis) ‚Äî Nivel: Avanzado

### 1) Intuici√≥n: la mejor foto

Imagina que tienes un objeto 3D (tus datos en alta dimensi√≥n) y solo puedes tomar una ‚Äúfoto‚Äù en 2D.

- Si tomas la foto desde un √°ngulo malo, la sombra se ve ‚Äúaplastada‚Äù y pierdes estructura.
- Si tomas la foto desde el √°ngulo correcto, la sombra conserva la mayor cantidad de informaci√≥n posible.

PCA busca matem√°ticamente ese √°ngulo: la proyecci√≥n donde la **varianza proyectada** es m√°xima.

### 2) Derivaci√≥n l√≥gica (covarianza ‚Üí eigen)

1) **Centrar**

Mueves el origen para que el promedio sea 0:

`X_c = X - Œº`

2) **Covarianza**

La matriz de covarianza captura c√≥mo ‚Äúse estiran‚Äù los datos:

`Œ£ = (1/(n-1)) X_c^T X_c`

Si (en 2D) `Œ£ = [[10, 0],[0, 1]]`, significa: hay mucha m√°s varianza en X que en Y.

3) **Eigenvectors y eigenvalues**

- Los **eigenvectors** de `Œ£` apuntan en direcciones principales de estiramiento.
- Los **eigenvalues** dicen cu√°nta varianza hay en esas direcciones.

PCA elige los eigenvectors con eigenvalues m√°s grandes y proyecta ah√≠.

### 3) Por qu√© SVD suele ser mejor que eigen en c√≥digo

Si calculas `X_c^T X_c` puedes amplificar problemas num√©ricos (est√°s ‚Äúcuadrando‚Äù escalas).

En cambio, con SVD:

`X_c = U S V^T`

se obtienen las componentes principales directamente desde `V` sin formar expl√≠citamente `Œ£`.

Regla pr√°ctica:

- **En teor√≠a:** PCA = eigen de la covarianza.
- **En pr√°ctica:** PCA = SVD de `X_c` (m√°s estable; es lo que usan implementaciones modernas).

---

## üíª Parte 1: K-Means Clustering

### 1.0 K-Means ‚Äî Nivel: intermedio (core del Pathway)

**Prop√≥sito:** pasar de ‚Äús√© que K-Means agrupa puntos‚Äù a **poder implementarlo desde cero, elegir `k` con criterio y detectar cu√°ndo NO usarlo**.

#### Objetivos de aprendizaje (medibles)

Al terminar esta parte podr√°s:

- **Explicar** la funci√≥n objetivo de K-Means (inercia) y por qu√© usa distancia euclidiana.
- **Aplicar** el algoritmo de Lloyd (asignar ‚Üí actualizar ‚Üí repetir) y reconocer convergencia.
- **Implementar** K-Means++ y justificar por qu√© mejora la inicializaci√≥n.
- **Analizar** fallas t√≠picas: clusters vac√≠os, sensibilidad a escala, m√≠nimos locales.
- **Evaluar** resultados usando inercia y silhouette (y entender limitaciones de ambas).

#### Prerrequisitos

- De `M√≥dulo 01`: NumPy (broadcasting, `axis`, shapes).
- De `M√≥dulo 02`: norma L2 / distancia euclidiana.

Enlaces r√°pidos:

- [GLOSARIO: K-Means](GLOSARIO.md#k-means)
- [GLOSARIO: K-Means++](GLOSARIO.md#k-means-1)
- [GLOSARIO: Inertia](GLOSARIO.md#inertia)
- [GLOSARIO: Clustering](GLOSARIO.md#clustering)

#### Resumen ejecutivo (big idea)

K-Means alterna dos pasos que **siempre reducen (o no aumentan)** la inercia:

- **Asignaci√≥n:** cada punto va al centroide m√°s cercano.
- **Actualizaci√≥n:** cada centroide se mueve al promedio de sus puntos.

Esto garantiza que el algoritmo converge (en iteraciones finitas), pero **no garantiza el m√≠nimo global**: por eso la inicializaci√≥n (K-Means++) importa.

#### Intuici√≥n ‚Üí formalizaci√≥n

##### a) Intuici√≥n

K-Means intenta poner `k` ‚Äúimanes‚Äù (centroides) y moverlos hasta que cada im√°n represente bien a los puntos que atrajo.

##### a.1 Intuici√≥n geom√©trica: Voronoi tessellation (territorios)

Una forma visual de entender K-Means:

- pones `k` centroides como ‚Äúsemillas‚Äù en el plano
- cada semilla **reclama el territorio** de los puntos m√°s cercanos

Eso induce un particionado del espacio en **celdas de Voronoi**: regiones poligonales donde todos los puntos est√°n m√°s cerca de un centroide que de cualquier otro.

En cada iteraci√≥n de Lloyd:

- **Asignaci√≥n:** recalculas las celdas (qui√©n pertenece a qui√©n)
- **Actualizaci√≥n:** cada semilla se mueve al centro de masa de su celda

##### b) Formalizaci√≥n

Funci√≥n objetivo:

`J = Œ£·µ¢ Œ£_{x‚ààC·µ¢} ||x - Œº·µ¢||¬≤`

Donde:

- `Œº·µ¢` es el centroide del cluster `i`.
- `C·µ¢` es el conjunto de puntos asignados al cluster `i`.

##### c) Condiciones donde K-Means funciona bien

- clusters ‚Äúredondos‚Äù / aproximadamente esf√©ricos
- tama√±os similares
- distancia euclidiana tiene sentido (features en la misma escala)

##### d) Casos donde falla (y c√≥mo detectarlo)

- clusters alargados/no convexos (ej.: ‚Äúdos lunas‚Äù)
- escalas distintas sin normalizar (una feature domina)
- outliers fuertes arrastran centroides

#### Actividades activas (aprendizaje activo)

- **Retrieval practice (3‚Äì5 min):** escribe sin mirar:
  - los dos pasos del algoritmo de Lloyd
  - la funci√≥n objetivo `J`
- **Ejercicio de diagn√≥stico:** crea 2 features con escalas distintas y observa c√≥mo cambia el clustering si normalizas.

#### Debugging / validaci√≥n (v5)

- Si obtienes resultados raros, revisa primero:
  - shapes (`X: (n_samples, n_features)`, `centroids: (k, n_features)`, `labels: (n_samples,)`)
  - `NaN` por clusters vac√≠os (centroide sin puntos)
- Registra hallazgos en `study_tools/DIARIO_ERRORES.md`.
- Antes de usar un dataset real ‚Äúsucio‚Äù, aplica `study_tools/DIRTY_DATA_CHECK.md`.
- Para integrar el protocolo completo:
  - [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)
  - [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md)

#### Cheat sheet (repaso r√°pido)

- **Paso 1:** `labels = argmin(||x - Œº·µ¢||¬≤)`
- **Paso 2:** `Œº·µ¢ = mean(points_in_cluster_i)`
- **Convergencia:** `||Œº_new - Œº_old||¬≤ < tol`
- **Riesgo:** m√≠nimos locales ‚Üí usar K-Means++ y/o m√∫ltiples inicializaciones

### 1.1 Algoritmo de Lloyd

```python
import numpy as np

"""
K-MEANS CLUSTERING (Algoritmo de Lloyd)

Objetivo: Particionar n puntos en k clusters, minimizando la
varianza intra-cluster (inercia).

Algoritmo:
1. Inicializar k centroides (aleatorio o k-means++)
2. Repetir hasta convergencia:
   a. ASIGNAR: cada punto al centroide m√°s cercano
   b. ACTUALIZAR: mover cada centroide al promedio de sus puntos
3. Retornar centroides y asignaciones

Funci√≥n objetivo (minimizar):
    J = Œ£·µ¢ Œ£‚±º ||x‚±º - Œº·µ¢||¬≤

Donde x‚±º pertenece al cluster i con centroide Œº·µ¢
"""

def euclidean_distance(a: np.ndarray, b: np.ndarray) -> float:
    """Distancia euclidiana entre dos puntos."""
    return np.sqrt(np.sum((a - b) ** 2))

def assign_clusters(X: np.ndarray, centroids: np.ndarray) -> np.ndarray:
    """
    Asigna cada punto al centroide m√°s cercano.

    Args:
        X: datos (n_samples, n_features)
        centroids: centroides actuales (k, n_features)

    Returns:
        labels: √≠ndice del cluster para cada punto (n_samples,)
    """
    n_samples = X.shape[0]
    k = centroids.shape[0]

    # Calcular distancia de cada punto a cada centroide
    distances = np.zeros((n_samples, k))
    for i in range(k):
        distances[:, i] = np.sqrt(np.sum((X - centroids[i]) ** 2, axis=1))

    # Asignar al m√°s cercano
    return np.argmin(distances, axis=1)

def update_centroids(X: np.ndarray, labels: np.ndarray, k: int) -> np.ndarray:
    """
    Actualiza centroides como el promedio de los puntos asignados.

    Args:
        X: datos
        labels: asignaciones actuales
        k: n√∫mero de clusters

    Returns:
        nuevos centroides
    """
    n_features = X.shape[1]
    centroids = np.zeros((k, n_features))

    for i in range(k):
        points_in_cluster = X[labels == i]
        if len(points_in_cluster) > 0:
            centroids[i] = np.mean(points_in_cluster, axis=0)

    return centroids
```

### 1.2 K-Means++ Initialization

```python
import numpy as np

def kmeans_plus_plus_init(X: np.ndarray, k: int, random_state: int = None) -> np.ndarray:
    """
    Inicializaci√≥n K-Means++.

    Mejor que inicializaci√≥n aleatoria porque:
    - Elige centroides que est√°n lejos entre s√≠
    - Reduce la probabilidad de mala convergencia
    - Garantiza O(log k) de la soluci√≥n √≥ptima

    Algoritmo:
    1. Elegir primer centroide aleatoriamente
    2. Para cada centroide restante:
       a. Calcular distancia de cada punto al centroide m√°s cercano
       b. Elegir nuevo centroide con probabilidad proporcional a d¬≤
    """
    if random_state is not None:
        np.random.seed(random_state)

    n_samples, n_features = X.shape
    centroids = np.zeros((k, n_features))

    # Primer centroide aleatorio
    first_idx = np.random.randint(n_samples)
    centroids[0] = X[first_idx]

    # Centroides restantes
    for c in range(1, k):
        # Calcular distancia al centroide m√°s cercano para cada punto
        distances = np.zeros(n_samples)
        for i in range(n_samples):
            min_dist = float('inf')
            for j in range(c):
                dist = np.sum((X[i] - centroids[j]) ** 2)
                min_dist = min(min_dist, dist)
            distances[i] = min_dist

        # Probabilidad proporcional a d¬≤
        probabilities = distances / np.sum(distances)

        # Elegir nuevo centroide
        new_idx = np.random.choice(n_samples, p=probabilities)
        centroids[c] = X[new_idx]

    return centroids
```

### 1.3 Implementaci√≥n Completa

```python
import numpy as np
from typing import Tuple

class KMeans:
    """K-Means Clustering implementado desde cero."""

    def __init__(
        self,
        n_clusters: int = 3,
        max_iter: int = 300,
        tol: float = 1e-4,
        init: str = 'kmeans++',
        random_state: int = None
    ):
        """
        Args:
            n_clusters: n√∫mero de clusters (k)
            max_iter: m√°ximo de iteraciones
            tol: tolerancia para convergencia
            init: 'kmeans++' o 'random'
            random_state: semilla para reproducibilidad
        """
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.tol = tol
        self.init = init
        self.random_state = random_state

        self.centroids = None
        self.labels_ = None
        self.inertia_ = None
        self.n_iter_ = 0

    def _init_centroids(self, X: np.ndarray) -> np.ndarray:
        """Inicializa centroides."""
        if self.random_state is not None:
            np.random.seed(self.random_state)

        if self.init == 'kmeans++':
            return kmeans_plus_plus_init(X, self.n_clusters, self.random_state)
        else:
            # Inicializaci√≥n aleatoria
            indices = np.random.choice(len(X), self.n_clusters, replace=False)
            return X[indices].copy()

    def _compute_inertia(self, X: np.ndarray) -> float:
        """
        Calcula inercia (within-cluster sum of squares).

        Inercia = Œ£·µ¢ Œ£‚±º ||x‚±º - Œº·µ¢||¬≤
        """
        inertia = 0
        for i in range(self.n_clusters):
            cluster_points = X[self.labels_ == i]
            if len(cluster_points) > 0:
                inertia += np.sum((cluster_points - self.centroids[i]) ** 2)
        return inertia

    def fit(self, X: np.ndarray) -> 'KMeans':
        """Entrena el modelo."""
        # Inicializar centroides
        self.centroids = self._init_centroids(X)

        for iteration in range(self.max_iter):
            # Guardar centroides anteriores
            old_centroids = self.centroids.copy()

            # Paso 1: Asignar puntos a clusters
            self.labels_ = assign_clusters(X, self.centroids)

            # Paso 2: Actualizar centroides
            self.centroids = update_centroids(X, self.labels_, self.n_clusters)

            # Verificar convergencia
            centroid_shift = np.sum((self.centroids - old_centroids) ** 2)
            if centroid_shift < self.tol:
                break

        self.n_iter_ = iteration + 1
        self.inertia_ = self._compute_inertia(X)

        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        """Predice clusters para nuevos datos."""
        return assign_clusters(X, self.centroids)

    def fit_predict(self, X: np.ndarray) -> np.ndarray:
        """Entrena y predice."""
        self.fit(X)
        return self.labels_


# Demo
np.random.seed(42)

# Generar datos sint√©ticos (3 clusters)
cluster1 = np.random.randn(100, 2) + [0, 0]
cluster2 = np.random.randn(100, 2) + [5, 5]
cluster3 = np.random.randn(100, 2) + [10, 0]
X = np.vstack([cluster1, cluster2, cluster3])

# Entrenar
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)

print(f"Iteraciones: {kmeans.n_iter_}")
print(f"Inercia: {kmeans.inertia_:.2f}")
print(f"Centroides:\n{kmeans.centroids}")
```

---

## üíª Parte 2: Evaluaci√≥n de Clusters

### 2.0 Evaluaci√≥n ‚Äî c√≥mo decidir si el clustering ‚Äútiene sentido‚Äù

**Prop√≥sito:** evitar el error com√∫n de ‚ÄúK-Means siempre devuelve clusters, entonces siempre sirve‚Äù. Aqu√≠ aprendes a **medir calidad** y a entender por qu√© esas m√©tricas pueden enga√±ar.

#### Objetivos de aprendizaje (medibles)

- **Explicar** qu√© mide la inercia y por qu√© siempre baja al subir `k`.
- **Aplicar** el m√©todo del codo como heur√≠stica (no como regla matem√°tica).
- **Interpretar** silhouette score (qu√© significa cerca de 1, 0 y valores negativos).
- **Analizar** cu√°ndo no puedes validar bien (porque no hay ground truth).

Enlaces r√°pidos:

- [GLOSARIO: Inertia](GLOSARIO.md#inertia)
- [GLOSARIO: Silhouette Score](GLOSARIO.md#silhouette-score)

#### Resumen ejecutivo

- **Inercia:** mide compactaci√≥n interna; √∫til para comparar `k`, pero sesgada (siempre favorece `k` grande).
- **Silhouette:** mezcla cohesi√≥n y separaci√≥n; √∫til para comparar modelos, pero costosa de calcular de forma exacta.

#### Actividades activas

- Ejecuta elbow + silhouette sobre el mismo dataset y escribe una conclusi√≥n:
  - ¬øcoinciden en el `k`?
  - si no coinciden, ¬øpor qu√© podr√≠a pasar?

### 2.1 Inercia (Within-Cluster Sum of Squares)

```python
def compute_inertia(X: np.ndarray, labels: np.ndarray, centroids: np.ndarray) -> float:
    """
    Inercia: suma de distancias cuadradas al centroide.

    Menor inercia = clusters m√°s compactos.

    Problema: siempre disminuye al aumentar k.
    Soluci√≥n: usar m√©todo del codo.
    """
    inertia = 0
    for i, centroid in enumerate(centroids):
        cluster_points = X[labels == i]
        inertia += np.sum((cluster_points - centroid) ** 2)
    return inertia
```

### 2.2 M√©todo del Codo (Elbow Method)

```python
import numpy as np
import matplotlib.pyplot as plt

def elbow_method(X: np.ndarray, k_range: range) -> list:
    """
    M√©todo del codo para elegir k √≥ptimo.

    Busca el punto donde a√±adir m√°s clusters
    no reduce significativamente la inercia.
    """
    inertias = []

    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42)
        kmeans.fit(X)
        inertias.append(kmeans.inertia_)

    return inertias

def plot_elbow(k_range: range, inertias: list):
    """Visualiza el m√©todo del codo."""
    plt.figure(figsize=(8, 5))
    plt.plot(list(k_range), inertias, 'bo-')
    plt.xlabel('N√∫mero de clusters (k)')
    plt.ylabel('Inercia')
    plt.title('M√©todo del Codo')
    plt.grid(True)
    plt.show()

# Demo
# inertias = elbow_method(X, range(1, 11))
# plot_elbow(range(1, 11), inertias)
```

### 2.3 Silhouette Score

```python
import numpy as np

def silhouette_sample(X: np.ndarray, labels: np.ndarray, idx: int) -> float:
    """
    Calcula silhouette para un solo punto.

    s(i) = (b(i) - a(i)) / max(a(i), b(i))

    Donde:
    - a(i): distancia promedio a puntos del mismo cluster
    - b(i): distancia promedio m√≠nima a puntos de otro cluster

    Rango: [-1, 1]
    - 1: punto bien asignado
    - 0: punto en frontera entre clusters
    - -1: punto mal asignado
    """
    point = X[idx]
    label = labels[idx]

    # a(i): distancia promedio intra-cluster
    same_cluster = X[labels == label]
    if len(same_cluster) > 1:
        a = np.mean([np.sqrt(np.sum((point - p) ** 2))
                     for p in same_cluster if not np.array_equal(p, point)])
    else:
        a = 0

    # b(i): distancia promedio al cluster m√°s cercano
    unique_labels = np.unique(labels)
    b = float('inf')
    for other_label in unique_labels:
        if other_label != label:
            other_cluster = X[labels == other_label]
            if len(other_cluster) > 0:
                avg_dist = np.mean([np.sqrt(np.sum((point - p) ** 2))
                                   for p in other_cluster])
                b = min(b, avg_dist)

    if b == float('inf'):
        return 0

    return (b - a) / max(a, b)

def silhouette_score(X: np.ndarray, labels: np.ndarray) -> float:
    """
    Silhouette Score promedio para todos los puntos.

    Mayor es mejor (max = 1).
    """
    scores = [silhouette_sample(X, labels, i) for i in range(len(X))]
    return np.mean(scores)


# Demo
# score = silhouette_score(X, labels)
# print(f"Silhouette Score: {score:.4f}")
```

---

## üíª Parte 3: PCA (Principal Component Analysis)

### 3.0 PCA ‚Äî Nivel: intermedio (reducci√≥n dimensional con rigor)

**Prop√≥sito:** pasar de ‚ÄúPCA reduce dimensiones‚Äù a **poder derivar su l√≥gica, implementarlo con SVD y usar varianza explicada para tomar decisiones**.

#### Objetivos de aprendizaje (medibles)

Al terminar esta parte podr√°s:

- **Explicar** por qu√© PCA encuentra direcciones de m√°xima varianza (y qu√© NO significa eso).
- **Aplicar** el pipeline correcto: centrar ‚Üí descomponer (SVD) ‚Üí proyectar ‚Üí reconstruir.
- **Implementar** PCA con SVD y calcular `explained_variance_ratio_`.
- **Elegir** `n_components` por varianza acumulada y justificar el trade-off.
- **Diagnosticar** errores t√≠picos: no centrar datos, confundir componentes con scores, reconstrucci√≥n incorrecta.

#### Motivaci√≥n / por qu√© importa

En la mayor√≠a de los problemas reales, la intuici√≥n visual se pierde en espacios de alta dimensi√≥n (ej.: cientos o miles de features). PCA te permite:

- **Visualizar** en 2D/3D sin tirar informaci√≥n ‚Äúa ojo‚Äù.
- **Eliminar ruido** (qued√°ndote con las direcciones dominantes de variaci√≥n).
- **Comprimir** (reconstruir aproximaciones controlando el error).

Regla pr√°ctica: PCA no ‚Äúencuentra lo que separa clases‚Äù; encuentra lo que **m√°s var√≠a**.

#### Prerrequisitos

- De `M√≥dulo 02`: SVD (intuici√≥n) y producto matricial.
- De `M√≥dulo 02`: matriz de covarianza, eigenvalues y eigenvectors.
- De `M√≥dulo 02`: proyecci√≥n (producto punto) y norma.
- De `M√≥dulo 01`: manipulaci√≥n de shapes y `axis`.

Enlaces r√°pidos:

- [GLOSARIO: PCA](GLOSARIO.md#pca-principal-component-analysis)
- [GLOSARIO: SVD](GLOSARIO.md#svd-singular-value-decomposition)
- [RECURSOS.md](RECURSOS.md)

#### Resumen ejecutivo (big idea)

PCA crea un nuevo sistema de coordenadas donde:

- el eje 1 (PC1) captura la mayor varianza,
- el eje 2 (PC2) captura la mayor varianza restante, y as√≠ sucesivamente,

y luego te permite quedarte con los primeros `k` ejes para comprimir.

#### Intuici√≥n ‚Üí formalizaci√≥n

##### a) Intuici√≥n

Si tus datos viven cerca de un plano dentro de un espacio 100D, PCA intenta encontrar ese plano (o subespacio) para representar los datos con menos n√∫meros.

Analog√≠a: ‚Äúbuscar el mejor √°ngulo para tomar una foto‚Äù

- Tienes un objeto 3D (tus datos en alta dimensi√≥n).
- Una foto 2D pierde informaci√≥n.
- PCA elige el **√°ngulo de c√°mara** que preserva la mayor ‚Äúinformaci√≥n‚Äù medible como **varianza**.

Met√°fora complementaria (baguette): imagina una nube de puntos alargada como una baguette flotando en 3D. Si tomas la foto desde la punta, parece un c√≠rculo (pierdes estructura). Si la tomas de lado, ves su longitud real. PCA busca ese ‚Äúlado‚Äù matem√°ticamente.

Ojo: ‚Äúm√°s varianza‚Äù no significa ‚Äúm√°s √∫til para clasificar‚Äù; solo significa ‚Äúm√°s dispersi√≥n‚Äù.

##### b) Conceptos clave (glosario m√≠nimo)

- **Varianza:** dispersi√≥n de los datos; PCA busca maximizarla *despu√©s* de proyectar.
- **Matriz de covarianza (`Œ£`):** matriz sim√©trica que describe c√≥mo var√≠an las variables y c√≥mo co-var√≠an entre s√≠.
- **Eigenvector (vector propio):** direcci√≥n que no cambia (salvo escala) al aplicar `Œ£`; en PCA, son los ejes principales.
- **Eigenvalue (valor propio):** varianza capturada en la direcci√≥n de su eigenvector.
- **Componente principal:** eje (eigenvector) ordenado por eigenvalue descendente.

##### c) Formalizaci√≥n m√≠nima

- Centrar: `X_c = X - mean(X)`
- SVD: `X_c = U S V·µÄ`
- Componentes principales: columnas de `V` (o filas de `V·µÄ`)
- Proyecci√≥n a `k` componentes: `Z = X_c @ V_k`
- Reconstrucci√≥n: `X_hat = Z @ V_k·µÄ + mean`

##### c.1 Maximizando la varianza (derivaci√≥n l√≥gica ‚Üí ecuaci√≥n de eigenvalores)

Idea: buscas un vector unitario `u` (direcci√≥n) tal que la varianza de la proyecci√≥n `u·µÄx` sea m√°xima.

Si `x` est√° centrado, la varianza proyectada es:

`Var(u·µÄx) = u·µÄ Œ£ u`

Planteas el problema:

`max_u  u·µÄ Œ£ u   s.a.  ||u||‚ÇÇ = 1`

Con multiplicadores de Lagrange, la condici√≥n de √≥ptimo lleva a:

`Œ£u = Œªu`

Interpretaci√≥n directa:

- `u` es un componente principal.
- `Œª` es la varianza capturada por ese componente.

##### c.2 Relaci√≥n SVD ‚Üî eigenvalues (por qu√© SVD es el m√©todo preferido)

Si `X_c` son los datos centrados y haces:

```
X_c = U S V·µÄ
```

Entonces la covarianza muestral es:

```
Œ£ = (1/(n-1)) X_c·µÄ X_c
  = (1/(n-1)) (V S U·µÄ)(U S V·µÄ)
  = V (S¬≤/(n-1)) V·µÄ
```

Conclusi√≥n:

- **Los eigenvectors de `Œ£`** son las columnas de `V`.
- **Los eigenvalues de `Œ£`** son `S¬≤/(n-1)`.

Esto conecta directamente con `M√≥dulo 02` (eigenvalues/eigenvectors) y explica por qu√© PCA ‚Äúv√≠a SVD‚Äù suele ser m√°s estable.

##### c.3 Worked example: PCA manual en 2D (rotaci√≥n de ejes)

Sup√≥n datos 2D que ‚Äúviven‚Äù casi sobre la diagonal `y = x`.

1) Centrar los datos:

```
X_c = X - mean(X)
```

2) Imagina que la covarianza queda (caso idealizado):

```
Œ£ = [[1, 1],
     [1, 1]]
```

3) Sus eigenvectors (direcciones principales) son:

- `v1 = (1, 1)/‚àö2`  (direcci√≥n diagonal)
- `v2 = (1, -1)/‚àö2` (direcci√≥n anti-diagonal)

Y sus eigenvalues:

- `Œª1 = 2` (mucha varianza en la diagonal)
- `Œª2 = 0` (casi nada en la anti-diagonal)

4) Proyecci√≥n a 1D:

```
z = X_c @ v1
```

Interpretaci√≥n: rotaste ejes y te quedaste solo con el eje donde ‚Äúvive‚Äù casi toda la variaci√≥n.

##### c.4 Worked example (num√©rico): covarianza y primer componente a mano

Datos centrados (6 puntos):

```text
X = [(-1,-1), (-2,-1), (-3,-2), (1,1), (2,1), (3,2)]
```

1) Construye `X` como matriz `(n_samples, 2)` y calcula:

`Œ£ = (1/(n-1)) X·µÄ X`

Aqu√≠:

```text
X·µÄX = [[28, 18],
       [18, 12]]
n-1 = 5
```

Por tanto:

```text
Œ£ = [[5.6, 3.6],
     [3.6, 2.4]]
```

2) Eigenvalues/eigenvectors (aprox.)

- `Œª1 ‚âà 7.94`, `Œª2 ‚âà 0.06`
- primer eigenvector (normalizado) `u1 ‚âà (0.84, 0.55)`

3) Varianza explicada del primer componente:

`Œª1/(Œª1+Œª2) ‚âà 7.94/8.00 ‚âà 99.3%`

Lectura: la nube est√° casi en una l√≠nea; proyectar a 1D conserva casi toda la estructura.

#### Algoritmo (paso a paso)

1) Centrar (y t√≠picamente escalar si tus features tienen unidades distintas).
2) SVD de `X_c` (recomendado) o eigen de `Œ£`.
3) Elegir `k` por varianza acumulada (y/o error de reconstrucci√≥n).
4) Proyectar `Z = X_c @ V_k`.
5) (Opcional) Reconstruir `X_hat = Z @ V_k·µÄ + mean` para medir p√©rdida.

#### Implementaci√≥n pr√°ctica (c√≥digo)

En esta gu√≠a ya tienes:

- `pca_eigen(...)` en **3.2** (√∫til para entender la teor√≠a).
- `pca_svd(...)` y la clase `PCA` en **3.3‚Äì3.4** (recomendado para pr√°ctica).

#### Evaluaci√≥n formativa (r√°pida)

Pregunta: si `Œª1 = 9` y `Œª2 = 1`, ¬øqu√© proporci√≥n de varianza captura PC1?

Respuesta: `9/(9+1) = 90%`.

#### Actividades activas

- **Retrieval practice:** escribe las 4 ecuaciones (centrar, SVD, proyectar, reconstruir).
- **Experimento m√≠nimo:** genera datos 3D correlacionados, reduce a 1D y reporta:
  - varianza explicada
  - error de reconstrucci√≥n

#### Errores comunes

- **No centrar**: PCA se sesga hacia la media (resultado incorrecto).
- **Confundir `components` vs `X_pca`**: componentes son ejes; `X_pca` son coordenadas en esos ejes.
- **Elegir `n_components` ‚Äúa ojo‚Äù**: usar varianza acumulada.

### 3.1 Concepto

```python
"""
PCA - AN√ÅLISIS DE COMPONENTES PRINCIPALES

Objetivo: Reducir dimensionalidad preservando la m√°xima varianza.

Idea:
1. Centrar los datos (restar media)
2. Encontrar direcciones de m√°xima varianza (eigenvectors)
3. Proyectar datos en las top-k direcciones

Matem√°ticamente:
- Las componentes principales son los eigenvectors de la matriz de covarianza
- Los eigenvalues indican cu√°nta varianza captura cada componente

Aplicaciones:
- Visualizaci√≥n (reducir a 2D/3D)
- Preprocesamiento (eliminar ruido, reducir features)
- Compresi√≥n de datos/im√°genes
"""
```

### 3.2 PCA via Eigendecomposition

```python
import numpy as np

def pca_eigen(X: np.ndarray, n_components: int) -> tuple:
    """
    PCA usando eigendecomposition de la matriz de covarianza.

    Pasos:
    1. Centrar datos: X_centered = X - mean(X)
    2. Calcular matriz de covarianza: Œ£ = (1/(n-1)) X^T X
    3. Eigendecomposition: Œ£v = Œªv
    4. Ordenar eigenvectors por eigenvalue descendente
    5. Proyectar: X_pca = X_centered @ V[:, :k]

    Returns:
        X_pca: datos transformados
        components: eigenvectors (componentes principales)
        explained_variance_ratio: proporci√≥n de varianza por componente
    """
    # 1. Centrar
    mean = np.mean(X, axis=0)
    X_centered = X - mean

    # 2. Matriz de covarianza
    n_samples = X.shape[0]
    cov_matrix = (X_centered.T @ X_centered) / (n_samples - 1)

    # 3. Eigendecomposition
    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

    # Convertir a reales (puede haber componentes imaginarias peque√±as)
    eigenvalues = eigenvalues.real
    eigenvectors = eigenvectors.real

    # 4. Ordenar por eigenvalue descendente
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]

    # 5. Seleccionar top k componentes
    components = eigenvectors[:, :n_components]

    # 6. Proyectar
    X_pca = X_centered @ components

    # 7. Varianza explicada
    total_variance = np.sum(eigenvalues)
    explained_variance_ratio = eigenvalues[:n_components] / total_variance

    return X_pca, components, explained_variance_ratio, mean
```

### 3.3 PCA via SVD (M√°s Estable)

```python
import numpy as np

def pca_svd(X: np.ndarray, n_components: int) -> tuple:
    """
    PCA usando SVD (Singular Value Decomposition).

    M√°s estable num√©ricamente que eigendecomposition.

    Si X = UŒ£V^T, entonces:
    - V contiene las componentes principales
    - Œ£¬≤/(n-1) son los eigenvalues (varianzas)
    """
    # 1. Centrar
    mean = np.mean(X, axis=0)
    X_centered = X - mean

    # 2. SVD
    U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)

    # 3. Componentes principales (filas de Vt, o columnas de V)
    components = Vt[:n_components].T

    # 4. Proyectar
    X_pca = X_centered @ components

    # 5. Varianza explicada
    n_samples = X.shape[0]
    variance = (S ** 2) / (n_samples - 1)
    explained_variance_ratio = variance[:n_components] / np.sum(variance)

    return X_pca, components, explained_variance_ratio, mean
```

### 3.4 Implementaci√≥n Completa

```python
import numpy as np

class PCA:
    """Principal Component Analysis implementado desde cero."""

    def __init__(self, n_components: int = 2):
        """
        Args:
            n_components: n√∫mero de componentes a retener
        """
        self.n_components = n_components
        self.components_ = None  # (n_features, n_components)
        self.explained_variance_ratio_ = None
        self.mean_ = None

    def fit(self, X: np.ndarray) -> 'PCA':
        """Calcula componentes principales."""
        # Centrar
        self.mean_ = np.mean(X, axis=0)
        X_centered = X - self.mean_

        # SVD
        U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)

        # Componentes principales
        self.components_ = Vt[:self.n_components].T

        # Varianza explicada
        n_samples = X.shape[0]
        variance = (S ** 2) / (n_samples - 1)
        self.explained_variance_ratio_ = variance[:self.n_components] / np.sum(variance)
        self.singular_values_ = S[:self.n_components]

        return self

    def transform(self, X: np.ndarray) -> np.ndarray:
        """Proyecta datos a espacio de componentes principales."""
        X_centered = X - self.mean_
        return X_centered @ self.components_

    def fit_transform(self, X: np.ndarray) -> np.ndarray:
        """Fit y transform en un paso."""
        self.fit(X)
        return self.transform(X)

    def inverse_transform(self, X_pca: np.ndarray) -> np.ndarray:
        """
        Reconstruye datos desde el espacio PCA.

        X_reconstructed = X_pca @ components.T + mean

        Nota: hay p√©rdida de informaci√≥n si n_components < n_features
        """
        return X_pca @ self.components_.T + self.mean_

    def get_covariance(self) -> np.ndarray:
        """Retorna matriz de covarianza aproximada."""
        return self.components_ @ np.diag(self.singular_values_ ** 2) @ self.components_.T


# Demo
np.random.seed(42)

# Datos correlacionados en 3D
n_samples = 200
X = np.random.randn(n_samples, 3)
X[:, 1] = X[:, 0] * 2 + np.random.randn(n_samples) * 0.1  # y correlacionado con x
X[:, 2] = X[:, 0] + X[:, 1] + np.random.randn(n_samples) * 0.1

# PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print(f"Shape original: {X.shape}")
print(f"Shape reducido: {X_pca.shape}")
print(f"Varianza explicada: {pca.explained_variance_ratio_}")
print(f"Varianza total: {np.sum(pca.explained_variance_ratio_):.2%}")
```

### 3.5 Reconstrucci√≥n y Error

```python
import numpy as np

def reconstruction_error(X: np.ndarray, pca: PCA) -> float:
    """
    Calcula el error de reconstrucci√≥n.

    Error = ||X - X_reconstructed||¬≤ / ||X||¬≤
    """
    X_pca = pca.transform(X)
    X_reconstructed = pca.inverse_transform(X_pca)

    error = np.sum((X - X_reconstructed) ** 2)
    total = np.sum((X - np.mean(X, axis=0)) ** 2)

    return error / total

def choose_n_components(X: np.ndarray, variance_threshold: float = 0.95) -> int:
    """
    Elige n√∫mero de componentes para retener cierta varianza.

    Args:
        variance_threshold: proporci√≥n de varianza a retener (ej: 0.95 = 95%)
    """
    # PCA con todos los componentes
    pca = PCA(n_components=min(X.shape))
    pca.fit(X)

    # Varianza acumulada
    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)

    # Encontrar n_components
    n_components = np.argmax(cumulative_variance >= variance_threshold) + 1

    return n_components, cumulative_variance
```

---

## üß© Consolidaci√≥n (PCA)

### Errores comunes

- **No centrar:** si no restas la media, el primer componente puede capturar ‚Äúoffset‚Äù en vez de estructura.
- **Confundir `components` con `X_pca`:**
  - `components` = ejes
  - `X_pca` = coordenadas en esos ejes
- **Elegir `n_components` sin criterio:** usa varianza acumulada + error de reconstrucci√≥n.

### Debugging / validaci√≥n (v5)

- Verifica:
  - `X_centered.mean(axis=0)` cerca de 0
  - shapes: `components: (n_features, k)`, `X_pca: (n_samples, k)`
- Si tu reconstrucci√≥n explota, revisa `X_hat = Z @ V_k·µÄ + mean`.
- Registra hallazgos en `study_tools/DIARIO_ERRORES.md`.
- Protocolos completos:
  - [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)
  - [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md)

### Reto Feynman (tablero blanco)

Explica en 5 l√≠neas o menos:

1) ¬øPor qu√© PCA ‚Äúelige un √°ngulo‚Äù y qu√© significa ‚Äúm√°xima varianza‚Äù?
2) ¬øPor qu√© `S¬≤/(n-1)` son eigenvalues de la covarianza?
3) ¬øQu√© pierde la reconstrucci√≥n cuando `k < n_features`?

---

## üíª Parte 5: Gaussian Mixture Models (GMM)

### 5.0 GMM ‚Äî Nivel: intermedio/avanzado (clustering probabil√≠stico)

**Prop√≥sito:** pasar de ‚ÄúK-Means agrupa‚Äù a **entender cu√°ndo K-Means es geom√©tricamente incorrecto** y usar un modelo que capture **clusters el√≠pticos** y asignaci√≥n ‚Äúsuave‚Äù (*soft clustering*).

#### Objetivos de aprendizaje (medibles)

Al terminar este bloque podr√°s:

- **Explicar** por qu√© K-Means asume clusters aproximadamente esf√©ricos (misma varianza en todas direcciones).
- **Describir** un GMM como ‚Äúmezcla de Gaussianas‚Äù con una variable latente de componente.
- **Derivar** la idea operacional del algoritmo EM (E-step y M-step) a nivel implementable.
- **Interpretar** *responsibilities* `Œ≥(z_k)` como probabilidad de pertenencia.
- **Diagnosticar** fallas t√≠picas: colapso de covarianzas, sensibilidad a inicializaci√≥n, singularidad.

#### Intuici√≥n geom√©trica: clusters el√≠pticos y pertenencia suave

Imagina que tus datos forman ‚Äúnubes‚Äù alargadas:

- K-Means solo puede poner centroides y partir el espacio por regiones de Voronoi con distancia euclidiana.
- GMM asume que cada cluster es una **Gaussiana** con su propia forma:
  - media `Œº_k` (centro)
  - covarianza `Œ£_k` (orientaci√≥n y elongaci√≥n)

La diferencia clave es que GMM no dice ‚Äúeste punto es del cluster 2‚Äù. Dice:

> ‚ÄúEste punto es 70% del componente 2 y 30% del componente 1‚Äù.

Eso es extremadamente √∫til cuando los clusters se solapan.

#### Conceptos clave (glosario m√≠nimo)

- **Mezcla:** combinaci√≥n ponderada de distribuciones.
- **Pesos `œÄ_k`:** probabilidades a priori de cada componente (suman 1).
- **Variable latente `z`:** indica qu√© componente ‚Äúgener√≥‚Äù el punto.
- **Responsibilities `Œ≥_{ik}`:** `P(z=k | x_i)`.
- **EM (Expectation-Maximization):** alterna ‚Äúasignar probabilidades‚Äù y ‚Äúre-estimar par√°metros‚Äù.

#### Formalizaci√≥n m√≠nima

Modelo:

`p(x) = Œ£_{k=1..K} œÄ_k  N(x | Œº_k, Œ£_k)`

Log-likelihood de datos `X = {x_i}`:

`‚Ñì = Œ£_i log( Œ£_k œÄ_k N(x_i | Œº_k, Œ£_k) )`

No puedes maximizar esto de forma cerrada por el `log(Œ£ ...)`. EM lo hace iterativamente.

#### EM (idea implementable)

**E-step:** calcula responsibilities

`Œ≥_{ik} = P(z=k | x_i) = (œÄ_k N(x_i|Œº_k,Œ£_k)) / (Œ£_j œÄ_j N(x_i|Œº_j,Œ£_j))`

**M-step:** actualiza par√°metros usando promedios ponderados

- `N_k = Œ£_i Œ≥_{ik}`
- `œÄ_k = N_k / n`
- `Œº_k = (1/N_k) Œ£_i Œ≥_{ik} x_i`
- `Œ£_k = (1/N_k) Œ£_i Œ≥_{ik} (x_i-Œº_k)(x_i-Œº_k)·µÄ`

#### Worked example (m√≠nimo, 1D para ver EM sin √°lgebra pesada)

Sup√≥n puntos 1D `x = [-2, -1, 0, 2, 3]` y `K=2`.

Idea:

1) Inicializas dos Gaussianas (medias distintas).
2) En E-step, los puntos negativos tienen `Œ≥` alto para el componente ‚Äúizquierdo‚Äù y bajo para el derecho.
3) En M-step, la media izquierda se va hacia el promedio ponderado de los negativos, la derecha hacia los positivos.
4) Repites hasta que el log-likelihood deja de mejorar.

La intuici√≥n: es como K-Means, pero en vez de asignar ‚Äúduro‚Äù, asignas *responsabilities* y actualizas con pesos.

#### Cu√°ndo usar GMM vs K-Means (regla pr√°ctica)

- **Usa K-Means** si esperas clusters aproximadamente esf√©ricos, bien separados y quieres simplicidad/velocidad.
- **Usa GMM** si:
  - esperas **clusters el√≠pticos** o con varianzas distintas por direcci√≥n
  - hay **solapamiento** y necesitas pertenencia probabil√≠stica
  - quieres un modelo generativo simple para densidad

#### Errores comunes / debugging

- **No estandarizar features:** si una dimensi√≥n domina, la covarianza se distorsiona.
- **Singularidad/collapse:** una `Œ£_k` puede volverse casi singular si un componente ‚Äúse queda‚Äù con muy pocos puntos.
- **Inicializaci√≥n pobre:** EM converge a √≥ptimos locales; iniciar con K-Means suele ayudar.

---

## üö´ Cu√°ndo NO usar K-Means / PCA (y qu√© hacer en su lugar)

### Diagn√≥stico r√°pido (regla pr√°ctica)

Si no puedes justificar ‚Äúpor qu√© este m√©todo tiene sentido para este dataset‚Äù, asume que est√°s en zona de riesgo.

#### K-Means: se√±ales de que NO es buena idea

- **Geometr√≠a incorrecta:** clusters no convexos (formas tipo ‚Äúdos lunas‚Äù) o estructuras alargadas.
- **Densidades muy distintas:** un cluster muy denso y otro muy disperso.
- **Outliers fuertes:** centroides se mueven para ‚Äúperseguir‚Äù outliers.
- **Escalas distintas:** una feature domina la distancia euclidiana.

**S√≠ntomas medibles t√≠picos:**

- El **m√©todo del codo** no muestra un ‚Äúcodo‚Äù claro.
- **Silhouette score** bajo (cerca de 0) o negativo.
- Resultados muy distintos entre distintas inicializaciones.

**Qu√© hacer en su lugar (seg√∫n el problema):**

- **Clusters el√≠pticos (varianza diferente por direcci√≥n):** GMM (Gaussian Mixture Models).
- **Clusters con formas arbitrarias y ruido:** DBSCAN / HDBSCAN (no implementados aqu√≠, pero recomendados).
- **Estructura jer√°rquica:** Hierarchical clustering.

#### PCA: se√±ales de que NO es buena idea

- **Se√±al no alineada con varianza:** la direcci√≥n con mayor varianza no es la que separa clases (com√∫n en tareas supervisadas).
- **Relaci√≥n no lineal:** datos sobre un manifold curvo (PCA lineal pierde estructura).
- **Interpretaci√≥n equivocada:** usar PCA como ‚Äúselector de features importantes‚Äù sin analizar varianza explicada y reconstrucci√≥n.

**S√≠ntomas medibles t√≠picos:**

- Necesitas muchos componentes para llegar a 95% de varianza (PCA no est√° comprimiendo bien).
- La visualizaci√≥n en 2D parece ‚Äúmezclar‚Äù todo sin estructura (ojo: esto no prueba que no haya estructura, pero es una se√±al).

**Qu√© hacer en su lugar (seg√∫n el objetivo):**

- **Visualizaci√≥n no lineal:** t-SNE / UMAP (√∫tiles para explorar, no para entrenar modelos lineales directamente).
- **Compresi√≥n aprendida:** autoencoders (M√≥dulo 07, enfoque DL).
- **Si solo quieres acelerar:** reducir features por ingenier√≠a o seleccionar por dominio.

### Checklist de decisi√≥n (antes de usar el m√©todo)

- **Datos escalados:** ¬øfeatures comparables? (si no, normaliza).
- **Outliers:** ¬øhay outliers? (si s√≠, documenta su impacto).
- **Objetivo real:** ¬øquieres compresi√≥n, visualizaci√≥n, o clustering interpretable?

Integraci√≥n con ejecuci√≥n y validaci√≥n:

- [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)
- [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md)
- Diario: `study_tools/DIARIO_ERRORES.md`

## üíª Parte 4: Aplicaciones de PCA

### 4.1 Compresi√≥n de Im√°genes

```python
import numpy as np

def compress_image_pca(image: np.ndarray, n_components: int) -> tuple:
    """
    Comprime una imagen usando PCA.

    Args:
        image: imagen grayscale (height, width)
        n_components: n√∫mero de componentes a retener

    Returns:
        imagen comprimida, pca model
    """
    # Tratar filas como muestras
    pca = PCA(n_components=n_components)
    image_pca = pca.fit_transform(image)

    # Reconstruir
    image_reconstructed = pca.inverse_transform(image_pca)

    return image_reconstructed, pca

def compression_ratio_pca(original_shape: tuple, n_components: int) -> float:
    """Calcula ratio de compresi√≥n."""
    height, width = original_shape
    original_size = height * width
    # Almacenamos: componentes + proyecciones + media
    compressed_size = n_components * width + height * n_components + width
    return compressed_size / original_size
```

### 4.2 Visualizaci√≥n en 2D

```python
import numpy as np
import matplotlib.pyplot as plt

def visualize_pca_2d(X: np.ndarray, labels: np.ndarray = None, title: str = "PCA"):
    """Reduce a 2D y visualiza."""
    pca = PCA(n_components=2)
    X_2d = pca.fit_transform(X)

    plt.figure(figsize=(10, 6))

    if labels is not None:
        for label in np.unique(labels):
            mask = labels == label
            plt.scatter(X_2d[mask, 0], X_2d[mask, 1],
                       label=f'Clase {label}', alpha=0.7)
        plt.legend()
    else:
        plt.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.7)

    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} var)')
    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} var)')
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.show()
```

---

## üì¶ Entregable del M√≥dulo

### `unsupervised_learning.py`

```python
"""
Unsupervised Learning Module

Implementaci√≥n desde cero de:
- K-Means Clustering (con K-Means++ initialization)
- PCA (Principal Component Analysis)
- M√©tricas de evaluaci√≥n de clusters

Autor: [Tu nombre]
M√≥dulo: 06 - Unsupervised Learning
"""

import numpy as np
from typing import Tuple, List


# ============================================================
# K-MEANS CLUSTERING
# ============================================================

def kmeans_plus_plus(X: np.ndarray, k: int, seed: int = None) -> np.ndarray:
    """Inicializaci√≥n K-Means++."""
    if seed: np.random.seed(seed)
    n = len(X)
    centroids = [X[np.random.randint(n)]]

    for _ in range(1, k):
        distances = np.array([min(np.sum((x - c)**2) for c in centroids) for x in X])
        probs = distances / distances.sum()
        centroids.append(X[np.random.choice(n, p=probs)])

    return np.array(centroids)


class KMeans:
    def __init__(self, n_clusters=3, max_iter=300, tol=1e-4, seed=None):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.tol = tol
        self.seed = seed
        self.centroids = None
        self.labels_ = None
        self.inertia_ = None
        self.n_iter_ = 0

    def fit(self, X: np.ndarray) -> 'KMeans':
        self.centroids = kmeans_plus_plus(X, self.n_clusters, self.seed)

        for i in range(self.max_iter):
            old_centroids = self.centroids.copy()

            # Asignar
            distances = np.array([[np.sum((x - c)**2) for c in self.centroids] for x in X])
            self.labels_ = np.argmin(distances, axis=1)

            # Actualizar
            for j in range(self.n_clusters):
                points = X[self.labels_ == j]
                if len(points) > 0:
                    self.centroids[j] = points.mean(axis=0)

            if np.sum((self.centroids - old_centroids)**2) < self.tol:
                break

        self.n_iter_ = i + 1
        self.inertia_ = sum(np.sum((X[self.labels_ == j] - self.centroids[j])**2)
                           for j in range(self.n_clusters))
        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        distances = np.array([[np.sum((x - c)**2) for c in self.centroids] for x in X])
        return np.argmin(distances, axis=1)

    def fit_predict(self, X: np.ndarray) -> np.ndarray:
        self.fit(X)
        return self.labels_


# ============================================================
# PCA
# ============================================================

class PCA:
    def __init__(self, n_components: int = 2):
        self.n_components = n_components
        self.components_ = None
        self.explained_variance_ratio_ = None
        self.mean_ = None

    def fit(self, X: np.ndarray) -> 'PCA':
        self.mean_ = X.mean(axis=0)
        X_centered = X - self.mean_

        U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)

        self.components_ = Vt[:self.n_components].T
        variance = (S**2) / (len(X) - 1)
        self.explained_variance_ratio_ = variance[:self.n_components] / variance.sum()

        return self

    def transform(self, X: np.ndarray) -> np.ndarray:
        return (X - self.mean_) @ self.components_

    def fit_transform(self, X: np.ndarray) -> np.ndarray:
        self.fit(X)
        return self.transform(X)

    def inverse_transform(self, X_pca: np.ndarray) -> np.ndarray:
        return X_pca @ self.components_.T + self.mean_


# ============================================================
# M√âTRICAS
# ============================================================

def inertia(X: np.ndarray, labels: np.ndarray, centroids: np.ndarray) -> float:
    """Within-cluster sum of squares."""
    return sum(np.sum((X[labels == i] - centroids[i])**2)
               for i in range(len(centroids)))

def silhouette_score(X: np.ndarray, labels: np.ndarray) -> float:
    """Silhouette score promedio."""
    n = len(X)
    scores = []

    for i in range(n):
        # a: distancia promedio intra-cluster
        same = X[labels == labels[i]]
        a = np.mean([np.sqrt(np.sum((X[i] - x)**2)) for x in same if not np.array_equal(x, X[i])])

        # b: distancia promedio al cluster m√°s cercano
        b = float('inf')
        for label in np.unique(labels):
            if label != labels[i]:
                other = X[labels == label]
                if len(other) > 0:
                    b = min(b, np.mean([np.sqrt(np.sum((X[i] - x)**2)) for x in other]))

        if b == float('inf'):
            scores.append(0)
        else:
            scores.append((b - a) / max(a, b))

    return np.mean(scores)


# ============================================================
# TESTS
# ============================================================

if __name__ == "__main__":
    np.random.seed(42)

    # Test K-Means
    c1 = np.random.randn(50, 2) + [0, 0]
    c2 = np.random.randn(50, 2) + [5, 5]
    c3 = np.random.randn(50, 2) + [10, 0]
    X = np.vstack([c1, c2, c3])

    kmeans = KMeans(n_clusters=3, seed=42)
    labels = kmeans.fit_predict(X)

    print(f"K-Means Inertia: {kmeans.inertia_:.2f}")
    print(f"Silhouette Score: {silhouette_score(X, labels):.4f}")

    # Test PCA
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)
    X_reconstructed = pca.inverse_transform(X_pca)

    print(f"\nPCA Varianza explicada: {pca.explained_variance_ratio_}")
    print(f"Error reconstrucci√≥n: {np.mean((X - X_reconstructed)**2):.6f}")

    print("\n‚úì Todos los tests pasaron!")
```

---

## üîç Shadow Mode: Validaci√≥n con sklearn (v3.3)

> ‚ö†Ô∏è **Regla:** sklearn est√° **prohibido para aprender**, pero es **√∫til para validar**. Si tus resultados difieren de forma grande y consistente, primero asume bug.

### Protocolo m√≠nimo

- **K-Means:** comparar inercia y silhouette para el mismo `k`.
- **PCA:** comparar `explained_variance_ratio_` y reconstrucci√≥n aproximada.

```python
"""
Shadow Mode - Unsupervised Learning
Comparaci√≥n: implementaciones desde cero vs sklearn.
"""

import numpy as np
from sklearn.cluster import KMeans as SklearnKMeans
from sklearn.decomposition import PCA as SklearnPCA


def shadow_mode_kmeans(X: np.ndarray, k: int = 3, seed: int = 42) -> None:
    """Compara inercia de tu K-Means vs sklearn."""
    # Tu implementaci√≥n
    # my = KMeans(n_clusters=k, random_state=seed)
    # my_labels = my.fit_predict(X)
    # my_inertia = my.inertia_

    # Placeholder (reemplazar con tu c√≥digo)
    my_inertia = 0.0

    # sklearn
    sk = SklearnKMeans(n_clusters=k, init="k-means++", n_init=10, random_state=seed)
    sk.fit(X)

    print("=" * 60)
    print("SHADOW MODE: K-Means")
    print("=" * 60)
    print(f"Tu inercia:      {my_inertia:.4f}")
    print(f"sklearn inertia: {sk.inertia_:.4f}")


def shadow_mode_pca(X: np.ndarray, n_components: int = 2) -> None:
    """Compara varianza explicada de tu PCA vs sklearn."""
    # Tu implementaci√≥n
    # my = PCA(n_components=n_components)
    # X_my = my.fit_transform(X)

    # sklearn
    sk = SklearnPCA(n_components=n_components)
    sk.fit(X)

    print("=" * 60)
    print("SHADOW MODE: PCA")
    print("=" * 60)
    print(f"sklearn explained_variance_ratio_: {sk.explained_variance_ratio_}")
```

---

## üß≠ Puente al M√≥dulo 08 (MNIST Analyst)

En la Semana 21 del proyecto:

- **PCA:** lo usas para reducir MNIST y visualizar estructura en 2D (y para acelerar m√©todos posteriores).
- **K-Means:** lo usas para agrupar d√≠gitos sin etiquetas y visualizar centroides como ‚Äúprototipos‚Äù.

Checklist de integraci√≥n:

- **Entrada:** MNIST normalizado a `[0, 1]`.
- **PCA 2D:** gr√°fico con clusters/colores.
- **K-Means:** elegir `k=10` y analizar si los clusters se alinean con d√≠gitos.
- **Salida:** guarda figuras y conclusiones para el informe.

---

## ‚úÖ Checklist de Finalizaci√≥n

- [ ] Implement√© K-Means con inicializaci√≥n K-Means++
- [ ] Entiendo el algoritmo de Lloyd (asignar-actualizar)
- [ ] Puedo calcular inercia y usarla para el m√©todo del codo
- [ ] Implement√© silhouette score
- [ ] Implement√© PCA usando SVD
- [ ] Entiendo varianza explicada y puedo elegir n_components
- [ ] Puedo reconstruir datos desde PCA
- [ ] Apliqu√© PCA para visualizaci√≥n 2D
- [ ] Todos los tests del m√≥dulo pasan

---

## üîó Navegaci√≥n

| Anterior | √çndice | Siguiente |
|----------|--------|-----------|
| [05_SUPERVISED_LEARNING](05_SUPERVISED_LEARNING.md) | [00_INDICE](00_INDICE.md) | [07_DEEP_LEARNING](07_DEEP_LEARNING.md) |
