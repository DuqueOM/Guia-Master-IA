# M√≥dulo 08 - Proyecto Final: NLP Disaster Tweets Pipeline

> **üéØ Objetivo:** Pipeline end-to-end de NLP que demuestra competencia en las 3 √°reas del Pathway
> **Fase:** 3 - Proyecto Integrador | **Semanas 21-24** (4 semanas)
> **Dataset:** **Kaggle NLP with Disaster Tweets** (7,613 tweets etiquetados, clasificaci√≥n binaria)
> **Nivel:** Avanzado (requiere dominio de M05, M06, M07)

---

## üß† ¬øQu√© Estamos Construyendo?

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ   PROYECTO: NLP DISASTER TWEET CLASSIFICATION PIPELINE                      ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                     ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   L√çNEA 1: SUPERVISED LEARNING (CSCA 5622) - Semanas 21-22                  ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Semana 21: EDA + Preprocessing + Vectorizaci√≥n (TF-IDF)               ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Semana 22: Baselines (Logistic Regression, Naive Bayes, SVM)          ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   L√çNEA 2: UNSUPERVISED/REPRESENTATIONS (CSCA 5632) - Impl√≠cito             ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Word Embeddings (GloVe), Representaciones Latentes                    ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   L√çNEA 3: DEEP LEARNING (CSCA 5642) - Semanas 23-24                        ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Semana 23: Bidirectional LSTM + GloVe Embeddings                      ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Semana 24: Transfer Learning (BERT) + Reporte Final                   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   RESULTADO:                                                                ‚îÇ
‚îÇ   Un pipeline que clasifica tweets como desastres reales o metaf√≥ricos      ‚îÇ
‚îÇ   usando t√©cnicas desde TF-IDF hasta Transformers.                          ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

> üí° **Nota:** Este proyecto es M√ÅS COMPLEJO que MNIST porque el texto tiene ambig√ºedad sem√°ntica, ruido, y requiere preprocesamiento especializado.

## üéØ Benchmark principal: F1-Score (no Accuracy)

La gu√≠a enfatiza **F1-Score** sobre Accuracy porque:

- **Desbalance de clases:** ~57% No-Disaster, ~43% Disaster
- **Costo asim√©trico:** Un falso negativo (no detectar desastre real) es m√°s grave que un falso positivo
- **Est√°ndar de la industria:** En detecci√≥n de eventos, F1 es la m√©trica de referencia

Checklist de diagn√≥stico (m√≠nimo):

- **Datos**: distribuci√≥n de clases, longitud de tweets, palabras frecuentes por clase
- **Preprocesamiento**: URLs eliminadas, menciones procesadas, tokenizaci√≥n consistente
- **Vectorizaci√≥n**: coverage de vocabulario, dimensionalidad de TF-IDF
- **Evaluaci√≥n**: F1-Score, Precision, Recall, Matriz de Confusi√≥n

---

## üìö Estructura del Proyecto

### Cronograma (4 Semanas)

| Semana | Fase | Materia Demostrada | Entregable |
|--------|------|-------------------|------------|
| 21 | EDA + Preprocessing | Supervised Learning (prep) | Pipeline de limpieza + EDA notebook |
| 22 | Baselines ML | Supervised Learning | LogReg + NB + m√©tricas |
| 23 | Deep Learning | Deep Learning | Bi-LSTM + GloVe funcionando |
| 24 | Transfer Learning + Reporte | Integraci√≥n | BERT + REPORT.md + comparaci√≥n |

### Estructura de Archivos

```
nlp-disaster-tweets/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py       # Pipeline de limpieza de texto (M√≥dulo 01+05)
‚îÇ   ‚îú‚îÄ‚îÄ features.py            # TF-IDF, embeddings (M√≥dulo 06)
‚îÇ   ‚îú‚îÄ‚îÄ models.py              # Definiciones de modelos (M√≥dulo 05+07)
‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py          # M√©tricas y visualizaci√≥n (M√≥dulo 05)
‚îÇ   ‚îî‚îÄ‚îÄ utils.py               # Funciones auxiliares
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_EDA_Preprocessing.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_Baseline_Models.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 03_Deep_Learning_LSTM.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 04_Transfer_Learning_BERT.ipynb
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ tfidf_logreg.pkl
‚îÇ   ‚îú‚îÄ‚îÄ lstm_best.h5
‚îÇ   ‚îî‚îÄ‚îÄ bert_finetuned/
‚îÇ
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îú‚îÄ‚îÄ REPORT.md              # Reporte acad√©mico final
‚îÇ   ‚îî‚îÄ‚îÄ MODEL_COMPARISON.md    # Benchmark de modelos
‚îÇ
‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îî‚îÄ‚îÄ README_DATASETS.md     # Instrucciones de descarga
‚îÇ
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt
```

---

## üíª Parte 1: Carga de Datos y EDA (Semana 21)

### 1.1 Data Loader para Disaster Tweets

```python
"""SEMANA 21: Carga y Exploraci√≥n del Dataset de Disaster Tweets

El dataset contiene:
- 7,613 tweets etiquetados para entrenamiento
- 3,263 tweets sin etiquetar para test (Kaggle submission)
- Cada tweet: texto + keyword (opcional) + location (opcional)
- 2 clases: 0 = No disaster, 1 = Real disaster

Columnas:
- id: identificador √∫nico del tweet
- keyword: palabra clave relacionada con desastres (puede ser NaN)
- location: ubicaci√≥n del usuario (puede ser NaN, muy ruidosa)
- text: contenido del tweet (m√°ximo 280 caracteres)
- target: 1 = desastre real, 0 = no desastre (solo en train)
"""  # Cierra docstring del m√≥dulo; si faltara, imports quedar√≠an dentro del string

import pandas as pd  # Importa pandas para manipulaci√≥n de DataFrames y lectura de CSV
import numpy as np  # Importa NumPy para operaciones num√©ricas y estad√≠sticas
import matplotlib.pyplot as plt  # Importa matplotlib para visualizaci√≥n de datos
import seaborn as sns  # Importa seaborn para gr√°ficos estad√≠sticos m√°s elegantes
from collections import Counter  # Importa Counter para conteo eficiente de frecuencias
from typing import Tuple, List, Dict  # Importa tipos para anotaciones (no afecta runtime)
import re  # Importa re para expresiones regulares en limpieza de texto


def load_disaster_tweets(train_path: str, test_path: str = None) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """Carga el dataset de Disaster Tweets desde archivos CSV.

    Args:
        train_path: ruta al archivo train.csv
        test_path: ruta al archivo test.csv (opcional, para submission)

    Returns:
        df_train: DataFrame con datos de entrenamiento (incluye 'target')
        df_test: DataFrame con datos de test (sin 'target') o None
    """  # Cierra docstring; el c√≥digo siguiente ejecuta la carga real
    df_train = pd.read_csv(train_path)  # Lee CSV de entrenamiento; pandas infiere dtypes autom√°ticamente
    df_test = pd.read_csv(test_path) if test_path else None  # Lee test CSV si se proporciona ruta
    return df_train, df_test  # Retorna tupla (train, test); test puede ser None


def explore_dataset(df: pd.DataFrame) -> Dict:
    """Realiza EDA b√°sico del dataset y retorna estad√≠sticas.

    Esta funci√≥n es cr√≠tica para entender:
    1. Balance de clases (¬ønecesitamos class_weight?)
    2. Valores faltantes (¬økeyword/location √∫tiles?)
    3. Distribuci√≥n de longitudes (¬ømax_length para padding?)
    """  # Cierra docstring; c√≥digo de exploraci√≥n sigue
    stats = {}  # Diccionario para almacenar estad√≠sticas; se ir√° poblando

    # Estad√≠sticas b√°sicas
    stats['n_samples'] = len(df)  # N√∫mero total de muestras (filas)
    stats['n_features'] = len(df.columns)  # N√∫mero de columnas/features

    # Balance de clases (solo si existe 'target')
    if 'target' in df.columns:  # Verifica que sea conjunto de entrenamiento
        class_counts = df['target'].value_counts()  # Cuenta ocurrencias por clase
        stats['class_distribution'] = class_counts.to_dict()  # Convierte a dict {0: n0, 1: n1}
        stats['class_balance'] = class_counts[1] / class_counts[0]  # Ratio clase_1 / clase_0
        print(f"\nüìä Distribuci√≥n de Clases:")  # Header informativo
        print(f"   No Disaster (0): {class_counts[0]:,} ({class_counts[0]/len(df):.1%})")  # Cuenta clase 0
        print(f"   Disaster (1):    {class_counts[1]:,} ({class_counts[1]/len(df):.1%})")  # Cuenta clase 1

    # Valores faltantes
    missing = df.isnull().sum()  # Cuenta NaN por columna
    stats['missing_values'] = missing.to_dict()  # Almacena como dict
    print(f"\nüìä Valores Faltantes:")  # Header
    for col, count in missing.items():  # Itera columnas con valores faltantes
        if count > 0:  # Solo muestra columnas con NaN
            print(f"   {col}: {count:,} ({count/len(df):.1%})")  # Imprime columna y porcentaje

    # Longitud de tweets
    df['text_length'] = df['text'].apply(len)  # Calcula longitud en caracteres por tweet
    df['word_count'] = df['text'].apply(lambda x: len(x.split()))  # Cuenta palabras por tweet
    stats['avg_text_length'] = df['text_length'].mean()  # Longitud promedio en caracteres
    stats['avg_word_count'] = df['word_count'].mean()  # Conteo promedio de palabras
    stats['max_text_length'] = df['text_length'].max()  # Longitud m√°xima (para padding)

    print(f"\nüìä Estad√≠sticas de Texto:")  # Header
    print(f"   Longitud promedio: {stats['avg_text_length']:.1f} caracteres")  # Promedio chars
    print(f"   Palabras promedio: {stats['avg_word_count']:.1f} palabras")  # Promedio words
    print(f"   Longitud m√°xima: {stats['max_text_length']} caracteres")  # M√°ximo chars

    return stats  # Retorna diccionario con todas las estad√≠sticas


def visualize_class_distribution(df: pd.DataFrame, save_path: str = None):
    """Visualiza distribuci√≥n de clases con gr√°fico de barras.

    Importante para detectar desbalance antes de entrenar.
    """  # Cierra docstring
    fig, ax = plt.subplots(figsize=(8, 5))  # Crea figura de 8x5 pulgadas

    class_counts = df['target'].value_counts()  # Cuenta por clase
    colors = ['#3498db', '#e74c3c']  # Azul para 0, rojo para 1 (convenci√≥n: rojo=alerta)

    bars = ax.bar(['No Disaster (0)', 'Disaster (1)'], class_counts.values, color=colors)  # Barras

    # A√±adir valores encima de las barras
    for bar, count in zip(bars, class_counts.values):  # Itera barras y conteos
        height = bar.get_height()  # Altura de la barra
        ax.annotate(f'{count:,}\n({count/len(df):.1%})',  # Texto con conteo y porcentaje
                   xy=(bar.get_x() + bar.get_width()/2, height),  # Posici√≥n centrada
                   ha='center', va='bottom', fontsize=12)  # Alineaci√≥n y tama√±o

    ax.set_ylabel('N√∫mero de Tweets', fontsize=12)  # Etiqueta eje Y
    ax.set_title('Distribuci√≥n de Clases - Disaster Tweets', fontsize=14)  # T√≠tulo
    ax.set_ylim(0, max(class_counts.values) * 1.15)  # Espacio para anotaciones

    plt.tight_layout()  # Ajusta espaciado
    if save_path:  # Guarda si se proporciona ruta
        plt.savefig(save_path, dpi=150, bbox_inches='tight')  # Alta resoluci√≥n
    plt.show()  # Muestra figura


def visualize_text_lengths(df: pd.DataFrame, save_path: str = None):
    """Visualiza distribuci√≥n de longitudes de texto por clase.

    √ötil para decidir max_length en padding y detectar outliers.
    """  # Cierra docstring
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))  # Dos subplots lado a lado

    # Longitud en caracteres
    for label, color in [(0, '#3498db'), (1, '#e74c3c')]:  # Itera clases con colores
        subset = df[df['target'] == label]['text_length']  # Filtra por clase
        axes[0].hist(subset, bins=50, alpha=0.6, label=f'Class {label}', color=color)  # Histograma
    axes[0].set_xlabel('Longitud (caracteres)', fontsize=11)  # Etiqueta X
    axes[0].set_ylabel('Frecuencia', fontsize=11)  # Etiqueta Y
    axes[0].set_title('Distribuci√≥n de Longitud por Clase', fontsize=12)  # T√≠tulo
    axes[0].legend()  # Leyenda

    # Conteo de palabras
    for label, color in [(0, '#3498db'), (1, '#e74c3c')]:  # Itera clases
        subset = df[df['target'] == label]['word_count']  # Filtra por clase
        axes[1].hist(subset, bins=30, alpha=0.6, label=f'Class {label}', color=color)  # Histograma
    axes[1].set_xlabel('N√∫mero de Palabras', fontsize=11)  # Etiqueta X
    axes[1].set_ylabel('Frecuencia', fontsize=11)  # Etiqueta Y
    axes[1].set_title('Distribuci√≥n de Palabras por Clase', fontsize=12)  # T√≠tulo
    axes[1].legend()  # Leyenda

    plt.tight_layout()  # Ajusta espaciado
    if save_path:  # Guarda si se proporciona ruta
        plt.savefig(save_path, dpi=150, bbox_inches='tight')  # Alta resoluci√≥n
    plt.show()  # Muestra figura


def get_top_keywords(df: pd.DataFrame, n: int = 20) -> pd.DataFrame:
    """Obtiene las keywords m√°s frecuentes por clase.

    Las keywords son proporcionadas por Kaggle y pueden ser √∫tiles como features.
    """  # Cierra docstring
    results = []  # Lista para almacenar resultados

    for label in [0, 1]:  # Itera por cada clase
        subset = df[df['target'] == label]['keyword'].dropna()  # Filtra y elimina NaN
        top_kw = Counter(subset).most_common(n)  # Top n keywords m√°s frecuentes
        for kw, count in top_kw:  # Itera keywords y conteos
            results.append({'class': label, 'keyword': kw, 'count': count})  # A√±ade a resultados

    return pd.DataFrame(results)  # Retorna como DataFrame para an√°lisis


# === DEMO: C√≥mo usar el Data Loader ===
if __name__ == "__main__":
    # Cargar datos (ajustar rutas seg√∫n ubicaci√≥n)
    df_train, _ = load_disaster_tweets('data/train.csv')

    # Explorar
    stats = explore_dataset(df_train)

    # Visualizar
    visualize_class_distribution(df_train)
    visualize_text_lengths(df_train)

    # Keywords
    top_kw = get_top_keywords(df_train)
    print("\nüìä Top Keywords por Clase:")
    print(top_kw.head(20))
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 1.1: Data Loader (Disaster Tweets)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Data loader robusto: EDA sistem√°tico para texto y detecci√≥n de desbalance
- **ID (opcional):** `M08-NLP-01_1`
- **Duraci√≥n estimada:** 90‚Äì150 min
- **Nivel:** Intermedio-Avanzado
- **Dependencias:** Pandas b√°sico, matplotlib, comprensi√≥n de clasificaci√≥n binaria

#### 2) Objetivos
- Cargar correctamente el dataset y entender la estructura de columnas.
- Identificar desbalance de clases y decidir estrategias (class_weight, oversampling).
- Analizar distribuci√≥n de longitudes para decidir max_length en tokenizaci√≥n.
- Detectar valores faltantes y decidir si keyword/location son √∫tiles.

#### 3) Relevancia
- Si no entiendes tus datos, entrenar√°s modelos con sesgos ocultos.
- El desbalance (57/43) requiere usar F1-Score en lugar de Accuracy.
- La longitud de tweets afecta directamente el padding y la arquitectura LSTM/BERT.

#### 4) Mapa conceptual m√≠nimo
- **CSV** ‚Üí DataFrame ‚Üí **EDA** (clases, longitudes, missing) ‚Üí Decisiones de dise√±o.
- **Desbalance** ‚Üí `class_weight='balanced'` o F1 como m√©trica.
- **Longitud** ‚Üí `max_length` para padding.

#### 5) Definiciones esenciales
- **Target:** Variable objetivo binaria (0/1).
- **Keyword:** Palabra clave pre-asignada por Kaggle (puede ser NaN).
- **Desbalance:** Cuando una clase tiene significativamente m√°s muestras.

#### 6) Explicaci√≥n did√°ctica
- El ratio 57/43 es desbalance "leve" pero suficiente para que Accuracy enga√±e.
- Tweets cortos (< 10 palabras) pueden ser m√°s ambiguos; tweets largos dan m√°s contexto.

#### 7) Ejemplo modelado
- Si `class_balance ‚âà 0.75`, significa que hay ~3 tweets de clase 0 por cada 4 de clase 1.

#### 8) Pr√°ctica guiada
- Imprime 5 ejemplos de cada clase y analiza manualmente si son f√°ciles/dif√≠ciles de clasificar.

#### 9) Pr√°ctica independiente
- Crea un histograma de longitudes separado por clase y analiza si hay diferencias sistem√°ticas.

#### 10) Autoevaluaci√≥n
- ¬øPor qu√© no usamos `location` como feature directamente?
- ¬øQu√© pasa si ignoramos el desbalance y usamos Accuracy?

#### 11) Errores comunes
- Asumir que el dataset est√° balanceado sin verificar.
- No revisar valores faltantes en keyword/location.
- Usar max_length demasiado peque√±o y truncar informaci√≥n √∫til.

#### 12) Retenci√≥n
- Checklist EDA: `shape`, `dtypes`, `class_counts`, `missing`, `length_distribution`.

#### 13) Diferenciaci√≥n
- Avanzado: Analizar si hay correlaci√≥n entre `keyword` y `target` (podr√≠a ser leakage).

#### 14) Recursos
- Pandas documentation, Kaggle competition page, papers sobre text classification.

#### 15) Nota docente
- Pide que el alumno identifique 3 tweets "dif√≠ciles" y explique por qu√© son ambiguos.
</details>

---

### 1.2 Pipeline de Preprocesamiento de Texto

```python
"""SEMANA 21: Preprocesamiento de Texto para NLP

El preprocesamiento es CR√çTICO en NLP porque:
1. Tweets tienen ruido √∫nico: URLs, menciones, hashtags, emojis
2. La misma informaci√≥n puede expresarse de muchas formas
3. Errores aqu√≠ se propagan a todo el pipeline

Pipeline est√°ndar:
1. Normalizaci√≥n (lowercase)
2. Limpieza (URLs, menciones, caracteres especiales)
3. Tokenizaci√≥n (dividir en palabras/tokens)
4. Normalizaci√≥n l√©xica (stemming/lemmatization)
5. Filtrado (stopwords, tokens cortos)
"""  # Cierra docstring del m√≥dulo

import re  # Importa re para expresiones regulares; core de limpieza de texto
import string  # Importa string para constantes como punctuation
from typing import List, Optional  # Tipos para anotaciones
import numpy as np  # NumPy para operaciones vectorizadas


def download_nltk_resources():
    """Descarga recursos NLTK necesarios (solo primera vez).

    NLTK requiere datos externos que no vienen con el paquete.
    """  # Cierra docstring
    import nltk  # Import local para evitar dependencia si no se usa
    resources = ['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger']  # Recursos necesarios
    for resource in resources:  # Itera recursos
        try:
            nltk.data.find(f'tokenizers/{resource}' if resource == 'punkt'  # Ruta var√≠a por tipo
                          else f'corpora/{resource}' if resource in ['stopwords', 'wordnet']
                          else f'taggers/{resource}')  # Intenta encontrar recurso
        except LookupError:  # Si no existe
            print(f"Descargando {resource}...")  # Informa al usuario
            nltk.download(resource, quiet=True)  # Descarga silenciosamente


class TextPreprocessor:
    """Pipeline de preprocesamiento para tweets.

    Dise√±ado espec√≠ficamente para texto de redes sociales:
    - Maneja URLs, menciones, hashtags
    - Normaliza elongaciones ("loooove" ‚Üí "love")
    - Opcionalmente remueve stopwords y lematiza

    Attributes:
        remove_stopwords: Si True, elimina stopwords comunes
        lemmatize: Si True, aplica lematizaci√≥n
        min_word_length: Longitud m√≠nima de palabras a conservar
        stop_words: Set de stopwords en ingl√©s
        lemmatizer: Instancia de WordNetLemmatizer
    """  # Cierra docstring de clase

    # Patrones regex compilados (m√°s eficiente que compilar cada vez)
    URL_PATTERN = re.compile(r'http\S+|www\S+|https\S+', re.MULTILINE)  # Patr√≥n para URLs
    MENTION_PATTERN = re.compile(r'@\w+')  # Patr√≥n para menciones (@usuario)
    HASHTAG_PATTERN = re.compile(r'#(\w+)')  # Patr√≥n para hashtags; captura palabra sin #
    HTML_PATTERN = re.compile(r'<[^>]+>')  # Patr√≥n para tags HTML
    SPECIAL_CHAR_PATTERN = re.compile(r'[^a-zA-Z\s]')  # Todo excepto letras y espacios
    WHITESPACE_PATTERN = re.compile(r'\s+')  # M√∫ltiples espacios
    ELONGATION_PATTERN = re.compile(r'(.)\1{2,}')  # Caracteres repetidos 3+ veces

    def __init__(  # Constructor: configura el pipeline
        self,
        remove_stopwords: bool = True,  # Si eliminar stopwords
        lemmatize: bool = True,  # Si aplicar lematizaci√≥n
        min_word_length: int = 2,  # Longitud m√≠nima de tokens
        lowercase: bool = True  # Si convertir a min√∫sculas
    ):
        """Inicializa el preprocesador con opciones configurables."""  # Docstring breve
        self.remove_stopwords = remove_stopwords  # Guarda flag de stopwords
        self.lemmatize = lemmatize  # Guarda flag de lematizaci√≥n
        self.min_word_length = min_word_length  # Guarda longitud m√≠nima
        self.lowercase = lowercase  # Guarda flag de lowercase

        # Inicializar recursos NLTK
        download_nltk_resources()  # Asegura que recursos est√©n disponibles

        from nltk.corpus import stopwords  # Import local para evitar error si NLTK no est√°
        from nltk.stem import WordNetLemmatizer  # Lematizador basado en WordNet
        from nltk.tokenize import TweetTokenizer  # Tokenizador especializado para tweets

        self.stop_words = set(stopwords.words('english'))  # Set de stopwords (b√∫squeda O(1))
        self.lemmatizer = WordNetLemmatizer()  # Instancia de lematizador
        self.tokenizer = TweetTokenizer(  # Tokenizador para tweets
            preserve_case=False,  # Convierte a min√∫sculas
            reduce_len=True,  # Reduce elongaciones ("loooove" ‚Üí "loove")
            strip_handles=True  # Elimina menciones @usuario
        )

    def clean_text(self, text: str) -> str:
        """Limpieza b√°sica de texto: URLs, menciones, caracteres especiales.

        Args:
            text: Texto crudo del tweet

        Returns:
            Texto limpio sin URLs, menciones, ni caracteres especiales
        """  # Cierra docstring
        if not isinstance(text, str):  # Maneja NaN o tipos no-string
            return ""  # Retorna string vac√≠o para valores inv√°lidos

        # 1. Lowercase (opcional)
        if self.lowercase:  # Si flag est√° activo
            text = text.lower()  # Convierte todo a min√∫sculas

        # 2. Eliminar URLs
        text = self.URL_PATTERN.sub('', text)  # Reemplaza URLs con string vac√≠o

        # 3. Eliminar menciones (@usuario)
        text = self.MENTION_PATTERN.sub('', text)  # Reemplaza menciones con vac√≠o

        # 4. Procesar hashtags (conservar palabra, eliminar #)
        text = self.HASHTAG_PATTERN.sub(r'\1', text)  # Captura grupo 1 (palabra sin #)

        # 5. Eliminar HTML tags
        text = self.HTML_PATTERN.sub('', text)  # Elimina <tags>

        # 6. Reducir elongaciones ("loooove" ‚Üí "loo")
        text = self.ELONGATION_PATTERN.sub(r'\1\1', text)  # M√°ximo 2 repeticiones

        # 7. Eliminar caracteres especiales y n√∫meros
        text = self.SPECIAL_CHAR_PATTERN.sub(' ', text)  # Reemplaza con espacio

        # 8. Normalizar espacios
        text = self.WHITESPACE_PATTERN.sub(' ', text).strip()  # Un solo espacio, sin trailing

        return text  # Retorna texto limpio

    def tokenize(self, text: str) -> List[str]:
        """Tokeniza texto en lista de palabras.

        Usa TweetTokenizer de NLTK optimizado para redes sociales.
        """  # Cierra docstring
        return self.tokenizer.tokenize(text)  # Tokeniza usando NLTK TweetTokenizer

    def remove_stops(self, tokens: List[str]) -> List[str]:
        """Elimina stopwords de la lista de tokens.

        Stopwords son palabras muy frecuentes que aportan poco significado:
        "the", "is", "at", "which", "on", etc.
        """  # Cierra docstring
        return [t for t in tokens if t not in self.stop_words]  # Filtra stopwords

    def lemmatize_tokens(self, tokens: List[str]) -> List[str]:
        """Aplica lematizaci√≥n: reduce palabras a su forma base.

        Ejemplos:
        - "running" ‚Üí "run"
        - "better" ‚Üí "good" (con POS tag correcto)
        - "studies" ‚Üí "study"
        """  # Cierra docstring
        return [self.lemmatizer.lemmatize(t) for t in tokens]  # Lematiza cada token

    def filter_by_length(self, tokens: List[str]) -> List[str]:
        """Filtra tokens por longitud m√≠nima.

        Tokens muy cortos (1 car√°cter) suelen ser ruido.
        """  # Cierra docstring
        return [t for t in tokens if len(t) >= self.min_word_length]  # Filtra por longitud

    def preprocess(self, text: str, return_tokens: bool = False):
        """Pipeline completo de preprocesamiento.

        Ejecuta todos los pasos en orden:
        1. clean_text() - Limpieza b√°sica
        2. tokenize() - Dividir en tokens
        3. remove_stops() - Eliminar stopwords (opcional)
        4. lemmatize_tokens() - Lematizar (opcional)
        5. filter_by_length() - Filtrar tokens cortos

        Args:
            text: Texto crudo a preprocesar
            return_tokens: Si True, retorna lista de tokens; si False, string

        Returns:
            Texto preprocesado como string o lista de tokens
        """  # Cierra docstring
        # Paso 1: Limpieza
        text = self.clean_text(text)  # Aplica limpieza b√°sica

        # Paso 2: Tokenizaci√≥n
        tokens = self.tokenize(text)  # Divide en tokens

        # Paso 3: Eliminar stopwords (opcional)
        if self.remove_stopwords:  # Si flag activo
            tokens = self.remove_stops(tokens)  # Filtra stopwords

        # Paso 4: Lematizaci√≥n (opcional)
        if self.lemmatize:  # Si flag activo
            tokens = self.lemmatize_tokens(tokens)  # Lematiza tokens

        # Paso 5: Filtrar por longitud
        tokens = self.filter_by_length(tokens)  # Elimina tokens cortos

        # Retornar en formato solicitado
        if return_tokens:  # Si se piden tokens
            return tokens  # Lista de strings
        return ' '.join(tokens)  # String con tokens separados por espacio

    def preprocess_batch(self, texts: List[str], return_tokens: bool = False) -> List:
        """Preprocesa m√∫ltiples textos.

        √ötil para procesar todo el DataFrame de una vez.
        """  # Cierra docstring
        return [self.preprocess(text, return_tokens) for text in texts]  # Aplica a cada texto


# === DEMO: C√≥mo usar el Preprocesador ===
if __name__ == "__main__":
    # Ejemplos de tweets
    tweets = [
        "BREAKING: Massive earthquake hits California! Stay safe! http://t.co/xyz @CNN #earthquake",
        "My mixtape is so fire it's causing earthquakes üî•üî•üî• @DJ_Fire",
        "Prayers for the victims of the flooding in Houston. #HoustonStrong",
        "I'm DYINGGGG of laughter at this video üòÇüòÇüòÇ #dead #funny"
    ]

    # Crear preprocesador
    preprocessor = TextPreprocessor(remove_stopwords=True, lemmatize=True)

    # Procesar tweets
    print("=" * 60)
    print("DEMO: TextPreprocessor")
    print("=" * 60)

    for tweet in tweets:
        clean = preprocessor.preprocess(tweet)
        print(f"\nOriginal: {tweet}")
        print(f"Limpio:   {clean}")
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 1.2: Pipeline de Preprocesamiento</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Preprocesamiento de texto: de tweet crudo a tokens limpios
- **ID (opcional):** `M08-NLP-01_2`
- **Duraci√≥n estimada:** 120‚Äì180 min
- **Nivel:** Intermedio
- **Dependencias:** Regex b√°sico, NLTK, comprensi√≥n de tokenizaci√≥n

#### 2) Objetivos
- Implementar un pipeline de limpieza robusto para tweets.
- Entender cada paso del preprocesamiento y su justificaci√≥n.
- Saber cu√°ndo usar/no usar stopwords y lematizaci√≥n.
- Manejar casos edge: NaN, texto vac√≠o, caracteres especiales.

#### 3) Relevancia
- El 80% del √©xito en NLP depende del preprocesamiento.
- Errores aqu√≠ (ej: eliminar negaciones "not") destruyen el modelo.
- Tweets tienen ruido √∫nico que pipelines gen√©ricos no manejan.

#### 4) Mapa conceptual m√≠nimo
- **Texto crudo** ‚Üí limpieza ‚Üí tokenizaci√≥n ‚Üí normalizaci√≥n ‚Üí **tokens limpios**.
- **Regex** es la herramienta core para patrones de texto.
- **Lematizaci√≥n** reduce vocabulario sin perder sem√°ntica.

#### 5) Definiciones esenciales
- **Token:** Unidad at√≥mica de texto (palabra, s√≠mbolo).
- **Stopword:** Palabra muy frecuente con poco contenido sem√°ntico.
- **Lematizaci√≥n:** Reducci√≥n a forma can√≥nica ("running" ‚Üí "run").
- **Stemming:** Reducci√≥n heur√≠stica que puede generar no-palabras.

#### 6) Explicaci√≥n did√°ctica
- Cada regex tiene un prop√≥sito espec√≠fico; docum√©ntalos.
- El orden de operaciones importa: lowercase antes de regex case-sensitive.
- Elongaciones ("loooove") son comunes en redes sociales y deben normalizarse.

#### 7) Ejemplo modelado
- Tweet: "BREAKING: Fire in LA! http://t.co/x @LAFD #LAFire"
- Despu√©s de clean_text: "breaking fire la lafire"
- Despu√©s de lematizaci√≥n: "break fire la lafire"

#### 8) Pr√°ctica guiada
- Procesa 10 tweets manualmente y verifica que el output tiene sentido.
- Identifica un caso donde lematizaci√≥n cambia el significado incorrectamente.

#### 9) Pr√°ctica independiente
- A√±ade manejo de emojis: ¬øconvertirlos a texto o eliminarlos?
- Implementa detecci√≥n de negaciones para no eliminar "not" como stopword.

#### 10) Autoevaluaci√≥n
- ¬øPor qu√© conservamos la palabra del hashtag pero eliminamos el #?
- ¬øQu√© problemas causa eliminar stopwords para frases como "this is not good"?

#### 11) Errores comunes
- Eliminar "not", "no", "never" como stopwords (destruye negaciones).
- No manejar NaN/None correctamente (crash del pipeline).
- Regex demasiado agresivos que eliminan informaci√≥n √∫til.
- Olvidar normalizar espacios m√∫ltiples.

#### 12) Retenci√≥n
- Mantra: "Preprocesa conservadoramente; es m√°s f√°cil limpiar m√°s que recuperar informaci√≥n."

#### 13) Diferenciaci√≥n
- Avanzado: Usar SpaCy para NER y conservar entidades nombradas.
- Avanzado: Implementar spell correction para errores tipogr√°ficos.

#### 14) Recursos
- NLTK Book cap√≠tulo 3, regex101.com para probar patrones, SpaCy docs.

#### 15) Nota docente
- Pide que el alumno encuentre un tweet donde el preprocesamiento falla y proponga una soluci√≥n.
</details>

---

## ÔøΩ Parte 2: Vectorizaci√≥n y Modelos Baseline (Semana 22)

### 2.1 TF-IDF Vectorization

```python
"""SEMANA 22: Vectorizaci√≥n de Texto con TF-IDF

TF-IDF (Term Frequency - Inverse Document Frequency) convierte texto a vectores num√©ricos.
Es el est√°ndar para modelos de ML cl√°sicos (LogReg, SVM, Naive Bayes).

F√≥rmula matem√°tica:
    TF-IDF(t, d, D) = TF(t, d) √ó IDF(t, D)

    donde:
    - TF(t, d) = frecuencia del t√©rmino t en documento d (o 1 + log(tf) si sublinear)
    - IDF(t, D) = log(N / df(t)) + 1  (sklearn a√±ade 1 para suavizar)
    - N = n√∫mero total de documentos
    - df(t) = n√∫mero de documentos que contienen t

Intuici√≥n: Palabras frecuentes en un documento pero raras en el corpus son m√°s informativas.
Ejemplo: "earthquake" es raro globalmente pero frecuente en tweets de desastres ‚Üí alto TF-IDF.
"""  # Cierra docstring del m√≥dulo; c√≥digo ejecutable sigue

import numpy as np  # NumPy para operaciones num√©ricas y manejo de arrays sparse
import pandas as pd  # Pandas para manejo de DataFrames
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer  # Vectorizadores de sklearn
from sklearn.model_selection import train_test_split  # Split estratificado train/test
from typing import Tuple, List, Dict  # Tipos para anotaciones de funciones


class TextVectorizer:
    """Wrapper para vectorizaci√≥n de texto con TF-IDF o Bag of Words.

    Esta clase encapsula TfidfVectorizer de sklearn con configuraci√≥n
    optimizada para clasificaci√≥n de texto en redes sociales.

    Par√°metros clave y su efecto:
    - max_features: Limita vocabulario para evitar overfitting y reducir memoria
    - ngram_range: (1,2) captura unigramas Y bigramas ("new york" como feature)
    - min_df: Ignora palabras que aparecen en < N documentos (ruido)
    - max_df: Ignora palabras que aparecen en > X% de documentos (stopwords impl√≠citas)
    - sublinear_tf: Usa 1+log(tf) para reducir impacto de alta frecuencia

    Attributes:
        vectorizer: Instancia de TfidfVectorizer o CountVectorizer
        method: 'tfidf' o 'bow' seg√∫n m√©todo elegido
    """  # Cierra docstring de clase; atributos de clase siguen

    def __init__(  # Constructor: configura el vectorizador con hiperpar√°metros
        self,
        method: str = 'tfidf',  # M√©todo: 'tfidf' (recomendado) o 'bow'
        max_features: int = 5000,  # Tama√±o m√°ximo del vocabulario (5000 es buen balance)
        ngram_range: Tuple[int, int] = (1, 2),  # Rango de n-gramas: unigramas + bigramas
        min_df: int = 2,  # Frecuencia m√≠nima de documento (elimina palabras muy raras)
        max_df: float = 0.95,  # Frecuencia m√°xima (proporci√≥n; elimina palabras muy comunes)
        sublinear_tf: bool = True  # Usa 1 + log(tf) en lugar de tf crudo
    ):
        """Inicializa el vectorizador con par√°metros configurables."""  # Docstring breve
        self.method = method  # Guarda m√©todo para referencia posterior
        self.max_features = max_features  # Guarda para reporting
        self.ngram_range = ngram_range  # Guarda para reporting

        if method == 'tfidf':  # Si se elige TF-IDF (recomendado para clasificaci√≥n)
            self.vectorizer = TfidfVectorizer(  # Crea instancia de TfidfVectorizer
                max_features=max_features,  # Limita vocabulario; evita curse of dimensionality
                ngram_range=ngram_range,  # (1,2) = unigramas + bigramas
                min_df=min_df,  # Ignora palabras muy raras (aparecen en < 2 docs)
                max_df=max_df,  # Ignora palabras muy comunes (aparecen en > 95% docs)
                sublinear_tf=sublinear_tf,  # Escala logar√≠tmica para TF (mejor para texto)
                strip_accents='unicode',  # Normaliza acentos (caf√© ‚Üí cafe)
                lowercase=True,  # Convierte a min√∫sculas (Ya lo hace preprocesador, pero por seguridad)
                token_pattern=r'\b[a-zA-Z]{2,}\b'  # Solo palabras de 2+ letras (ignora n√∫meros sueltos)
            )
        else:  # Si se elige Bag of Words (conteos crudos)
            self.vectorizer = CountVectorizer(  # Crea instancia de CountVectorizer
                max_features=max_features,  # Limita vocabulario
                ngram_range=ngram_range,  # Rango de n-gramas
                min_df=min_df,  # Frecuencia m√≠nima
                max_df=max_df,  # Frecuencia m√°xima
                strip_accents='unicode',  # Normaliza acentos
                lowercase=True,  # Convierte a min√∫sculas
                token_pattern=r'\b[a-zA-Z]{2,}\b'  # Solo palabras de 2+ letras
            )

    def fit(self, texts: List[str]) -> 'TextVectorizer':
        """Ajusta el vectorizador al corpus (aprende vocabulario).

        CR√çTICO: Solo hacer fit en datos de ENTRENAMIENTO.
        Hacer fit en test causa data leakage (el modelo "ve" palabras del futuro).

        Args:
            texts: Lista de textos preprocesados

        Returns:
            self para permitir chaining: vectorizer.fit(X).transform(X)
        """  # Cierra docstring; c√≥digo de fit sigue
        self.vectorizer.fit(texts)  # Aprende vocabulario del corpus de entrenamiento
        return self  # Retorna self para chaining

    def transform(self, texts: List[str]) -> np.ndarray:
        """Transforma textos a vectores usando vocabulario ya aprendido.

        Los textos de test se transforman con el vocabulario de train.
        Palabras nuevas (OOV - Out of Vocabulary) se ignoran.

        Args:
            texts: Lista de textos a transformar

        Returns:
            Matriz sparse de shape (n_samples, n_features)
        """  # Cierra docstring
        return self.vectorizer.transform(texts)  # Aplica transformaci√≥n con vocab existente

    def fit_transform(self, texts: List[str]) -> np.ndarray:
        """Fit y transform en un solo paso (SOLO para datos de train).

        Equivalente a: vectorizer.fit(texts).transform(texts)
        M√°s eficiente porque no recorre los datos dos veces.
        """  # Cierra docstring
        return self.vectorizer.fit_transform(texts)  # Fit + transform combinados

    def get_feature_names(self) -> np.ndarray:
        """Retorna nombres de las features (vocabulario aprendido).

        √ötil para:
        1. Inspeccionar qu√© palabras/n-gramas est√°n en el vocabulario
        2. Interpretar coeficientes de modelos lineales
        3. Debug: verificar que el vocabulario tiene sentido
        """  # Cierra docstring
        return self.vectorizer.get_feature_names_out()  # Retorna array de palabras/n-gramas

    def get_vocabulary_stats(self) -> Dict:
        """Retorna estad√≠sticas del vocabulario aprendido."""  # Docstring breve
        vocab = self.get_feature_names()  # Obtiene vocabulario
        unigrams = [w for w in vocab if ' ' not in w]  # Palabras sin espacio = unigramas
        bigrams = [w for w in vocab if ' ' in w]  # Palabras con espacio = bigramas
        return {  # Dict con estad√≠sticas
            'total_features': len(vocab),  # Total de features
            'unigrams': len(unigrams),  # N√∫mero de unigramas
            'bigrams': len(bigrams),  # N√∫mero de bigramas
            'sample_unigrams': unigrams[:10],  # Muestra de unigramas
            'sample_bigrams': bigrams[:10] if bigrams else []  # Muestra de bigramas
        }


def prepare_train_test_split(  # Funci√≥n de utilidad para preparar datos completos
    df: pd.DataFrame,
    text_column: str = 'text_clean',  # Columna con texto preprocesado
    target_column: str = 'target',  # Columna con labels
    test_size: float = 0.2,  # 20% para test (est√°ndar)
    random_state: int = 42  # Semilla para reproducibilidad
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, TextVectorizer]:
    """Prepara datos para entrenamiento: split + vectorizaci√≥n.

    Pipeline completo:
    1. Split estratificado (mantiene proporci√≥n de clases)
    2. Fit vectorizador SOLO en train
    3. Transform ambos conjuntos con el mismo vocabulario

    IMPORTANTE: Siempre fit en train, transform en test.

    Args:
        df: DataFrame con columnas de texto y target
        text_column: Nombre de columna con texto preprocesado
        target_column: Nombre de columna con labels (0/1)
        test_size: Proporci√≥n para test (0.2 = 20%)
        random_state: Semilla para reproducibilidad

    Returns:
        Tuple de (X_train, X_test, y_train, y_test, vectorizer)
    """  # Cierra docstring; c√≥digo de preparaci√≥n sigue
    # 1. Split estratificado (mantiene proporci√≥n 57/43 en ambos conjuntos)
    X_train_text, X_test_text, y_train, y_test = train_test_split(
        df[text_column],  # Textos preprocesados
        df[target_column],  # Labels binarios
        test_size=test_size,  # Proporci√≥n de test
        random_state=random_state,  # Reproducibilidad
        stratify=df[target_column]  # CR√çTICO: estratificar por clase para mantener balance
    )

    # 2. Crear y ajustar vectorizador SOLO en train
    vectorizer = TextVectorizer(  # Instancia con configuraci√≥n optimizada
        method='tfidf',  # TF-IDF es mejor que BoW para clasificaci√≥n
        max_features=5000,  # 5000 features es buen balance precisi√≥n/eficiencia
        ngram_range=(1, 2)  # Unigramas + bigramas
    )

    # 3. Fit en train, transform ambos
    X_train = vectorizer.fit_transform(X_train_text.tolist())  # FIT + transform en train
    X_test = vectorizer.transform(X_test_text.tolist())  # Solo transform en test (usa vocab de train)

    # 4. Reportar estad√≠sticas
    print(f"\nüìä Datos Preparados para ML:")  # Header informativo
    print(f"   X_train: {X_train.shape} (samples √ó features)")  # Shape de matriz train
    print(f"   X_test:  {X_test.shape}")  # Shape de matriz test
    print(f"   y_train: {len(y_train)} ({y_train.mean():.1%} positivos)")  # Balance en train
    print(f"   y_test:  {len(y_test)} ({y_test.mean():.1%} positivos)")  # Balance en test

    vocab_stats = vectorizer.get_vocabulary_stats()  # Estad√≠sticas de vocabulario
    print(f"\nüìä Vocabulario:")  # Header
    print(f"   Total features: {vocab_stats['total_features']:,}")  # Total
    print(f"   Unigramas: {vocab_stats['unigrams']:,}")  # Unigramas
    print(f"   Bigramas: {vocab_stats['bigrams']:,}")  # Bigramas

    return X_train, X_test, y_train.values, y_test.values, vectorizer  # Retorna todo
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 2.1: TF-IDF Vectorization</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** TF-IDF: transformando texto a vectores num√©ricos para ML
- **ID (opcional):** `M08-NLP-02_1`
- **Duraci√≥n estimada:** 90‚Äì120 min
- **Nivel:** Intermedio
- **Dependencias:** Preprocesamiento de texto (1.2), conceptos de matrices sparse

#### 2) Objetivos
- Entender la intuici√≥n matem√°tica detr√°s de TF-IDF.
- Configurar TfidfVectorizer con par√°metros apropiados para el problema.
- Evitar data leakage: fit SOLO en train, transform en test.
- Analizar el vocabulario aprendido e interpretar features.

#### 3) Relevancia
- TF-IDF es el baseline est√°ndar en NLP cl√°sico; si falla, revisa preprocesamiento.
- Es completamente interpretable: puedes ver exactamente qu√© palabras importan.
- Es extremadamente r√°pido: segundos de entrenamiento vs horas para BERT.
- Funciona sorprendentemente bien para muchos problemas de clasificaci√≥n.

#### 4) Mapa conceptual m√≠nimo
```
Texto ‚Üí Tokenizaci√≥n ‚Üí Conteo (TF) ‚Üí Ponderaci√≥n (IDF) ‚Üí Vector TF-IDF
                                          ‚Üì
                       Palabras raras tienen mayor peso
```

#### 5) Definiciones esenciales
- **TF (Term Frequency):** Frecuencia del t√©rmino en el documento. Mide relevancia local.
- **IDF (Inverse Document Frequency):** log(N/df). Penaliza t√©rminos que aparecen en muchos documentos.
- **N-grama:** Secuencia de n tokens. Bigrama "new york" captura contexto que unigramas pierden.
- **Sparse matrix:** Matriz donde mayor√≠a de elementos son 0. Eficiente en memoria (CSR format).

#### 6) Explicaci√≥n did√°ctica
- Si "earthquake" aparece en 5% de documentos pero 10 veces en un tweet espec√≠fico ‚Üí alto TF-IDF.
- Si "the" aparece en 99% de documentos ‚Üí IDF ‚âà 0 ‚Üí contribuci√≥n m√≠nima.
- Bigramas como "breaking news" o "stay safe" pueden ser m√°s discriminativos que unigramas.

#### 7) Ejemplo modelado
- Tweet: "earthquake in california, buildings collapsed"
- Unigramas con alto TF-IDF: "earthquake", "collapsed", "buildings"
- Bigramas con alto TF-IDF: "buildings collapsed"
- Palabras con bajo TF-IDF: "in" (muy com√∫n)

#### 8) Pr√°ctica guiada
- Vectoriza el corpus de train y examina `vectorizer.get_feature_names()`.
- Encuentra los 10 bigramas m√°s frecuentes y analiza si son √∫tiles.

#### 9) Pr√°ctica independiente
- Experimenta con `ngram_range=(1,3)` (trigramas) y compara el tama√±o del vocabulario.
- Prueba diferentes valores de `max_features` (1000, 5000, 10000) y mide impacto en F1.

#### 10) Autoevaluaci√≥n
- ¬øPor qu√© hacemos `fit` solo en train y no en todo el dataset?
- ¬øQu√© pasa si `max_df=0.5`? ¬øQu√© palabras se eliminan?
- ¬øPor qu√© `sublinear_tf=True` es mejor para texto?

#### 11) Errores comunes
- **Data leakage:** Hacer fit en todo el dataset (train + test). El modelo "ve el futuro".
- **Vocabulario gigante:** No limitar `max_features`. Causa overfitting y lentitud.
- **Ignorar sparse:** Convertir a dense con `.toarray()` innecesariamente. Explota la memoria.
- **No verificar vocabulario:** Asumir que tiene sentido sin inspeccionarlo.

#### 12) Retenci√≥n
- Mantra: "Fit on train, transform on both. Never fit on test."
- Regla: TF-IDF > BoW para clasificaci√≥n (casi siempre).

#### 13) Diferenciaci√≥n
- Avanzado: Comparar TF-IDF con BM25 (usado en motores de b√∫squeda).
- Avanzado: Implementar TF-IDF desde cero para entender la matem√°tica.

#### 14) Recursos
- Sklearn TfidfVectorizer docs: par√°metros y ejemplos
- Paper original: "A Statistical Interpretation of Term Specificity" (Sparck Jones, 1972)

#### 15) Nota docente
- Pide que el alumno explique en sus palabras por qu√© IDF penaliza palabras comunes.
- Ejercicio: Calcular TF-IDF a mano para un documento de 3 palabras.
</details>

---

### 2.2 Modelos Baseline: Logistic Regression

```python
"""SEMANA 22: Logistic Regression para Clasificaci√≥n de Texto

Logistic Regression es el modelo baseline por excelencia para NLP porque:
1. Es r√°pido de entrenar (segundos incluso con millones de features)
2. Es interpretable (coeficientes = importancia de palabras)
3. Funciona bien con datos sparse (TF-IDF)
4. Sirve como baseline s√≥lido para comparar con deep learning

Modelo matem√°tico:
    P(y=1|x) = œÉ(w¬∑x + b) = 1 / (1 + exp(-(w¬∑x + b)))

    donde:
    - x: vector TF-IDF del documento (sparse, ~5000 dims)
    - w: pesos aprendidos (uno por feature/palabra)
    - b: bias (intercepto)
    - œÉ: funci√≥n sigmoide que mapea a [0,1]

Interpretaci√≥n de coeficientes:
    - w_i > 0: palabra i incrementa P(disaster)
    - w_i < 0: palabra i decrementa P(disaster)
    - |w_i| grande: palabra i es muy discriminativa
"""  # Cierra docstring del m√≥dulo

import numpy as np  # NumPy para operaciones num√©ricas
import time  # Para medir tiempos de entrenamiento
from sklearn.linear_model import LogisticRegression  # Modelo lineal para clasificaci√≥n
from sklearn.metrics import (  # M√©tricas de evaluaci√≥n
    classification_report,  # Reporte detallado por clase
    confusion_matrix,  # Matriz de confusi√≥n
    f1_score,  # F1-Score (m√©trica principal)
    precision_score,  # Precision
    recall_score,  # Recall
    roc_auc_score,  # √Årea bajo curva ROC
    precision_recall_curve,  # Curva precision-recall
    roc_curve  # Curva ROC
)
from typing import Dict, Tuple, List  # Tipos para anotaciones
import matplotlib.pyplot as plt  # Para visualizaciones


def train_logistic_regression(  # Funci√≥n principal de entrenamiento
    X_train: np.ndarray,  # Matriz TF-IDF de train (sparse)
    y_train: np.ndarray,  # Labels de train (0/1)
    C: float = 1.0,  # Inverso de regularizaci√≥n (menor = m√°s regularizaci√≥n)
    class_weight: str = 'balanced',  # Manejo de desbalance de clases
    max_iter: int = 1000  # Iteraciones m√°ximas del solver
) -> LogisticRegression:
    """Entrena Logistic Regression para clasificaci√≥n binaria de tweets.

    Par√°metros importantes:
    - C: Controla regularizaci√≥n L2. C=1 es default, C<1 m√°s regularizaci√≥n.
    - class_weight='balanced': Ajusta pesos inversamente proporcionales a frecuencia.
      Si clase 0 tiene 57% y clase 1 tiene 43%, los pesos son ~1.75 y ~2.33.
    - solver='lbfgs': Algoritmo de optimizaci√≥n eficiente para L2.

    Args:
        X_train: Matriz de features (n_samples, n_features), t√≠picamente sparse
        y_train: Vector de labels (n_samples,), valores 0 o 1
        C: Par√°metro de regularizaci√≥n inverso
        class_weight: 'balanced' ajusta por frecuencia, None ignora desbalance
        max_iter: M√°ximo de iteraciones (aumentar si no converge)

    Returns:
        Modelo LogisticRegression entrenado
    """  # Cierra docstring; c√≥digo de entrenamiento sigue
    print("\nüî¨ Entrenando Logistic Regression...")  # Status informativo
    print(f"   Par√°metros: C={C}, class_weight='{class_weight}'")  # Hiperpar√°metros

    start_time = time.time()  # Marca tiempo inicial para medir duraci√≥n

    model = LogisticRegression(  # Crea instancia del modelo
        C=C,  # Regularizaci√≥n: C grande = menos regularizaci√≥n, m√°s riesgo de overfitting
        class_weight=class_weight,  # 'balanced' compensa desbalance autom√°ticamente
        max_iter=max_iter,  # Iteraciones del solver (aumentar si warning de convergencia)
        solver='lbfgs',  # L-BFGS: eficiente para L2, maneja sparse matrices
        random_state=42,  # Reproducibilidad
        n_jobs=-1  # Usar todos los cores disponibles
    )

    model.fit(X_train, y_train)  # Entrena el modelo (optimiza w y b)

    train_time = time.time() - start_time  # Calcula duraci√≥n
    print(f"   ‚úÖ Entrenamiento completado en {train_time:.2f} segundos")  # Reporta tiempo

    return model  # Retorna modelo entrenado


def evaluate_model(  # Funci√≥n de evaluaci√≥n completa
    model,  # Modelo entrenado (LogReg, NB, etc.)
    X_test: np.ndarray,  # Features de test
    y_test: np.ndarray,  # Labels de test
    model_name: str = "Model"  # Nombre para reportes
) -> Dict[str, float]:
    """Eval√∫a modelo y retorna m√©tricas completas.

    M√©tricas calculadas:
    - Accuracy: (TP+TN)/(TP+TN+FP+FN) - NO usar como m√©trica principal con desbalance
    - Precision: TP/(TP+FP) - De los predichos positivos, ¬øcu√°ntos son correctos?
    - Recall: TP/(TP+FN) - De los positivos reales, ¬øcu√°ntos detectamos?
    - F1-Score: 2√óP√óR/(P+R) - Media arm√≥nica de Precision y Recall
    - ROC-AUC: √Årea bajo curva ROC - Mide separabilidad de clases

    En clasificaci√≥n de desastres:
    - Alto Recall es cr√≠tico (no queremos perder desastres reales)
    - Precision tambi√©n importa (muchos falsos positivos causan fatiga)
    - F1 balancea ambas

    Args:
        model: Modelo con m√©todos predict() y predict_proba()
        X_test: Features de test
        y_test: Labels verdaderos de test
        model_name: Nombre para los reportes

    Returns:
        Dict con todas las m√©tricas
    """  # Cierra docstring; c√≥digo de evaluaci√≥n sigue
    # Obtener predicciones
    y_pred = model.predict(X_test)  # Predicciones binarias (0/1)
    y_proba = model.predict_proba(X_test)[:, 1]  # Probabilidades de clase 1

    # Calcular m√©tricas
    metrics = {  # Diccionario con todas las m√©tricas
        'accuracy': (y_pred == y_test).mean(),  # Accuracy (solo referencia)
        'precision': precision_score(y_test, y_pred),  # Precision para clase 1
        'recall': recall_score(y_test, y_pred),  # Recall para clase 1
        'f1_score': f1_score(y_test, y_pred),  # F1-Score (m√©trica principal)
        'roc_auc': roc_auc_score(y_test, y_proba)  # ROC-AUC
    }

    # Imprimir resultados
    print(f"\nüìä Resultados de {model_name}:")  # Header
    print(f"   Accuracy:  {metrics['accuracy']:.4f}  (‚ö†Ô∏è no usar como m√©trica principal)")  # Accuracy con warning
    print(f"   Precision: {metrics['precision']:.4f}")  # Precision
    print(f"   Recall:    {metrics['recall']:.4f}")  # Recall
    print(f"   F1-Score:  {metrics['f1_score']:.4f}  ‚≠ê (m√©trica principal)")  # F1 destacado
    print(f"   ROC-AUC:   {metrics['roc_auc']:.4f}")  # AUC

    # Classification report detallado
    print(f"\n   Classification Report:")  # Header
    print(classification_report(  # Reporte de sklearn
        y_test, y_pred,
        target_names=['No Disaster', 'Disaster'],  # Nombres de clases
        digits=4  # 4 decimales
    ))

    return metrics  # Retorna diccionario de m√©tricas


def get_feature_importance(  # Funci√≥n para interpretar el modelo
    model: LogisticRegression,
    feature_names: np.ndarray,
    top_n: int = 20
) -> Tuple[List, List]:
    """Obtiene las palabras m√°s importantes para cada clase.

    En LogReg binaria, los coeficientes indican importancia:
    - Coeficiente positivo grande ‚Üí incrementa P(disaster)
    - Coeficiente negativo grande ‚Üí incrementa P(no disaster)

    Esta interpretabilidad es una GRAN ventaja sobre deep learning.

    Args:
        model: LogisticRegression entrenado
        feature_names: Array con nombres de features (palabras/n-gramas)
        top_n: N√∫mero de features a retornar por clase

    Returns:
        Tuple de (top_disaster_words, top_no_disaster_words)
    """  # Cierra docstring
    coefs = model.coef_[0]  # Coeficientes del modelo (1 fila para binario)
    sorted_idx = np.argsort(coefs)  # √çndices ordenados de menor a mayor

    # Top palabras para clase "Disaster" (coeficientes m√°s positivos)
    top_disaster_idx = sorted_idx[-top_n:][::-1]  # √öltimos N, invertidos
    top_disaster = [(feature_names[i], coefs[i]) for i in top_disaster_idx]

    # Top palabras para clase "No Disaster" (coeficientes m√°s negativos)
    top_no_disaster_idx = sorted_idx[:top_n]  # Primeros N
    top_no_disaster = [(feature_names[i], coefs[i]) for i in top_no_disaster_idx]

    return top_disaster, top_no_disaster  # Retorna ambas listas


def plot_confusion_matrix(  # Visualizaci√≥n de matriz de confusi√≥n
    y_true: np.ndarray,
    y_pred: np.ndarray,
    model_name: str = "Model"
):
    """Visualiza matriz de confusi√≥n con anotaciones.

    La matriz de confusi√≥n muestra:
    - True Negatives (TN): Predicho 0, Real 0 ‚úì
    - False Positives (FP): Predicho 1, Real 0 ‚úó
    - False Negatives (FN): Predicho 0, Real 1 ‚úó (cr√≠tico en desastres)
    - True Positives (TP): Predicho 1, Real 1 ‚úì
    """  # Cierra docstring
    cm = confusion_matrix(y_true, y_pred)  # Calcula matriz de confusi√≥n

    fig, ax = plt.subplots(figsize=(8, 6))  # Crea figura
    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)  # Heatmap
    ax.figure.colorbar(im, ax=ax)  # Colorbar

    # Etiquetas
    classes = ['No Disaster', 'Disaster']  # Nombres de clases
    ax.set(  # Configura ejes
        xticks=np.arange(len(classes)),
        yticks=np.arange(len(classes)),
        xticklabels=classes,
        yticklabels=classes,
        title=f'Confusion Matrix - {model_name}',
        ylabel='True Label',
        xlabel='Predicted Label'
    )

    # Anotaciones en cada celda
    thresh = cm.max() / 2  # Umbral para color de texto
    for i in range(len(classes)):  # Itera filas
        for j in range(len(classes)):  # Itera columnas
            ax.text(j, i, format(cm[i, j], 'd'),  # N√∫mero en celda
                   ha="center", va="center",
                   color="white" if cm[i, j] > thresh else "black")  # Color seg√∫n fondo

    plt.tight_layout()  # Ajusta layout
    plt.show()  # Muestra figura
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 2.2: Logistic Regression</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Logistic Regression: el workhorse de clasificaci√≥n de texto
- **ID (opcional):** `M08-NLP-02_2`
- **Duraci√≥n estimada:** 90‚Äì120 min
- **Nivel:** Intermedio
- **Dependencias:** TF-IDF (2.1), conceptos de clasificaci√≥n binaria, regularizaci√≥n

#### 2) Objetivos
- Entrenar y evaluar LogReg para clasificaci√≥n de tweets.
- Entender el rol de regularizaci√≥n (C) y class_weight.
- Interpretar coeficientes para entender qu√© palabras importan.
- Usar F1-Score como m√©trica principal (no Accuracy).

#### 3) Relevancia
- LogReg es el baseline obligatorio antes de cualquier modelo complejo.
- Si LogReg+TF-IDF da F1=0.78 y BERT da F1=0.80, ¬øvale la pena BERT?
- La interpretabilidad de LogReg es invaluable para debugging y confianza.

#### 4) Mapa conceptual m√≠nimo
```
x (TF-IDF vector) ‚Üí w¬∑x + b ‚Üí sigmoid ‚Üí P(disaster) ‚Üí threshold ‚Üí 0/1
                      ‚Üì
            coeficientes interpretables
```

#### 5) Definiciones esenciales
- **Regularizaci√≥n L2:** Penaliza ||w||¬≤ para evitar coeficientes extremos (overfitting).
- **C:** Inverso de fuerza de regularizaci√≥n. C peque√±o = m√°s regularizaci√≥n.
- **class_weight='balanced':** Peso de clase = n_samples / (n_classes √ó n_samples_per_class).
- **Threshold:** Por default 0.5. Se puede ajustar para balancear precision/recall.

#### 6) Explicaci√≥n did√°ctica
- LogReg aprende un hiperplano en el espacio TF-IDF de ~5000 dimensiones.
- Cada palabra tiene un "voto" (coeficiente) a favor o en contra de "disaster".
- Si un tweet tiene muchas palabras con coeficientes positivos ‚Üí P(disaster) alta.

#### 7) Ejemplo modelado
- Palabras con coef > 0: "earthquake", "flood", "emergency", "victims"
- Palabras con coef < 0: "love", "music", "lol", "game"
- Tweet "earthquake in LA, emergency services responding" ‚Üí alto P(disaster)

#### 8) Pr√°ctica guiada
- Entrena LogReg y obt√©n las 20 palabras m√°s predictivas por clase.
- Verifica que las palabras tienen sentido sem√°ntico.

#### 9) Pr√°ctica independiente
- Experimenta con diferentes valores de C (0.1, 1, 10) y observa el efecto en F1.
- Ajusta el threshold (0.3, 0.5, 0.7) y analiza el trade-off precision/recall.

#### 10) Autoevaluaci√≥n
- ¬øPor qu√© class_weight='balanced' mejora el recall?
- ¬øQu√© pasa si C es muy grande (C=1000)?
- ¬øPor qu√© no usamos Accuracy como m√©trica principal?

#### 11) Errores comunes
- **Reportar solo Accuracy:** Enga√±oso con desbalance (57/43).
- **Ignorar convergencia:** Warning "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT" significa que no convergi√≥.
- **No verificar coeficientes:** Si las palabras importantes no tienen sentido, hay bug en preprocesamiento.

#### 12) Retenci√≥n
- Mantra: "F1 for classification, not Accuracy. Always check class_weight."
- Regla: Si LogReg falla, el problema est√° en los datos o preprocesamiento.

#### 13) Diferenciaci√≥n
- Avanzado: Implementar b√∫squeda de threshold √≥ptimo con curva PR.
- Avanzado: Comparar L1 vs L2 regularizaci√≥n (L1 produce sparsity).

#### 14) Recursos
- Sklearn LogisticRegression docs
- Paper: "Regularization Paths for Generalized Linear Models via Coordinate Descent"

#### 15) Nota docente
- Pide que el alumno explique por qu√© las palabras m√°s predictivas tienen sentido.
- Ejercicio: Encontrar un tweet mal clasificado y explicar por qu√© el modelo fall√≥.
</details>

---

## üíª Parte 3: Deep Learning con LSTM (Semana 23)

### 3.1 Word Embeddings y GloVe

```python
"""SEMANA 23: Word Embeddings con GloVe Pre-entrenados

Word Embeddings representan palabras como vectores densos donde palabras
similares tienen vectores similares. A diferencia de TF-IDF (sparse, ~5000 dims),
los embeddings son densos (~100-300 dims) y capturan sem√°ntica.

Evoluci√≥n de representaciones:
    One-Hot:  "cat" = [1,0,0,...,0]  dim=vocabulario (~50,000)
              "dog" = [0,1,0,...,0]
              Similaridad(cat, dog) = 0  ‚Üê PROBLEMA

    Embedding: "cat" = [0.2, -0.4, 0.7, ...]  dim=100-300
               "dog" = [0.3, -0.3, 0.6, ...]
               Similaridad(cat, dog) ‚âà 0.85  ‚Üê CORRECTO

GloVe (Global Vectors for Word Representation):
- Pre-entrenado en Wikipedia + Gigaword (6B tokens)
- Captura relaciones sem√°nticas: king - man + woman ‚âà queen
- Descarga: https://nlp.stanford.edu/projects/glove/
"""  # Cierra docstring del m√≥dulo

import numpy as np  # NumPy para operaciones vectoriales
from typing import Dict, Tuple  # Tipos para anotaciones


def load_glove_embeddings(  # Funci√≥n para cargar GloVe
    glove_path: str,  # Ruta al archivo glove.6B.100d.txt
    embedding_dim: int = 100  # Dimensi√≥n de los embeddings (50, 100, 200, 300)
) -> Dict[str, np.ndarray]:
    """Carga embeddings GloVe pre-entrenados desde archivo.

    El archivo tiene formato: palabra dim1 dim2 ... dimN (una l√≠nea por palabra).
    Ejemplo: "the 0.418 0.24968 -0.41242 ..."

    Args:
        glove_path: Ruta al archivo GloVe descargado
        embedding_dim: Dimensi√≥n esperada (debe coincidir con archivo)

    Returns:
        Dict mapping palabra ‚Üí vector numpy de shape (embedding_dim,)
    """  # Cierra docstring
    print(f"üì• Cargando GloVe embeddings desde {glove_path}...")  # Status
    embeddings_index = {}  # Dict para almacenar palabra ‚Üí vector

    with open(glove_path, encoding='utf-8') as f:  # Abre archivo con encoding UTF-8
        for line_num, line in enumerate(f):  # Itera l√≠neas con n√∫mero de l√≠nea
            values = line.split()  # Divide l√≠nea por espacios
            word = values[0]  # Primera palabra es el token
            try:
                coefs = np.asarray(values[1:], dtype='float32')  # Resto son coeficientes
                if len(coefs) == embedding_dim:  # Verifica dimensi√≥n correcta
                    embeddings_index[word] = coefs  # Guarda en dict
            except ValueError:  # Si hay error de conversi√≥n (l√≠nea malformada)
                continue  # Salta esa l√≠nea

    print(f"   ‚úÖ Cargados {len(embeddings_index):,} word vectors")  # Reporta total
    return embeddings_index  # Retorna diccionario


def create_embedding_matrix(  # Funci√≥n para crear matriz de embeddings
    word_index: Dict[str, int],  # Mapeo palabra ‚Üí √≠ndice del Tokenizer
    embeddings_index: Dict[str, np.ndarray],  # Embeddings GloVe cargados
    max_words: int = 10000,  # N√∫mero m√°ximo de palabras en vocabulario
    embedding_dim: int = 100  # Dimensi√≥n de embeddings
) -> Tuple[np.ndarray, int, int]:
    """Crea matriz de embeddings para usar en capa Embedding de Keras.

    La matriz tiene shape (max_words, embedding_dim) donde la fila i
    contiene el embedding de la palabra con √≠ndice i en word_index.

    Palabras sin embedding en GloVe (OOV) se inicializan con ceros.

    Args:
        word_index: Dict de Tokenizer.word_index (palabra ‚Üí √≠ndice)
        embeddings_index: Dict de embeddings GloVe cargados
        max_words: Tama√±o del vocabulario (debe coincidir con Tokenizer)
        embedding_dim: Dimensi√≥n de embeddings GloVe

    Returns:
        Tuple de (embedding_matrix, num_found, num_missing)
    """  # Cierra docstring
    print(f"\nüî® Creando embedding matrix...")  # Status

    # Inicializar matriz con ceros
    embedding_matrix = np.zeros((max_words, embedding_dim), dtype='float32')  # Shape (vocab, dim)

    found = 0  # Contador de palabras encontradas
    missing = 0  # Contador de palabras no encontradas (OOV)

    for word, i in word_index.items():  # Itera palabras del vocabulario
        if i >= max_words:  # Si √≠ndice excede vocabulario m√°ximo
            continue  # Salta (palabras menos frecuentes)

        embedding_vector = embeddings_index.get(word)  # Busca embedding en GloVe
        if embedding_vector is not None:  # Si existe
            embedding_matrix[i] = embedding_vector  # Asigna a la fila i
            found += 1  # Incrementa contador
        else:  # Si no existe (OOV)
            missing += 1  # Incrementa contador de missing
            # La fila queda con ceros (se aprender√° durante training si trainable=True)

    coverage = found / (found + missing) * 100  # Porcentaje de cobertura
    print(f"   ‚úÖ Palabras con embedding: {found:,} ({coverage:.1f}%)")  # Reporta encontradas
    print(f"   ‚ö†Ô∏è  Palabras sin embedding (OOV): {missing:,}")  # Reporta faltantes

    return embedding_matrix, found, missing  # Retorna matriz y estad√≠sticas
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 3.1: Word Embeddings</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Word Embeddings: de representaciones sparse a sem√°ntica densa
- **ID:** `M08-NLP-03_1`
- **Duraci√≥n estimada:** 60‚Äì90 min
- **Nivel:** Intermedio-Avanzado
- **Dependencias:** TF-IDF (2.1), conceptos de similitud coseno

#### 2) Objetivos
- Entender por qu√© embeddings densos capturan sem√°ntica mejor que one-hot.
- Cargar y usar embeddings pre-entrenados (GloVe).
- Crear matriz de embeddings para Keras.
- Analizar cobertura del vocabulario (OOV problem).

#### 3) Relevancia
- Embeddings son la base de todo NLP moderno (LSTM, BERT, GPT).
- Transfer learning: usar conocimiento de Wikipedia para tu tarea.
- Reducen dimensionalidad de ~50,000 (one-hot) a ~100-300 (dense).

#### 4) Mapa conceptual m√≠nimo
```
Palabras ‚Üí One-Hot (sparse) ‚Üí No sem√°ntica
Palabras ‚Üí Embedding (dense) ‚Üí Sem√°ntica capturada
king - man + woman ‚âà queen
```

#### 5) Definiciones esenciales
- **Embedding:** Vector denso que representa una palabra en espacio continuo.
- **GloVe:** "Global Vectors" - embeddings basados en co-ocurrencia global.
- **OOV (Out of Vocabulary):** Palabras no vistas en el corpus de pre-entrenamiento.
- **Cobertura:** Porcentaje de tu vocabulario con embedding disponible.

#### 6) Explicaci√≥n did√°ctica
- Palabras que aparecen en contextos similares ‚Üí vectores similares.
- "cat" y "dog" aparecen cerca de "pet", "animal" ‚Üí vectores cercanos.
- Analog√≠as: las direcciones en el espacio codifican relaciones (g√©nero, tiempo verbal).

#### 7) Ejemplo modelado
- `glove['disaster']` y `glove['emergency']` tienen alta similitud coseno.
- `glove['fire']` est√° entre `glove['flames']` (literal) y `glove['passion']` (figurativo).

#### 8) Pr√°ctica guiada
- Calcula similitud coseno entre pares de palabras relacionadas con desastres.
- Verifica la analog√≠a: earthquake - ground + water ‚âà flood.

#### 9) Pr√°ctica independiente
- Analiza palabras OOV del dataset. ¬øSon jerga de Twitter, typos, o t√©rminos t√©cnicos?
- Implementa inicializaci√≥n aleatoria para OOV en lugar de ceros.

#### 10) Autoevaluaci√≥n
- ¬øPor qu√© la cobertura de GloVe puede ser <100% para tweets?
- ¬øQu√© pasa si inicializamos OOV con el promedio de todos los embeddings?

#### 11) Errores comunes
- **Dimensi√≥n incorrecta:** Usar glove.100d con embedding_dim=300.
- **No verificar cobertura:** Asumir que todas las palabras tienen embedding.
- **Encoding incorrecto:** No usar UTF-8 al leer el archivo GloVe.

#### 12) Retenci√≥n
- Regla: "Embeddings pre-entrenados > entrenar desde cero (para datasets peque√±os)."

#### 13) Diferenciaci√≥n
- Avanzado: Comparar GloVe vs Word2Vec vs FastText (maneja OOV con subwords).

#### 14) Recursos
- Paper GloVe: "GloVe: Global Vectors for Word Representation" (Pennington et al.)
- Descarga: https://nlp.stanford.edu/projects/glove/

#### 15) Nota docente
- Pide que el alumno visualice embeddings con t-SNE y agrupe palabras por tema.
</details>

---

### 3.2 Arquitectura Bidirectional LSTM

```python
"""SEMANA 23: Bidirectional LSTM para Clasificaci√≥n de Texto

LSTM (Long Short-Term Memory) es una arquitectura de red neuronal recurrente
que puede capturar dependencias de largo alcance en secuencias.

¬øPor qu√© Bidirectional?
    Unidirectional: ‚Üí ‚Üí ‚Üí ‚Üí ‚Üí
    Solo ve contexto pasado al procesar cada palabra.

    Bidirectional:  ‚Üí ‚Üí ‚Üí ‚Üí ‚Üí
                    ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê
    Ve contexto pasado Y futuro. Crucial para entender negaciones:
    "The fire was NOT a real emergency" - "NOT" afecta a "emergency" que viene despu√©s.

Arquitectura para este proyecto:
    Input (secuencia de √≠ndices) ‚Üí Embedding (GloVe) ‚Üí Bi-LSTM ‚Üí Dropout ‚Üí Dense ‚Üí Output
"""  # Cierra docstring del m√≥dulo

import numpy as np  # NumPy para operaciones num√©ricas
from tensorflow.keras.models import Model  # API Funcional de Keras
from tensorflow.keras.layers import (  # Capas de Keras
    Input,  # Capa de entrada
    Embedding,  # Capa de embeddings
    LSTM,  # Capa LSTM
    Bidirectional,  # Wrapper para bidireccionalidad
    Dense,  # Capa fully connected
    Dropout,  # Regularizaci√≥n por dropout
    GlobalMaxPooling1D,  # Pooling global
    Concatenate  # Para combinar salidas
)
from tensorflow.keras.callbacks import (  # Callbacks para entrenamiento
    EarlyStopping,  # Detener si no mejora
    ModelCheckpoint,  # Guardar mejor modelo
    ReduceLROnPlateau  # Reducir learning rate si se estanca
)
from tensorflow.keras.preprocessing.text import Tokenizer  # Tokenizador de Keras
from tensorflow.keras.preprocessing.sequence import pad_sequences  # Padding de secuencias
from typing import Tuple  # Tipos


def prepare_sequences(  # Prepara datos para LSTM
    texts: list,  # Lista de textos preprocesados
    max_words: int = 10000,  # Tama√±o del vocabulario
    max_len: int = 100  # Longitud m√°xima de secuencia
) -> Tuple[np.ndarray, Tokenizer]:
    """Convierte textos a secuencias de √≠ndices con padding.

    Pipeline:
    1. Tokenizer aprende vocabulario (palabra ‚Üí √≠ndice)
    2. texts_to_sequences convierte textos a listas de √≠ndices
    3. pad_sequences asegura longitud uniforme (trunca o a√±ade ceros)

    Args:
        texts: Lista de textos preprocesados
        max_words: N√∫mero m√°ximo de palabras en vocabulario
        max_len: Longitud m√°xima de secuencia (truncar/pad)

    Returns:
        Tuple de (X_padded, tokenizer)
    """  # Cierra docstring
    print(f"\nüî® Preparando secuencias para LSTM...")  # Status

    # 1. Crear y ajustar Tokenizer
    tokenizer = Tokenizer(  # Crea tokenizer
        num_words=max_words,  # Limita vocabulario a las max_words m√°s frecuentes
        oov_token='<OOV>'  # Token para palabras fuera de vocabulario
    )
    tokenizer.fit_on_texts(texts)  # Aprende vocabulario del corpus

    # 2. Convertir textos a secuencias de √≠ndices
    sequences = tokenizer.texts_to_sequences(texts)  # Lista de listas de enteros

    # 3. Aplicar padding para longitud uniforme
    X_padded = pad_sequences(  # Padding
        sequences,
        maxlen=max_len,  # Longitud objetivo
        padding='post',  # A√±adir ceros al final
        truncating='post'  # Truncar al final si excede max_len
    )

    # Reportar estad√≠sticas
    print(f"   Vocabulario: {min(len(tokenizer.word_index), max_words):,} palabras")
    print(f"   Shape de secuencias: {X_padded.shape}")
    print(f"   Longitud m√°xima: {max_len}")

    return X_padded, tokenizer  # Retorna datos y tokenizer


def build_bilstm_model(  # Construye modelo Bi-LSTM
    max_words: int = 10000,  # Tama√±o del vocabulario
    max_len: int = 100,  # Longitud de secuencia
    embedding_dim: int = 100,  # Dimensi√≥n de embeddings
    embedding_matrix: np.ndarray = None,  # Matriz de embeddings pre-entrenados
    lstm_units: int = 64,  # Unidades en LSTM
    dropout_rate: float = 0.3,  # Tasa de dropout
    trainable_embeddings: bool = False  # Si entrenar embeddings
) -> Model:
    """Construye modelo Bidirectional LSTM para clasificaci√≥n binaria.

    Arquitectura:
        Input (max_len,) ‚Üí Embedding (max_words, embedding_dim)
        ‚Üí Bidirectional(LSTM(lstm_units, return_sequences=True))
        ‚Üí Dropout ‚Üí Bidirectional(LSTM(lstm_units//2))
        ‚Üí Dropout ‚Üí Dense(64, relu) ‚Üí Dropout ‚Üí Dense(1, sigmoid)

    Args:
        max_words: Tama√±o del vocabulario
        max_len: Longitud de secuencia de entrada
        embedding_dim: Dimensi√≥n de embeddings
        embedding_matrix: Pesos pre-entrenados (GloVe) o None para random
        lstm_units: N√∫mero de unidades en primera capa LSTM
        dropout_rate: Tasa de dropout para regularizaci√≥n
        trainable_embeddings: Si True, los embeddings se actualizan en training

    Returns:
        Modelo Keras compilado
    """  # Cierra docstring
    print(f"\nüî® Construyendo modelo Bi-LSTM...")  # Status

    # Capa de entrada
    inputs = Input(shape=(max_len,), name='input')  # Shape: (batch, max_len)

    # Capa de Embedding
    if embedding_matrix is not None:  # Si tenemos embeddings pre-entrenados
        x = Embedding(  # Capa Embedding con pesos inicializados
            input_dim=max_words,  # Tama√±o del vocabulario
            output_dim=embedding_dim,  # Dimensi√≥n de salida
            weights=[embedding_matrix],  # Inicializar con GloVe
            input_length=max_len,  # Longitud de secuencia
            trainable=trainable_embeddings,  # Congelar o no los embeddings
            name='embedding_glove'
        )(inputs)
        print(f"   Usando embeddings pre-entrenados (trainable={trainable_embeddings})")
    else:  # Si no tenemos pre-entrenados
        x = Embedding(  # Embedding aleatorio (se aprende)
            input_dim=max_words,
            output_dim=embedding_dim,
            input_length=max_len,
            trainable=True,  # Siempre entrenable si es aleatorio
            name='embedding_random'
        )(inputs)
        print(f"   Usando embeddings aleatorios (se aprender√°n)")

    # Primera capa Bidirectional LSTM
    x = Bidirectional(  # Wrapper bidireccional
        LSTM(  # LSTM base
            units=lstm_units,  # N√∫mero de unidades
            return_sequences=True,  # Retornar secuencia completa para siguiente LSTM
            dropout=0.2,  # Dropout en input
            recurrent_dropout=0.2  # Dropout en conexiones recurrentes
        ),
        name='bilstm_1'
    )(x)
    x = Dropout(dropout_rate, name='dropout_1')(x)  # Dropout adicional

    # Segunda capa Bidirectional LSTM
    x = Bidirectional(  # Wrapper bidireccional
        LSTM(  # LSTM base
            units=lstm_units // 2,  # Menos unidades (pir√°mide)
            return_sequences=False  # Solo retornar √∫ltimo output
        ),
        name='bilstm_2'
    )(x)
    x = Dropout(dropout_rate, name='dropout_2')(x)  # Dropout

    # Capas Dense para clasificaci√≥n
    x = Dense(64, activation='relu', name='dense_1')(x)  # Capa oculta
    x = Dropout(dropout_rate, name='dropout_3')(x)  # Dropout

    # Capa de salida (clasificaci√≥n binaria)
    outputs = Dense(1, activation='sigmoid', name='output')(x)  # Sigmoid para probabilidad

    # Crear modelo
    model = Model(inputs=inputs, outputs=outputs, name='BiLSTM_Classifier')

    # Compilar
    model.compile(
        optimizer='adam',  # Adam optimizer (adaptativo)
        loss='binary_crossentropy',  # Loss para clasificaci√≥n binaria
        metrics=['accuracy']  # M√©trica de monitoreo (F1 se calcula aparte)
    )

    # Resumen
    print(f"\nüìä Arquitectura del modelo:")
    model.summary()

    return model  # Retorna modelo compilado


def get_callbacks(  # Obtiene callbacks para entrenamiento
    model_path: str = 'models/lstm_best.h5',  # Ruta para guardar mejor modelo
    patience_early: int = 5,  # Paciencia para early stopping
    patience_lr: int = 3  # Paciencia para reducir LR
) -> list:
    """Crea lista de callbacks para entrenamiento.

    Callbacks:
    - EarlyStopping: Detiene si val_loss no mejora en N √©pocas
    - ModelCheckpoint: Guarda el mejor modelo seg√∫n val_loss
    - ReduceLROnPlateau: Reduce learning rate si se estanca

    Returns:
        Lista de callbacks configurados
    """  # Cierra docstring
    callbacks = [
        EarlyStopping(  # Detener si no mejora
            monitor='val_loss',  # M√©trica a monitorear
            patience=patience_early,  # √âpocas sin mejora antes de detener
            restore_best_weights=True,  # Restaurar pesos del mejor epoch
            verbose=1  # Imprimir cuando se detiene
        ),
        ModelCheckpoint(  # Guardar mejor modelo
            filepath=model_path,  # Ruta del archivo
            monitor='val_loss',  # M√©trica a monitorear
            save_best_only=True,  # Solo guardar si mejora
            verbose=1  # Imprimir cuando guarda
        ),
        ReduceLROnPlateau(  # Reducir learning rate
            monitor='val_loss',  # M√©trica a monitorear
            factor=0.5,  # Factor de reducci√≥n (LR *= 0.5)
            patience=patience_lr,  # √âpocas sin mejora antes de reducir
            min_lr=1e-6,  # LR m√≠nimo
            verbose=1  # Imprimir cuando reduce
        )
    ]
    return callbacks  # Retorna lista
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 3.2: Bidirectional LSTM</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Bi-LSTM: capturando contexto bidireccional para NLP
- **ID:** `M08-NLP-03_2`
- **Duraci√≥n estimada:** 120‚Äì180 min
- **Nivel:** Avanzado
- **Dependencias:** Word embeddings (3.1), conceptos de RNN, backpropagation

#### 2) Objetivos
- Construir arquitectura Bi-LSTM con Keras Functional API.
- Entender por qu√© bidireccional es mejor que unidireccional para NLP.
- Aplicar regularizaci√≥n (Dropout, EarlyStopping) para evitar overfitting.
- Usar embeddings pre-entrenados vs entrenables.

#### 3) Relevancia
- LSTM es el paso intermedio entre ML cl√°sico y Transformers.
- Bi-LSTM fue state-of-the-art antes de BERT (2018).
- M√°s r√°pido de entrenar que BERT, √∫til para datasets peque√±os/recursos limitados.

#### 4) Mapa conceptual m√≠nimo
```
Secuencia ‚Üí Embedding ‚Üí LSTM forward  ‚Üí Concatenate ‚Üí Dense ‚Üí Sigmoid
                     ‚Üí LSTM backward ‚Üí
```

#### 5) Definiciones esenciales
- **LSTM:** Red recurrente con celdas de memoria que evitan vanishing gradients.
- **Bidirectional:** Procesa secuencia en ambas direcciones y concatena outputs.
- **return_sequences:** Si True, retorna output de cada timestep; si False, solo el √∫ltimo.
- **Dropout:** Desactiva neuronas aleatoriamente durante training para regularizar.

#### 6) Explicaci√≥n did√°ctica
- LSTM tiene "compuertas" (forget, input, output) que controlan flujo de informaci√≥n.
- Bidirectional permite que "not" (al inicio) afecte la interpretaci√≥n de "emergency" (al final).
- Dropout "obliga" a la red a no depender de features espec√≠ficas.

#### 7) Ejemplo modelado
- Tweet: "This is NOT a real emergency, just a drill"
- Forward LSTM: procesa "NOT" antes de "emergency" ‚Üí puede modular
- Backward LSTM: procesa "drill" primero, luego "emergency" ‚Üí contexto adicional

#### 8) Pr√°ctica guiada
- Entrena el modelo y observa las curvas de loss/accuracy.
- Compara F1 con trainable_embeddings=True vs False.

#### 9) Pr√°ctica independiente
- Experimenta con diferentes lstm_units (32, 64, 128).
- A√±ade una tercera capa LSTM y observa el efecto en overfitting.

#### 10) Autoevaluaci√≥n
- ¬øPor qu√© usamos return_sequences=True en la primera LSTM pero no en la segunda?
- ¬øQu√© indica si train_loss baja pero val_loss sube?
- ¬øPor qu√© congelamos embeddings inicialmente?

#### 11) Errores comunes
- **Overfitting:** Modelo memoriza train, no generaliza. Soluci√≥n: m√°s dropout, early stopping.
- **OOM (Out of Memory):** batch_size muy grande. Soluci√≥n: reducir a 16 o 32.
- **No converge:** Learning rate muy alto o bajo. Soluci√≥n: usar ReduceLROnPlateau.

#### 12) Retenci√≥n
- Regla: "Siempre usar EarlyStopping. El n√∫mero de √©pocas es un upper bound, no un target."

#### 13) Diferenciaci√≥n
- Avanzado: A√±adir capa de Attention sobre los outputs de LSTM.
- Avanzado: Implementar LSTM desde cero para entender las compuertas.

#### 14) Recursos
- Paper LSTM: "Long Short-Term Memory" (Hochreiter & Schmidhuber, 1997)
- Keras LSTM documentation

#### 15) Nota docente
- Pide que el alumno dibuje el flujo de datos a trav√©s del modelo.
- Ejercicio: Identificar tweets donde Bi-LSTM mejora sobre LogReg y explicar por qu√©.
</details>

---

## ÔøΩ Parte 4: Transfer Learning con BERT (Semana 24)

### 4.1 Introducci√≥n a BERT y Transformers

```python
"""SEMANA 24: Transfer Learning con BERT para Clasificaci√≥n de Texto

BERT (Bidirectional Encoder Representations from Transformers) revolucion√≥ NLP en 2018.
A diferencia de LSTM que procesa secuencialmente, BERT usa atenci√≥n para ver
TODAS las palabras simult√°neamente.

¬øPor qu√© BERT es mejor que LSTM?
1. Atenci√≥n bidireccional real (no concatenaci√≥n de forward+backward)
2. Pre-entrenado en corpus masivo (Wikipedia + BookCorpus, 3.3B palabras)
3. Transfer learning: conocimiento de lenguaje general se transfiere a tu tarea
4. State-of-the-art en la mayor√≠a de benchmarks de NLP

Arquitectura BERT:
    Input: [CLS] token1 token2 ... tokenN [SEP]
    ‚Üì
    12 capas de Transformer Encoder (BERT-base) o 24 (BERT-large)
    ‚Üì
    Output: Embedding contextualizado para cada token

Para clasificaci√≥n: Usamos el embedding de [CLS] como representaci√≥n del documento.
"""  # Cierra docstring del m√≥dulo

import numpy as np  # NumPy para operaciones num√©ricas
import tensorflow as tf  # TensorFlow para deep learning
from transformers import (  # HuggingFace Transformers
    BertTokenizer,  # Tokenizador de BERT
    TFBertForSequenceClassification,  # Modelo BERT para clasificaci√≥n
    BertConfig  # Configuraci√≥n del modelo
)
from typing import Dict, Tuple, List  # Tipos para anotaciones


def load_bert_model(  # Carga modelo BERT pre-entrenado
    model_name: str = 'bert-base-uncased',  # Nombre del modelo en HuggingFace
    num_labels: int = 2  # N√∫mero de clases (2 para binario)
) -> Tuple:
    """Carga tokenizer y modelo BERT pre-entrenado.

    Modelos disponibles:
    - bert-base-uncased: 110M params, lowercase (recomendado para empezar)
    - bert-base-cased: 110M params, mantiene may√∫sculas
    - bert-large-uncased: 340M params, m√°s potente pero m√°s lento
    - distilbert-base-uncased: 66M params, m√°s r√°pido, ~97% performance

    Args:
        model_name: Nombre del modelo en HuggingFace Hub
        num_labels: N√∫mero de clases de salida

    Returns:
        Tuple de (tokenizer, model)
    """  # Cierra docstring
    print(f"\nüì• Cargando modelo BERT: {model_name}...")  # Status

    # Cargar tokenizer
    tokenizer = BertTokenizer.from_pretrained(model_name)  # Descarga/carga tokenizer
    print(f"   ‚úÖ Tokenizer cargado (vocab size: {tokenizer.vocab_size:,})")

    # Cargar modelo para clasificaci√≥n
    model = TFBertForSequenceClassification.from_pretrained(
        model_name,  # Modelo base
        num_labels=num_labels  # Configura capa de clasificaci√≥n
    )
    print(f"   ‚úÖ Modelo cargado ({model.num_parameters():,} par√°metros)")

    return tokenizer, model  # Retorna ambos


def encode_texts_for_bert(  # Prepara datos para BERT
    texts: List[str],  # Lista de textos
    tokenizer: BertTokenizer,  # Tokenizer de BERT
    max_length: int = 128  # Longitud m√°xima (BERT m√°ximo: 512)
) -> Dict:
    """Tokeniza textos para BERT usando el tokenizer de HuggingFace.

    BERT requiere:
    - input_ids: √çndices de tokens en el vocabulario
    - attention_mask: 1 para tokens reales, 0 para padding
    - token_type_ids: 0 para primera oraci√≥n (no usado en clasificaci√≥n simple)

    Args:
        texts: Lista de textos a tokenizar
        tokenizer: BertTokenizer cargado
        max_length: Longitud m√°xima de secuencia (truncar si excede)

    Returns:
        Dict con input_ids, attention_mask, token_type_ids como tensores TF
    """  # Cierra docstring
    print(f"\nüî® Tokenizando {len(texts):,} textos para BERT...")  # Status

    # Tokenizar batch de textos
    encodings = tokenizer(
        texts,  # Lista de textos (puede ser lista de strings)
        padding='max_length',  # A√±adir padding hasta max_length
        truncation=True,  # Truncar si excede max_length
        max_length=max_length,  # Longitud objetivo
        return_tensors='tf'  # Retornar tensores de TensorFlow
    )

    print(f"   ‚úÖ Shape de input_ids: {encodings['input_ids'].shape}")
    print(f"   ‚úÖ Max length: {max_length}")

    return encodings  # Retorna dict de tensores


def create_tf_dataset(  # Crea dataset de TensorFlow
    encodings: Dict,  # Encodings de BERT
    labels: np.ndarray,  # Labels (0/1)
    batch_size: int = 16,  # Tama√±o de batch (peque√±o por memoria)
    shuffle: bool = True  # Si mezclar datos
) -> tf.data.Dataset:
    """Crea tf.data.Dataset para entrenamiento eficiente.

    tf.data.Dataset permite:
    - Prefetching: carga siguiente batch mientras procesa actual
    - Shuffling: mezcla datos para mejor generalizaci√≥n
    - Batching: agrupa ejemplos para procesamiento paralelo

    Args:
        encodings: Dict de tensores de BERT tokenizer
        labels: Array de labels
        batch_size: Tama√±o de batch (16 es t√≠pico para BERT por memoria)
        shuffle: Si True, mezcla los datos

    Returns:
        tf.data.Dataset listo para training/evaluation
    """  # Cierra docstring
    # Crear dataset desde tensores
    dataset = tf.data.Dataset.from_tensor_slices((
        dict(encodings),  # Convierte encodings a dict para el modelo
        labels  # Labels
    ))

    if shuffle:  # Si se pide mezclar
        dataset = dataset.shuffle(buffer_size=1000)  # Buffer de 1000 ejemplos

    dataset = dataset.batch(batch_size)  # Agrupa en batches
    dataset = dataset.prefetch(tf.data.AUTOTUNE)  # Prefetch autom√°tico

    return dataset  # Retorna dataset


def fine_tune_bert(  # Fine-tuning de BERT
    model: TFBertForSequenceClassification,  # Modelo BERT cargado
    train_dataset: tf.data.Dataset,  # Dataset de entrenamiento
    val_dataset: tf.data.Dataset,  # Dataset de validaci√≥n
    epochs: int = 3,  # √âpocas (BERT converge r√°pido, 2-4 t√≠pico)
    learning_rate: float = 2e-5  # LR bajo para fine-tuning (2e-5 a 5e-5)
) -> Dict:
    """Fine-tune BERT para clasificaci√≥n de tweets.

    Estrategia de fine-tuning:
    1. Learning rate muy bajo (2e-5) para no destruir pesos pre-entrenados
    2. Pocas √©pocas (2-4) porque BERT ya sabe mucho de lenguaje
    3. Batch size peque√±o (8-32) por limitaciones de memoria GPU

    Args:
        model: Modelo BERT pre-cargado
        train_dataset: tf.data.Dataset de entrenamiento
        val_dataset: tf.data.Dataset de validaci√≥n
        epochs: N√∫mero de √©pocas (2-4 t√≠pico)
        learning_rate: Learning rate (2e-5 a 5e-5 t√≠pico)

    Returns:
        Dict con history del entrenamiento
    """  # Cierra docstring
    print(f"\nüöÄ Fine-tuning BERT...")  # Status
    print(f"   Epochs: {epochs}")
    print(f"   Learning rate: {learning_rate}")

    # Configurar optimizer con LR bajo
    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

    # Loss para clasificaci√≥n
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    # Compilar modelo
    model.compile(
        optimizer=optimizer,
        loss=loss,
        metrics=['accuracy']
    )

    # Entrenar
    history = model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=epochs
    )

    print(f"   ‚úÖ Fine-tuning completado")
    return history.history  # Retorna history como dict
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 4.1: BERT Transfer Learning</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** BERT: Transfer Learning para NLP state-of-the-art
- **ID:** `M08-NLP-04_1`
- **Duraci√≥n estimada:** 150‚Äì240 min
- **Nivel:** Avanzado
- **Dependencias:** Bi-LSTM (3.2), conceptos de attention, GPU recomendada

#### 2) Objetivos
- Entender la arquitectura Transformer y el mecanismo de atenci√≥n.
- Cargar y usar modelos pre-entrenados de HuggingFace.
- Fine-tune BERT para clasificaci√≥n de tweets.
- Comparar performance con modelos anteriores (LogReg, LSTM).

#### 3) Relevancia
- BERT y sus variantes son el est√°ndar actual en NLP industrial.
- Transfer learning reduce dr√°sticamente datos necesarios.
- Entender BERT es prerequisito para GPT, T5, y modelos m√°s recientes.

#### 4) Mapa conceptual m√≠nimo
```
Input ‚Üí BERT Encoder (12 capas) ‚Üí [CLS] embedding ‚Üí Dense ‚Üí Clasificaci√≥n
         ‚Üë
    Atenci√≥n: cada token "atiende" a todos los dem√°s
```

#### 5) Definiciones esenciales
- **Transformer:** Arquitectura basada en atenci√≥n, sin recurrencia.
- **Self-Attention:** Cada posici√≥n puede atender a todas las dem√°s.
- **[CLS] token:** Token especial cuyo embedding representa todo el documento.
- **Fine-tuning:** Ajustar pesos pre-entrenados para tarea espec√≠fica.

#### 6) Explicaci√≥n did√°ctica
- BERT "ley√≥" Wikipedia y aprendi√≥ estructura del lenguaje.
- Fine-tuning transfiere ese conocimiento a clasificar desastres.
- Learning rate bajo evita "olvidar" lo aprendido (catastrophic forgetting).

#### 7) Ejemplo modelado
- Sin BERT: Necesitas ~100k ejemplos etiquetados para buen modelo.
- Con BERT: ~7k ejemplos (nuestro dataset) son suficientes para F1 > 0.80.

#### 8) Pr√°ctica guiada
- Fine-tune BERT con 2 y 4 √©pocas, compara val_loss.
- Analiza la curva de entrenamiento: ¬øhay overfitting?

#### 9) Pr√°ctica independiente
- Prueba distilbert-base-uncased (m√°s r√°pido) y compara F1.
- Experimenta con diferentes learning rates (1e-5, 2e-5, 5e-5).

#### 10) Autoevaluaci√≥n
- ¬øPor qu√© usamos learning rate tan bajo (2e-5 vs 1e-3 t√≠pico)?
- ¬øQu√© es el [CLS] token y por qu√© lo usamos para clasificaci√≥n?
- ¬øPor qu√© BERT necesita menos √©pocas que LSTM?

#### 11) Errores comunes
- **OOM (Out of Memory):** Reducir batch_size a 8 o 16.
- **LR muy alto:** Destruye pesos pre-entrenados. Usar 2e-5.
- **Muchas √©pocas:** BERT overfittea r√°pido. M√°ximo 4-5 √©pocas.
- **No usar GPU:** BERT es muy lento en CPU.

#### 12) Retenci√≥n
- Regla: "Para BERT: LR bajo (2e-5), pocas √©pocas (2-4), batch peque√±o (16)."

#### 13) Diferenciaci√≥n
- Avanzado: Implementar gradual unfreezing (descongelar capas progresivamente).
- Avanzado: Probar RoBERTa o ALBERT como alternativas.

#### 14) Recursos
- Paper: "BERT: Pre-training of Deep Bidirectional Transformers" (Devlin et al.)
- HuggingFace Course: https://huggingface.co/course
- The Illustrated Transformer: http://jalammar.github.io/illustrated-transformer/

#### 15) Nota docente
- Pide que el alumno explique por qu√© BERT puede entender "fire" diferente en contextos diferentes.
- Ejercicio: Comparar F1 de LogReg vs LSTM vs BERT y discutir trade-offs.
</details>

---

### 4.2 Comparaci√≥n Final de Modelos

```python
"""SEMANA 24: Comparaci√≥n de Todos los Modelos

Este script compara todos los modelos entrenados:
1. Logistic Regression + TF-IDF (baseline)
2. Naive Bayes + TF-IDF (baseline probabil√≠stico)
3. Bidirectional LSTM + GloVe
4. BERT fine-tuned

M√©tricas de comparaci√≥n:
- F1-Score (m√©trica principal)
- Precision y Recall
- Tiempo de entrenamiento
- Requisitos de recursos (CPU vs GPU)
"""  # Cierra docstring

import pandas as pd  # Pandas para tablas de resultados
import numpy as np  # NumPy para c√°lculos
import matplotlib.pyplot as plt  # Matplotlib para visualizaci√≥n
from typing import Dict, List  # Tipos


def compare_models(results: Dict[str, Dict]) -> pd.DataFrame:
    """Crea tabla comparativa de todos los modelos.

    Args:
        results: Dict con estructura {model_name: {metric: value}}

    Returns:
        DataFrame con comparaci√≥n
    """  # Cierra docstring
    df = pd.DataFrame(results).T  # Transponer: modelos como filas
    df = df.sort_values('f1_score', ascending=False)  # Ordenar por F1
    return df


def plot_model_comparison(results: Dict[str, Dict], save_path: str = None):
    """Visualiza comparaci√≥n de modelos con gr√°fico de barras.

    Args:
        results: Dict de resultados por modelo
        save_path: Ruta para guardar figura (opcional)
    """  # Cierra docstring
    models = list(results.keys())  # Nombres de modelos
    metrics = ['precision', 'recall', 'f1_score']  # M√©tricas a comparar

    x = np.arange(len(models))  # Posiciones en X
    width = 0.25  # Ancho de barras

    fig, ax = plt.subplots(figsize=(12, 6))  # Figura grande

    # Barras para cada m√©trica
    for i, metric in enumerate(metrics):
        values = [results[m][metric] for m in models]
        ax.bar(x + i*width, values, width, label=metric.replace('_', ' ').title())

    # Configuraci√≥n
    ax.set_xlabel('Modelo', fontsize=12)
    ax.set_ylabel('Score', fontsize=12)
    ax.set_title('Comparaci√≥n de Modelos - NLP Disaster Tweets', fontsize=14)
    ax.set_xticks(x + width)
    ax.set_xticklabels(models, rotation=15, ha='right')
    ax.legend()
    ax.set_ylim(0, 1)
    ax.grid(axis='y', alpha=0.3)

    # A√±adir valores encima de las barras
    for i, metric in enumerate(metrics):
        values = [results[m][metric] for m in models]
        for j, v in enumerate(values):
            ax.annotate(f'{v:.2f}', xy=(x[j] + i*width, v), ha='center', va='bottom', fontsize=8)

    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()


# === RESULTADOS ESPERADOS (ejemplo) ===
EXPECTED_RESULTS = {
    'LogReg + TF-IDF': {
        'precision': 0.78,
        'recall': 0.75,
        'f1_score': 0.76,
        'training_time': '2s',
        'gpu_required': 'No'
    },
    'Naive Bayes': {
        'precision': 0.72,
        'recall': 0.80,
        'f1_score': 0.76,
        'training_time': '1s',
        'gpu_required': 'No'
    },
    'Bi-LSTM + GloVe': {
        'precision': 0.80,
        'recall': 0.78,
        'f1_score': 0.79,
        'training_time': '5-10min',
        'gpu_required': 'Recomendada'
    },
    'BERT Fine-tuned': {
        'precision': 0.84,
        'recall': 0.82,
        'f1_score': 0.83,
        'training_time': '30-60min',
        'gpu_required': 'Necesaria'
    }
}

# Imprimir tabla de resultados esperados
print("\n" + "="*70)
print("RESULTADOS ESPERADOS (benchmark)")
print("="*70)
df_results = compare_models(EXPECTED_RESULTS)
print(df_results.to_string())
print("\n‚≠ê Modelo recomendado para producci√≥n: LogReg + TF-IDF")
print("   Raz√≥n: Balance √≥ptimo entre F1 (0.76) y simplicidad/velocidad")
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 4.2: Comparaci√≥n de Modelos</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** An√°lisis comparativo: elegir el modelo correcto para producci√≥n
- **ID:** `M08-NLP-04_2`
- **Duraci√≥n estimada:** 60‚Äì90 min
- **Nivel:** Avanzado
- **Dependencias:** Todos los modelos anteriores entrenados y evaluados

#### 2) Objetivos
- Comparar objetivamente todos los modelos con las mismas m√©tricas.
- Analizar trade-offs: performance vs complejidad vs recursos.
- Tomar decisi√≥n informada sobre modelo para producci√≥n.
- Documentar resultados en formato acad√©mico (REPORT.md).

#### 3) Relevancia
- En la industria, el "mejor" modelo no siempre es el m√°s preciso.
- Costos de deployment (GPU, latencia) importan tanto como F1.
- Saber comunicar trade-offs es skill cr√≠tico para data scientists.

#### 4) Mapa conceptual m√≠nimo
```
Modelos ‚Üí M√©tricas (F1, P, R) ‚Üí Trade-offs ‚Üí Decisi√≥n ‚Üí Producci√≥n
              ‚Üì
         Recursos (tiempo, GPU)
```

#### 5) Definiciones esenciales
- **Trade-off:** Sacrificar una cosa por otra (ej: accuracy vs velocidad).
- **Producci√≥n:** Ambiente donde el modelo se usa con datos reales.
- **Latencia:** Tiempo que toma una predicci√≥n.
- **Deployment:** Proceso de poner modelo en producci√≥n.

#### 6) Explicaci√≥n did√°ctica
- BERT gana en F1 pero necesita GPU y es lento.
- LogReg es casi tan bueno y funciona en cualquier servidor.
- La decisi√≥n depende del contexto: ¬øvelocidad o precisi√≥n importa m√°s?

#### 7) Ejemplo modelado
- Sistema de alertas en tiempo real: LogReg (baja latencia).
- An√°lisis batch diario: BERT (m√°xima precisi√≥n, tiempo no cr√≠tico).

#### 8) Pr√°ctica guiada
- Llena la tabla de resultados con tus propios modelos.
- Calcula mejora relativa: (BERT_F1 - LogReg_F1) / LogReg_F1.

#### 9) Pr√°ctica independiente
- A√±ade an√°lisis de errores: ¬øqu√© tweets falla cada modelo?
- Implementa ensemble (combinar predicciones de varios modelos).

#### 10) Autoevaluaci√≥n
- Si BERT tiene F1=0.83 y LogReg F1=0.76, ¬øcu√°l usar√≠as y por qu√©?
- ¬øEn qu√© casos pagar√≠as el costo extra de BERT?

#### 11) Errores comunes
- **Solo mirar F1:** Ignorar costos de recursos.
- **No reproducibilidad:** No fijar random seeds.
- **Comparaci√≥n injusta:** Usar diferentes splits de datos.

#### 12) Retenci√≥n
- Regla: "El mejor modelo es el que resuelve el problema de negocio, no el que tiene mayor F1."

#### 13) Diferenciaci√≥n
- Avanzado: Calcular costo/beneficio monetario de cada punto de F1.

#### 14) Recursos
- Paper: "Model Selection for NLP" (varios autores)
- Blog: "Deploying ML Models in Production" (Google AI)

#### 15) Nota docente
- Pide que el alumno presente recomendaci√≥n como si fuera para un cliente.
- Ejercicio: Escribir p√°rrafo de conclusiones para REPORT.md.
</details>

---

## ÔøΩüìä Evaluaci√≥n del Proyecto

### Criterios de Evaluaci√≥n (Total: 100 puntos)

| Componente | Puntos | Criterios |
|------------|--------|-----------|
| **EDA + Preprocessing** | 20 | Pipeline robusto, decisiones justificadas |
| **Baselines ML** | 20 | LogReg + NB funcionando, F1-Score reportado |
| **Deep Learning** | 25 | LSTM entrenando, regularizaci√≥n aplicada |
| **Transfer Learning** | 20 | BERT fine-tuned, comparaci√≥n con baselines |
| **Reporte REPORT.md** | 15 | Estructura acad√©mica, an√°lisis de errores |

### Condici√≥n de Aprobaci√≥n
- **F1-Score m√≠nimo:** 0.75 en test (con al menos un modelo)
- **Reporte completo:** Todas las secciones cubiertas

---

## üìö Recursos y Referencias

### Papers Fundamentales
1. **"Attention Is All You Need"** (Vaswani et al., 2017) - Transformers
2. **"BERT"** (Devlin et al., 2018) - Pre-training bidireccional
3. **"GloVe"** (Pennington et al., 2014) - Word embeddings

### Documentaci√≥n
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- [NLTK Book](https://www.nltk.org/book/)
- [Keras Text Processing](https://keras.io/api/preprocessing/text/)

---

*Material desarrollado para el MS-AI Pathway - University of Colorado Boulder*
*Semanas 21-24 - Proyecto Capstone NLP*
