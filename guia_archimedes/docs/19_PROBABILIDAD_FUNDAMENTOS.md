# M√≥dulo 04 - Fundamentos de Probabilidad para IA

> **üéØ Objetivo:** Dominar los conceptos probabil√≠sticos esenciales para ML/IA  
> **‚≠ê PATHWAY L√çNEA 2:** Probability Fundamentals for Data Science and AI

---

## üß† Analog√≠a: La Probabilidad como Lenguaje de la Incertidumbre

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ   LA IA VIVE EN LA INCERTIDUMBRE                                            ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Determin√≠stico (Algoritmos cl√°sicos):                                     ‚îÇ
‚îÇ   if x > 5: return "grande"  ‚Üí SIEMPRE la misma respuesta                   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Probabil√≠stico (Machine Learning):                                        ‚îÇ
‚îÇ   P(spam | email) = 0.87     ‚Üí "Probablemente spam, 87% seguro"             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   ¬øPOR QU√â PROBABILIDAD?                                                    ‚îÇ
‚îÇ   ‚Ä¢ Datos ruidosos e incompletos                                            ‚îÇ
‚îÇ   ‚Ä¢ Predicciones sobre el futuro                                            ‚îÇ
‚îÇ   ‚Ä¢ Cuantificar confianza en decisiones                                     ‚îÇ
‚îÇ   ‚Ä¢ Generalizar de muestras a poblaciones                                   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã Contenido

1. [Fundamentos de Probabilidad](#1-fundamentos)
2. [Probabilidad Condicional y Bayes](#2-bayes)
3. [Variables Aleatorias](#3-variables-aleatorias)
4. [Distribuciones de Probabilidad](#4-distribuciones)
5. [Esperanza, Varianza y Momentos](#5-momentos)

---

## 1. Fundamentos de Probabilidad {#1-fundamentos}

### 1.1 Espacio Muestral y Eventos

```python
from typing import Set, Dict
import math

# Espacio muestral: todos los resultados posibles
# Evento: subconjunto del espacio muestral

def probability_basic(favorable: int, total: int) -> float:
    """Basic probability: P(A) = favorable outcomes / total outcomes.
    
    Example:
        >>> probability_basic(1, 6)  # Sacar un 6 en un dado
        0.16666666666666666
    """
    if total == 0:
        raise ValueError("Total outcomes cannot be zero")
    return favorable / total


def complement_probability(p_a: float) -> float:
    """P(not A) = 1 - P(A).
    
    Example:
        >>> complement_probability(0.3)  # P(no llueve) si P(llueve) = 0.3
        0.7
    """
    return 1.0 - p_a
```

### 1.2 Axiomas de Kolmogorov

```
AXIOMAS DE PROBABILIDAD:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
1. P(A) ‚â• 0          (No negativas)
2. P(Œ©) = 1          (Espacio muestral tiene prob. 1)
3. P(A ‚à™ B) = P(A) + P(B)   si A ‚à© B = ‚àÖ  (Aditividad)

PROPIEDADES DERIVADAS:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚Ä¢ P(‚àÖ) = 0
‚Ä¢ P(A') = 1 - P(A)
‚Ä¢ P(A ‚à™ B) = P(A) + P(B) - P(A ‚à© B)
‚Ä¢ Si A ‚äÜ B, entonces P(A) ‚â§ P(B)
```

### 1.3 Operaciones con Eventos

```python
def union_probability(p_a: float, p_b: float, p_intersection: float) -> float:
    """P(A ‚à™ B) = P(A) + P(B) - P(A ‚à© B).
    
    Inclusion-exclusion principle.
    
    Example:
        >>> # P(rey o coraz√≥n) en baraja
        >>> union_probability(4/52, 13/52, 1/52)
        0.3076923076923077
    """
    return p_a + p_b - p_intersection


def intersection_independent(p_a: float, p_b: float) -> float:
    """P(A ‚à© B) = P(A) √ó P(B) for independent events.
    
    Example:
        >>> # Dos monedas, ambas cara
        >>> intersection_independent(0.5, 0.5)
        0.25
    """
    return p_a * p_b
```

---

## 2. Probabilidad Condicional y Bayes {#2-bayes}

### 2.1 Probabilidad Condicional

```python
def conditional_probability(p_a_and_b: float, p_b: float) -> float:
    """P(A|B) = P(A ‚à© B) / P(B).
    
    Probability of A given B has occurred.
    
    Example:
        >>> # P(llueve | nublado)
        >>> conditional_probability(0.3, 0.4)
        0.75
    """
    if p_b == 0:
        raise ValueError("P(B) cannot be zero")
    return p_a_and_b / p_b
```

### 2.2 Teorema de Bayes ‚≠ê FUNDAMENTAL PARA ML

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ   TEOREMA DE BAYES                                                          ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                         ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ                    P(B|A) √ó P(A)                                            ‚îÇ
‚îÇ   P(A|B) = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                        ‚îÇ
‚îÇ                     P(B)                                                    ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Donde:                                                                    ‚îÇ
‚îÇ   ‚Ä¢ P(A|B) = POSTERIOR (lo que queremos saber)                              ‚îÇ
‚îÇ   ‚Ä¢ P(B|A) = LIKELIHOOD (evidencia dado la hip√≥tesis)                       ‚îÇ
‚îÇ   ‚Ä¢ P(A)   = PRIOR (creencia inicial)                                       ‚îÇ
‚îÇ   ‚Ä¢ P(B)   = EVIDENCE (normalizador)                                        ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   EJEMPLO SPAM:                                                             ‚îÇ
‚îÇ   P(spam | "gratis") = P("gratis"|spam) √ó P(spam) / P("gratis")             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

```python
def bayes_theorem(
    p_b_given_a: float,  # Likelihood
    p_a: float,          # Prior
    p_b: float           # Evidence
) -> float:
    """Bayes' Theorem: P(A|B) = P(B|A) √ó P(A) / P(B).
    
    The foundation of probabilistic machine learning.
    
    Example:
        >>> # Test m√©dico: P(enfermo | test positivo)
        >>> # P(test+|enfermo) = 0.99, P(enfermo) = 0.01, P(test+) = 0.02
        >>> bayes_theorem(0.99, 0.01, 0.02)
        0.495
    """
    return (p_b_given_a * p_a) / p_b


def bayes_with_total_probability(
    p_b_given_a: float,
    p_a: float,
    p_b_given_not_a: float
) -> float:
    """Bayes with P(B) calculated via total probability.
    
    P(B) = P(B|A)P(A) + P(B|¬¨A)P(¬¨A)
    
    Example:
        >>> # Spam classifier
        >>> # P("free"|spam)=0.7, P(spam)=0.3, P("free"|not spam)=0.1
        >>> bayes_with_total_probability(0.7, 0.3, 0.1)
        0.75
    """
    p_not_a = 1 - p_a
    p_b = p_b_given_a * p_a + p_b_given_not_a * p_not_a
    return (p_b_given_a * p_a) / p_b
```

### 2.3 Aplicaci√≥n: Clasificador Naive Bayes

```python
from collections import defaultdict
from typing import List, Tuple

class NaiveBayesClassifier:
    """Simple Naive Bayes for text classification.
    
    Assumes features are conditionally independent given class.
    
    P(class|features) ‚àù P(class) √ó ‚àè P(feature|class)
    """
    
    def __init__(self) -> None:
        self.class_counts: Dict[str, int] = defaultdict(int)
        self.feature_counts: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))
        self.total_samples: int = 0
    
    def fit(self, X: List[List[str]], y: List[str]) -> None:
        """Train the classifier.
        
        Args:
            X: List of feature lists (e.g., words in documents)
            y: List of class labels
        """
        for features, label in zip(X, y):
            self.class_counts[label] += 1
            self.total_samples += 1
            for feature in features:
                self.feature_counts[label][feature] += 1
    
    def predict(self, features: List[str]) -> str:
        """Predict class for given features.
        
        Returns class with highest posterior probability.
        """
        best_class = None
        best_score = float('-inf')
        
        for cls in self.class_counts:
            # Log probability to avoid underflow
            score = math.log(self.class_counts[cls] / self.total_samples)
            
            total_features_in_class = sum(self.feature_counts[cls].values())
            vocab_size = len(set(
                f for counts in self.feature_counts.values() 
                for f in counts
            ))
            
            for feature in features:
                # Laplace smoothing
                count = self.feature_counts[cls].get(feature, 0) + 1
                prob = count / (total_features_in_class + vocab_size)
                score += math.log(prob)
            
            if score > best_score:
                best_score = score
                best_class = cls
        
        return best_class
```

---

## 3. Variables Aleatorias {#3-variables-aleatorias}

### 3.1 Discretas vs Continuas

```
VARIABLES ALEATORIAS:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

DISCRETAS: Valores contables
‚Ä¢ N√∫mero de emails spam
‚Ä¢ Cara o cruz
‚Ä¢ Clasificaci√≥n (0, 1, 2, ...)

CONTINUAS: Valores en un rango
‚Ä¢ Temperatura
‚Ä¢ Altura
‚Ä¢ Probabilidad predicha

FUNCI√ìN DE PROBABILIDAD:
‚Ä¢ Discreta: PMF (Probability Mass Function)
  P(X = x)

‚Ä¢ Continua: PDF (Probability Density Function)
  P(a ‚â§ X ‚â§ b) = ‚à´[a,b] f(x)dx
```

### 3.2 Funci√≥n de Distribuci√≥n Acumulativa (CDF)

```python
def cdf_from_pmf(pmf: Dict[int, float], x: int) -> float:
    """CDF: F(x) = P(X ‚â§ x) = Œ£ P(X = k) for k ‚â§ x.
    
    Example:
        >>> pmf = {1: 1/6, 2: 1/6, 3: 1/6, 4: 1/6, 5: 1/6, 6: 1/6}
        >>> cdf_from_pmf(pmf, 3)
        0.5
    """
    return sum(prob for val, prob in pmf.items() if val <= x)
```

---

## 4. Distribuciones de Probabilidad {#4-distribuciones}

### 4.1 Distribuci√≥n Bernoulli

```python
def bernoulli_pmf(k: int, p: float) -> float:
    """Bernoulli: single trial with success probability p.
    
    P(X = k) = p^k √ó (1-p)^(1-k) for k ‚àà {0, 1}
    
    Example:
        >>> bernoulli_pmf(1, 0.7)  # P(success) with p=0.7
        0.7
    """
    if k == 1:
        return p
    elif k == 0:
        return 1 - p
    else:
        return 0.0
```

### 4.2 Distribuci√≥n Binomial

```python
def factorial(n: int) -> int:
    """Calculate n! iteratively."""
    result = 1
    for i in range(2, n + 1):
        result *= i
    return result


def binomial_coefficient(n: int, k: int) -> int:
    """C(n, k) = n! / (k! √ó (n-k)!)."""
    return factorial(n) // (factorial(k) * factorial(n - k))


def binomial_pmf(k: int, n: int, p: float) -> float:
    """Binomial: k successes in n independent trials.
    
    P(X = k) = C(n,k) √ó p^k √ó (1-p)^(n-k)
    
    Example:
        >>> # P(3 caras en 5 lanzamientos)
        >>> binomial_pmf(3, 5, 0.5)
        0.3125
    """
    return binomial_coefficient(n, k) * (p ** k) * ((1 - p) ** (n - k))
```

### 4.3 Distribuci√≥n Normal (Gaussiana) ‚≠ê FUNDAMENTAL

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ   DISTRIBUCI√ìN NORMAL                                                       ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                       ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ                    1              (x - Œº)¬≤                                  ‚îÇ
‚îÇ   f(x) = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ √ó exp(- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ)                               ‚îÇ
‚îÇ          œÉ √ó ‚àö(2œÄ)                  2œÉ¬≤                                     ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   Par√°metros:                                                               ‚îÇ
‚îÇ   ‚Ä¢ Œº (mu) = media (centro de la campana)                                   ‚îÇ
‚îÇ   ‚Ä¢ œÉ (sigma) = desviaci√≥n est√°ndar (ancho)                                 ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ              .---.                                                          ‚îÇ
‚îÇ            .'     '.            68% dentro de 1œÉ                            ‚îÇ
‚îÇ           /    Œº    \           95% dentro de 2œÉ                            ‚îÇ
‚îÇ         _/           \_         99.7% dentro de 3œÉ                          ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ/‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                               ‚îÇ
‚îÇ       Œº-2œÉ  Œº-œÉ  Œº  Œº+œÉ  Œº+2œÉ                                               ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ   ¬øPOR QU√â ES TAN IMPORTANTE?                                               ‚îÇ
‚îÇ   ‚Ä¢ Teorema del L√≠mite Central                                              ‚îÇ
‚îÇ   ‚Ä¢ Muchos fen√≥menos naturales                                              ‚îÇ
‚îÇ   ‚Ä¢ Base de modelos lineales                                                ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

```python
def normal_pdf(x: float, mu: float = 0.0, sigma: float = 1.0) -> float:
    """Probability density function of normal distribution.
    
    Example:
        >>> normal_pdf(0, 0, 1)  # Standard normal at mean
        0.3989422804014327
    """
    coefficient = 1 / (sigma * math.sqrt(2 * math.pi))
    exponent = -((x - mu) ** 2) / (2 * sigma ** 2)
    return coefficient * math.exp(exponent)


def standard_normal_cdf_approx(x: float) -> float:
    """Approximation of standard normal CDF using error function.
    
    Uses the relationship: Œ¶(x) = 0.5 √ó (1 + erf(x/‚àö2))
    """
    return 0.5 * (1 + math.erf(x / math.sqrt(2)))


def z_score(x: float, mu: float, sigma: float) -> float:
    """Standardize a value: z = (x - Œº) / œÉ.
    
    Converts any normal distribution to standard normal.
    
    Example:
        >>> z_score(85, 70, 10)  # Score of 85 with mean 70, std 10
        1.5
    """
    return (x - mu) / sigma
```

### 4.4 Otras Distribuciones Importantes

```python
def poisson_pmf(k: int, lam: float) -> float:
    """Poisson: events in fixed interval.
    
    P(X = k) = (Œª^k √ó e^(-Œª)) / k!
    
    Used for: emails per hour, arrivals per minute.
    
    Example:
        >>> poisson_pmf(3, 2.5)  # 3 events when average is 2.5
        0.21376...
    """
    return (lam ** k * math.exp(-lam)) / factorial(k)


def exponential_pdf(x: float, lam: float) -> float:
    """Exponential: time between Poisson events.
    
    f(x) = Œª √ó e^(-Œªx) for x ‚â• 0
    
    Used for: time until next event.
    """
    if x < 0:
        return 0.0
    return lam * math.exp(-lam * x)
```

---

## 5. Esperanza, Varianza y Momentos {#5-momentos}

### 5.1 Valor Esperado (Media)

```python
def expected_value_discrete(pmf: Dict[float, float]) -> float:
    """E[X] = Œ£ x √ó P(X = x).
    
    The "center of mass" of the distribution.
    
    Example:
        >>> pmf = {1: 1/6, 2: 1/6, 3: 1/6, 4: 1/6, 5: 1/6, 6: 1/6}
        >>> expected_value_discrete(pmf)
        3.5
    """
    return sum(x * prob for x, prob in pmf.items())


def expected_value_sample(data: List[float]) -> float:
    """Sample mean as estimate of E[X].
    
    xÃÑ = (1/n) √ó Œ£ x·µ¢
    """
    return sum(data) / len(data)
```

### 5.2 Varianza y Desviaci√≥n Est√°ndar

```python
def variance_discrete(pmf: Dict[float, float]) -> float:
    """Var(X) = E[(X - Œº)¬≤] = E[X¬≤] - (E[X])¬≤.
    
    Measures spread around the mean.
    """
    mu = expected_value_discrete(pmf)
    return sum((x - mu) ** 2 * prob for x, prob in pmf.items())


def variance_sample(data: List[float]) -> float:
    """Sample variance (unbiased estimator).
    
    s¬≤ = (1/(n-1)) √ó Œ£ (x·µ¢ - xÃÑ)¬≤
    """
    n = len(data)
    mean = expected_value_sample(data)
    return sum((x - mean) ** 2 for x in data) / (n - 1)


def std_dev_sample(data: List[float]) -> float:
    """Sample standard deviation."""
    return math.sqrt(variance_sample(data))
```

### 5.3 Covarianza y Correlaci√≥n

```python
def covariance(x: List[float], y: List[float]) -> float:
    """Cov(X, Y) = E[(X - Œº‚Çì)(Y - Œº·µß)].
    
    Measures linear relationship between variables.
    ‚Ä¢ Cov > 0: positive relationship
    ‚Ä¢ Cov < 0: negative relationship
    ‚Ä¢ Cov = 0: no linear relationship (not necessarily independent!)
    """
    n = len(x)
    mean_x = sum(x) / n
    mean_y = sum(y) / n
    return sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y)) / (n - 1)


def correlation(x: List[float], y: List[float]) -> float:
    """Pearson correlation: œÅ = Cov(X,Y) / (œÉ‚Çì √ó œÉ·µß).
    
    Normalized to [-1, 1].
    ‚Ä¢ œÅ = 1: perfect positive linear relationship
    ‚Ä¢ œÅ = -1: perfect negative linear relationship
    ‚Ä¢ œÅ = 0: no linear relationship
    
    IMPORTANT for ML: Correlation ‚â† Causation!
    """
    cov = covariance(x, y)
    std_x = std_dev_sample(x)
    std_y = std_dev_sample(y)
    
    if std_x == 0 or std_y == 0:
        return 0.0
    
    return cov / (std_x * std_y)
```

---

## ‚ö†Ô∏è Conceptos Clave para ML

### Independence vs Conditional Independence

```
INDEPENDENCIA:
P(A ‚à© B) = P(A) √ó P(B)
P(A|B) = P(A)  (conocer B no cambia A)

INDEPENDENCIA CONDICIONAL (crucial para Naive Bayes):
P(A ‚à© B | C) = P(A|C) √ó P(B|C)

Aunque A y B no sean independientes, pueden serlo dado C.
Naive Bayes ASUME que features son independientes dado la clase.
```

### Law of Large Numbers

```
A medida que n ‚Üí ‚àû:
‚Ä¢ Sample mean ‚Üí True mean
‚Ä¢ Sample variance ‚Üí True variance

Justifica usar estad√≠sticas muestrales como estimadores.
```

### Central Limit Theorem ‚≠ê

```
La suma/promedio de muchas variables aleatorias independientes
tiende a una distribuci√≥n NORMAL, sin importar la distribuci√≥n original.

IMPLICACI√ìN PARA ML:
‚Ä¢ Muchos errores se distribuyen normalmente
‚Ä¢ Justifica asumir normalidad en muchos modelos
‚Ä¢ Base te√≥rica de muchos m√©todos estad√≠sticos
```

---

## üîß Ejercicios Pr√°cticos

### Ejercicio 19.1: Bayes para Diagn√≥stico
Un test tiene 99% sensibilidad, 95% especificidad. La enfermedad afecta al 1% de la poblaci√≥n. ¬øCu√°l es P(enfermo | test+)?

### Ejercicio 19.2: Distribuci√≥n Binomial
Si 30% de emails son spam, ¬øcu√°l es la probabilidad de recibir exactamente 4 spam en 10 emails?

### Ejercicio 19.3: Naive Bayes
Implementar clasificador de sentimiento usando el c√≥digo de ejemplo.

---

## üìö Recursos Externos

| Recurso | Tipo | Prioridad |
|---------|------|-----------|
| [Probability for Data Science](https://www.coursera.org/learn/machine-learning-probability-and-statistics) | Curso | üî¥ Obligatorio |
| [3Blue1Brown: Bayes](https://www.youtube.com/watch?v=HZGCoVF3YvM) | Video | üî¥ Obligatorio |
| [Khan Academy: Statistics](https://www.khanacademy.org/math/statistics-probability) | Curso | üü° Recomendado |

---

## üîó Referencias del Glosario

- [Probabilidad Condicional](GLOSARIO.md#probabilidad-condicional)
- [Teorema de Bayes](GLOSARIO.md#teorema-de-bayes)
- [Distribuci√≥n Normal](GLOSARIO.md#distribucion-normal)
- [Esperanza Matem√°tica](GLOSARIO.md#esperanza-matematica)
- [Varianza](GLOSARIO.md#varianza)

---

## üß≠ Navegaci√≥n

| ‚Üê Anterior | √çndice | Siguiente ‚Üí |
|------------|--------|-------------|
| [18_HEAPS](18_HEAPS.md) | [00_INDICE](00_INDICE.md) | [20_ESTADISTICA_INFERENCIAL](20_ESTADISTICA_INFERENCIAL.md) |
