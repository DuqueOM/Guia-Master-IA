{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec036ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lab 1: MLE/MAP y Estimadores Estadísticos\n",
    "==========================================\n",
    "\n",
    "Módulo: M04 - Probabilidad y Estadística\n",
    "Tiempo Estimado: 2-3 horas\n",
    "Prerequisitos: Cálculo diferencial, probabilidad básica\n",
    "\n",
    "Objetivos de Aprendizaje:\n",
    "-------------------------\n",
    "1. Derivar MLE para distribuciones comunes (Bernoulli, Gaussiana, Poisson)\n",
    "2. Implementar MLE numéricamente con scipy.optimize\n",
    "3. Comparar MLE vs MAP con diferentes priors\n",
    "4. Visualizar el trade-off sesgo-varianza\n",
    "\n",
    "Referencias:\n",
    "------------\n",
    "- Murphy, \"ML: A Probabilistic Perspective\", Cap. 3\n",
    "- Bishop, \"Pattern Recognition and ML\", Cap. 2\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from scipy import optimize, stats\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef45ae05",
   "metadata": {},
   "source": [
    "# Lab 1: Maximum Likelihood y Maximum A Posteriori\n",
    "\n",
    "## Introducción\n",
    "\n",
    "| Método | Fórmula | Interpretación |\n",
    "|--------|---------|----------------|\n",
    "| **MLE** | θ̂ = argmax P(D\\|θ) | Maximiza verosimilitud |\n",
    "| **MAP** | θ̂ = argmax P(θ\\|D) = argmax P(D\\|θ)P(θ) | Incluye prior |\n",
    "\n",
    "MAP = MLE cuando el prior es uniforme (no informativo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb702c5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Parte 1: MLE Analítico (45 min)\n",
    "\n",
    "### 1.1 MLE para Distribución de Bernoulli\n",
    "\n",
    "Datos: x₁, x₂, ..., xₙ ∈ {0, 1}\n",
    "\n",
    "Likelihood: L(θ) = ∏ θ^xᵢ (1-θ)^(1-xᵢ)\n",
    "\n",
    "Log-likelihood: ℓ(θ) = Σxᵢ log(θ) + Σ(1-xᵢ) log(1-θ)\n",
    "\n",
    "Derivando y igualando a 0: θ̂_MLE = (1/n) Σxᵢ = x̄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ad6b9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def mle_bernoulli(data: NDArray[np.int64]) -> float:\n",
    "    \"\"\"\n",
    "    MLE para parámetro θ de Bernoulli.\n",
    "\n",
    "    θ̂_MLE = media muestral = número de éxitos / n\n",
    "    \"\"\"\n",
    "    return float(np.mean(data))\n",
    "\n",
    "\n",
    "# Demostración\n",
    "print(\"=== MLE para Bernoulli ===\\n\")\n",
    "true_theta = 0.7\n",
    "n_samples = 100\n",
    "data_bernoulli = rng.binomial(1, true_theta, n_samples)\n",
    "\n",
    "theta_mle = mle_bernoulli(data_bernoulli)\n",
    "print(f\"θ verdadero:  {true_theta}\")\n",
    "print(f\"θ̂_MLE:        {theta_mle:.4f}\")\n",
    "print(f\"Datos: {sum(data_bernoulli)} éxitos de {n_samples} intentos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9428ac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### 1.2 MLE para Distribución Gaussiana\n",
    "\n",
    "Datos: x₁, x₂, ..., xₙ ~ N(μ, σ²)\n",
    "\n",
    "Log-likelihood:\n",
    "ℓ(μ, σ²) = -n/2 log(2π) - n/2 log(σ²) - 1/(2σ²) Σ(xᵢ - μ)²\n",
    "\n",
    "Derivando:\n",
    "- μ̂_MLE = (1/n) Σxᵢ = x̄\n",
    "- σ̂²_MLE = (1/n) Σ(xᵢ - x̄)² (¡sesgado!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b9afdb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def mle_gaussian(data: NDArray[np.float64]) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    MLE para parámetros μ y σ² de Gaussiana.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    tuple[float, float]\n",
    "        - μ̂_MLE (media muestral)\n",
    "        - σ̂²_MLE (varianza muestral sesgada, dividiendo por n)\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    mu_mle = float(np.mean(data))\n",
    "    sigma2_mle = float(np.sum((data - mu_mle) ** 2) / n)  # Sesgado (divide por n)\n",
    "    return mu_mle, sigma2_mle\n",
    "\n",
    "\n",
    "# Demostración\n",
    "print(\"\\n=== MLE para Gaussiana ===\\n\")\n",
    "true_mu, true_sigma2 = 5.0, 4.0\n",
    "n_samples = 50\n",
    "data_gaussian = rng.normal(true_mu, np.sqrt(true_sigma2), n_samples)\n",
    "\n",
    "mu_mle, sigma2_mle = mle_gaussian(data_gaussian)\n",
    "print(f\"Parámetros verdaderos: μ = {true_mu}, σ² = {true_sigma2}\")\n",
    "print(f\"MLE: μ̂ = {mu_mle:.4f}, σ̂² = {sigma2_mle:.4f}\")\n",
    "print(\"\\n⚠️ Nota: σ̂²_MLE está sesgado. Estimador insesgado divide por (n-1).\")\n",
    "print(f\"   σ̂² insesgado = {np.var(data_gaussian, ddof=1):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9150d1d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### 1.3 MLE para Distribución de Poisson\n",
    "\n",
    "Datos: x₁, x₂, ..., xₙ ~ Poisson(λ)\n",
    "\n",
    "Likelihood: L(λ) = ∏ (λ^xᵢ e^(-λ)) / xᵢ!\n",
    "\n",
    "Log-likelihood: ℓ(λ) = Σxᵢ log(λ) - nλ - Σlog(xᵢ!)\n",
    "\n",
    "Derivando: λ̂_MLE = (1/n) Σxᵢ = x̄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045fa33d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def mle_poisson(data: NDArray[np.int64]) -> float:\n",
    "    \"\"\"MLE para parámetro λ de Poisson.\"\"\"\n",
    "    return float(np.mean(data))\n",
    "\n",
    "\n",
    "print(\"\\n=== MLE para Poisson ===\\n\")\n",
    "true_lambda = 3.5\n",
    "data_poisson = rng.poisson(true_lambda, 100)\n",
    "\n",
    "lambda_mle = mle_poisson(data_poisson)\n",
    "print(f\"λ verdadero:  {true_lambda}\")\n",
    "print(f\"λ̂_MLE:        {lambda_mle:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e37266",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Parte 2: MLE Numérico (30 min)\n",
    "\n",
    "Cuando no hay solución analítica, usamos optimización numérica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ee0ed",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def negative_log_likelihood_gaussian(\n",
    "    params: tuple[float, float],\n",
    "    data: NDArray[np.float64],\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Negative log-likelihood para Gaussiana (para minimizar).\n",
    "\n",
    "    params: (μ, log_σ²) - usamos log para evitar σ² < 0\n",
    "    \"\"\"\n",
    "    mu, log_sigma2 = params\n",
    "    sigma2 = np.exp(log_sigma2)\n",
    "    n = len(data)\n",
    "\n",
    "    nll = 0.5 * n * np.log(2 * np.pi * sigma2) + np.sum((data - mu) ** 2) / (2 * sigma2)\n",
    "    return float(nll)\n",
    "\n",
    "\n",
    "def mle_gaussian_numeric(data: NDArray[np.float64]) -> tuple[float, float]:\n",
    "    \"\"\"MLE numérico para Gaussiana usando scipy.optimize.\"\"\"\n",
    "    # Inicialización\n",
    "    mu_init = np.mean(data)\n",
    "    sigma2_init = np.var(data)\n",
    "\n",
    "    result = optimize.minimize(\n",
    "        negative_log_likelihood_gaussian,\n",
    "        x0=[mu_init, np.log(sigma2_init)],\n",
    "        args=(data,),\n",
    "        method=\"BFGS\",\n",
    "    )\n",
    "\n",
    "    mu_mle = result.x[0]\n",
    "    sigma2_mle = np.exp(result.x[1])\n",
    "\n",
    "    return mu_mle, sigma2_mle\n",
    "\n",
    "\n",
    "# Comparar analítico vs numérico\n",
    "print(\"\\n=== Comparación MLE Analítico vs Numérico ===\\n\")\n",
    "mu_analytic, sigma2_analytic = mle_gaussian(data_gaussian)\n",
    "mu_numeric, sigma2_numeric = mle_gaussian_numeric(data_gaussian)\n",
    "\n",
    "print(f\"Analítico: μ̂ = {mu_analytic:.6f}, σ̂² = {sigma2_analytic:.6f}\")\n",
    "print(f\"Numérico:  μ̂ = {mu_numeric:.6f}, σ̂² = {sigma2_numeric:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9083f99f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Parte 3: MAP con Prior Conjugado (45 min)\n",
    "\n",
    "### Beta-Binomial: Prior conjugado para Bernoulli\n",
    "\n",
    "Prior: θ ~ Beta(α, β)\n",
    "Likelihood: x₁, ..., xₙ | θ ~ Bernoulli(θ)\n",
    "Posterior: θ | x ~ Beta(α + Σxᵢ, β + n - Σxᵢ)\n",
    "\n",
    "θ̂_MAP = (α + Σxᵢ - 1) / (α + β + n - 2)\n",
    "\n",
    "Para α = β = 1 (prior uniforme): θ̂_MAP = θ̂_MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de865b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_bernoulli(data: NDArray[np.int64], alpha: float, beta: float) -> float:\n",
    "    \"\"\"\n",
    "    MAP para Bernoulli con prior Beta(α, β).\n",
    "\n",
    "    θ̂_MAP = (α + k - 1) / (α + β + n - 2)\n",
    "\n",
    "    donde k = número de éxitos, n = tamaño muestral.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    k = np.sum(data)\n",
    "\n",
    "    # Modo de Beta(α + k, β + n - k)\n",
    "    alpha_post = alpha + k\n",
    "    beta_post = beta + n - k\n",
    "\n",
    "    if alpha_post > 1 and beta_post > 1:\n",
    "        theta_map = (alpha_post - 1) / (alpha_post + beta_post - 2)\n",
    "    else:\n",
    "        # Si posterior no tiene modo bien definido, usar media\n",
    "        theta_map = alpha_post / (alpha_post + beta_post)\n",
    "\n",
    "    return float(theta_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f8f2d0",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Comparar MLE vs MAP con diferentes priors"
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== MLE vs MAP con Prior Beta ===\\n\")\n",
    "\n",
    "# Escenario: pocos datos, θ real = 0.8\n",
    "true_theta = 0.8\n",
    "n_small = 10\n",
    "data_small = rng.binomial(1, true_theta, n_small)\n",
    "k = sum(data_small)\n",
    "print(f\"Datos: {k} éxitos de {n_small} intentos (θ verdadero = {true_theta})\\n\")\n",
    "\n",
    "# MLE\n",
    "theta_mle = mle_bernoulli(data_small)\n",
    "print(f\"MLE:                        θ̂ = {theta_mle:.4f}\")\n",
    "\n",
    "# MAP con diferentes priors\n",
    "priors = [\n",
    "    (1, 1, \"Uniforme (α=1, β=1)\"),\n",
    "    (2, 2, \"Débil centrado en 0.5 (α=2, β=2)\"),\n",
    "    (5, 5, \"Fuerte centrado en 0.5 (α=5, β=5)\"),\n",
    "    (10, 2, \"Fuerte sesgado hacia 0.8 (α=10, β=2)\"),\n",
    "]\n",
    "\n",
    "for alpha, beta, desc in priors:\n",
    "    theta_map = map_bernoulli(data_small, alpha, beta)\n",
    "    print(f\"MAP {desc}: θ̂ = {theta_map:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316bcbc2",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Visualizar efecto del prior"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "theta_range = np.linspace(0.001, 0.999, 200)\n",
    "\n",
    "# Prior débil\n",
    "alpha, beta = 2, 2\n",
    "k, n = sum(data_small), len(data_small)\n",
    "prior = stats.beta.pdf(theta_range, alpha, beta)\n",
    "likelihood = stats.binom.pmf(k, n, theta_range)\n",
    "posterior = stats.beta.pdf(theta_range, alpha + k, beta + n - k)\n",
    "\n",
    "axes[0].plot(theta_range, prior / max(prior), \"b--\", label=\"Prior\")\n",
    "axes[0].plot(theta_range, likelihood / max(likelihood), \"g-.\", label=\"Likelihood\")\n",
    "axes[0].plot(\n",
    "    theta_range, posterior / max(posterior), \"r-\", linewidth=2, label=\"Posterior\"\n",
    ")\n",
    "axes[0].axvline(\n",
    "    mle_bernoulli(data_small),\n",
    "    color=\"green\",\n",
    "    linestyle=\":\",\n",
    "    label=f\"MLE={mle_bernoulli(data_small):.2f}\",\n",
    ")\n",
    "axes[0].axvline(\n",
    "    map_bernoulli(data_small, alpha, beta),\n",
    "    color=\"red\",\n",
    "    linestyle=\":\",\n",
    "    label=f\"MAP={map_bernoulli(data_small, alpha, beta):.2f}\",\n",
    ")\n",
    "axes[0].set_title(f\"Prior débil: Beta({alpha}, {beta})\")\n",
    "axes[0].set_xlabel(\"θ\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Prior fuerte centrado\n",
    "alpha, beta = 10, 10\n",
    "posterior = stats.beta.pdf(theta_range, alpha + k, beta + n - k)\n",
    "prior = stats.beta.pdf(theta_range, alpha, beta)\n",
    "\n",
    "axes[1].plot(theta_range, prior / max(prior), \"b--\", label=\"Prior\")\n",
    "axes[1].plot(theta_range, likelihood / max(likelihood), \"g-.\", label=\"Likelihood\")\n",
    "axes[1].plot(\n",
    "    theta_range, posterior / max(posterior), \"r-\", linewidth=2, label=\"Posterior\"\n",
    ")\n",
    "axes[1].axvline(\n",
    "    map_bernoulli(data_small, alpha, beta),\n",
    "    color=\"red\",\n",
    "    linestyle=\":\",\n",
    "    label=f\"MAP={map_bernoulli(data_small, alpha, beta):.2f}\",\n",
    ")\n",
    "axes[1].set_title(f\"Prior fuerte: Beta({alpha}, {beta})\")\n",
    "axes[1].set_xlabel(\"θ\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Prior informativo correcto\n",
    "alpha, beta = 8, 2\n",
    "posterior = stats.beta.pdf(theta_range, alpha + k, beta + n - k)\n",
    "prior = stats.beta.pdf(theta_range, alpha, beta)\n",
    "\n",
    "axes[2].plot(theta_range, prior / max(prior), \"b--\", label=\"Prior\")\n",
    "axes[2].plot(theta_range, likelihood / max(likelihood), \"g-.\", label=\"Likelihood\")\n",
    "axes[2].plot(\n",
    "    theta_range, posterior / max(posterior), \"r-\", linewidth=2, label=\"Posterior\"\n",
    ")\n",
    "axes[2].axvline(\n",
    "    map_bernoulli(data_small, alpha, beta),\n",
    "    color=\"red\",\n",
    "    linestyle=\":\",\n",
    "    label=f\"MAP={map_bernoulli(data_small, alpha, beta):.2f}\",\n",
    ")\n",
    "axes[2].set_title(f\"Prior informativo: Beta({alpha}, {beta})\")\n",
    "axes[2].set_xlabel(\"θ\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../assets/mle_vs_map.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01895f2d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Parte 4: Sesgo-Varianza (30 min)\n",
    "\n",
    "### Trade-off fundamental\n",
    "\n",
    "MSE(θ̂) = Bias(θ̂)² + Var(θ̂)\n",
    "\n",
    "- MLE: bajo sesgo, alta varianza (con pocos datos)\n",
    "- MAP: puede tener sesgo pero menor varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2b56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_estimator_bias_variance(\n",
    "    true_theta: float,\n",
    "    n_samples: int,\n",
    "    n_simulations: int,\n",
    "    alpha: float,\n",
    "    beta: float,\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Simula múltiples datasets para calcular sesgo y varianza de MLE y MAP.\n",
    "    \"\"\"\n",
    "    mle_estimates_list: list[float] = []\n",
    "    map_estimates_list: list[float] = []\n",
    "\n",
    "    for _ in range(n_simulations):\n",
    "        data = rng.binomial(1, true_theta, n_samples)\n",
    "        mle_estimates_list.append(mle_bernoulli(data))\n",
    "        map_estimates_list.append(map_bernoulli(data, alpha, beta))\n",
    "\n",
    "    mle_arr = np.array(mle_estimates_list)\n",
    "    map_arr = np.array(map_estimates_list)\n",
    "\n",
    "    # Sesgo\n",
    "    bias_mle = float(np.mean(mle_arr) - true_theta)\n",
    "    bias_map = float(np.mean(map_arr) - true_theta)\n",
    "\n",
    "    # Varianza\n",
    "    var_mle = float(np.var(mle_arr))\n",
    "    var_map = float(np.var(map_arr))\n",
    "\n",
    "    return bias_mle, var_mle, bias_map, var_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2534e9fe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Análisis Sesgo-Varianza ===\\n\")\n",
    "\n",
    "true_theta = 0.7\n",
    "n_simulations = 1000\n",
    "alpha_prior, beta_prior = 5, 5  # Prior centrado en 0.5\n",
    "\n",
    "print(f\"θ verdadero = {true_theta}, Prior = Beta({alpha_prior}, {beta_prior})\\n\")\n",
    "print(\n",
    "    f\"{'n':>5} | {'Bias MLE':>10} | {'Var MLE':>10} | {'MSE MLE':>10} | \"\n",
    "    f\"{'Bias MAP':>10} | {'Var MAP':>10} | {'MSE MAP':>10}\"\n",
    ")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "sample_sizes = [5, 10, 20, 50, 100, 500]\n",
    "mse_mle_list = []\n",
    "mse_map_list = []\n",
    "\n",
    "for n in sample_sizes:\n",
    "    bias_mle, var_mle, bias_map, var_map = simulate_estimator_bias_variance(\n",
    "        true_theta, n, n_simulations, alpha_prior, beta_prior\n",
    "    )\n",
    "    mse_mle = bias_mle**2 + var_mle\n",
    "    mse_map = bias_map**2 + var_map\n",
    "    mse_mle_list.append(mse_mle)\n",
    "    mse_map_list.append(mse_map)\n",
    "\n",
    "    print(\n",
    "        f\"{n:>5} | {bias_mle:>10.4f} | {var_mle:>10.4f} | {mse_mle:>10.4f} | \"\n",
    "        f\"{bias_map:>10.4f} | {var_map:>10.4f} | {mse_map:>10.4f}\"\n",
    "    )\n",
    "\n",
    "# Visualizar\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(sample_sizes, mse_mle_list, \"o-\", label=\"MSE (MLE)\", linewidth=2)\n",
    "ax.plot(sample_sizes, mse_map_list, \"s-\", label=\"MSE (MAP)\", linewidth=2)\n",
    "ax.set_xlabel(\"Tamaño de muestra (n)\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.set_title(\"MSE de MLE vs MAP en función del tamaño de muestra\")\n",
    "ax.legend()\n",
    "ax.set_xscale(\"log\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../assets/bias_variance_mle_map.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Observación: MAP tiene menor MSE con pocos datos (el prior ayuda),\")\n",
    "print(\"   pero converge a MLE cuando n es grande (los datos dominan).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae46e2d5",
   "metadata": {},
   "source": [
    "## Ejercicios para el Estudiante\n",
    "\n",
    "### Ejercicio 1: MLE para Exponencial\n",
    "Deriva el MLE para λ en una distribución Exponencial(λ).\n",
    "Pista: f(x|λ) = λ exp(-λx)\n",
    "\n",
    "### Ejercicio 2: MAP para Gaussiana con prior Gaussiano\n",
    "Si x ~ N(μ, σ²) con σ² conocido y prior μ ~ N(μ₀, σ₀²),\n",
    "deriva la fórmula del posterior y el MAP.\n",
    "\n",
    "### Ejercicio 3: Regularización como MAP\n",
    "Demuestra que Ridge Regression (L2) corresponde a MLE\n",
    "con prior Gaussiano en los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab744205",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FIN DEL LAB 1: MLE/MAP Y ESTIMADORES\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
