# M√≥dulo 05 - Supervised Learning

> **üéØ Objetivo:** Dominar regresi√≥n lineal, log√≠stica y m√©tricas de evaluaci√≥n
> **Fase:** 2 - N√∫cleo de ML | **Semanas 9-12**
> **Curso del Pathway:** Introduction to Machine Learning: Supervised Learning

---

<a id="m05-0"></a>

## üß≠ C√≥mo usar este m√≥dulo (modo 0‚Üí100)

**Prop√≥sito:** que puedas construir un pipeline supervisado ‚Äúde examen‚Äù:

- entrenar (regresi√≥n lineal/log√≠stica)
- evaluar (m√©tricas)
- validar (train/test, K-fold)
- controlar overfitting (regularizaci√≥n)

### Objetivos de aprendizaje (medibles)

Al terminar este m√≥dulo podr√°s:

- **Implementar** regresi√≥n lineal y regresi√≥n log√≠stica desde cero.
- **Derivar** el gradiente de MSE y de cross-entropy (con la forma `X·µÄ(≈∑ - y)`).
- **Elegir** m√©tricas correctas seg√∫n el costo de FP/FN.
- **Aplicar** validaci√≥n (split y K-fold) evitando leakage.
- **Validar** tu implementaci√≥n con Shadow Mode (sklearn) como ground truth.
- **Explicar** Entrop√≠a/Gini, Information Gain y el contraste **Bagging vs Boosting** (Random Forest vs Gradient Boosting) a nivel conceptual.

### üß™ Ver para Entender (Laboratorios Interactivos)

- Gu√≠a central: [INTERACTIVE_LABS.md](INTERACTIVE_LABS.md)
- Regresi√≥n lineal (t√°ctil):
  - `streamlit run interactive_labs/m05_supervised/visualizacion_regresion.py`
- Overfitting / Bias‚ÄìVariance (grado 1‚Üí20):
  - `streamlit run interactive_labs/m05_supervised/overfitting_bias_variance_app.py`

### C√°psula (obligatoria): Vectorizaci√≥n extrema (prohibido usar loops)

Regla pr√°ctica para todo el m√≥dulo:

- **Prohibido** iterar con `for` sobre muestras (`N`) o features (`D`) para computar predicciones, p√©rdidas o gradientes.
- **Permitido** iterar sobre iteraciones de entrenamiento (`for step in range(...)`) o √©pocas.

Objetivo: que el *core* de ML sea una composici√≥n de operaciones tipo:

- `logits = X @ W`
- `grad = X.T @ something`

Ejemplos can√≥nicos (con **disciplina de shapes** y sin loops):

```python
import numpy as np  # NumPy: √°lgebra lineal y vectorizaci√≥n


# ============================================================
# 1) Forward multiclase: logits = X @ W
# ============================================================
N = 5  # N: n√∫mero de muestras
D = 4  # D: n√∫mero de features
K = 3  # K: n√∫mero de clases

X = np.random.randn(N, D).astype(float)  # X:(N,D) batch de entrada
assert X.shape == (N, D)  # Contrato de shape para X

W = np.random.randn(D, K).astype(float)  # W:(D,K) pesos por clase
assert W.shape == (D, K)  # Contrato de shape para W

logits = X @ W  # logits:(N,K) porque (N,D)@(D,K)=(N,K)
assert logits.shape == (N, K)  # Contrato: logits debe ser 2D (batch x clases)


# ============================================================
# 2) Log√≠stica binaria: gradiente vectorizado ‚àáw = (1/N) X^T(≈∑ - y)
# ============================================================
w = np.random.randn(D).astype(float)  # w:(D,) pesos binarios (una clase)
assert w.shape == (D,)  # Contrato de shape para w

y = (np.random.rand(N) > 0.5).astype(float)  # y:(N,) etiquetas binarias en {0,1}
assert y.shape == (N,)  # Contrato de shape para y

z = X @ w  # z:(N,) logits binarios
assert z.shape == (N,)  # Contrato de shape para z

y_hat = 1.0 / (1.0 + np.exp(-z))  # sigmoid(z) vectorizada (sin loops)
assert y_hat.shape == (N,)  # Contrato de shape para ≈∑

grad_w = (X.T @ (y_hat - y)) / N  # (D,N)@(N,)=(D,) (forma de examen)
assert grad_w.shape == (D,)  # Contrato: gradiente debe tener el shape de w
# ============================================================
# 3) Distancias pairwise sin loops (kNN / clustering):
#    dist2[i,j] = ||X_query[i] - X_train[j]||^2
# ============================================================
M = 6  # M: n√∫mero de queries
X_train = np.random.randn(N, D).astype(float)  # X_train:(N,D)
X_query = np.random.randn(M, D).astype(float)  # X_query:(M,D)
assert X_train.shape == (N, D)  # Shape correcto para broadcasting
assert X_query.shape == (M, D)  # Shape correcto para broadcasting

# Trick algebraico: ||a-b||^2 = ||a||^2 + ||b||^2 - 2 a¬∑b
q_norm2 = np.sum(X_query ** 2, axis=1, keepdims=True)  # (M,1) ||q_i||^2
t_norm2 = np.sum(X_train ** 2, axis=1, keepdims=True).T  # (1,N) ||t_j||^2
cross = X_query @ X_train.T  # (M,N) producto punto entre cada par (q_i, t_j)

dist2 = q_norm2 + t_norm2 - 2.0 * cross  # (M,N) distancias cuadradas
dist2 = np.maximum(dist2, 0.0)  # Evita negativos por error num√©rico (float)
assert dist2.shape == (M, N)  # Shape correcto de matriz de distancias
```

Enlaces r√°pidos:

- [04_PROBABILIDAD_ML.md](04_PROBABILIDAD_ML.md) (MLE ‚Üí cross-entropy)
- [GLOSARIO.md](GLOSARIO.md)
- [RECURSOS.md](RECURSOS.md)
- [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)
- [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md)
- Evaluaci√≥n (r√∫brica): [study_tools/RUBRICA_v1.md](../study_tools/RUBRICA_v1.md) (scope `M05` en `rubrica.csv`)

### Recursos (cu√°ndo usarlos)

| Prioridad | Recurso | Cu√°ndo usarlo en este m√≥dulo | Para qu√© |
|----------|---------|------------------------------|----------|
| **Obligatorio** | [04_PROBABILIDAD_ML.md](04_PROBABILIDAD_ML.md) | Antes de implementar `log-loss`/cross-entropy y el gradiente de log√≠stica | Conectar MLE ‚Üí cross-entropy y evitar derivaciones ‚Äúde memoria‚Äù |
| **Obligatorio** | `study_tools/DIRTY_DATA_CHECK.md` | Antes del primer entrenamiento real (Semana 9‚Äì10), al preparar datasets | Evitar que el modelo ‚Äúaprenda basura‚Äù por fallas de datos |
| **Obligatorio** | `study_tools/DIARIO_ERRORES.md` | Cada vez que veas m√©tricas incoherentes, accuracy ‚Äúm√°gico‚Äù o divergencia | Registrar bugs, causas y fixes reproducibles |
| **Complementario** | [StatQuest ML (playlist)](https://www.youtube.com/playlist?list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF) | Semana 10‚Äì12 (log√≠stica, m√©tricas, regularizaci√≥n) | Refuerzo conceptual r√°pido + ejemplos |
| **Complementario** | [Stanford CS229](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) | Despu√©s de implementar regresi√≥n lineal/log√≠stica (para profundizar) | Profundizar en teor√≠a y derivaciones est√°ndar |
| **Opcional** | [RECURSOS.md](RECURSOS.md) | Al finalizar el m√≥dulo (para escoger pr√°ctica extra) | Expandir sin perder el foco del Pathway |

---

## üß† ¬øQu√© es Supervised Learning?

```text
APRENDIZAJE SUPERVISADO

Tenemos:
- Datos de entrada X (features)
- Etiquetas Y (targets/labels)

Objetivo: Aprender una funci√≥n f tal que f(X) ‚âà Y

Tipos principales:
‚îú‚îÄ‚îÄ REGRESI√ìN: Y es continuo (precio, temperatura)
‚îÇ   ‚îî‚îÄ‚îÄ Output: n√∫mero real
‚îî‚îÄ‚îÄ CLASIFICACI√ìN: Y es discreto (spam/no spam, d√≠gito 0-9)
    ‚îî‚îÄ‚îÄ Output: clase o probabilidad
```

---

## üìö Contenido del M√≥dulo

| Semana | Tema | Entregable |
|--------|------|------------|
| 9 | Regresi√≥n Lineal | `linear_regression.py` |
| 10 | Regresi√≥n Log√≠stica | `logistic_regression.py` |
| 11 | M√©tricas de Evaluaci√≥n | `metrics.py` |
| 12 | Validaci√≥n + Regularizaci√≥n + √Årboles | Cross-validation, L1/L2 + Tree-Based Models |

---

## üíª Parte 1: Regresi√≥n Lineal

### 1.1 Modelo

```python
import numpy as np  # Importa NumPy para arrays y operaciones num√©ricas

"""
REGRESI√ìN LINEAL

Hip√≥tesis: h(x) = Œ∏‚ÇÄ + Œ∏‚ÇÅx‚ÇÅ + Œ∏‚ÇÇx‚ÇÇ + ... + Œ∏‚Çôx‚Çô
         = Œ∏·µÄx (forma matricial)

Donde:
- Œ∏ (theta): par√°metros/pesos del modelo
- x: vector de features (con x‚ÇÄ = 1 para el bias)

En forma matricial para m√∫ltiples muestras:
    ≈∑ = XŒ∏

Donde:
- X: matriz (m √ó n+1) con m muestras y n features + columna de 1s
- Œ∏: vector (n+1 √ó 1) de par√°metros
- ≈∑: vector (m √ó 1) de predicciones
"""

def add_bias_term(X: np.ndarray) -> np.ndarray:  # a√±ade t√©rmino de bias (columna de 1s)
    """A√±ade columna de 1s para el t√©rmino de bias."""
    m = X.shape[0]  # n√∫mero de muestras (filas) en X
    return np.column_stack([np.ones(m), X])  # concatena columna de 1s (bias) con X

def predict_linear(X: np.ndarray, theta: np.ndarray) -> np.ndarray:  # predicci√≥n lineal: y_hat = XŒ∏
    """Predicci√≥n lineal: ≈∑ = XŒ∏"""
    return X @ theta  # producto matricial para obtener las predicciones
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 1.1: Modelo (Regresi√≥n Lineal)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** De la hip√≥tesis `≈∑=XŒ∏` a un contrato de shapes (y por qu√© el bias es ‚Äúfeature 0‚Äù)
- **ID (opcional):** `M05-T01_1`
- **Duraci√≥n estimada:** 60‚Äì120 min
- **Nivel:** B√°sico‚ÄìIntermedio
- **Dependencias:** √Ålgebra lineal m√≠nima (producto matriz-vector), noci√≥n de dataset tabular

#### 2) Objetivos
- Escribir la hip√≥tesis en forma escalar y matricial y explicar qu√© representa cada s√≠mbolo.
- Usar una convenci√≥n de shapes sin ambig√ºedad: `X:(m,n)` y `Œ∏:(n+1,)` tras agregar bias.
- Verificar r√°pidamente si una implementaci√≥n est√° ‚Äúbien cableada‚Äù (shape checks).

#### 3) Relevancia
- Todo el resto del m√≥dulo (log√≠stica, m√©tricas, regularizaci√≥n) depende de tener claro el *forward* `X @ Œ∏`.
- La mayor√≠a de bugs ‚Äúmisteriosos‚Äù en ML-from-scratch son bugs de shapes, no de matem√°ticas.

#### 4) Mapa conceptual m√≠nimo
- **Datos** `X` (features) + **par√°metros** `Œ∏` ‚Üí **predicci√≥n** `≈∑`.
- **Bias** ‚Üí se implementa como `x‚ÇÄ=1` y `Œ∏‚ÇÄ`.

#### 5) Definiciones esenciales
- `m`: n√∫mero de muestras.
- `n`: n√∫mero de features (sin bias).
- `Œ∏‚ÇÄ`: intercepto/bias.

#### 6) Explicaci√≥n did√°ctica
- Tr√°talo como ‚Äúcontrato‚Äù: si `add_bias_term(X)` devuelve `(m,n+1)`, entonces `Œ∏` debe tener longitud `n+1`.

#### 7) Ejemplo modelado
- Dataset 1D (`n=1`): `X:(m,1)` ‚Üí con bias `X_b:(m,2)` y `Œ∏:(2,)`.

#### 8) Pr√°ctica guiada
- Escribe 3 asserts: shapes de `X_b`, `Œ∏`, `X_b @ Œ∏`.

#### 9) Pr√°ctica independiente
- Convierte un dataset con 3 features a `X_b` y verifica que el forward funciona sin loops.

#### 10) Autoevaluaci√≥n
- ¬øPor qu√© `x‚ÇÄ=1` hace que el intercepto sea un peso m√°s?

#### 11) Errores comunes
- Duplicar bias (agregar columna de 1s dos veces).
- Usar `Œ∏` como columna `(n+1,1)` y luego mezclar con `(n+1,)` sin querer.

#### 12) Retenci√≥n
- Mantra: `≈∑ = X_b @ Œ∏` y el bias es `x‚ÇÄ=1`.

#### 13) Diferenciaci√≥n
- Avanzado: generaliza a multiclase `logits = X @ W`.

#### 14) Recursos
- Cheatsheet de shapes y producto matricial.

#### 15) Nota docente
- Pide que el alumno ‚Äúdebuggee en voz alta‚Äù un error de shape t√≠pico (ej. `(m,n)@(n+1,)`).
</details>

### 1.2 Funci√≥n de Costo (MSE)

```python
import numpy as np  # Importa NumPy para arrays y operaciones num√©ricas

def mse_cost(X: np.ndarray, y: np.ndarray, theta: np.ndarray) -> float:  # calcula el costo MSE para Œ∏
    """
    Mean Squared Error Cost Function.

    J(Œ∏) = (1/2m) Œ£·µ¢ (h(x·µ¢) - y·µ¢)¬≤
         = (1/2m) ||XŒ∏ - y||¬≤

    El factor 1/2 es por conveniencia (cancela con la derivada).
    """
    m = len(y)  # n√∫mero de muestras
    predictions = X @ theta  # predicciones del modelo (≈∑)
    errors = predictions - y  # residuos (≈∑ - y)
    return (1 / (2 * m)) * np.sum(errors ** 2)  # costo MSE con factor 1/2m

def mse_gradient(X: np.ndarray, y: np.ndarray, theta: np.ndarray) -> np.ndarray:  # gradiente del MSE respecto a Œ∏
    """
    Gradiente del MSE respecto a Œ∏.

    ‚àÇJ/‚àÇŒ∏ = (1/m) X·µÄ(XŒ∏ - y)
    """
    m = len(y)  # n√∫mero de muestras
    predictions = X @ theta  # predicciones del modelo (≈∑)
    errors = predictions - y  # residuos (≈∑ - y)
    return (1 / m) * X.T @ errors  # gradiente vectorizado: (1/m) X·µÄ(≈∑-y)
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 1.2: Funci√≥n de Costo (MSE)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** MSE como ‚Äúpenalizaci√≥n cuadr√°tica‚Äù y como `||XŒ∏-y||¬≤` (con lectura geom√©trica)
- **ID (opcional):** `M05-T01_2`
- **Duraci√≥n estimada:** 60‚Äì120 min
- **Nivel:** B√°sico‚ÄìIntermedio
- **Dependencias:** 1.1, suma de cuadrados, producto `X.T @ v`

#### 2) Objetivos
- Explicar el MSE en lenguaje natural (errores grandes se penalizan m√°s).
- Reconocer la forma vectorizada del gradiente `‚àáŒ∏ = (1/m) X·µÄ(XŒ∏ - y)`.
- Entender por qu√© aparece `X·µÄ` (proyecci√≥n del error hacia par√°metros).

#### 3) Relevancia
- Este patr√≥n de gradiente `X·µÄ(≈∑-y)` reaparece en log√≠stica (BCE) y en softmax (CCE).

#### 4) Mapa conceptual m√≠nimo
- **Predicci√≥n** `≈∑` ‚Üí **residuo** `(≈∑-y)` ‚Üí **gradiente** `X·µÄ(residuo)`.

#### 5) Definiciones esenciales
- **Residuo**: `r = ≈∑ - y`.
- **Costo**: promedio (o suma) de `r¬≤`.

#### 6) Explicaci√≥n did√°ctica
- El factor `1/2` en el costo suele usarse para simplificar derivadas; el m√≠nimo no cambia.

#### 7) Ejemplo modelado
- Si duplicas un error (de 2 a 4), la contribuci√≥n al costo se cuadruplica (4‚Üí16).

#### 8) Pr√°ctica guiada
- Implementa un test: si `theta` es perfecto (`X@theta==y`), entonces `mse_cost==0` y `mse_gradient==0`.

#### 9) Pr√°ctica independiente
- Compara `mse_cost` con `np.mean((X@theta - y)**2)` y explica la diferencia del `1/2`.

#### 10) Autoevaluaci√≥n
- ¬øQu√© significa que el gradiente apunte hacia donde el costo sube m√°s r√°pido?

#### 11) Errores comunes
- Confundir shapes: `y` como `(m,1)` vs `(m,)`.
- Olvidar el promedio por `m` (magnitud del gradiente depende del batch size).

#### 12) Retenci√≥n
- F√≥rmula clave: `‚àáŒ∏ MSE = (1/m) X·µÄ(XŒ∏-y)`.

#### 13) Diferenciaci√≥n
- Avanzado: conecta con m√≠nimos cuadrados y proyecciones (subespacios).

#### 14) Recursos
- Notas de least squares, interpretaci√≥n geom√©trica.

#### 15) Nota docente
- Pide que el alumno derive la forma vectorizada desde la forma sumatoria (una vez, con calma).
</details>

### 1.3 Soluci√≥n Cerrada (Normal Equation)

```python
import numpy as np  # Importa NumPy para arrays y operaciones num√©ricas

def normal_equation(X: np.ndarray, y: np.ndarray) -> np.ndarray:  # calcula Œ∏ por ecuaci√≥n normal
    """
    Soluci√≥n cerrada para regresi√≥n lineal.

    Œ∏ = (X·µÄX)‚Åª¬π X·µÄy

    Ventajas:
    - No requiere iteraciones
    - No hay hiperpar√°metros (learning rate)

    Desventajas:
    - O(n¬≥) por la inversi√≥n de matriz
    - No funciona si X·µÄX es singular
    - No escala bien para n grande (>10,000 features)
    """
    XtX = X.T @ X  # calcula X·µÄX
    Xty = X.T @ y  # calcula X·µÄy

    # Usar solve en lugar de inv para estabilidad num√©rica
    theta = np.linalg.solve(XtX, Xty)  # resuelve el sistema lineal XtX¬∑Œ∏ = Xty
    return theta  # devuelve el vector de par√°metros Œ∏
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 1.3: Soluci√≥n Cerrada (Normal Equation)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Normal Equation: cu√°ndo sirve, cu√°ndo falla y por qu√© `solve` es mejor que `inv`
- **ID (opcional):** `M05-T01_3`
- **Duraci√≥n estimada:** 45‚Äì90 min
- **Nivel:** Intermedio
- **Dependencias:** 1.1‚Äì1.2, noci√≥n de matriz singular/condicionamiento

#### 2) Objetivos
- Implementar `Œ∏ = argmin ||XŒ∏-y||¬≤` v√≠a ecuaciones normales.
- Explicar por qu√© `X·µÄX` puede ser singular o mal condicionada.
- Preferir `np.linalg.solve` sobre `inv` por estabilidad.

#### 3) Relevancia
- Te da un ‚Äúbaseline‚Äù para validar GD: si ambos dan resultados parecidos (cuando aplica), tu GD est√° bien.

#### 4) Mapa conceptual m√≠nimo
- Minimizar SSE ‚Üí derivada = 0 ‚Üí `X·µÄXŒ∏ = X·µÄy`.

#### 5) Definiciones esenciales
- **Singular**: no invertible.
- **Condicionamiento**: sensibilidad num√©rica a perturbaciones.

#### 6) Explicaci√≥n did√°ctica
- En alta dimensi√≥n o con colinealidad fuerte, `X·µÄX` puede ‚Äúromperse‚Äù num√©ricamente.

#### 7) Ejemplo modelado
- Si una feature es combinaci√≥n lineal de otra (duplicada), `X·µÄX` tiende a singular.

#### 8) Pr√°ctica guiada
- Crea una feature duplicada en `X` y observa qu√© ocurre con `np.linalg.solve`.

#### 9) Pr√°ctica independiente
- Implementa Ridge cerrada: `Œ∏=(X·µÄX+ŒªI)^{-1}X·µÄy` (solo conceptual aqu√≠).

#### 10) Autoevaluaci√≥n
- ¬øPor qu√© la complejidad crece como `O(n¬≥)`?

#### 11) Errores comunes
- Usar `inv` por costumbre.
- Olvidar agregar bias antes de la ecuaci√≥n normal.

#### 12) Retenci√≥n
- Regla: si puedes usar closed-form, √∫sala para validar GD (no necesariamente para producci√≥n).

#### 13) Diferenciaci√≥n
- Avanzado: `np.linalg.lstsq` y pseudo-inversa (SVD) como alternativa estable.

#### 14) Recursos
- Documentaci√≥n NumPy: `solve`, `lstsq`, conceptos de singularidad.

#### 15) Nota docente
- Pedir un ‚Äúdiagn√≥stico‚Äù cuando falla: ¬øsingularidad real o num√©rica?
</details>

### 1.4 Gradient Descent para Regresi√≥n

```python
import numpy as np  # Importa NumPy para operaciones matem√°ticas
from typing import List, Tuple  # Importa tipos para anotaciones

class LinearRegression:  # modelo de regresi√≥n lineal con GD o ecuaci√≥n normal
    """Regresi√≥n Lineal implementada desde cero."""

    def __init__(self):  # inicializa par√°metros e historial
        self.theta = None  # Par√°metros del modelo (pesos + bias)
        self.cost_history = []  # Historial de costos para monitoreo

    def fit(  # entrena el modelo usando GD o ecuaci√≥n normal
        self,  # instancia del modelo
        X: np.ndarray,  # features (m, n)
        y: np.ndarray,  # targets (m,)
        method: str = 'gradient_descent',  # m√©todo de entrenamiento
        learning_rate: float = 0.01,  # tasa de aprendizaje (solo GD)
        n_iterations: int = 1000  # n√∫mero de iteraciones (solo GD)
    ) -> 'LinearRegression':  # retorna self para estilo sklearn
        """
        Entrena el modelo.

        Args:
            X: features (m, n)
            y: targets (m,)
            method: 'gradient_descent' o 'normal_equation'
            learning_rate: tasa de aprendizaje (solo para GD)
            n_iterations: n√∫mero de iteraciones (solo para GD)
        """
        # A√±adir bias a las features
        X_b = add_bias_term(X)  # construye X con t√©rmino de bias
        m, n = X_b.shape  # m: muestras, n: features + bias

        if method == 'normal_equation':  # entrena con soluci√≥n cerrada
            self.theta = normal_equation(X_b, y)  # Soluci√≥n anal√≠tica directa
        else:  # entrena con descenso de gradiente
            # Inicializar theta con ceros o valores peque√±os
            self.theta = np.zeros(n)  # inicializa par√°metros

            for i in range(n_iterations):  # iteraciones de GD
                # Calcular gradiente del MSE
                gradient = mse_gradient(X_b, y, self.theta)  # gradiente del coste MSE

                # Actualizar theta usando gradient descent
                self.theta = self.theta - learning_rate * gradient  # update: Œ∏ ‚Üê Œ∏ ‚àí Œ±‚àáŒ∏

                # Guardar costo para monitoreo de convergencia
                cost = mse_cost(X_b, y, self.theta)  # calcula coste actual
                self.cost_history.append(cost)  # guarda hist√≥rico de coste

        return self  # permite chaining

    def predict(self, X: np.ndarray) -> np.ndarray:  # predice y_hat para nuevas muestras
        """Predice valores."""
        X_b = add_bias_term(X)  # A√±ade bias para predicci√≥n
        return X_b @ self.theta  # Predicci√≥n lineal: y = X¬∑Œ∏

    def score(self, X: np.ndarray, y: np.ndarray) -> float:  # calcula R¬≤ como m√©trica de ajuste
        """R¬≤ score."""
        y_pred = self.predict(X)  # Predicciones del modelo
        ss_res = np.sum((y - y_pred) ** 2)  # Suma de residuos al cuadrado
        ss_tot = np.sum((y - np.mean(y)) ** 2)  # Suma total de cuadrados
        return 1 - (ss_res / ss_tot)  # R¬≤ = 1 - (residuos/total)


# Demo de regresi√≥n lineal
np.random.seed(42)  # Fija semilla para reproducibilidad
X = 2 * np.random.rand(100, 1)  # 100 puntos entre 0 y 2
y = 4 + 3 * X.flatten() + np.random.randn(100) * 0.5  # y = 4 + 3x + ruido gaussiano

model = LinearRegression()  # Crea instancia del modelo
model.fit(X, y, method='gradient_descent', learning_rate=0.1, n_iterations=1000)  # Entrena

print(f"Par√°metros aprendidos: {model.theta}")  # Muestra Œ∏ aprendido
print(f"Esperados: [4, 3]")  # Valores te√≥ricos (bias=4, pendiente=3)
print(f"R¬≤ score: {model.score(X, y):.4f}")  # Calidad del ajuste
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 1.4: Gradient Descent para Regresi√≥n</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Gradient Descent ‚Äúde examen‚Äù: convergence, learning rate, y checks de sanidad
- **ID (opcional):** `M05-T01_4`
- **Duraci√≥n estimada:** 90‚Äì150 min
- **Nivel:** Intermedio
- **Dependencias:** 1.1‚Äì1.3, gradiente MSE, noci√≥n de iteraci√≥n/√©pocas

#### 2) Objetivos
- Entrenar regresi√≥n lineal por GD con un `learning_rate` razonable.
- Leer el `cost_history` y detectar divergencia o estancamiento.
- Entender por qu√© la vectorizaci√≥n es obligatoria (performance + claridad).

#### 3) Relevancia
- GD es la base del entrenamiento de modelos m√°s grandes (log√≠stica, MLP). Aqu√≠ practicas el ciclo ‚Äúforward ‚Üí loss ‚Üí grad ‚Üí update‚Äù.

#### 4) Mapa conceptual m√≠nimo
- Inicializar `Œ∏` ‚Üí repetir: `grad = X·µÄ(≈∑-y)/m` ‚Üí `Œ∏ ‚Üê Œ∏ - Œ± grad`.

#### 5) Definiciones esenciales
- **Learning rate (Œ±)**: tama√±o del paso.
- **Divergencia**: el costo sube o se vuelve NaN/inf.
- **Convergencia**: el costo baja y se estabiliza.

#### 6) Explicaci√≥n did√°ctica
- Si `Œ±` es muy grande: saltas el m√≠nimo y explota.
- Si `Œ±` es muy peque√±o: entrenas ‚Äúpara siempre‚Äù.

#### 7) Ejemplo modelado
- En el demo, la soluci√≥n esperada es ~`[4,3]` (con ruido). Si sale lej√≠simos, revisa shapes, bias y `Œ±`.

#### 8) Pr√°ctica guiada
- Imprime cada 100 iteraciones: costo actual. Debe decrecer (aprox).

#### 9) Pr√°ctica independiente
- Implementa early stopping: si la mejora del costo < `tol` por varias iteraciones, det√©n.

#### 10) Autoevaluaci√≥n
- ¬øQu√© pasa si omites el bias? ¬øC√≥mo cambia la recta aprendida?

#### 11) Errores comunes
- No normalizar features ‚Üí GD lento o inestable.
- Mezclar `X` con `X_b` en gradiente/predicci√≥n.
- Reportar R¬≤ en train y creer que generaliza (falta split).

#### 12) Retenci√≥n
- Checklist: bias, shapes, costo decrece, no NaNs, params razonables.

#### 13) Diferenciaci√≥n
- Avanzado: batch vs mini-batch vs SGD (conceptual) y efecto en el ruido del gradiente.

#### 14) Recursos
- Notas de optimizaci√≥n b√°sica, escalado de features.

#### 15) Nota docente
- Pide un ‚Äúprotocolo de debugging‚Äù: 1) overfit test en dataset peque√±o, 2) comparar con normal equation.
</details>

---

## üíª Parte 2: Regresi√≥n Log√≠stica

### 2.0 Regresi√≥n Log√≠stica ‚Äî Nivel: intermedio (core del Pathway)

**Prop√≥sito:** pasar de ‚Äús√© aplicar sigmoid‚Äù a **poder entrenar, derivar y validar** un clasificador binario (y extenderlo a multiclase con One-vs-All).

#### Objetivos de aprendizaje (medibles)

Al terminar esta parte podr√°s:

- **Explicar** por qu√© regresi√≥n log√≠stica es un modelo lineal *sobre el log-odds* (aunque la salida sea una probabilidad).
- **Derivar** (a mano) el gradiente de la p√©rdida *Binary Cross-Entropy* y reconocer la forma `X·µÄ(≈∑ - y)`.
- **Implementar** `fit()` con gradient descent estable (con `clip`/`eps`) y verificar convergencia.
- **Diagnosticar** errores t√≠picos: shapes, overflow en `exp`, signos invertidos, saturaci√≥n de sigmoid.
- **Validar** tu implementaci√≥n con **Shadow Mode** (comparaci√≥n con sklearn) y con un *overfit test* en dataset peque√±o.

#### Prerrequisitos

- De `M√≥dulo 03`: Chain Rule y gradiente.
- De `M√≥dulo 04`: interpretaci√≥n de MLE (conexi√≥n con cross-entropy).

Enlaces r√°pidos:

- [GLOSARIO: Logistic Regression](GLOSARIO.md#logistic-regression)
- [GLOSARIO: Sigmoid](GLOSARIO.md#sigmoid)
- [GLOSARIO: Binary Cross-Entropy](GLOSARIO.md#binary-cross-entropy)
- [GLOSARIO: Gradient Descent](GLOSARIO.md#gradient-descent)
- [RECURSOS.md](RECURSOS.md)

#### Explicaci√≥n progresiva (intuici√≥n ‚Üí formalizaci√≥n ‚Üí implementaci√≥n)

##### a) Intuici√≥n

Quieres un modelo que devuelva:

- un **score lineal** `z = Œ∏·µÄx` (como en regresi√≥n lineal), y
- lo convierta en una **probabilidad** en `(0, 1)`.

Eso lo hace `œÉ(z)`.

##### a.1 Odds, log-odds y por qu√© esto ‚Äúsigue siendo lineal‚Äù

Si el modelo produce `p = P(y=1|x)`, define:

```
odds = p / (1 - p)
logit(p) = log(odds)
```

La regresi√≥n log√≠stica asume que **el log-odds es lineal**:

```
logit(p) = Œ∏·µÄx
```

Y la sigmoide es simplemente la funci√≥n que vuelve de logit a probabilidad:

```
p = œÉ(Œ∏·µÄx) = 1 / (1 + exp(-Œ∏·µÄx))
```

Esto importa porque te permite interpretar el modelo:

- subir `Œ∏·µÄx` en +1 incrementa el **log-odds** en +1 (cambio multiplicativo en odds).

##### a.2 Por qu√© NO usar MSE para clasificaci√≥n

Podr√≠as intentar usar MSE con `≈∑ = œÉ(z)`, pero en pr√°ctica es mala idea:

- **La geometr√≠a del entrenamiento empeora:** el gradiente se vuelve poco informativo cuando `œÉ(z)` se satura (cerca de 0 o 1).
- **La funci√≥n objetivo deja de ser convexa** (puede tener m√≠nimos locales / mesetas), haciendo el descenso de gradiente menos confiable.
- **No penaliza bien el caso ‚Äúseguro y equivocado‚Äù:** si `y=1` pero `≈∑‚âà0`, quieres un castigo enorme; eso lo da `-log(≈∑)`.

Por eso usamos **Log-Loss / Binary Cross-Entropy**, que viene de MLE y es convexa para este modelo.

##### a.3 Visual: frontera de decisi√≥n

La frontera de decisi√≥n es el conjunto de puntos donde `p = 0.5`:

```
œÉ(Œ∏·µÄx) = 0.5  ‚áî  Œ∏·µÄx = 0
```

##### a.3.1 Intuici√≥n geom√©trica: el ‚Äúplano de corte‚Äù

Piensa en tus datos como puntos en un espacio.

- En 2D, `Œ∏·µÄx + b = 0` es una **l√≠nea**.
- En 3D, es un **plano**.
- En `n` dimensiones, es un **hiperplano**.

La cantidad `z = Œ∏·µÄx + b` es un **score con signo**:

- `z > 0` ‚Üí est√°s del lado ‚Äúpositivo‚Äù del plano
- `z < 0` ‚Üí est√°s del lado ‚Äúnegativo‚Äù

La sigmoide `œÉ(z)` convierte ese score (relacionado con la distancia al plano) en probabilidad:

- puntos muy lejos del plano (|z| grande) ‚Üí probabilidad cerca de 0 o 1
- puntos cerca del plano (`z ‚âà 0`) ‚Üí probabilidad cerca de 0.5

Visualizaci√≥n sugerida (dib√∫jalo): una nube roja/azul y una l√≠nea que la corta; marca puntos a distinta distancia y escribe su `z` y `œÉ(z)`.

##### a.3.2 Conexi√≥n conceptual: SVM y la idea de ‚Äúmargen‚Äù (sin implementar)

Aunque no implementes SVM aqu√≠, su intuici√≥n te mejora la comprensi√≥n de regularizaci√≥n.

Idea:

- En clasificaci√≥n lineal, hay muchas l√≠neas/planos que separan (si los datos lo permiten).
- SVM busca el separador que deja la ‚Äúcarretera‚Äù m√°s ancha entre clases: **m√°ximo margen**.

Conexi√≥n con lo que s√≠ implementas:

- La **regularizaci√≥n** (L2/L1) controla complejidad efectiva.
- En problemas separables o casi separables, regularizar suele empujar a soluciones m√°s estables, con fronteras menos extremas.

Visualizaci√≥n sugerida: dos l√≠neas separadoras posibles y dibujar cu√°l deja m√°s espacio m√≠nimo a los puntos m√°s cercanos (support vectors).

En 2D, `Œ∏·µÄx = 0` es una **l√≠nea**.

```
clase 1:   o o o o o
           o o o o o

frontera:  ---------

clase 0:   x x x x x
           x x x x x
```

##### a.4 Worked example (num√©rico) de BCE

Datos: `x=2`, `y=1`.

- `w=0.5`, `b=0`
- `z = wx + b = 1`
- `≈∑ = œÉ(1) ‚âà 0.731`

Como `y=1`, la loss por muestra es:

```
L = -log(≈∑) ‚âà -log(0.731) ‚âà 0.313
```

Interpretaci√≥n: la predicci√≥n es ‚Äúbastante‚Äù correcta, por eso la loss es peque√±a. Si `≈∑` fuera 0.01, la loss ser√≠a enorme.

##### a.5 C√≥digo generador de intuici√≥n (Protocolo D): frontera de decisi√≥n en 2D

Objetivo: ver que la **frontera de decisi√≥n** (`p=0.5`) es lineal, aunque la salida `œÉ(z)` sea curva (curva en *probabilidad*, no en geometr√≠a de la frontera).

```python
import numpy as np  # arrays, √°lgebra lineal y generaci√≥n de n√∫meros aleatorios
import matplotlib.pyplot as plt  # visualizaci√≥n de datos y frontera de decisi√≥n


def make_blobs_2d(n=200, seed=42):  # genera dos clusters 2D para clasificaci√≥n binaria
    rng = np.random.default_rng(seed)  # inicializa RNG reproducible
    c0 = rng.normal(loc=(-2.0, -1.5), scale=0.8, size=(n // 2, 2))  # clase 0: gaussiana 2D
    c1 = rng.normal(loc=(2.0, 1.5), scale=0.8, size=(n // 2, 2))  # clase 1: gaussiana 2D
    X = np.vstack([c0, c1])  # concatena muestras de ambas clases (features)
    y = np.array([0] * (n // 2) + [1] * (n // 2))  # vector de etiquetas 0/1
    return X, y  # retorna dataset sint√©tico


def sigmoid(z):  # sigmoide para mapear logits a (0,1)
    z = np.clip(z, -500, 500)  # evita overflow num√©rico en exp para |z| grande
    return 1 / (1 + np.exp(-z))  # aplica la funci√≥n sigmoide elemento a elemento


def add_bias(X):  # a√±ade columna de 1s para intercepto
    return np.column_stack([np.ones(len(X)), X])  # a√±ade columna de 1s para el t√©rmino bias


def plot_decision_boundary(model, X, y, title="Decision boundary"):  # grafica probabilidad y frontera p=0.5 en 2D
    x_min, x_max = X[:, 0].min() - 1.0, X[:, 0].max() + 1.0  # rango en x con margen
    y_min, y_max = X[:, 1].min() - 1.0, X[:, 1].max() + 1.0  # rango en y con margen

    xx, yy = np.meshgrid(  # crea una malla regular para evaluar el modelo
        np.linspace(x_min, x_max, 250),  # muestrea coordenadas x
        np.linspace(y_min, y_max, 250),  # muestrea coordenadas y
    )  # retorna matrices (xx, yy) con coordenadas del grid

    grid = np.column_stack([xx.ravel(), yy.ravel()])  # aplana la malla a lista de puntos (N, 2)
    proba = model.predict_proba(grid).reshape(xx.shape)  # eval√∫a p(y=1|x) y re-forma a la malla

    plt.figure(figsize=(7, 6))  # crea figura
    plt.contourf(xx, yy, proba, levels=20, cmap="RdBu", alpha=0.35)  # mapa de calor de probabilidad
    plt.contour(xx, yy, proba, levels=[0.5], colors="black", linewidths=2)  # l√≠nea de decisi√≥n p=0.5

    plt.scatter(X[y == 0, 0], X[y == 0, 1], s=18, label="Clase 0")  # puntos de la clase 0
    plt.scatter(X[y == 1, 0], X[y == 1, 1], s=18, label="Clase 1")  # puntos de la clase 1

    plt.title(title)  # t√≠tulo del gr√°fico
    plt.legend()  # leyenda de clases
    plt.grid(True, alpha=0.2)  # rejilla suave para lectura
    plt.show()  # renderiza la figura


# Usa TU LogisticRegression del m√≥dulo (la clase ya existe m√°s abajo)
# X, y = make_blobs_2d(n=300)
# model = LogisticRegression()
# model.fit(X, y, learning_rate=0.1, n_iterations=2000)
# plot_decision_boundary(model, X, y)
```

Reto visual (opcional, si usas sklearn solo para generar datos):

- genera `make_moons` y grafica la frontera
- ver√°s por qu√© log√≠stica falla (frontera lineal)
- luego entrena tu MLP (M07) y observa c√≥mo la frontera se curva

##### b) Formalizaci√≥n m√≠nima

- **Modelo:** `≈∑ = œÉ(XŒ∏)`
- **Decisi√≥n:** `≈∑ ‚â• 0.5 ‚Üí clase 1` (umbral configurable)
- **Loss (BCE):** penaliza fuerte cuando est√°s ‚Äúseguro y equivocado‚Äù (ej. `≈∑‚âà0` pero `y=1`).

##### c) Regla de oro de shapes

Evita bugs silenciosos usando una convenci√≥n consistente:

- `X`: `(m, n)`
- `Œ∏`: `(n,)` (o `(n, 1)` si prefieres columnas)
- `y`: `(m,)`

Y verifica que `X @ Œ∏` te da `(m,)`.

#### Actividades activas (para convertir teor√≠a en habilidad)

- **Retrieval practice (5 min):** escribe sin mirar:
  - la ecuaci√≥n de BCE,
  - el gradiente `‚àáŒ∏`.
- **Ejercicio de calibraci√≥n:** cambia el `threshold` de 0.5 a 0.3 y explica qu√© pasa con precision/recall.
- **Sanity check obligatorio:** entrena con 20 ejemplos hasta obtener accuracy ~100% (si no, hay bug).

#### Evaluaci√≥n (criterios de ‚Äúdominio‚Äù)

- **Dominio matem√°tico:** puedes explicar por qu√© aparece `(≈∑ - y)` en el gradiente.
- **Dominio de implementaci√≥n:** tu `fit()` reduce BCE de forma monot√≥nica (o casi) en un dataset simple.
- **Dominio de validaci√≥n:** tu accuracy difiere <5% de sklearn en Shadow Mode.

#### Errores comunes (los que m√°s queman tiempo)

- **Overflow/NaN:** `exp(500)` revienta. Soluci√≥n: `clip(z)` y `eps` en logs.
- **Saturaci√≥n:** si `|z|` crece, `œÉ(z)` se pega a 0/1 y el gradiente se hace peque√±o.
- **Signo invertido:** si actualizas en la direcci√≥n equivocada, la loss sube.
- **Sin normalizaci√≥n:** features en escalas muy distintas hacen que GD sea inestable.

#### Integraci√≥n con Plan v4/v5

- **v4.0:** usa `study_tools/SIMULACRO_EXAMEN_TEORICO.md` para preguntas tipo examen (sigmoid vs softmax, BCE vs MSE).
- **v5.0:** ejecuta **Shadow Mode** como verificaci√≥n externa antes de dar por terminado el m√≥dulo.

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 2.0: Regresi√≥n Log√≠stica (marco mental completo)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Qu√© est√°s construyendo realmente: probabilidades, decisi√≥n, loss y gradiente (una sola historia)
- **ID (opcional):** `M05-T02_0`
- **Duraci√≥n estimada:** 90‚Äì150 min
- **Nivel:** Intermedio (core)
- **Dependencias:** M03 (chain rule), M04 (MLE ‚Üí cross-entropy)

#### 2) Objetivos
- Unificar en una frase el pipeline: `z = XŒ∏` ‚Üí `p = œÉ(z)` ‚Üí `loss = BCE(p,y)` ‚Üí `grad = X·µÄ(p-y)/m` ‚Üí update.
- Explicar por qu√© log√≠stica es ‚Äúlineal‚Äù en la frontera, aunque `œÉ` sea no lineal.
- Saber qu√© debes observar cuando algo falla (NaNs, saturaci√≥n, signos, shapes).

#### 3) Relevancia
- Esta secci√≥n es el puente directo a MLP/softmax (M07): cambia `œÉ` por softmax y `BCE` por CCE, pero el esqueleto es el mismo.

#### 4) Mapa conceptual m√≠nimo
- **Modelo:** `p(y=1|x) = œÉ(Œ∏·µÄx)`.
- **Decisi√≥n:** `p ‚â• threshold`.
- **Entrenamiento (MLE):** minimizar NLL = BCE.
- **Gradiente vectorizado:** siempre termina en `X·µÄ(something)`.

#### 5) Definiciones esenciales
- **Logit:** `z = Œ∏·µÄx` (score sin acotar).
- **Probabilidad:** `p = œÉ(z)`.
- **Loss BCE:** castiga fuerte ‚Äúseguro y equivocado‚Äù.

#### 6) Explicaci√≥n did√°ctica
- Lo m√°s importante no es memorizar f√≥rmulas, sino saber qu√© variable inspeccionar:
  - si `p` es 0/1 exacto ‚Üí `log(0)` rompe ‚Üí `eps`.
  - si `|z|` es enorme ‚Üí saturaci√≥n ‚Üí gradiente peque√±o.

#### 7) Ejemplo modelado
- Si tu modelo predice `p=0.01` cuando `y=1`, BCE es grande; eso fuerza una correcci√≥n fuerte del gradiente.

#### 8) Pr√°ctica guiada
- Haz un ‚Äúoverfit test‚Äù con 20 ejemplos y confirma que BCE cae y accuracy sube.

#### 9) Pr√°ctica independiente
- Cambia `threshold` y observa el tradeoff precision/recall (lo conectar√°s con m√©tricas en Parte 3).

#### 10) Autoevaluaci√≥n
- ¬øCu√°l es la √∫nica pieza que convierte un score lineal en probabilidad? (respuesta: `œÉ`).

#### 11) Errores comunes
- Entrenar con `y‚àà{-1,1}` usando BCE est√°ndar.
- Olvidar bias.
- Mezclar `X` con `X_b` (con bias) en distintas funciones.

#### 12) Retenci√≥n
- Recita el mantra: `z‚ÜíœÉ(z)‚ÜíBCE‚ÜíX·µÄ(p-y)`.

#### 13) Diferenciaci√≥n
- Avanzado: interpreta `Œ∏` como direcci√≥n normal al hiperplano; magnitud controla ‚Äúconfianza‚Äù.

#### 14) Recursos
- M04 (MLE/cross-entropy) y glosario de sigmoid/logistic regression.

#### 15) Nota docente
- Pide al alumno un diagrama de flujo con shapes: `X:(m,n)`, `Œ∏:(n,)`, `z:(m,)`, `p:(m,)`, `grad:(n,)`.
</details>

### 2.1 Funci√≥n Sigmoid

```python
import numpy as np  # Importa NumPy para operaciones matem√°ticas

def sigmoid(z: np.ndarray) -> np.ndarray:  # aplica la funci√≥n log√≠stica elemento a elemento
    """
    Funci√≥n sigmoid/log√≠stica.

    œÉ(z) = 1 / (1 + e^(-z))

    Propiedades:
    - Rango: (0, 1) - perfecto para probabilidades
    - œÉ(0) = 0.5
    - œÉ'(z) = œÉ(z)(1 - œÉ(z))
    """
    # Clip para evitar overflow en exp() con valores extremos
    z = np.clip(z, -500, 500)  # limita z para evitar overflow/underflow en exp()
    return 1 / (1 + np.exp(-z))  # F√≥rmula matem√°tica de la sigmoide

# Visualizar la funci√≥n sigmoid
import matplotlib.pyplot as plt  # Importa matplotlib para gr√°ficos

z = np.linspace(-10, 10, 100)  # Valores de prueba de -10 a 10
plt.figure(figsize=(8, 4))  # Crea figura de 8x4 pulgadas
plt.plot(z, sigmoid(z))  # Grafica sigmoid(z)
plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5)  # L√≠nea horizontal en y=0.5
plt.axvline(x=0, color='r', linestyle='--', alpha=0.5)  # L√≠nea vertical en x=0
plt.xlabel('z')  # Etiqueta eje x
plt.ylabel('œÉ(z)')  # Etiqueta eje y
plt.title('Funci√≥n Sigmoid')  # T√≠tulo del gr√°fico
plt.grid(True)  # Activa cuadr√≠cula
# plt.show()  # Descomentar para mostrar gr√°fico
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 2.1: Sigmoid (intuici√≥n + estabilidad num√©rica)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Sigmoid como ‚Äúpuerta‚Äù a probabilidades y por qu√© hay que hacer `clip`
- **ID (opcional):** `M05-T02_1`
- **Duraci√≥n estimada:** 45‚Äì90 min
- **Nivel:** B√°sico‚ÄìIntermedio
- **Dependencias:** Exponencial/log, overflow/underflow

#### 2) Objetivos
- Entender que `œÉ(z)` solo reescala el score a `(0,1)`; no hace la frontera no lineal.
- Reconocer saturaci√≥n: `z>>0 ‚Üí œÉ‚âà1` y `z<<0 ‚Üí œÉ‚âà0`.
- Justificar `clip(z)` como protecci√≥n num√©rica.

#### 3) Relevancia
- Si no controlas overflow/saturaci√≥n, tu BCE se vuelve NaN y el entrenamiento colapsa.

#### 4) Mapa conceptual m√≠nimo
- `z` crece ‚Üí `exp(-z)` puede underflow; `z` muy negativo ‚Üí `exp(-z)` overflow.

#### 5) Definiciones esenciales
- `œÉ(0)=0.5`.
- `œÉ'(z)=œÉ(z)(1-œÉ(z))` (m√°xima en 0, m√≠nima en extremos).

#### 6) Explicaci√≥n did√°ctica
- Cuando `œÉ` se satura, el gradiente se vuelve peque√±o: puede ‚Äúaprender lento‚Äù aunque el error sea real.

#### 7) Ejemplo modelado
- Prueba `z=[-1000,0,1000]` y observa que sin `clip` puedes romper `exp`.

#### 8) Pr√°ctica guiada
- Escribe un test: `sigmoid(np.array([0.0]))==0.5` (aprox).

#### 9) Pr√°ctica independiente
- Implementa una sigmoid estable alternativa (log-sum-exp) y compara.

#### 10) Autoevaluaci√≥n
- ¬øQu√© pasa con `œÉ'(z)` cuando `z` es muy grande en valor absoluto?

#### 11) Errores comunes
- Creer que sigmoid ‚Äúhace no lineal‚Äù la frontera.

#### 12) Retenci√≥n
- ‚ÄúSigmoid curva la probabilidad, no la geometr√≠a del plano‚Äù.

#### 13) Diferenciaci√≥n
- Avanzado: relaci√≥n entre sigmoid y logit.

#### 14) Recursos
- Material de estabilidad num√©rica (overflow/underflow).

#### 15) Nota docente
- Pedir al alumno que explique por qu√© `clip` es un *guardrail* y no un ‚Äúhack‚Äù.
</details>

### 2.2 Hip√≥tesis Log√≠stica

```python
"""
REGRESI√ìN LOG√çSTICA

No predice un valor continuo, sino la PROBABILIDAD de pertenecer a la clase 1.

h(x) = P(y=1|x; Œ∏) = œÉ(Œ∏·µÄx)

Decisi√≥n:
- Si h(x) ‚â• 0.5 ‚Üí predicir clase 1
- Si h(x) < 0.5 ‚Üí predicir clase 0

Equivalente a:
- Si Œ∏·µÄx ‚â• 0 ‚Üí clase 1
- Si Œ∏·µÄx < 0 ‚Üí clase 0

El "decision boundary" est√° en Œ∏·µÄx = 0
"""

def predict_proba(X: np.ndarray, theta: np.ndarray) -> np.ndarray:  # predice probabilidad P(y=1|x) usando œÉ(XŒ∏)
    """Predice probabilidad de clase 1."""
    return sigmoid(X @ theta)  # calcula p=œÉ(XŒ∏)

def predict_class(X: np.ndarray, theta: np.ndarray, threshold: float = 0.5) -> np.ndarray:  # predice clase usando umbral
    """Predice clase (0 o 1)."""
    return (predict_proba(X, theta) >= threshold).astype(int)  # convierte probas a clases 0/1
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 2.2: Hip√≥tesis + umbral (qu√© significa predecir)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Probabilidad vs clase: `predict_proba` y `predict_class` no son lo mismo
- **ID (opcional):** `M05-T02_2`
- **Duraci√≥n estimada:** 45‚Äì90 min
- **Nivel:** Intermedio
- **Dependencias:** 2.1

#### 2) Objetivos
- Separar claramente: score `z`, probabilidad `p`, decisi√≥n `≈∑`.
- Entender el papel del `threshold` como decisi√≥n de negocio (no matem√°tica fija).

#### 3) Relevancia
- Cambiar `threshold` es una de las maneras m√°s simples y potentes de controlar FP vs FN (ver√°s esto en m√©tricas).

#### 4) Mapa conceptual m√≠nimo
- `predict_proba` te da un ranking de ‚Äúconfianza‚Äù.
- `predict_class` es una pol√≠tica: ‚Äúsi p‚â•t, digo 1‚Äù.

#### 5) Definiciones esenciales
- **Frontera:** `Œ∏·µÄx=0` si `t=0.5`.

#### 6) Explicaci√≥n did√°ctica
- `t=0.5` es convencional; si el costo de FN es alto, baja el umbral.

#### 7) Ejemplo modelado
- En spam: prefieres recall alto ‚Üí `threshold` m√°s bajo (aceptas m√°s FP).

#### 8) Pr√°ctica guiada
- Eval√∫a el mismo modelo con `t=0.3,0.5,0.7` y registra cambios de precision/recall.

#### 9) Pr√°ctica independiente
- Encuentra un `threshold` que maximice F1 en un dataset de validaci√≥n.

#### 10) Autoevaluaci√≥n
- ¬øPor qu√© dos modelos con igual accuracy pueden ser muy distintos cuando cambias `threshold`?

#### 11) Errores comunes
- Calcular m√©tricas usando probabilidades como si fueran clases.

#### 12) Retenci√≥n
- ‚ÄúPrimero calibro y eval√∫o probabilidades; luego decido clases con un umbral‚Äù.

#### 13) Diferenciaci√≥n
- Avanzado: curva ROC/PR (conceptual) como barrido de thresholds.

#### 14) Recursos
- Glosario: precision/recall y confusion matrix.

#### 15) Nota docente
- Pide que el alumno explique verbalmente qu√© significa: ‚Äúpredigo 1 si p‚â•0.3‚Äù.
</details>

### 2.3 Binary Cross-Entropy Loss

```python
import numpy as np  # arrays y operaciones num√©ricas

def binary_cross_entropy(  # BCE (log-loss) para regresi√≥n log√≠stica
    X: np.ndarray,  # matriz de features (m, d)
    y: np.ndarray,  # etiquetas binarias (m,)
    theta: np.ndarray,  # par√°metros (d,)
    eps: float = 1e-15  # epsilon para estabilidad num√©rica
) -> float:  # retorna el costo promedio
    """
    Binary Cross-Entropy (Log Loss).

    J(Œ∏) = -(1/m) Œ£·µ¢ [y·µ¢ log(h·µ¢) + (1-y·µ¢) log(1-h·µ¢)]

    Donde h·µ¢ = œÉ(Œ∏·µÄx·µ¢)

    Por qu√© esta funci√≥n de costo:
    - Es convexa (tiene un √∫nico m√≠nimo global)
    - Penaliza mucho las predicciones muy incorrectas
    - Es la derivaci√≥n de Maximum Likelihood Estimation
    """
    m = len(y)  # n√∫mero de muestras
    h = sigmoid(X @ theta)  # probabilidades predichas p=œÉ(XŒ∏)

    # Clip para evitar log(0)
    h = np.clip(h, eps, 1 - eps)  # limita p para evitar log(0) y log(1)

    cost = -(1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))  # BCE promedio
    return cost  # retorna costo escalar

def bce_gradient(X: np.ndarray, y: np.ndarray, theta: np.ndarray) -> np.ndarray:  # gradiente de BCE w.r.t. theta
    """
    Gradiente de Binary Cross-Entropy.

    ‚àÇJ/‚àÇŒ∏ = (1/m) X·µÄ(h - y)

    ¬°Tiene la misma forma que el gradiente del MSE!
    Esto es porque derivamos œÉ(z) y la derivada œÉ'(z) = œÉ(z)(1-œÉ(z))
    cancela parte de la expresi√≥n.
    """
    m = len(y)  # n√∫mero de muestras
    h = sigmoid(X @ theta)  # probabilidades predichas p=œÉ(XŒ∏)
    return (1/m) * X.T @ (h - y)  # ‚àáŒ∏ = (1/m) X·µÄ(p - y)
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 2.3: BCE + gradiente (lo que debes saber de memoria)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** BCE como NLL (MLE) y por qu√© el gradiente termina en `X·µÄ(p-y)`
- **ID (opcional):** `M05-T02_3`
- **Duraci√≥n estimada:** 60‚Äì120 min
- **Nivel:** Intermedio
- **Dependencias:** 2.1‚Äì2.2, logaritmos

#### 2) Objetivos
- Entender BCE como ‚Äúcastigo logar√≠tmico‚Äù a la probabilidad asignada a la clase correcta.
- Memorizar la forma del gradiente vectorizado.
- Entender por qu√© `eps` evita `log(0)` sin cambiar el objetivo conceptual.

#### 3) Relevancia
- Esta es la p√©rdida est√°ndar para binario y base de softmax cross-entropy en multiclase.

#### 4) Mapa conceptual m√≠nimo
- Si `y=1`: loss = `-log(p)`.
- Si `y=0`: loss = `-log(1-p)`.

#### 5) Definiciones esenciales
- `p = œÉ(XŒ∏)`.
- `‚àáŒ∏ = (1/m) X·µÄ(p-y)`.

#### 6) Explicaci√≥n did√°ctica
- El gradiente ‚Äúmide error en probabilidad‚Äù: si `p>y`, empuja hacia abajo; si `p<y`, empuja hacia arriba.

#### 7) Ejemplo modelado
- Una sola muestra: si `y=1` y `p=0.1`, el error `(p-y)` es negativo y el update mueve `Œ∏` para subir `z`.

#### 8) Pr√°ctica guiada
- Haz un gradient check num√©rico en 1 coordenada (diferencias centrales) con dataset peque√±o.

#### 9) Pr√°ctica independiente
- Grafica BCE vs `p` para `y=1` y `y=0` y explica la asimetr√≠a.

#### 10) Autoevaluaci√≥n
- ¬øPor qu√© BCE penaliza m√°s el caso ‚Äúseguro y equivocado‚Äù que MSE?

#### 11) Errores comunes
- Usar `y` como int pero con shape `(m,1)` y romper broadcasting.
- No hacer `clip` en `p` antes del log.

#### 12) Retenci√≥n
- F√≥rmula clave: `grad = X·µÄ(p-y)/m`.

#### 13) Diferenciaci√≥n
- Avanzado: relaci√≥n con la entrop√≠a cruzada y KL-divergence.

#### 14) Recursos
- M04 (MLE‚Üícross-entropy), glosario BCE.

#### 15) Nota docente
- Pide que el alumno derive el gradiente una vez y luego lo trate como ‚Äúpatr√≥n‚Äù reusable.
</details>

### 2.4 Implementaci√≥n Completa

```python
import numpy as np  # Importa NumPy: se usa para arrays, operaciones vectorizadas, RNG y construcci√≥n de datasets sint√©ticos
from typing import List  # Importa List: soporte de tipado (anotaciones) para listas; no afecta el runtime

class LogisticRegression:  # Regresi√≥n log√≠stica binaria desde cero: aprende Œ∏ para predecir P(y=1|x)=œÉ(X_bŒ∏)
    """Regresi√≥n Log√≠stica implementada desde cero."""

    def __init__(self):  # Inicializa el modelo: prepara par√°metros y contenedor de costes
        self.theta = None  # Vector de par√°metros (d+1,): incluye bias en theta[0]; se asigna en fit
        self.cost_history = []  # Historial de coste (BCE) por iteraci√≥n: √∫til para evaluar convergencia

    def fit(  # Entrena por gradient descent: minimiza BCE actualizando Œ∏ iterativamente
        self,  # Referencia al objeto: permite almacenar theta/cost_history en la instancia
        X: np.ndarray,  # Matriz de features (n,d): datos de entrada sin bias (se agrega dentro)
        y: np.ndarray,  # Vector de labels (n,): valores 0/1 para clasificaci√≥n binaria
        learning_rate: float = 0.1,  # Tasa de aprendizaje Œ±: escala el paso del update Œ∏ ‚Üê Œ∏ ‚àí Œ±¬∑grad
        n_iterations: int = 1000  # N√∫mero de iteraciones: cu√°ntos pasos de GD se ejecutan
    ) -> 'LogisticRegression':  # Retorna self tipado: estilo sklearn para encadenar llamadas
        """Entrena con gradient descent."""
        # A√±adir bias
        X_b = np.column_stack([np.ones(len(X)), X])  # Construye X con bias: agrega columna de 1s para intercepto
        m, n = X_b.shape  # Extrae shapes: m=n muestras, n=d+1 par√°metros (incluye bias)

        # Inicializar
        self.theta = np.zeros(n)  # Inicializa Œ∏ en cero: punto de partida est√°ndar para GD

        for i in range(n_iterations):  # Loop de entrenamiento: repite updates de GD n_iterations veces
            # Gradiente
            gradient = bce_gradient(X_b, y, self.theta)  # Calcula ‚àÇBCE/‚àÇŒ∏: gradiente del coste respecto a par√°metros

            # Actualizar
            self.theta = self.theta - learning_rate * gradient  # Update GD: mueve Œ∏ en direcci√≥n opuesta al gradiente

            # Guardar costo
            cost = binary_cross_entropy(X_b, y, self.theta)  # Calcula BCE actual: mide qu√© tan bien ajusta el modelo en esta iteraci√≥n
            self.cost_history.append(cost)  # Guarda coste: permite inspeccionar si baja y detectar divergencia/NaN

        return self  # Devuelve la instancia entrenada: habilita chaining (model.fit(...).predict(...))

    def predict_proba(self, X: np.ndarray) -> np.ndarray:  # Predice probabilidades: devuelve P(y=1|x) en (0,1)
        """Predice probabilidades."""
        X_b = np.column_stack([np.ones(len(X)), X])  # A√±ade bias: alinea X con theta aprendida
        return sigmoid(X_b @ self.theta)  # Aplica œÉ a logits: œÉ(X_bŒ∏) produce probabilidades por muestra

    def predict(self, X: np.ndarray, threshold: float = 0.5) -> np.ndarray:  # Predice clases: umbraliza probabilidades para obtener 0/1
        """Predice clases."""
        return (self.predict_proba(X) >= threshold).astype(int)  # Comparaci√≥n booleana y cast: devuelve etiquetas enteras

    def score(self, X: np.ndarray, y: np.ndarray) -> float:  # Eval√∫a accuracy: fracci√≥n de predicciones correctas
        """Accuracy."""
        return np.mean(self.predict(X) == y)  # Promedio de aciertos: compara arrays (True/False) y promedia


# Demo con datos sint√©ticos
np.random.seed(42)  # Fija semilla global: hace reproducible el dataset sint√©tico de la demo

# Generar datos de dos clases
n_samples = 200  # N√∫mero total de muestras: se divide en dos mitades (clase 0 y clase 1)
X_class0 = np.random.randn(n_samples // 2, 2) + np.array([-2, -2])  # Clase 0: nube gaussiana centrada en (-2,-2)
X_class1 = np.random.randn(n_samples // 2, 2) + np.array([2, 2])  # Clase 1: nube gaussiana centrada en (2,2)
X = np.vstack([X_class0, X_class1])  # Apila features: matriz final (n,2) con ambas clases
y = np.array([0] * (n_samples // 2) + [1] * (n_samples // 2))  # Labels: 0 para clase 0 y 1 para clase 1 (balanceado)

# Entrenar
model = LogisticRegression()  # Crea modelo: instancia la implementaci√≥n desde cero
model.fit(X, y, learning_rate=0.1, n_iterations=1000)  # Entrena por GD: ajusta theta usando el dataset sint√©tico

print(f"Accuracy: {model.score(X, y):.2%}")  # Reporta accuracy en train: en dataset separable deber√≠a ser alta (sanity check)
print(f"Par√°metros: {model.theta}")  # Imprime theta: permite inspeccionar signo/magnitud (incluye bias en theta[0])
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 2.4: Implementaci√≥n completa (checklist de robustez)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** C√≥mo saber que tu LogReg ‚Äúfunciona‚Äù: contratos, overfit test y Shadow Mode
- **ID (opcional):** `M05-T02_4`
- **Duraci√≥n estimada:** 90‚Äì150 min
- **Nivel:** Intermedio
- **Dependencias:** 2.1‚Äì2.3

#### 2) Objetivos
- Establecer invariantes: `theta` tama√±o correcto, `cost_history` finito, `predict_proba` en `(0,1)`.
- Detectar r√°pido si GD diverge (cost sube/NaN).
- Ejecutar ‚Äúoverfit test‚Äù como prueba unitaria del entrenamiento.

#### 3) Relevancia
- Un modelo que ‚Äúcorre‚Äù no necesariamente aprende. Necesitas una bater√≠a m√≠nima de checks.

#### 4) Mapa conceptual m√≠nimo
- **Datos** ‚Üí **bias** ‚Üí **sigmoid** ‚Üí **BCE** ‚Üí **grad** ‚Üí **update**.

#### 5) Definiciones esenciales
- `X_b = [1, X]`.
- `theta[0]` es bias.

#### 6) Explicaci√≥n did√°ctica
- Si el costo no baja en un dataset f√°cil, asume bug antes de ‚Äútocar hiperpar√°metros‚Äù.

#### 7) Ejemplo modelado
- Con datos separables (dos gaussianas separadas), deber√≠as obtener accuracy alta.

#### 8) Pr√°ctica guiada
- Imprime cada 100 iteraciones: `cost`. Debe caer en promedio.

#### 9) Pr√°ctica independiente
- A√±ade early stopping y guarda el mejor `theta` por costo.

#### 10) Autoevaluaci√≥n
- ¬øQu√© s√≠ntoma te indica signo invertido en el update? (costo sube sistem√°ticamente).

#### 11) Errores comunes
- No escalar features.
- Confundir `predict_proba` con `predict` en m√©tricas.

#### 12) Retenci√≥n
- Checklist m√≠nimo: `finite`, `monotonic-ish`, `overfit test`, `shadow mode`.

#### 13) Diferenciaci√≥n
- Avanzado: regularizaci√≥n L2 (MAP) y su efecto en estabilidad.

#### 14) Recursos
- Plan v5: validaci√≥n externa y rutina de checks.

#### 15) Nota docente
- Pide evidencia: captura de `cost_history` (inicio vs final) + comparaci√≥n con sklearn.
</details>

---

## üß© Consolidaci√≥n (Regresi√≥n Log√≠stica)

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Consolidaci√≥n LogReg: interpretaci√≥n y criterio de dominio</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** De ‚Äúentrenar un modelo‚Äù a ‚Äúentender qu√© aprendi√≥‚Äù (pesos como explicaci√≥n)
- **ID (opcional):** `M05-CONS-LOGREG`
- **Duraci√≥n estimada:** 60‚Äì120 min
- **Nivel:** Intermedio
- **Dependencias:** 2.4

#### 2) Objetivos
- Interpretar el vector de pesos como ‚Äúdirecci√≥n‚Äù que favorece una clase.
- En im√°genes (MNIST), mapear `theta[1:]` a 28√ó28 y explicar regiones importantes.

#### 3) Relevancia
- Esto te entrena para hacer informes: no solo reportar accuracy, sino justificar el comportamiento del modelo.

#### 4) Mapa conceptual m√≠nimo
- Pesos positivos aumentan `z` ‚Üí suben probabilidad de clase 1.
- Pesos negativos disminuyen `z` ‚Üí bajan probabilidad.

#### 5) Definiciones esenciales
- `theta[0]`: bias.
- `theta[1:]`: pesos por feature.

#### 6) Explicaci√≥n did√°ctica
- Interpretaci√≥n correcta es ‚Äúsi sube esta feature, sube/baja el logit‚Äù, no ‚Äúcausa‚Äù.

#### 7) Ejemplo modelado
- Para 0 vs 1, pesos en trazos t√≠picos del ‚Äú1‚Äù deber√≠an ser positivos (seg√∫n c√≥mo codifiques la clase).

#### 8) Pr√°ctica guiada
- Guarda el mapa de pesos y escribe 5 l√≠neas de interpretaci√≥n con hip√≥tesis verificables.

#### 9) Pr√°ctica independiente
- Repite con otra pareja (3 vs 8) y discute por qu√© es m√°s dif√≠cil.

#### 10) Autoevaluaci√≥n
- ¬øC√≥mo cambia la interpretaci√≥n si inviertes qu√© clase es 1 y cu√°l es 0?

#### 11) Errores comunes
- Olvidar remover el bias antes del reshape.
- Interpretar magnitudes sin normalizar features.

#### 12) Retenci√≥n
- ‚ÄúPesos ‚Üí logit ‚Üí probabilidad‚Äù: siempre explica primero qu√© clase corresponde a `y=1`.

#### 13) Diferenciaci√≥n
- Avanzado: inspeccionar errores (top confusiones) y correlacionarlos con regiones de peso.

#### 14) Recursos
- Herramientas de visualizaci√≥n y notas de interpretabilidad lineal.

#### 15) Nota docente
- Pide consistencia: la explicaci√≥n debe predecir qu√© p√≠xeles cambiar√≠an la predicci√≥n.
</details>

### Entregable conceptual (v3.3): Interpretaci√≥n de pesos (LogReg)

Objetivo: conectar el vector de pesos con ‚Äúqu√© est√° mirando‚Äù el modelo.

- Dataset recomendado: MNIST (28x28) en binario (p. ej. 0 vs 1) usando `sklearn.datasets.fetch_openml("mnist_784", as_frame=False)`.
- Entrena tu regresi√≥n log√≠stica sobre im√°genes aplanadas (`784` features).
- Visualiza:
  - toma `theta[1:]` (sin bias), reshapea a `(28, 28)` y grafica con `imshow`.
  - usa un mapa de color divergente (p. ej. centrado en 0) y guarda una imagen.
- Interpreta en 5‚Äì10 l√≠neas:
  - ¬øqu√© regiones tienen peso positivo/negativo?
  - ¬øpor qu√© eso tiene sentido para el d√≠gito?

### Errores comunes

- **Etiquetas incorrectas:** BCE asume `y ‚àà {0,1}` (no `{-1,1}`) si usas la f√≥rmula est√°ndar.
- **Olvidar el bias:** si no agregas columna de 1s, la frontera se forza a pasar por el origen.
- **`exp` overflow:** si `z` crece, `exp(-z)` puede overflow/underflow ‚Üí usa `clip`.
- **`log(0)`:** si `h` llega a 0 o 1 exactos, `log` revienta ‚Üí usa `eps`.
- **Sin escalado:** features con escalas distintas hacen el GD inestable.

### Debugging / validaci√≥n (v5)

- **Overfit test:** entrena con 20 ejemplos hasta casi 100% accuracy. Si no, asume bug.
- **Shadow Mode:** compara con sklearn para la misma semilla/dataset.
- Registra hallazgos en `study_tools/DIARIO_ERRORES.md`.
- Protocolos completos:
  - [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)
  - [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md)

### Reto Feynman (tablero blanco)

Explica en 5 l√≠neas o menos:

1) ¬øQu√© es el logit y por qu√© log√≠stica es lineal ‚Äúen el espacio de log-odds‚Äù?
2) ¬øPor qu√© `-log(≈∑)` explota cuando est√°s seguro y equivocado?
3) ¬øQu√© significa `X·µÄ(≈∑ - y)` y por qu√© aparece en el gradiente?

---

## üíª Parte 3: M√©tricas de Evaluaci√≥n

### 3.0 M√©tricas ‚Äî Nivel: intermedio (de ‚Äúcalcular‚Äù a ‚Äútomar decisiones‚Äù)

**Prop√≥sito:** que no te quedes en ‚Äús√© calcular accuracy‚Äù, sino que puedas **elegir la m√©trica correcta seg√∫n el riesgo** (FP vs FN), detectar desbalance de clases y justificar tus decisiones como en un informe.

#### Objetivos de aprendizaje (medibles)

Al terminar esta parte podr√°s:

- **Explicar** la matriz de confusi√≥n y derivar TP/TN/FP/FN sin mirar apuntes.
- **Aplicar** accuracy/precision/recall/F1/specificity y explicar cu√°ndo cada una es adecuada.
- **Analizar** el impacto del umbral (`threshold`) en precision/recall.
- **Diagnosticar** trampas comunes: accuracy alta con clases desbalanceadas, leakage, evaluar sobre train.

#### Prerrequisitos y conexiones

- Conexi√≥n directa con probabilidad/loss:
  - [04_PROBABILIDAD_ML.md](04_PROBABILIDAD_ML.md) (MLE ‚Üí cross-entropy)
- Glosario:
  - [GLOSARIO: Confusion Matrix](GLOSARIO.md#confusion-matrix)
  - [GLOSARIO: Accuracy](GLOSARIO.md#accuracy)
  - [GLOSARIO: Precision](GLOSARIO.md#precision)
  - [GLOSARIO: Recall](GLOSARIO.md#recall)
  - [GLOSARIO: F1 Score](GLOSARIO.md#f1-score)

#### Resumen ejecutivo (big idea)

La m√©trica es una traducci√≥n expl√≠cita de ‚Äúqu√© error es m√°s caro‚Äù:

- Si te preocupa **no perder positivos reales** ‚Üí prioriza **recall**.
- Si te preocupa **no disparar falsas alarmas** ‚Üí prioriza **precision**.
- Si necesitas balance ‚Üí **F1**.
- Si tu dataset est√° balanceado y el costo es sim√©trico ‚Üí **accuracy** puede servir.

#### Actividades activas (obligatorias)

- **Retrieval practice (5 min):** escribe la matriz 2x2 y define TP/TN/FP/FN.
- **Experimento de umbral:** eval√∫a con `threshold = 0.3, 0.5, 0.7` y anota c√≥mo cambian precision/recall.
- **Caso desbalanceado:** crea un dataset donde 95% sea clase 0 y muestra por qu√© accuracy enga√±a.

#### Errores comunes (los que m√°s da√±an resultados)
- **Evaluar en training:** te da una ‚Äúm√©trica falsa‚Äù por overfitting.
- **Leakage:** normalizar/seleccionar features usando todo el dataset antes del split.
- **No fijar semilla:** resultados no reproducibles.

Integraci√≥n con Plan v4/v5:

- [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md) (rutina + simulacros)
- [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md) (validaci√≥n externa / rigor)
- Diario: `study_tools/DIARIO_ERRORES.md`

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 3.0: M√©tricas (c√≥mo elegir y no autoenga√±arte)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** M√©tricas como decisi√≥n: qu√© optimizas depende del costo (FP vs FN) y del umbral
- **ID (opcional):** `M05-T03_0`
- **Duraci√≥n estimada:** 90‚Äì150 min
- **Nivel:** Intermedio
- **Dependencias:** LogReg (probabilidades + threshold), matriz de confusi√≥n

#### 2) Objetivos
- Pasar de ‚Äús√© calcular‚Äù a ‚Äús√© elegir‚Äù la m√©trica correcta seg√∫n el problema.
- Entender c√≥mo el `threshold` cambia precision/recall sin re-entrenar el modelo.
- Detectar el caso cl√°sico de autoenga√±o: accuracy alta con dataset desbalanceado.

#### 3) Relevancia
- En proyectos reales, la m√©trica es parte del producto: define qu√© errores toleras.
- M√©tricas conectan directamente con tu pol√≠tica de decisi√≥n (umbral) y con el tipo de informe.

#### 4) Mapa conceptual m√≠nimo
- **Modelo** produce `p(y=1|x)`.
- **Threshold** produce `≈∑`.
- `≈∑` + `y` ‚Üí **confusion matrix** ‚Üí m√©tricas.

#### 5) Definiciones esenciales
- **TP/TN/FP/FN:** conteos base.
- **Precision:** de lo que dije ‚Äúpositivo‚Äù, cu√°nto era positivo.
- **Recall:** de lo positivo real, cu√°nto captur√©.

#### 6) Explicaci√≥n did√°ctica
- Si tu modelo solo da clases, ya tomaste una decisi√≥n de threshold (impl√≠cita). Mejor separar: proba ‚Üí threshold ‚Üí m√©tricas.

#### 7) Ejemplo modelado
- Detecci√≥n de c√°ncer: FN es caro ‚Üí prioriza recall.
- Filtro de spam: FP es caro ‚Üí prioriza precision.

#### 8) Pr√°ctica guiada
- Para un mismo modelo, eval√∫a `threshold` en `{0.3, 0.5, 0.7}` y anota c√≥mo cambian precision/recall.

#### 9) Pr√°ctica independiente
- Crea un dataset con 95% clase 0 y muestra:
  - baseline ‚Äúsiempre 0‚Äù ‚Üí accuracy alta, pero recall para clase 1 = 0.

#### 10) Autoevaluaci√≥n
- ¬øPor qu√© no puedes comparar modelos con thresholds distintos sin decir el threshold?

#### 11) Errores comunes
- Reportar solo accuracy.
- Evaluar sobre train y reportar m√©tricas ‚Äúperfectas‚Äù.

#### 12) Retenci√≥n
- Regla: ‚Äúm√©trica = costo impl√≠cito‚Äù (si no lo defines, el modelo decide por ti).

#### 13) Diferenciaci√≥n
- Avanzado: curva PR/ROC como barrido de thresholds (sin cambiar el modelo).

#### 14) Recursos
- Glosario de confusion matrix/precision/recall/F1.

#### 15) Nota docente
- Pide que el alumno justifique una m√©trica con una frase de costo (‚ÄúFN cuesta m√°s que FP‚Äù).
</details>

### 3.1 Matriz de Confusi√≥n

```python
import numpy as np  # arrays y operaciones vectorizadas para conteos

def confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:  # matriz de confusi√≥n para clasificaci√≥n binaria/multiclase
    """
    Calcula la matriz de confusi√≥n.

    Para clasificaci√≥n binaria:

                    Predicho
                    0       1
    Real    0      TN      FP
            1      FN      TP

    - TP (True Positive): Predijo 1, era 1
    - TN (True Negative): Predijo 0, era 0
    - FP (False Positive): Predijo 1, era 0 (Error Tipo I)
    - FN (False Negative): Predijo 0, era 1 (Error Tipo II)
    """
    classes = np.unique(np.concatenate([y_true, y_pred]))  # obtiene clases presentes en true y pred
    n_classes = len(classes)  # n√∫mero de clases
    cm = np.zeros((n_classes, n_classes), dtype=int)  # inicializa matriz KxK de conteos

    for i, true_class in enumerate(classes):  # recorre clases reales (filas)
        for j, pred_class in enumerate(classes):  # recorre clases predichas (columnas)
            cm[i, j] = np.sum((y_true == true_class) & (y_pred == pred_class))  # cuenta ocurrencias (true=i, pred=j)

    return cm  # retorna matriz de confusi√≥n

def extract_tp_tn_fp_fn(y_true: np.ndarray, y_pred: np.ndarray):  # extrae TP/TN/FP/FN (asumiendo clase positiva=1)
    """Extrae TP, TN, FP, FN para clasificaci√≥n binaria."""
    tp = np.sum((y_true == 1) & (y_pred == 1))  # true positives
    tn = np.sum((y_true == 0) & (y_pred == 0))  # true negatives
    fp = np.sum((y_true == 0) & (y_pred == 1))  # false positives
    fn = np.sum((y_true == 1) & (y_pred == 0))  # false negatives
    return tp, tn, fp, fn  # retorna conteos
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 3.1: Matriz de Confusi√≥n (la base de todo)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** TP/TN/FP/FN como diagn√≥stico (no como tabla)
- **ID (opcional):** `M05-T03_1`
- **Duraci√≥n estimada:** 45‚Äì90 min
- **Nivel:** Intermedio
- **Dependencias:** Definir expl√≠citamente cu√°l es la clase positiva

#### 2) Objetivos
- Leer la matriz 2√ó2 sin confundirte entre FP y FN.
- Traducir el problema (spam, fraude, c√°ncer, etc.) a ‚Äúqu√© error es m√°s caro‚Äù.
- Usar la matriz para explicar cambios de precision/recall al mover el threshold.

#### 3) Relevancia
- Todas las m√©tricas son funciones de estos cuatro n√∫meros.
- Si FP/FN est√°n invertidos, todo el an√°lisis posterior queda inv√°lido.

#### 4) Mapa conceptual m√≠nimo
- `y_true` vs `y_pred` ‚Üí conteos ‚Üí m√©tricas.
- Cambiar `threshold` mueve masa entre celdas (no crea magia).

#### 5) Definiciones esenciales
- **FP:** predije 1 pero era 0 (alarma falsa).
- **FN:** predije 0 pero era 1 (caso perdido).

#### 6) Explicaci√≥n did√°ctica
- Subir threshold suele:
  - bajar FP (menos alarmas)
  - subir FN (pierdes positivos)

#### 7) Ejemplo modelado
- Si ‚Äúpositivo‚Äù = c√°ncer, FN suele ser m√°s grave que FP.

#### 8) Pr√°ctica guiada
- Crea 10 pares (true,pred) y llena la matriz a mano.

#### 9) Pr√°ctica independiente
- Repite con una definici√≥n distinta de ‚Äúpositivo‚Äù y observa c√≥mo cambia la interpretaci√≥n.

#### 10) Autoevaluaci√≥n
- ¬øQu√© celda corresponde a ‚Äúdije 0 pero era 1‚Äù?

#### 11) Errores comunes
- No declarar clase positiva.
- Intercambiar FP/FN.

#### 12) Retenci√≥n
- Atajo: FP = (pred 1, true 0), FN = (pred 0, true 1).

#### 13) Diferenciaci√≥n
- Multiclase: matriz K√óK, y cada clase se puede analizar como one-vs-rest.

#### 14) Recursos
- Glosario: Confusion Matrix.

#### 15) Nota docente
- Exige que el alumno explique un FP y un FN con un ejemplo de su dominio.
</details>

### 3.2 Accuracy, Precision, Recall, F1

```python
import numpy as np  # arrays y operaciones vectorizadas para m√©tricas

def accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:  # accuracy binario: proporci√≥n de aciertos
    """
    Accuracy = (TP + TN) / (TP + TN + FP + FN)

    Proporci√≥n de predicciones correctas.

    Problema: Puede ser enga√±oso con clases desbalanceadas.
    Si 99% son clase 0, predecir siempre 0 da 99% accuracy.
    """
    return np.mean(y_true == y_pred)  # promedio de booleanos True/False => accuracy en [0,1]

def precision(y_true: np.ndarray, y_pred: np.ndarray) -> float:  # precision binario: confiabilidad de positivos
    """
    Precision = TP / (TP + FP)

    De todos los que predije como positivos, ¬øcu√°ntos realmente lo son?

    Alta precisi√≥n = pocos falsos positivos.
    Importante cuando el costo de FP es alto (ej: spam ‚Üí inbox).
    """
    tp, tn, fp, fn = extract_tp_tn_fp_fn(y_true, y_pred)  # extrae conteos de confusi√≥n
    if tp + fp == 0:  # caso borde: no hay predicciones positivas
        return 0.0  # por convenci√≥n, precision=0
    return tp / (tp + fp)  # TP/(TP+FP)

def recall(y_true: np.ndarray, y_pred: np.ndarray) -> float:  # recall binario: cobertura de positivos reales
    """
    Recall (Sensitivity, True Positive Rate) = TP / (TP + FN)

    De todos los positivos reales, ¬øcu√°ntos captur√©?

    Alto recall = pocos falsos negativos.
    Importante cuando el costo de FN es alto (ej: detecci√≥n de c√°ncer).
    """
    tp, tn, fp, fn = extract_tp_tn_fp_fn(y_true, y_pred)  # extrae conteos de confusi√≥n
    if tp + fn == 0:  # caso borde: no hay positivos reales
        return 0.0  # por convenci√≥n, recall=0
    return tp / (tp + fn)  # TP/(TP+FN)

def f1_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:  # F1 binario: balance entre precision y recall
    """
    F1 = 2 * (precision * recall) / (precision + recall)

    Media arm√≥nica de precision y recall.

    √ötil cuando quieres un balance entre ambas m√©tricas.
    F1 alto solo si AMBAS precision y recall son altas.
    """
    p = precision(y_true, y_pred)  # calcula precision
    r = recall(y_true, y_pred)  # calcula recall
    if p + r == 0:  # caso borde: ambas m√©tricas en 0
        return 0.0  # por convenci√≥n, F1=0
    return 2 * (p * r) / (p + r)  # 2PR/(P+R)

def specificity(y_true: np.ndarray, y_pred: np.ndarray) -> float:  # especificidad: cobertura de negativos reales
    """
    Specificity (True Negative Rate) = TN / (TN + FP)

    De todos los negativos reales, ¬øcu√°ntos identifiqu√©?
    """
    tp, tn, fp, fn = extract_tp_tn_fp_fn(y_true, y_pred)  # extrae conteos de confusi√≥n
    if tn + fp == 0:  # caso borde: no hay negativos reales
        return 0.0  # por convenci√≥n, specificity=0
    return tn / (tn + fp)  # TN/(TN+FP)
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 3.2: Accuracy/Precision/Recall/F1 (cu√°ndo usar cada una)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Elegir m√©trica = declarar costo (y reportar threshold)
- **ID (opcional):** `M05-T03_2`
- **Duraci√≥n estimada:** 60‚Äì120 min
- **Nivel:** Intermedio
- **Dependencias:** 3.1

#### 2) Objetivos
- Identificar cu√°ndo accuracy es enga√±osa (desbalance).
- Elegir precision vs recall seg√∫n el costo de FP vs FN.
- Entender F1 como balance: cae si una de las dos es baja.

#### 3) Relevancia
- Aqu√≠ defines ‚Äúqu√© significa que el modelo sea bueno‚Äù.

#### 4) Mapa conceptual m√≠nimo
- accuracy: desempe√±o global.
- precision: control de FP.
- recall: control de FN.
- F1: balance precision/recall.

#### 5) Definiciones esenciales
- **Precision** responde: ‚Äúsi dije 1, ¬øcu√°ntas veces acert√©?‚Äù
- **Recall** responde: ‚Äúde los 1 reales, ¬øcu√°ntos encontr√©?‚Äù

#### 6) Explicaci√≥n did√°ctica
- Al subir threshold, normalmente sube precision y baja recall.

#### 7) Ejemplo modelado
- Modelo conservador: predice pocos 1 ‚Üí precision alta, recall baja.

#### 8) Pr√°ctica guiada
- Con el mismo conjunto, eval√∫a `threshold` en 0.3/0.5/0.7 y compara.

#### 9) Pr√°ctica independiente
- Busca un threshold que maximice F1 en validaci√≥n y reporta (F1, threshold).

#### 10) Autoevaluaci√≥n
- ¬øQu√© te falta para reproducir el mismo reporte ma√±ana?

#### 11) Errores comunes
- Reportar m√©tricas sin decir threshold.
- Optimizar F1 sin justificar el costo del error.

#### 12) Retenci√≥n
- Regla: costo ‚Üí m√©trica ‚Üí threshold.

#### 13) Diferenciaci√≥n
- Multiclase: macro vs micro (cuando las clases est√°n desbalanceadas).

#### 14) Recursos
- Glosario: Precision/Recall/F1.

#### 15) Nota docente
- Obliga a que el alumno elija una m√©trica y la defienda con una frase de costo.
</details>

### 3.3 Clase Metrics Completa

```python
import numpy as np  # arrays y operaciones num√©ricas para m√©tricas
from dataclasses import dataclass  # decorador para crear clases de datos

@dataclass  # genera __init__/__repr__/etc. para el reporte
class ClassificationReport:  # contenedor tipado para m√©tricas de clasificaci√≥n
    """Reporte de m√©tricas de clasificaci√≥n."""
    accuracy: float  # proporci√≥n de aciertos
    precision: float  # TP / (TP + FP)
    recall: float  # TP / (TP + FN)
    f1: float  # media arm√≥nica de precision/recall
    specificity: float  # TN / (TN + FP)
    confusion_matrix: np.ndarray  # matriz 2x2 de conteos

    def __str__(self) -> str:  # formatea el reporte como texto legible
        cm = self.confusion_matrix  # alias local para interpolar en el template
        return f"""
Classification Report
=====================
Accuracy:    {self.accuracy:.4f}
Precision:   {self.precision:.4f}
Recall:      {self.recall:.4f}
F1 Score:    {self.f1:.4f}
Specificity: {self.specificity:.4f}

Confusion Matrix:
           Pred 0  Pred 1
Actual 0   {cm[0,0]:5d}   {cm[0,1]:5d}
Actual 1   {cm[1,0]:5d}   {cm[1,1]:5d}
"""

def classification_report(y_true: np.ndarray, y_pred: np.ndarray) -> ClassificationReport:  # empaqueta m√©tricas en un objeto
    """Genera reporte completo de m√©tricas."""
    return ClassificationReport(  # construye el reporte con m√©tricas base
        accuracy=accuracy(y_true, y_pred),  # accuracy global
        precision=precision(y_true, y_pred),  # precision (clase positiva)
        recall=recall(y_true, y_pred),  # recall (clase positiva)
        f1=f1_score(y_true, y_pred),  # F1-score
        specificity=specificity(y_true, y_pred),  # especificidad (clase negativa)
        confusion_matrix=confusion_matrix(y_true, y_pred)  # matriz de confusi√≥n 2x2
    )  # retorna instancia de ClassificationReport

# Demo
y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])  # etiquetas reales
y_pred = np.array([0, 0, 1, 0, 1, 1, 0, 1, 1, 1])  # predicciones (ejemplo)

report = classification_report(y_true, y_pred)  # genera reporte de m√©tricas
print(report)  # imprime reporte formateado
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 3.3: Reporte de m√©tricas (de n√∫meros a diagn√≥stico)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Del reporte a la acci√≥n: qu√© cambiar si precision/recall no cumplen
- **ID (opcional):** `M05-T03_3`
- **Duraci√≥n estimada:** 60‚Äì120 min
- **Nivel:** Intermedio
- **Dependencias:** 3.1‚Äì3.2

#### 2) Objetivos
- Empaquetar m√©tricas para comparar experimentos sin confusi√≥n.
- Interpretar el reporte como diagn√≥stico: qu√© tipo de error domina.
- Mantener reproducibilidad: mismo split/seed/threshold.

#### 3) Relevancia
- En un proyecto, el reporte es lo que justifica decisiones (no solo el c√≥digo).

#### 4) Mapa conceptual m√≠nimo
- confusion matrix ‚Üí m√©tricas ‚Üí reporte ‚Üí decisi√≥n (threshold/feature/modelo).

#### 5) Definiciones esenciales
- ‚ÄúReporte‚Äù no es solo n√∫meros: requiere contexto (dataset/split/threshold).

#### 6) Explicaci√≥n did√°ctica
- Si el reporte no incluye contexto, es f√°cil autoenga√±arse con comparaciones inv√°lidas.

#### 7) Ejemplo modelado
- Recall bajo: baja threshold o mejora features; Precision baja: sube threshold o reduce ruido.

#### 8) Pr√°ctica guiada
- Cambia 2 predicciones del demo y observa c√≥mo cambian todas las m√©tricas.

#### 9) Pr√°ctica independiente
- Extiende a macro-F1 en multiclase (one-vs-rest).

#### 10) Autoevaluaci√≥n
- ¬øQu√© te falta para reproducir el mismo reporte ma√±ana?

#### 11) Errores comunes
- Comparar reportes de datasets distintos.

#### 12) Retenci√≥n
- ‚ÄúM√©trica sin contexto = n√∫mero sin significado‚Äù.

#### 13) Diferenciaci√≥n
- Avanzado: incluir `mean¬±std` v√≠a cross-validation.

#### 14) Recursos
- Plan v5: disciplina de validaci√≥n y registro de resultados.

#### 15) Nota docente
- Pide una recomendaci√≥n concreta basada en el reporte (threshold/features/datos).
</details>

---

## üíª Parte 4: Validaci√≥n y Regularizaci√≥n

### 4.0 Validaci√≥n y regularizaci√≥n ‚Äî Nivel: intermedio/avanzado

**Prop√≥sito:** aprender el ‚Äúworkflow real‚Äù que evita autoenga√±o:

- dividir datos correctamente
- validar de forma robusta
- controlar overfitting (regularizaci√≥n)

#### Objetivos de aprendizaje (medibles)

Al terminar esta parte podr√°s:

- **Explicar** la diferencia entre train/val/test y por qu√© el test no se toca.
- **Aplicar** K-fold cross validation y reportar media ¬± desviaci√≥n.
- **Diagnosticar** sesgo-varianza en t√©rminos pr√°cticos (qu√© cambia si aumentas `Œª` o si cambias el tama√±o del modelo).
- **Implementar** regularizaci√≥n L2 y justificar por qu√© se excluye el bias.

#### Resumen ejecutivo (big idea)

- **Validaci√≥n** te dice si generalizas.
- **Regularizaci√≥n** controla complejidad efectiva.

Conectar esto con el Pathway:

- En el curso, se eval√∫a tanto la *matem√°tica* como tu capacidad de **evitar leakage** y reportar resultados correctamente.

#### Actividades activas (obligatorias)

- Ejecuta `train_test_split` con al menos 2 semillas distintas y compara varianza en accuracy.
- Haz K-fold (k=5) y reporta `mean ¬± std`.
- Prueba `lambda_` en `{0, 0.01, 0.1, 1.0}` y describe el efecto.

#### Errores comunes

- **Data leakage** por normalizar antes del split.
- **Elegir hiperpar√°metros mirando el test** (invalidas el test).
- **Regularizar el bias** sin querer.

#### Integraci√≥n con Plan v4/v5

- v4.0: usa simulacros para preguntas tipo examen (`study_tools/SIMULACRO_EXAMEN_TEORICO.md`).
- v5.0: valida tu implementaci√≥n con Shadow Mode (sklearn) antes de cerrar el m√≥dulo.

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 4.0: Validaci√≥n + Regularizaci√≥n (workflow anti-autoenga√±o)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** C√≥mo saber si generalizas: split correcto, validaci√≥n y control de overfitting
- **ID (opcional):** `M05-T04_0`
- **Duraci√≥n estimada:** 90‚Äì150 min
- **Nivel:** Intermedio/Avanzado
- **Dependencias:** M√©tricas (Parte 3), LogReg (Parte 2)

#### 2) Objetivos
- Explicar la diferencia entre train/val/test y por qu√© el test no se toca.
- Entender qu√© pregunta responde K-fold (variancia de performance).
- Entender regularizaci√≥n como control de complejidad efectiva (bias-varianza).

#### 3) Relevancia
- Sin validaci√≥n, puedes ‚Äúganar‚Äù en train y fallar en producci√≥n.
- Regularizaci√≥n es una herramienta central para modelos lineales y redes.

#### 4) Mapa conceptual m√≠nimo
- Entrenar en train.
- Elegir hiperpar√°metros con val (o CV).
- Reportar final en test una sola vez.

#### 5) Definiciones esenciales
- **Leakage:** usar info del test/val al entrenar.
- **Overfitting:** buen train, mal test.

#### 6) Explicaci√≥n did√°ctica
- Si miras el test repetidamente, el test se convierte en ‚Äúval‚Äù sin querer.

#### 7) Ejemplo modelado
- Dos seeds distintas ‚Üí dos splits distintos ‚Üí accuracy distinta: eso es varianza.

#### 8) Pr√°ctica guiada
- Ejecuta 2 splits con semillas diferentes y reporta ambas m√©tricas.

#### 9) Pr√°ctica independiente
- Haz K-fold y reporta `mean ¬± std`.

#### 10) Autoevaluaci√≥n
- ¬øCu√°l conjunto se usa para elegir `lambda_`?

#### 11) Errores comunes
- Normalizar usando todo el dataset antes del split.
- Elegir hiperpar√°metros ‚Äúviendo‚Äù el test.

#### 12) Retenci√≥n
- Regla: test se usa una vez, al final.

#### 13) Diferenciaci√≥n
- Avanzado: nested CV (conceptual) para selecci√≥n + evaluaci√≥n robusta.

#### 14) Recursos
- Plan v5: Shadow Mode para validar implementaciones.

#### 15) Nota docente
- Exigir que el alumno declare expl√≠citamente qu√© datos us√≥ para cada decisi√≥n.
</details>

### 4.1 Train/Test Split

```python
import numpy as np  # arrays y utilidades de aleatoriedad

def train_test_split(  # divide X/y en train y test de forma reproducible
    X: np.ndarray,  # features
    y: np.ndarray,  # targets
    test_size: float = 0.2,  # proporci√≥n para test
    random_state: int = None  # semilla opcional
) -> tuple:  # retorna (X_train, X_test, y_train, y_test)
    """
    Divide datos en conjuntos de entrenamiento y prueba.

    Args:
        X: features
        y: targets
        test_size: proporci√≥n para test (0-1)
        random_state: semilla para reproducibilidad
    """
    if random_state is not None:  # si se pide reproducibilidad
        np.random.seed(random_state)  # fija semilla global de numpy

    n = len(y)  # n√∫mero de muestras
    indices = np.random.permutation(n)  # permuta √≠ndices de 0..n-1

    test_size_n = int(n * test_size)  # tama√±o del test en n√∫mero de muestras
    test_indices = indices[:test_size_n]  # √≠ndices para test
    train_indices = indices[test_size_n:]  # √≠ndices para train

    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]  # indexa y retorna splits
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 4.1: Train/Test Split (contratos y fugas)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Split reproducible: qu√© debe ser aleatorio y qu√© debe ser determinista
- **ID (opcional):** `M05-T04_1`
- **Duraci√≥n estimada:** 45‚Äì90 min
- **Nivel:** Intermedio
- **Dependencias:** 4.0

#### 2) Objetivos
- Verificar invariantes: tama√±os de split, alineaci√≥n X/y, sin duplicados.
- Entender el rol de `random_state`.

#### 3) Relevancia
- Si tu split est√° mal, todo el benchmark se vuelve irrelevante.

#### 4) Mapa conceptual m√≠nimo
- Permutar √≠ndices ‚Üí cortar ‚Üí indexar X/y.

#### 5) Definiciones esenciales
- **Reproducibilidad:** misma semilla ‚Üí mismo split.

#### 6) Explicaci√≥n did√°ctica
- Split debe hacerse antes de normalizar/seleccionar features (evita leakage).

#### 7) Ejemplo modelado
- Verifica que `len(train)+len(test)=n`.

#### 8) Pr√°ctica guiada
- Imprime tama√±os y distribuciones de clase por split.

#### 9) Pr√°ctica independiente
- Implementa split estratificado (conceptual) para clasificaci√≥n desbalanceada.

#### 10) Autoevaluaci√≥n
- ¬øQu√© se rompe si `X` y `y` se permutan con √≠ndices distintos?

#### 11) Errores comunes
- Reusar el test para ‚Äúajustar‚Äù el modelo.

#### 12) Retenci√≥n
- Regla: split primero; transformaciones despu√©s (fit en train).

#### 13) Diferenciaci√≥n
- Avanzado: train/val/test + pipelines.

#### 14) Recursos
- Plan v4/v5: disciplina de evaluaci√≥n.

#### 15) Nota docente
- Pide que el alumno identifique 2 formas de leakage y c√≥mo evitarlas.
</details>

### 4.2 K-Fold Cross Validation

```python
import numpy as np  # Importa NumPy: se usa para generar √≠ndices, barajar, concatenar y calcular estad√≠sticas
from typing import List, Tuple  # Importa tipos: anota listas de folds y tuplas (train_idx, val_idx) para claridad

def k_fold_split(n: int, k: int) -> List[Tuple[np.ndarray, np.ndarray]]:  # Genera folds: devuelve pares (train_indices, val_indices)
    """
    Genera √≠ndices para K-Fold Cross Validation.

    Returns:
        Lista de (train_indices, val_indices) para cada fold
    """
    indices = np.arange(n)  # Crea √≠ndices 0..n-1: base para particionar el dataset en folds
    np.random.shuffle(indices)  # Baraja in-place: randomiza el orden para evitar sesgos por ordenaci√≥n

    fold_size = n // k  # Tama√±o de cada fold: divisi√≥n entera (si sobra, el √∫ltimo fold se ajusta)
    folds = []  # Lista acumuladora: contendr√° k tuplas (train_idx, val_idx)

    for i in range(k):  # Itera k folds: cada i define un segmento distinto para validaci√≥n
        start = i * fold_size  # Inicio del fold i dentro del vector de √≠ndices barajados
        end = start + fold_size if i < k - 1 else n  # Fin del fold: √∫ltimo fold captura el remanente para cubrir todos

        val_indices = indices[start:end]  # √çndices de validaci√≥n: subarray del fold i
        train_indices = np.concatenate([indices[:start], indices[end:]])  # √çndices de entrenamiento: todos los √≠ndices fuera del fold i

        folds.append((train_indices, val_indices))  # Guarda fold: par (train,val) para consumirlo en cross_validate

    return folds  # Devuelve lista de folds: cada elemento permite indexar X e y con train/val

def cross_validate(  # ejecuta validaci√≥n cruzada K-Fold para una clase de modelo
    model_class,  # Clase del modelo a evaluar: debe poder instanciarse como model_class() y tener fit/score
    X: np.ndarray,  # Matriz de features (n,d): datos completos a partir de los cuales se generan folds
    y: np.ndarray,  # Vector de etiquetas (n,): targets alineados con X
    k: int = 5,  # N√∫mero de folds: m√°s folds => menos sesgo pero m√°s costo computacional
    **model_params  # Hiperpar√°metros: se pasan a model.fit(...) en cada fold
) -> dict:  # Retorna un dict con scores por fold y agregados (mean/std)
    """
    Realiza K-Fold Cross Validation.

    Returns:
        Dict con scores de cada fold y promedio
    """
    folds = k_fold_split(len(y), k)  # Genera folds: lista de pares (train_idx, val_idx) para iterar
    scores = []  # Acumula scores por fold: se agregan para media y desviaci√≥n

    for i, (train_idx, val_idx) in enumerate(folds):  # Recorre folds: cada iteraci√≥n entrena en train y eval√∫a en val
        # Split
        X_train, X_val = X[train_idx], X[val_idx]  # Subconjuntos de features: train/val para el fold actual
        y_train, y_val = y[train_idx], y[val_idx]  # Subconjuntos de labels: targets alineados con los √≠ndices

        # Train
        model = model_class()  # Instancia un modelo nuevo: evita contaminaci√≥n de estado entre folds
        model.fit(X_train, y_train, **model_params)  # Entrena en train: aplica hiperpar√°metros pasados por **model_params

        # Evaluate
        score = model.score(X_val, y_val)  # Eval√∫a desempe√±o en validaci√≥n: m√©trica provista por el modelo (p.ej., accuracy)
        scores.append(score)  # Guarda score del fold: se usar√° para estad√≠stica final

    return {  # Devuelve resumen de CV: permite reportar distribuci√≥n y estabilidad del rendimiento
        'scores': scores,  # Lista de scores por fold: muestra variaci√≥n por partici√≥n
        'mean': np.mean(scores),  # Promedio de scores: estimaci√≥n puntual del rendimiento
        'std': np.std(scores)  # Desviaci√≥n est√°ndar: cuantifica sensibilidad al split (inestabilidad)
    }  # Cierra dict: estructura final de resultados para logging/decisiones

# Demo
# cv_results = cross_validate(LogisticRegression, X, y, k=5, learning_rate=0.1, n_iterations=500)
# print(f"CV Accuracy: {cv_results['mean']:.4f} ¬± {cv_results['std']:.4f}")
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 4.2: K-Fold (qu√© estima y qu√© no)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** K-fold como estimador de varianza y robustez (no como ‚Äúmejorar accuracy‚Äù)
- **ID (opcional):** `M05-T04_2`
- **Duraci√≥n estimada:** 60‚Äì120 min
- **Nivel:** Intermedio
- **Dependencias:** 4.1

#### 2) Objetivos
- Entender que K-fold produce una distribuci√≥n de scores.
- Reportar `mean ¬± std`.
- Evitar errores de leakage en CV (fit transforms dentro de cada fold).

#### 3) Relevancia
- Te da confianza en la estabilidad del modelo.

#### 4) Mapa conceptual m√≠nimo
- Repartir √≠ndices en folds ‚Üí entrenar k veces ‚Üí evaluar k veces.

#### 5) Definiciones esenciales
- **Fold:** partici√≥n usada como validaci√≥n.

#### 6) Explicaci√≥n did√°ctica
- Si la std es alta, tu rendimiento depende demasiado del split.

#### 7) Ejemplo modelado
- `k=5` ‚Üí 5 scores; promedia y reporta dispersi√≥n.

#### 8) Pr√°ctica guiada
- Ejecuta CV con 2 seeds y compara la std.

#### 9) Pr√°ctica independiente
- Implementa un ‚Äúgrid‚Äù peque√±o sobre `learning_rate` y compara medias.

#### 10) Autoevaluaci√≥n
- ¬øPor qu√© CV no reemplaza el test final?

#### 11) Errores comunes
- Elegir hiperpar√°metros y evaluar todo en el mismo CV sin un test final (sobreajuste de selecci√≥n).

#### 12) Retenci√≥n
- Regla: CV para selecci√≥n/estimaci√≥n; test para cierre.

#### 13) Diferenciaci√≥n
- Avanzado: nested CV (conceptual).

#### 14) Recursos
- Notas de validaci√≥n y bias-varianza.

#### 15) Nota docente
- Pide que el alumno explique qu√© significa ‚Äústd alta‚Äù con una analog√≠a.
</details>

### 4.3 Regularizaci√≥n

```python
import numpy as np  # Importa NumPy: se usa para √°lgebra vectorizada, concatenaci√≥n, abs/sign y operaciones en coste/gradiente

class LogisticRegressionRegularized:  # Logistic Regression con regularizaci√≥n: a√±ade penalizaci√≥n L1/L2 para controlar complejidad
    """Logistic Regression con regularizaci√≥n L1/L2."""

    def __init__(self, regularization: str = 'l2', lambda_: float = 0.01):  # Inicializa configuraci√≥n de regularizaci√≥n (tipo y fuerza)
        """
        Args:
            regularization: 'l1', 'l2', o None
            lambda_: fuerza de regularizaci√≥n
        """
        self.regularization = regularization  # Tipo de penalizaci√≥n: 'l2' (ridge), 'l1' (lasso) o None (sin regularizar)
        self.lambda_ = lambda_  # Par√°metro Œª: escala cu√°nto pesa la penalizaci√≥n respecto a la loss base
        self.theta = None  # Par√°metros del modelo (d+1,): incluye bias en theta[0]; se inicializa al entrenar
        self.cost_history = []  # Historial del coste total por iteraci√≥n: √∫til para ver convergencia y efecto de Œª

    def _cost(self, X: np.ndarray, y: np.ndarray) -> float:  # calcula el coste BCE + t√©rmino de regularizaci√≥n
        """Costo con regularizaci√≥n."""
        m = len(y)  # N√∫mero de muestras: se usa para promediar la loss y escalar el t√©rmino de regularizaci√≥n
        h = sigmoid(X @ self.theta)  # Probabilidades predichas: aplica œÉ a logits XŒ∏ (X ya incluye bias)
        h = np.clip(h, 1e-15, 1 - 1e-15)  # Evita log(0): recorta h para estabilidad num√©rica en cross-entropy

        # Cross-entropy base
        bce = -(1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))  # BCE promedio: log-loss binaria (cross-entropy)

        # Regularizaci√≥n (excluir bias theta[0])
        if self.regularization == 'l2':  # L2/Ridge: penaliza suma de cuadrados de pesos (excluyendo el bias)
            # Ridge: Œª/2m * Œ£Œ∏‚±º¬≤
            reg = (self.lambda_ / (2 * m)) * np.sum(self.theta[1:] ** 2)  # T√©rmino L2: (Œª/2m)||Œ∏_{1:}||¬≤
        elif self.regularization == 'l1':  # L1/Lasso: penaliza suma de valores absolutos (promueve sparsity)
            # Lasso: Œª/m * Œ£|Œ∏‚±º|
            reg = (self.lambda_ / m) * np.sum(np.abs(self.theta[1:]))  # T√©rmino L1: (Œª/m)||Œ∏_{1:}||‚ÇÅ
        else:  # Sin regularizaci√≥n: no se a√±ade penalizaci√≥n al coste
            reg = 0  # Penalizaci√≥n nula: coste total coincide con BCE

        return bce + reg  # Coste total: loss base + penalizaci√≥n (controla overfitting)

    def _gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:  # calcula gradiente BCE + contribuci√≥n de regularizaci√≥n
        """Gradiente con regularizaci√≥n."""
        m = len(y)  # N√∫mero de muestras: normaliza el gradiente (promedio)
        h = sigmoid(X @ self.theta)  # Probabilidades: œÉ(XŒ∏) para construir el gradiente de la loss

        # Gradiente base
        grad = (1/m) * X.T @ (h - y)  # Gradiente BCE: (1/m) X·µÄ(h‚àíy) para log√≠stica (sin regularizaci√≥n)

        # Regularizaci√≥n (excluir bias)
        if self.regularization == 'l2':  # Gradiente L2: proporcional a Œ∏ (excepto bias)
            reg_grad = np.concatenate([[0], (self.lambda_ / m) * self.theta[1:]])  # [0, (Œª/m)Œ∏‚ÇÅ:]: excluye bias en la penalizaci√≥n
        elif self.regularization == 'l1':  # Gradiente L1: usa signo (subgradiente) para empujar a 0
            reg_grad = np.concatenate([[0], (self.lambda_ / m) * np.sign(self.theta[1:])])  # [0, (Œª/m)sign(Œ∏‚ÇÅ:)]: excluye bias
        else:  # Sin regularizaci√≥n: no se suma gradiente adicional
            reg_grad = 0  # Penalizaci√≥n nula: gradiente total coincide con grad base

        return grad + reg_grad  # Gradiente total: grad base + contribuci√≥n de regularizaci√≥n

    def fit(self, X: np.ndarray, y: np.ndarray,  # entrena el modelo con GD (firma multi-l√≠nea)
            learning_rate: float = 0.1, n_iterations: int = 1000):  # Entrena por GD: actualiza Œ∏ usando gradiente con regularizaci√≥n
        X_b = np.column_stack([np.ones(len(X)), X])  # A√±ade bias: crea X_b (n,d+1) con columna de 1s al inicio
        self.theta = np.zeros(X_b.shape[1])  # Inicializa Œ∏: un par√°metro por columna de X_b (incluye bias)

        for _ in range(n_iterations):  # Loop de entrenamiento: repite actualizaciones de GD
            gradient = self._gradient(X_b, y)  # Calcula gradiente total en el punto actual (incluye regularizaci√≥n)
            self.theta -= learning_rate * gradient  # Actualiza Œ∏: Œ∏ ‚Üê Œ∏ ‚àí Œ±¬∑grad
            self.cost_history.append(self._cost(X_b, y))  # Registra coste: permite inspeccionar convergencia y overfitting

        return self  # Devuelve self: permite chaining estilo sklearn

    def predict(self, X: np.ndarray) -> np.ndarray:  # predice etiquetas binarias aplicando œÉ y umbral 0.5
        X_b = np.column_stack([np.ones(len(X)), X])  # A√±ade bias: alinea features con theta aprendida (incluye intercepto)
        return (sigmoid(X_b @ self.theta) >= 0.5).astype(int)  # Umbraliza probas: devuelve etiquetas 0/1 seg√∫n prob>=0.5

    def score(self, X: np.ndarray, y: np.ndarray) -> float:  # calcula accuracy promedio sobre (X,y)
        return np.mean(self.predict(X) == y)  # Accuracy: promedio de aciertos del clasificador sobre (X,y)
```

<details open>
<summary><strong>üìå Complemento pedag√≥gico ‚Äî Secci√≥n 4.3: Regularizaci√≥n (L1/L2 y por qu√© se excluye el bias)</strong></summary>

#### 1) Metadatos
- **T√≠tulo:** Regularizaci√≥n como control de complejidad: L2 (suaviza) vs L1 (sparse)
- **ID (opcional):** `M05-T04_3`
- **Duraci√≥n estimada:** 90‚Äì150 min
- **Nivel:** Intermedio
- **Dependencias:** 2.3 (BCE/gradiente), 4.0

#### 2) Objetivos
- Entender qu√© t√©rmino se agrega al costo y c√≥mo afecta el gradiente.
- Justificar por qu√© el bias no se regulariza.
- Relacionar `lambda_` con bias-varianza.

#### 3) Relevancia
- Regularizaci√≥n suele ser la diferencia entre generalizar o sobreajustar en modelos lineales.

#### 4) Mapa conceptual m√≠nimo
- Loss base + penalizaci√≥n a pesos ‚Üí update m√°s ‚Äúconservador‚Äù.

#### 5) Definiciones esenciales
- **L2:** penaliza cuadrados (shrink continuo).
- **L1:** penaliza valores absolutos (promueve sparsity).

#### 6) Explicaci√≥n did√°ctica
- Regularizar el bias puede desplazar la frontera innecesariamente.

#### 7) Ejemplo modelado
- Si `lambda_` sube, t√≠picamente bajan magnitudes de `theta[1:]`.

#### 8) Pr√°ctica guiada
- Prueba `lambda_` en `{0,0.01,0.1,1.0}` y observa train/test.

#### 9) Pr√°ctica independiente
- Grafica norma de `theta` vs `lambda_`.

#### 10) Autoevaluaci√≥n
- ¬øQu√© efecto esperas en el gap train-test cuando aumenta `lambda_`?

#### 11) Errores comunes
- Regularizar tambi√©n `theta[0]`.
- Olvidar ajustar el gradiente con el t√©rmino de regularizaci√≥n.

#### 12) Retenci√≥n
- Regla: penaliza pesos, no el bias.

#### 13) Diferenciaci√≥n
- Avanzado: conexi√≥n con MAP (prior gaussiano / laplaciano).

#### 14) Recursos
- Notas de Ridge/Lasso y sesgo-varianza.

#### 15) Nota docente
- Pide que el alumno explique por qu√© L1 puede hacer pesos exactamente 0.
</details>

---

### ‚ö†Ô∏è Aviso cr√≠tico antes de √Årboles: Recursividad (Semana 12)

La implementaci√≥n de √°rboles se basa en **recursi√≥n**. Si no defines y pruebas condiciones de parada, vas a generar √°rboles infinitos o muy profundos.

- Condiciones de parada m√≠nimas: `max_depth`, pureza (todas las etiquetas iguales), `min_samples_split`, ‚Äúno split improves‚Äù.
- Recurso recomendado: https://realpython.com/python-recursion/
- Debug m√≠nimo: imprime `depth`, `n_samples` y el criterio elegido por nodo durante desarrollo.

### Micro-sprint (15 minutos): recursividad m√≠nima para √°rboles

Dos reglas que debes internalizar:

- **Caso base:** el caso m√°s peque√±o que puedes responder inmediatamente (aqu√≠ se detiene la recursi√≥n).
- **Paso recursivo:** reduces el problema a una versi√≥n m√°s peque√±a de s√≠ mismo.

Si no puedes decir el caso base en 1 l√≠nea, tu implementaci√≥n del √°rbol probablemente recursar√° para siempre.

#### Ejemplo: suma recursiva (practica el modelo mental)

```python
from typing import Sequence  # Importa Sequence para anotar entradas indexables (listas/tuplas)

def sum_recursive(xs: Sequence[float]) -> float:  # suma recursiva de una secuencia de n√∫meros
    # Caso base: la suma de una lista vac√≠a es 0
    if len(xs) == 0:  # condici√≥n de parada: secuencia vac√≠a
        return 0.0  # retorna 0 como elemento neutro de la suma

    # Paso recursivo: reduces el problema quitando el primer elemento
    return float(xs[0]) + sum_recursive(xs[1:])  # suma primer elemento y recurre con el resto


assert sum_recursive([]) == 0.0  # test: caso base (lista vac√≠a)
assert sum_recursive([3.0]) == 3.0  # test: un solo elemento
assert sum_recursive([3.0, 2.0, 5.0]) == 10.0  # test: suma de varios elementos
```

#### Pila de llamadas (lo que Python est√° haciendo)

```text
sum_recursive([3, 2, 5])
= 3 + sum_recursive([2, 5])
    = 2 + sum_recursive([5])
        = 5 + sum_recursive([])
            = 0
```

#### Conexi√≥n con Decision Trees: condiciones de parada = casos base

Al construir un nodo, tu caso base deber√≠a dispararse cuando:

- `depth >= max_depth`
- el nodo es **puro** (todas las etiquetas son iguales)
- `n_samples < min_samples_split`
- ning√∫n split mejora impureza (information gain <= 0)

## üå≥ Parte 5: Tree-Based Models (Semana 12)

Esta semana cubre modelos supervisados **no diferenciables** (no entrenan con Gradient Descent). La l√≥gica de entrenamiento es:

- elegir un *split* (feature + threshold)
- medir qu√© tan ‚Äúpuro‚Äù queda cada lado (Entrop√≠a o Gini)
- repetir recursivamente

### 5.1 Entrop√≠a, Gini e Information Gain

Definiciones base (para clasificaci√≥n):

- **Entrop√≠a:** `H(y) = - Œ£ p(c) log2 p(c)`
- **Gini:** `G(y) = 1 - Œ£ p(c)^2`

Un split `(j, t)` divide el dataset en:

- izquierda: `x_j ‚â§ t`
- derecha: `x_j > t`

La idea es maximizar la mejora en pureza:

- **Information Gain:** `IG = impurity(parent) - weighted_impurity(children)`

### 5.2 Entrenable desde cero (entregable)

Entregable runnable:

- `scripts/decision_tree_from_scratch.py`

Ejecuta:

```bash
python3 scripts/decision_tree_from_scratch.py --criterion gini --max-depth 5
```

Objetivo m√≠nimo:

- que el script entrene un √°rbol y reporte accuracy train/test en un dataset toy
- que puedas explicar (en 5 l√≠neas) c√≥mo el √°rbol decide el mejor split

### 5.3 Ensembles (intro): Bagging vs Boosting

Conceptos clave:

- **Bagging (Random Forest):** muchos √°rboles entrenados en *bootstrap samples*; reduce varianza.
- **Boosting (Gradient Boosting/XGBoost):** √°rboles entrenados secuencialmente corrigiendo errores; reduce bias (pero puede sobreajustar).

---

## üéØ Ejercicios por tema (progresivos) + Soluciones

Reglas:

- **Intenta primero** sin mirar la soluci√≥n.
- **Timebox sugerido:** 20‚Äì45 min por ejercicio.
- **√âxito m√≠nimo:** tu soluci√≥n debe pasar los `assert`.

---

### Ejercicio 5.1: Regresi√≥n lineal (Normal Equation) + recuperaci√≥n de pesos

#### Enunciado

1) **B√°sico**

- Genera un dataset sint√©tico: `y = Xw + noise`.

2) **Intermedio**

- Estima `w_hat` usando la ecuaci√≥n normal con `np.linalg.solve`.

3) **Avanzado**

- Verifica que `w_hat` se aproxima a `w_true` y que el MSE es peque√±o.

#### Soluci√≥n

```python
import numpy as np  # arrays y operaciones num√©ricas

np.random.seed(0)  # fija semilla para reproducibilidad
n, d = 500, 3  # n√∫mero de muestras y dimensi√≥n
X = np.random.randn(n, d)  # features gaussianas
w_true = np.array([0.7, -1.5, 2.0])  # pesos "verdaderos" para generar y
noise = 0.05 * np.random.randn(n)  # ruido gaussiano aditivo
y = X @ w_true + noise  # targets con ruido

# Normal equation: (X^T X) w = X^T y
XtX = X.T @ X  # matriz X^T X
Xty = X.T @ y  # vector X^T y
w_hat = np.linalg.solve(XtX, Xty)  # estima w por ecuaci√≥n normal

mse = np.mean((X @ w_hat - y) ** 2)  # error cuadr√°tico medio

assert w_hat.shape == (d,)  # sanity check: shape de w_hat
assert np.linalg.norm(w_hat - w_true) < 0.15  # sanity check: recupera pesos
assert mse < 0.01  # sanity check: MSE peque√±o
```

---

### Ejercicio 5.2: Regresi√≥n lineal (Gradient Descent) + comparaci√≥n con Normal Equation

#### Enunciado

1) **B√°sico**

- Implementa GD para minimizar MSE: `w <- w - Œ± (1/n) X^T (Xw - y)`.

2) **Intermedio**

- Compara `w_gd` contra `w_ne` (normal equation).

3) **Avanzado**

- Verifica que el loss disminuye (al menos al final es menor que al inicio).

#### Soluci√≥n

```python
import numpy as np  # arrays y operaciones num√©ricas

np.random.seed(1)  # fija semilla para reproducibilidad
n, d = 400, 4  # n√∫mero de muestras y dimensi√≥n
X = np.random.randn(n, d)  # features gaussianas
w_true = np.array([1.0, -2.0, 0.5, 3.0])  # pesos "verdaderos" para generar y
y = X @ w_true + 0.1 * np.random.randn(n)  # targets con ruido gaussiano

XtX = X.T @ X  # matriz X^T X
Xty = X.T @ y  # vector X^T y
w_ne = np.linalg.solve(XtX, Xty)  # soluci√≥n cerrada (normal equation)

w = np.zeros(d)  # inicializa pesos para GD
alpha = 0.05  # learning rate
losses = []  # historial de loss (MSE) para verificar descenso
for _ in range(3000):  # iteraciones de GD
    r = X @ w - y  # residuo (pred - real)
    grad = (X.T @ r) / n  # gradiente de MSE: (1/n) X^T (Xw - y)
    w = w - alpha * grad  # actualizaci√≥n: w <- w - alpha * grad
    losses.append(float(np.mean(r**2)))  # guarda MSE actual

w_gd = w  # pesos aprendidos por GD

assert losses[-1] <= losses[0]  # sanity check: el loss baja
assert np.linalg.norm(w_gd - w_ne) < 0.2  # sanity check: GD se aproxima a normal equation
```

---

### Ejercicio 5.3: M√©tricas desde una matriz de confusi√≥n (TP/TN/FP/FN)

#### Enunciado

1) **B√°sico**

- Implementa una funci√≥n que compute TP/TN/FP/FN para un problema binario.

2) **Intermedio**

- Implementa accuracy, precision, recall, F1.

3) **Avanzado**

- Valida con un caso conocido y `assert`.

#### Soluci√≥n

```python
import numpy as np  # arrays y operaciones vectorizadas

def confusion_counts(y_true: np.ndarray, y_pred: np.ndarray):  # calcula TP/TN/FP/FN para binario
    y_true = np.asarray(y_true).astype(int)  # normaliza a ndarray de enteros
    y_pred = np.asarray(y_pred).astype(int)  # normaliza a ndarray de enteros
    tp = int(np.sum((y_true == 1) & (y_pred == 1)))  # true positives
    tn = int(np.sum((y_true == 0) & (y_pred == 0)))  # true negatives
    fp = int(np.sum((y_true == 0) & (y_pred == 1)))  # false positives
    fn = int(np.sum((y_true == 1) & (y_pred == 0)))  # false negatives
    return tp, tn, fp, fn  # retorna conteos


def precision_recall_f1(y_true: np.ndarray, y_pred: np.ndarray):  # calcula m√©tricas binarias desde conteos
    tp, tn, fp, fn = confusion_counts(y_true, y_pred)  # obtiene TP/TN/FP/FN
    eps = 1e-12  # estabilizador para evitar divisi√≥n por cero
    acc = (tp + tn) / (tp + tn + fp + fn + eps)  # accuracy
    prec = tp / (tp + fp + eps)  # precision
    rec = tp / (tp + fn + eps)  # recall
    f1 = 2 * prec * rec / (prec + rec + eps)  # F1-score
    return float(acc), float(prec), float(rec), float(f1)  # retorna m√©tricas como floats


y_true = np.array([1, 1, 1, 0, 0, 0])  # etiquetas verdaderas (binarias)
y_pred = np.array([1, 0, 1, 0, 1, 0])  # predicciones (binarias)
tp, tn, fp, fn = confusion_counts(y_true, y_pred)  # computa conteos de confusi√≥n

assert (tp, tn, fp, fn) == (2, 2, 1, 1)  # valida conteos esperados

acc, prec, rec, f1 = precision_recall_f1(y_true, y_pred)  # calcula m√©tricas
assert np.isclose(acc, 4/6)  # valida accuracy
assert np.isclose(prec, 2/3)  # valida precision
assert np.isclose(rec, 2/3)  # valida recall
assert np.isclose(f1, 2/3)  # valida F1
```

---

### Ejercicio 5.4: Logistic Regression - sigmoid + BCE estable

#### Enunciado

1) **B√°sico**

- Implementa `sigmoid(z)` con `np.clip` para evitar overflow.

2) **Intermedio**

- Implementa Binary Cross-Entropy estable (con `clip`).

3) **Avanzado**

- Verifica:
  - BCE cerca de 0 para predicciones casi perfectas.
  - BCE ‚âà `-log(0.9)` cuando `y=1` y `p=0.9`.

#### Soluci√≥n

```python
import numpy as np  # arrays y operaciones num√©ricas

def sigmoid(z: np.ndarray) -> np.ndarray:  # sigmoide estable para logits
    z = np.asarray(z, dtype=float)  # asegura ndarray float
    z = np.clip(z, -500, 500)  # evita overflow en exp para |z| grande
    return 1.0 / (1.0 + np.exp(-z))  # œÉ(z)=1/(1+e^{-z})


def bce(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-15) -> float:  # BCE estable con clipping
    y_true = np.asarray(y_true, dtype=float)  # normaliza etiquetas a float
    y_pred = np.asarray(y_pred, dtype=float)  # normaliza predicciones a float
    y_pred = np.clip(y_pred, eps, 1.0 - eps)  # evita log(0) y log(1)
    return float(-np.mean(y_true * np.log(y_pred) + (1.0 - y_true) * np.log(1.0 - y_pred)))  # BCE promedio


y_true = np.array([1.0, 0.0, 1.0, 0.0])  # etiquetas reales
y_pred_good = np.array([0.999, 0.001, 0.999, 0.001])  # predicciones casi perfectas
assert bce(y_true, y_pred_good) < 0.01  # BCE debe ser cercana a 0
assert np.isclose(bce(np.array([1.0]), np.array([0.9])), -np.log(0.9), atol=1e-12)  # caso y=1, p=0.9
```

---

### Ejercicio 5.5: Gradiente de Logistic Regression (verificaci√≥n num√©rica)

#### Enunciado

1) **B√°sico**

- Implementa el gradiente de BCE para Logistic Regression:
  - `≈∑ = sigmoid(Xw)`
  - `‚àáw = (1/n) X^T (≈∑ - y)`

2) **Intermedio**

- Implementa una funci√≥n de p√©rdida `L(w)`.

3) **Avanzado**

- Verifica 1 coordenada del gradiente con diferencias centrales.

#### Soluci√≥n

```python
import numpy as np  # arrays, √°lgebra lineal y utilidades num√©ricas

def sigmoid(z: np.ndarray) -> np.ndarray:  # funci√≥n log√≠stica para mapear logits a probabilidades
    z = np.asarray(z, dtype=float)  # asegura ndarray float para operaciones estables
    z = np.clip(z, -500, 500)  # evita overflow en exp para valores extremos
    return 1.0 / (1.0 + np.exp(-z))  # aplica sigmoide elemento a elemento


def bce_from_logits(X: np.ndarray, y: np.ndarray, w: np.ndarray, eps: float = 1e-15) -> float:  # BCE dada X,y,w
    logits = X @ w  # calcula logits z = Xw
    y_hat = sigmoid(logits)  # convierte logits a probabilidades
    y_hat = np.clip(y_hat, eps, 1.0 - eps)  # evita log(0) en la BCE
    return float(-np.mean(y * np.log(y_hat) + (1.0 - y) * np.log(1.0 - y_hat)))  # BCE promedio


def grad_bce(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:  # gradiente de BCE para log√≠stica
    y_hat = sigmoid(X @ w)  # predicci√≥n probabil√≠stica ≈∑
    return (X.T @ (y_hat - y)) / X.shape[0]  # ‚àáw = (1/n) X^T (≈∑ - y)


np.random.seed(2)  # fija semilla global para reproducibilidad
n, d = 200, 3  # n√∫mero de muestras y dimensi√≥n
X = np.random.randn(n, d)  # features gaussianas
w0 = np.array([0.3, -0.7, 1.2])  # pesos "verdaderos" para generar etiquetas
probs = sigmoid(X @ w0)  # probabilidades base p(y=1|x) seg√∫n w0
y = (np.random.rand(n) < probs).astype(float)  # samplea y ~ Bernoulli(probs)

w = np.random.randn(d)  # inicializa pesos para evaluar gradiente
g = grad_bce(X, y, w)  # gradiente anal√≠tico

idx = 1  # coordenada del gradiente a verificar
h = 1e-6  # tama√±o de paso para diferencias centrales
e = np.zeros(d)  # vector base para perturbar una sola coordenada
e[idx] = 1.0  # activa la coordenada idx
L_plus = bce_from_logits(X, y, w + h * e)  # p√©rdida con w[idx] + h
L_minus = bce_from_logits(X, y, w - h * e)  # p√©rdida con w[idx] - h
g_num = (L_plus - L_minus) / (2.0 * h)  # aproximaci√≥n num√©rica del gradiente (central)

assert np.isclose(g[idx], g_num, rtol=1e-4, atol=1e-6)  # valida gradiente anal√≠tico vs num√©rico
```

---

### Ejercicio 5.6: Umbral (threshold) y trade-off precision/recall

#### Enunciado

1) **B√°sico**

- Dadas probabilidades `p` y etiquetas `y`, construye predicciones con umbral `t`.

2) **Intermedio**

- Calcula precision/recall para `t=0.5` y `t=0.3`.

3) **Avanzado**

- Verifica que al bajar el umbral t√≠picamente sube el recall (en el mismo dataset).

#### Soluci√≥n

```python
import numpy as np  # arrays y operaciones vectorizadas

def predict_threshold(p: np.ndarray, t: float) -> np.ndarray:  # aplica umbral t a probabilidades
    return (np.asarray(p) >= t).astype(int)  # boolean -> int (0/1)


def precision_recall(y_true: np.ndarray, y_pred: np.ndarray):  # calcula precision/recall desde etiquetas
    y_true = np.asarray(y_true).astype(int)  # normaliza etiquetas reales
    y_pred = np.asarray(y_pred).astype(int)  # normaliza etiquetas predichas
    tp = np.sum((y_true == 1) & (y_pred == 1))  # true positives
    fp = np.sum((y_true == 0) & (y_pred == 1))  # false positives
    fn = np.sum((y_true == 1) & (y_pred == 0))  # false negatives
    eps = 1e-12  # estabilizador num√©rico
    prec = tp / (tp + fp + eps)  # precision = TP/(TP+FP)
    rec = tp / (tp + fn + eps)  # recall = TP/(TP+FN)
    return float(prec), float(rec)  # retorna m√©tricas


np.random.seed(3)  # semilla del demo (no estrictamente necesaria aqu√≠)
y_true = np.array([1, 0, 1, 0, 1, 0, 1, 0])  # etiquetas reales
p = np.array([0.9, 0.6, 0.55, 0.52, 0.4, 0.35, 0.2, 0.1])  # probabilidades predichas

pred_05 = predict_threshold(p, 0.5)  # predicciones con umbral 0.5
pred_03 = predict_threshold(p, 0.3)  # predicciones con umbral 0.3

prec05, rec05 = precision_recall(y_true, pred_05)  # m√©tricas con t=0.5
prec03, rec03 = precision_recall(y_true, pred_03)  # m√©tricas con t=0.3

assert rec03 >= rec05  # al bajar umbral suele subir recall (capturas m√°s positivos)
```

---

### Ejercicio 5.7: Regularizaci√≥n L2 (Ridge) y norma de pesos

#### Enunciado

1) **B√°sico**

- Implementa Ridge Regression: `(X^T X + ŒªI) w = X^T y`.

2) **Intermedio**

- Compara `||w_ridge||` contra `||w_ols||`.

3) **Avanzado**

- Verifica que para `Œª>0`, t√≠picamente `||w_ridge|| <= ||w_ols||`.

#### Soluci√≥n

```python
import numpy as np  # arrays y √°lgebra lineal

np.random.seed(4)  # fija semilla para reproducibilidad
n, d = 300, 5  # n√∫mero de muestras y dimensi√≥n
X = np.random.randn(n, d)  # features gaussianas
w_true = np.array([2.0, -1.0, 0.5, 0.0, 3.0])  # pesos "verdaderos" para generar y
y = X @ w_true + 0.2 * np.random.randn(n)  # targets con ruido

XtX = X.T @ X  # matriz X^T X
Xty = X.T @ y  # vector X^T y
w_ols = np.linalg.solve(XtX, Xty)  # soluci√≥n OLS (lambda=0)

lam = 10.0  # fuerza de regularizaci√≥n L2
w_ridge = np.linalg.solve(XtX + lam * np.eye(d), Xty)  # soluci√≥n ridge: (XtX + ŒªI)w = Xty

assert np.linalg.norm(w_ridge) <= np.linalg.norm(w_ols) + 1e-8  # ridge tiende a reducir la norma de w
```

---

### Ejercicio 5.8: Train/Test split reproducible (semilla)

#### Enunciado

1) **B√°sico**

- Implementa `train_test_split(X,y,test_size,seed)`.

2) **Intermedio**

- Verifica que con la misma semilla el split es id√©ntico.

3) **Avanzado**

- Verifica que no se pierden muestras y que shapes son correctos.

#### Soluci√≥n

```python
import numpy as np  # arrays y RNG para generar datos y permutar √≠ndices

def train_test_split(X: np.ndarray, y: np.ndarray, test_size: float = 0.2, seed: int = 0):  # split reproducible
    X = np.asarray(X)  # asegura ndarray para indexaci√≥n
    y = np.asarray(y)  # asegura ndarray para indexaci√≥n
    n = X.shape[0]  # n√∫mero de muestras
    rng = np.random.default_rng(seed)  # RNG reproducible
    idx = np.arange(n)  # √≠ndices 0..n-1
    rng.shuffle(idx)  # permuta √≠ndices in-place
    n_test = int(round(n * test_size))  # tama√±o del test (redondeo)
    test_idx = idx[:n_test]  # √≠ndices del split de test
    train_idx = idx[n_test:]  # √≠ndices del split de train
    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]  # retorna (X_train, X_test, y_train, y_test)


np.random.seed(0)  # fija semilla global para reproducibilidad del demo
X = np.random.randn(100, 2)  # dataset sint√©tico (100,2)
y = (np.random.rand(100) < 0.5).astype(int)  # etiquetas binarias aleatorias

Xtr1, Xte1, ytr1, yte1 = train_test_split(X, y, test_size=0.25, seed=42)  # primer split
Xtr2, Xte2, ytr2, yte2 = train_test_split(X, y, test_size=0.25, seed=42)  # segundo split (misma seed)

assert np.allclose(Xtr1, Xtr2)  # verifica reproducibilidad en train
assert np.allclose(Xte1, Xte2)  # verifica reproducibilidad en test
assert np.all(ytr1 == ytr2)  # verifica reproducibilidad de y_train
assert np.all(yte1 == yte2)  # verifica reproducibilidad de y_test
assert Xtr1.shape[0] + Xte1.shape[0] == 100  # verifica que no se pierden muestras
```

---

### Ejercicio 5.9: K-Fold cross-validation (partici√≥n correcta)

#### Enunciado

1) **B√°sico**

- Implementa un generador de folds (√≠ndices train/val).

2) **Intermedio**

- Verifica que cada √≠ndice aparece exactamente una vez en validaci√≥n.

3) **Avanzado**

- Verifica que train/val no se solapan.

#### Soluci√≥n

```python
import numpy as np  # arrays y RNG para particiones

def kfold_indices(n: int, k: int, seed: int = 0):  # genera √≠ndices (train, val) para K-fold
    rng = np.random.default_rng(seed)  # RNG reproducible
    idx = np.arange(n)  # √≠ndices 0..n-1
    rng.shuffle(idx)  # permuta √≠ndices
    folds = np.array_split(idx, k)  # divide en k folds (tama√±os casi iguales)
    for i in range(k):  # itera folds como validaci√≥n
        val_idx = folds[i]  # fold i como validaci√≥n
        train_idx = np.concatenate([folds[j] for j in range(k) if j != i])  # resto como train
        yield train_idx, val_idx  # retorna par (train_idx, val_idx)


n = 23  # n√∫mero de muestras
k = 5  # n√∫mero de folds
seen = np.zeros(n, dtype=int)  # contador de apariciones en validaci√≥n
for tr, va in kfold_indices(n, k, seed=123):  # recorre folds
    assert len(np.intersect1d(tr, va)) == 0  # train/val no se solapan
    seen[va] += 1  # marca que estos √≠ndices aparecieron en validaci√≥n
assert np.all(seen == 1)  # cada √≠ndice aparece exactamente una vez en validaci√≥n
```

---

### Ejercicio 5.10: √Årboles - Gini e Information Gain (split 1D)

#### Enunciado

1) **B√°sico**

- Implementa impurity Gini para etiquetas binarias.

2) **Intermedio**

- Para un feature 1D y un umbral `t`, computa el Information Gain.

3) **Avanzado**

- Encuentra el mejor umbral entre varios candidatos y verifica el resultado.

#### Soluci√≥n

```python
import numpy as np  # arrays y operaciones num√©ricas

def gini(y: np.ndarray) -> float:  # impurity Gini para etiquetas binarias
    y = np.asarray(y).astype(int)  # normaliza a ndarray de enteros
    if y.size == 0:  # caso borde: sin muestras
        return 0.0  # por convenci√≥n, impurity nula
    p1 = np.mean(y == 1)  # proporci√≥n de la clase 1
    p0 = 1.0 - p1  # proporci√≥n de la clase 0
    return float(1.0 - (p0**2 + p1**2))  # Gini = 1 - Œ£_k p_k^2


def info_gain_gini(x: np.ndarray, y: np.ndarray, t: float) -> float:  # information gain usando Gini como impurity
    x = np.asarray(x, dtype=float)  # normaliza feature 1D
    y = np.asarray(y, dtype=int)  # normaliza etiquetas
    parent = gini(y)  # impurity del nodo padre
    left = y[x <= t]  # etiquetas que caen a la izquierda del umbral
    right = y[x > t]  # etiquetas que caen a la derecha del umbral
    w_left = left.size / y.size  # peso (fracci√≥n) del hijo izquierdo
    w_right = right.size / y.size  # peso (fracci√≥n) del hijo derecho
    child = w_left * gini(left) + w_right * gini(right)  # impurity ponderada de hijos
    return float(parent - child)  # gain = impurity_padre - impurity_hijos


x = np.array([0.1, 0.2, 0.25, 0.8, 0.85, 0.9])  # feature 1D de juguete
y = np.array([0, 0, 0, 1, 1, 1])  # etiquetas binarias asociadas

candidates = [0.2, 0.25, 0.8]  # umbrales candidatos a evaluar
gains = [info_gain_gini(x, y, t) for t in candidates]  # ganancia por cada umbral
best_t = candidates[int(np.argmax(gains))]  # selecciona umbral con mayor gain

assert best_t in [0.25, 0.8]  # valida que el mejor umbral separa las clases
assert max(gains) > 0.0  # valida que hay ganancia positiva
```

---

### (Bonus) Ejercicio 5.11: Shadow Mode - comparar contra soluci√≥n cerrada en mini-dataset

#### Enunciado

- Entrena regresi√≥n lineal por GD y compara predicci√≥n con soluci√≥n cerrada en un conjunto peque√±o.

#### Soluci√≥n

```python
import numpy as np  # arrays y operaciones num√©ricas

np.random.seed(5)  # fija semilla para reproducibilidad
n, d = 30, 2  # n√∫mero de muestras y dimensi√≥n
X = np.random.randn(n, d)  # features sint√©ticas
w_true = np.array([1.2, -0.4])  # pesos "verdaderos" para generar y
y = X @ w_true + 0.01 * np.random.randn(n)  # targets con ruido peque√±o

w_ne = np.linalg.solve(X.T @ X, X.T @ y)  # soluci√≥n cerrada (normal equation)

w = np.zeros(d)  # inicializa pesos para GD
alpha = 0.1  # learning rate
for _ in range(2000):  # iteraciones de GD
    grad = (X.T @ (X @ w - y)) / n  # gradiente MSE: (1/n) X·µÄ(Xw - y)
    w = w - alpha * grad  # actualizaci√≥n: w <- w - alpha * grad

y_ne = X @ w_ne  # predicci√≥n usando soluci√≥n cerrada
y_gd = X @ w  # predicci√≥n usando pesos de GD

assert np.mean((y_ne - y_gd) ** 2) < 1e-4  # verifica que GD aproxima la soluci√≥n cerrada
```


## üì¶ Entregable del M√≥dulo

- `supervised_learning.py` (regresi√≥n lineal + log√≠stica + m√©tricas + validaci√≥n).
- `scripts/decision_tree_from_scratch.py` (√°rbol de decisi√≥n simple desde cero, sin gradientes).

### `supervised_learning.py`

```python
"""
Supervised Learning Module

Implementaci√≥n desde cero de:
- Linear Regression (con Normal Equation y Gradient Descent)
- Logistic Regression (con regularizaci√≥n L1/L2)
- M√©tricas de evaluaci√≥n
- Cross Validation

Autor: [Tu nombre]
M√≥dulo: 05 - Supervised Learning
"""

import numpy as np  # Importa NumPy: base para √°lgebra lineal, vectorizaci√≥n, RNG y operaciones usadas en regresi√≥n y m√©tricas
from typing import Tuple, List, Optional  # Importa tipos: anota firmas para claridad/herramientas (no afecta el runtime)
from dataclasses import dataclass  # Importa dataclass: permite definir contenedores de datos simples si se usan en el m√≥dulo


# ============================================================
# FUNCIONES AUXILIARES
# ============================================================

def sigmoid(z: np.ndarray) -> np.ndarray:  # Sigmoide: transforma logits reales a probabilidades (0,1), usada en regresi√≥n log√≠stica
    z = np.clip(z, -500, 500)  # Estabilidad num√©rica: evita overflow en exp() con valores muy grandes en magnitud
    return 1 / (1 + np.exp(-z))  # œÉ(z)=1/(1+e^{-z}): c√°lculo vectorizado para arrays

def add_bias(X: np.ndarray) -> np.ndarray:  # A√±ade bias: inserta columna de 1s para modelar intercepto como theta[0]
    return np.column_stack([np.ones(len(X)), X])  # Construye X_b (n,d+1): concatena 1s con features para usar X_b @ theta


# ============================================================
# REGRESI√ìN LINEAL
# ============================================================

class LinearRegression:  # Regresi√≥n lineal: aprende par√°metros Œ∏ para predecir y‚âàXŒ∏ (incluye intercepto via bias)
    def __init__(self):  # Inicializa el modelo: prepara par√°metros y un historial opcional de coste
        self.theta = None  # Par√°metros del modelo (d+1,): se aprenden en fit y se usan en predict
        self.cost_history = []  # Historial de MSE por iteraci√≥n: √∫til si se entrena con descenso de gradiente

    def fit(self, X: np.ndarray, y: np.ndarray,  # Ajusta Œ∏: elige soluci√≥n cerrada (normal) o iterativa (GD)
            method: str = 'normal', lr: float = 0.01, n_iter: int = 1000):  # Hiperpar√°metros: m√©todo, learning rate y n√∫mero de iteraciones
        X_b = add_bias(X)  # A√±ade columna de 1s: permite aprender el intercepto en theta[0]

        if method == 'normal':  # Ecuaci√≥n normal: obtiene Œ∏ √≥ptimo para MSE sin iterar (m√≠nimos cuadrados)
            self.theta = np.linalg.solve(X_b.T @ X_b, X_b.T @ y)  # Resuelve (X·µÄX)Œ∏=X·µÄy (m√°s estable que invertir X·µÄX)
        else:  # Descenso de gradiente batch: optimiza MSE iterativamente
            m, n = X_b.shape  # m=n muestras (normaliza gradiente), n=d+1 par√°metros (incluye bias)
            self.theta = np.zeros(n)  # Inicializa Œ∏: vector de ceros como punto de partida de GD
            for _ in range(n_iter):  # Itera n_iter pasos de GD: cada paso reduce (idealmente) el coste
                grad = (1/m) * X_b.T @ (X_b @ self.theta - y)  # Gradiente MSE: (1/m) X·µÄ(XŒ∏‚àíy)
                self.theta -= lr * grad  # Actualizaci√≥n: Œ∏ ‚Üê Œ∏ ‚àí Œ±¬∑grad
                self.cost_history.append(np.mean((X_b @ self.theta - y)**2))  # Guarda MSE actual: permite trazar convergencia
        return self  # Devuelve self: permite chaining estilo sklearn (model.fit(...).predict(...))

    def predict(self, X: np.ndarray) -> np.ndarray:  # Predice valores continuos: y_hat = X_b Œ∏
        return add_bias(X) @ self.theta  # Multiplicaci√≥n matriz-vector: produce predicciones (n,)

    def score(self, X: np.ndarray, y: np.ndarray) -> float:  # R¬≤: mide varianza explicada por el modelo (1 es perfecto)
        y_pred = self.predict(X)  # Predicci√≥n sobre X: base para medir error vs y
        ss_res = np.sum((y - y_pred)**2)  # SSE/SS_res: suma de cuadrados residual (error no explicado)
        ss_tot = np.sum((y - np.mean(y))**2)  # SST/SS_tot: suma de cuadrados total (variaci√≥n respecto a la media)
        return 1 - ss_res / ss_tot  # R¬≤ = 1 - SSE/SST: compara el modelo contra baseline de predecir la media


# ============================================================
# REGRESI√ìN LOG√çSTICA
# ============================================================

class LogisticRegression:  # Regresi√≥n log√≠stica binaria: modela P(y=1|x)=œÉ(X_b Œ∏) y aprende Œ∏ por descenso de gradiente
    def __init__(self, reg: str = None, lambda_: float = 0.01):  # Inicializa configuraci√≥n de regularizaci√≥n (opcional)
        self.reg = reg  # Tipo de regularizaci√≥n: None, 'l1' o 'l2' (define penalizaci√≥n aplicada a los pesos)
        self.lambda_ = lambda_  # Fuerza de regularizaci√≥n Œª: escala el t√©rmino de penalizaci√≥n en el gradiente
        self.theta = None  # Par√°metros del modelo (d+1,): incluye bias; se inicializan en fit
        self.cost_history = []  # Historial de coste (si se registra): √∫til para debug/convergencia (aqu√≠ no se llena)

    def fit(self, X: np.ndarray, y: np.ndarray,  # Entrena el clasificador: optimiza cross-entropy impl√≠cita v√≠a GD
            lr: float = 0.1, n_iter: int = 1000):  # Hiperpar√°metros: learning rate y n√∫mero de iteraciones
        X_b = add_bias(X)  # A√±ade bias: permite intercepto (theta[0]) en el modelo lineal antes de la sigmoide
        m, n = X_b.shape  # Extrae shapes: m=n muestras (normaliza gradiente), n=d+1 par√°metros (incluye bias)
        self.theta = np.zeros(n)  # Inicializa par√°metros: vector de ceros como punto de partida del descenso de gradiente

        for _ in range(n_iter):  # Itera GD batch: actualiza Œ∏ para reducir el error de clasificaci√≥n
            h = sigmoid(X_b @ self.theta)  # Probabilidades predichas: aplica œÉ a los logits X_bŒ∏
            grad = (1/m) * X_b.T @ (h - y)  # Gradiente base: deriva de log-loss para logistic regression sin regularizaci√≥n

            if self.reg == 'l2':  # L2 (ridge): penaliza magnitudes cuadr√°ticas, favorece pesos peque√±os (shrinkage)
                grad[1:] += (self.lambda_/m) * self.theta[1:]  # No regulariza bias (√≠ndice 0): solo aplica a pesos de features
            elif self.reg == 'l1':  # L1 (lasso): penaliza norma L1, induce sparsity (pesos exactamente 0)
                grad[1:] += (self.lambda_/m) * np.sign(self.theta[1:])  # Subgradiente L1: signo por componente (maneja no diferenciabilidad en 0)

            self.theta -= lr * grad  # Actualizaci√≥n de GD: Œ∏ ‚Üê Œ∏ ‚àí Œ±¬∑grad
        return self  # Devuelve self: estilo sklearn para encadenar llamadas

    def predict_proba(self, X: np.ndarray) -> np.ndarray:  # Devuelve probabilidad de clase positiva (y=1) para cada muestra
        return sigmoid(add_bias(X) @ self.theta)  # Calcula œÉ(X_bŒ∏): salida en (0,1) interpretada como probabilidad

    def predict(self, X: np.ndarray) -> np.ndarray:  # Devuelve etiqueta binaria {0,1} aplicando un umbral a predict_proba
        return (self.predict_proba(X) >= 0.5).astype(int)  # Umbral 0.5: clasifica como 1 si prob>=0.5; castea booleanos a int

    def score(self, X: np.ndarray, y: np.ndarray) -> float:  # Accuracy: proporci√≥n de aciertos (m√©trica simple de clasificaci√≥n)
        return np.mean(self.predict(X) == y)  # Promedio de comparaciones: True=1, False=0 => accuracy en [0,1]


# ============================================================
# M√âTRICAS
# ============================================================

def accuracy(y_true, y_pred):  # Accuracy: proporci√≥n de aciertos (clasificaci√≥n) = mean(y_true == y_pred)
    return np.mean(y_true == y_pred)  # Calcula promedio de booleanos: True=1, False=0 => valor en [0,1]

def precision(y_true, y_pred):  # Precision: TP/(TP+FP), qu√© tan confiables son los positivos predichos
    tp = np.sum((y_true == 1) & (y_pred == 1))  # True positives: positivos reales predichos como positivos
    fp = np.sum((y_true == 0) & (y_pred == 1))  # False positives: negativos reales predichos como positivos
    return tp / (tp + fp) if (tp + fp) > 0 else 0  # Evita divisi√≥n por cero: si no hay predicciones positivas, precision=0

def recall(y_true, y_pred):  # Recall: TP/(TP+FN), cobertura de positivos reales detectados
    tp = np.sum((y_true == 1) & (y_pred == 1))  # True positives: positivos reales capturados por el modelo
    fn = np.sum((y_true == 1) & (y_pred == 0))  # False negatives: positivos reales que el modelo se perdi√≥
    return tp / (tp + fn) if (tp + fn) > 0 else 0  # Evita divisi√≥n por cero: si no hay positivos reales, recall=0

def f1_score(y_true, y_pred):  # F1: media arm√≥nica de precision y recall, balancea ambos cuando hay desbalance de clases
    p, r = precision(y_true, y_pred), recall(y_true, y_pred)  # Calcula precision y recall: ingredientes del F1
    return 2*p*r/(p+r) if (p+r) > 0 else 0  # Evita divisi√≥n por cero: si p+r=0, no hay se√±al => F1=0

def confusion_matrix(y_true, y_pred):  # Matriz 2x2: filas=clase real, columnas=clase predicha (√∫til para analizar errores)
    cm = np.zeros((2, 2), dtype=int)  # Inicializa conteos: [[TN, FP],[FN, TP]] con dtype int
    cm[0, 0] = np.sum((y_true == 0) & (y_pred == 0))  # TN: negativos reales predichos como negativos
    cm[0, 1] = np.sum((y_true == 0) & (y_pred == 1))  # FP: negativos reales predichos como positivos
    cm[1, 0] = np.sum((y_true == 1) & (y_pred == 0))  # FN: positivos reales predichos como negativos
    cm[1, 1] = np.sum((y_true == 1) & (y_pred == 1))  # TP: positivos reales predichos como positivos
    return cm  # Devuelve matriz: permite derivar precision/recall y ver tradeoffs por tipo de error


# ============================================================
# VALIDACI√ìN
# ============================================================

def train_test_split(X, y, test_size=0.2, seed=None):  # Divide en train/test: baraja √≠ndices y separa seg√∫n proporci√≥n `test_size`
    if seed: np.random.seed(seed)  # Si hay semilla, fija RNG global: hace reproducible la partici√≥n
    n = len(y)  # N√∫mero de muestras: define el rango de √≠ndices a permutar
    idx = np.random.permutation(n)  # Permutaci√≥n aleatoria de √≠ndices: mezcla el dataset antes de partir
    split = int(n * test_size)  # Tama√±o del test: cantidad de √≠ndices que ir√°n al conjunto de evaluaci√≥n
    return X[idx[split:]], X[idx[:split]], y[idx[split:]], y[idx[:split]]  # Retorna X_train, X_test, y_train, y_test (seg√∫n √≠ndices permutados)

def cross_validate(model_class, X, y, k=5, **params):  # K-fold CV: eval√∫a el modelo entrenando k veces y promediando el score
    n = len(y)  # N√∫mero de muestras: base para construir folds
    idx = np.random.permutation(n)  # Baraja √≠ndices: evita sesgo por orden de los datos
    fold_size = n // k  # Tama√±o del fold: divisi√≥n entera (si sobra, algunos ejemplos pueden quedar fuera)
    scores = []  # Scores por fold: se agregan para calcular media y desviaci√≥n

    for i in range(k):  # Itera folds: en cada i, un segmento es validaci√≥n y el resto es entrenamiento
        val_idx = idx[i*fold_size:(i+1)*fold_size]  # √çndices de validaci√≥n: slice del fold i
        train_idx = np.concatenate([idx[:i*fold_size], idx[(i+1)*fold_size:]])  # √çndices de train: concatena todos los dem√°s folds

        model = model_class()  # Instancia modelo nuevo por fold: evita reutilizar estado/Œ∏ entre entrenamientos
        model.fit(X[train_idx], y[train_idx], **params)  # Entrena en el subset de train: pasa hiperpar√°metros por **params
        scores.append(model.score(X[val_idx], y[val_idx]))  # Eval√∫a en validaci√≥n: guarda score para estad√≠stica final

    return {'scores': scores, 'mean': np.mean(scores), 'std': np.std(scores)}  # Resume CV: lista de scores + media + desviaci√≥n est√°ndar


# ============================================================
# TESTS
# ============================================================

if __name__ == "__main__":  # Entry point: ejecuta pruebas r√°pidas cuando se corre como script
    np.random.seed(42)  # Fija semilla global: hace reproducibles los datos sint√©ticos

    # Test Linear Regression
    X = 2 * np.random.rand(100, 1)  # Features: 100 valores uniformes en [0,2) con 1 feature (shape (100,1))
    y = 4 + 3 * X.flatten() + np.random.randn(100) * 0.5  # Target: relaci√≥n lineal y=4+3x + ruido gaussiano (simula datos reales)

    lr = LinearRegression()  # Instancia modelo de regresi√≥n lineal
    lr.fit(X, y)  # Ajusta Œ∏ con el m√©todo por defecto (ecuaci√≥n normal)
    print(f"Linear Regression R¬≤: {lr.score(X, y):.4f}")  # Imprime R¬≤: sanity check de que el ajuste explica bien la varianza

    # Test Logistic Regression
    X_c0 = np.random.randn(50, 2) + [-2, -2]  # Clase 0: 50 puntos alrededor de (-2,-2) (distribuci√≥n normal)
    X_c1 = np.random.randn(50, 2) + [2, 2]  # Clase 1: 50 puntos alrededor de (2,2)
    X_clf = np.vstack([X_c0, X_c1])  # Junta features: (100,2) concatenando ambas nubes
    y_clf = np.array([0]*50 + [1]*50)  # Labels: 0 para clase 0 y 1 para clase 1 (shape (100,))

    log_reg = LogisticRegression()  # Instancia regresi√≥n log√≠stica (sin regularizaci√≥n por defecto)
    log_reg.fit(X_clf, y_clf)  # Entrena Œ∏ por GD sobre el dataset sint√©tico
    print(f"Logistic Regression Accuracy: {log_reg.score(X_clf, y_clf):.4f}")  # Imprime accuracy: verifica que clasifica razonablemente

    # Test metrics
    y_true = np.array([0,0,0,1,1,1,1,1])  # Labels reales: ejemplo peque√±o para comprobar m√©tricas manualmente
    y_pred = np.array([0,0,1,1,1,0,1,1])  # Predicciones: incluye FP y FN para que precision/recall no sean triviales
    print(f"Precision: {precision(y_true, y_pred):.4f}")  # Calcula e imprime precision del ejemplo
    print(f"Recall: {recall(y_true, y_pred):.4f}")  # Calcula e imprime recall del ejemplo
    print(f"F1: {f1_score(y_true, y_pred):.4f}")  # Calcula e imprime F1 del ejemplo

    # Test CV
    cv = cross_validate(LogisticRegression, X_clf, y_clf, k=5, lr=0.1, n_iter=500)  # Ejecuta CV 5-fold pasando hiperpar√°metros al fit
    print(f"CV Score: {cv['mean']:.4f} ¬± {cv['std']:.4f}")  # Reporta media¬±std: estima estabilidad del rendimiento

    print("\n‚úì Todos los tests pasaron!")  # Mensaje final: indica que se ejecutaron pruebas sin errores
```

---

## üìù Derivaci√≥n Anal√≠tica: El Entregable de L√°piz y Papel (v3.3)

> üéì **Simulaci√≥n de Examen:** En la maestr√≠a te pedir√°n: *"Derive la regla de actualizaci√≥n de pesos para Logistic Regression"*. Debes poder hacerlo a mano.

### Derivaci√≥n del Gradiente de Logistic Regression

**Objetivo:** Derivar `‚àÇL/‚àÇw` para la funci√≥n de costo Cross-Entropy.

#### Paso 1: Definir la Funci√≥n de Costo

```
L(w) = -(1/n) Œ£_{i=1..n} [ y_i log(≈∑_i) + (1 - y_i) log(1 - ≈∑_i) ]
```

Donde:
- `≈∑_i = œÉ(w·µÄ x_i) = 1 / (1 + e^{-w·µÄ x_i})`
- `œÉ(z)` es la funci√≥n sigmoid

#### Paso 2: Derivar la Sigmoid

```
dœÉ/dz = œÉ(z)(1 - œÉ(z))
```

**Demostraci√≥n:**
```
œÉ(z) = 1 / (1 + e^{-z})

dœÉ/dz = e^{-z} / (1 + e^{-z})^2
      = (1 / (1 + e^{-z})) ¬∑ (e^{-z} / (1 + e^{-z}))
      = œÉ(z)(1 - œÉ(z))
```

#### Paso 3: Aplicar la Regla de la Cadena

Para un solo ejemplo `(x_i, y_i)`:

```
‚àÇL_i/‚àÇw = (‚àÇL_i/‚àÇ≈∑_i) ¬∑ (‚àÇ≈∑_i/‚àÇz_i) ¬∑ (‚àÇz_i/‚àÇw)
```

Donde `z_i = w·µÄ x_i`

**Calcular cada t√©rmino:**

1. `‚àÇL_i/‚àÇ≈∑_i = -y_i/≈∑_i + (1 - y_i)/(1 - ≈∑_i)`

2. `‚àÇ≈∑_i/‚àÇz_i = ≈∑_i(1 - ≈∑_i)`

3. `‚àÇz_i/‚àÇw = x_i`

#### Paso 4: Simplificar

```
‚àÇL_i/‚àÇw = ( -y_i/≈∑_i + (1 - y_i)/(1 - ≈∑_i) ) ¬∑ ≈∑_i(1 - ≈∑_i) ¬∑ x_i
```

Simplificando el t√©rmino entre par√©ntesis:
```
= ( (-y_i(1 - ≈∑_i) + (1 - y_i)≈∑_i) / (≈∑_i(1 - ≈∑_i)) ) ¬∑ ≈∑_i(1 - ≈∑_i) ¬∑ x_i
= (-y_i + y_i≈∑_i + ≈∑_i - y_i≈∑_i) ¬∑ x_i
= (≈∑_i - y_i) ¬∑ x_i
```

#### Resultado Final

```
‚àÇL/‚àÇw = (1/n) Œ£_{i=1..n} (≈∑_i - y_i) x_i
      = (1/n) X·µÄ (≈∑ - y)
```

**Forma vectorizada (para c√≥digo):**
```python
gradient = (1/n) * X.T @ (y_pred - y_true)  # gradiente vectorizado: (1/n) X·µÄ(≈∑ - y)
```

### Tu Entregable

Escribe en un documento (Markdown o LaTeX):
1. La derivaci√≥n completa del gradiente de Cross-Entropy
2. La derivaci√≥n de la regla de actualizaci√≥n: `w <- w - Œ± ‚àáL`
3. Por qu√© el gradiente tiene la forma `(≈∑ - y)` (interpretaci√≥n geom√©trica)

---

## üéØ El Reto del Tablero Blanco (Metodolog√≠a Feynman)

Explica en **m√°ximo 5 l√≠neas** sin jerga t√©cnica:

1. **¬øPor qu√© usamos sigmoid en clasificaci√≥n?**
   > Pista: Piensa en probabilidades entre 0 y 1.

2. **¬øPor qu√© Cross-Entropy y no MSE para clasificaci√≥n?**
   > Pista: Piensa en qu√© pasa cuando `≈∑ ‚âà 0` pero `y = 1`.

3. **¬øQu√© significa "One-vs-All"?**
   > Pista: Piensa en c√≥mo clasificar 10 d√≠gitos con clasificadores binarios.

---

## üîç Shadow Mode: Validaci√≥n con sklearn (v3.3)

> ‚ö†Ô∏è **Regla:** sklearn est√° **prohibido para aprender**, pero es **necesario para validar**. Si tu implementaci√≥n difiere significativamente de sklearn, tienes un bug.

### Protocolo de Validaci√≥n (Viernes de Fase 2)

```python
"""
Shadow Mode - Validaci√≥n de Implementaciones
Compara tu c√≥digo desde cero vs sklearn para detectar bugs.

Regla: Si la diferencia de accuracy es >5%, revisar matem√°ticas.
"""
import numpy as np  # Importa NumPy: se usa para zeros, norma L2, abs y operaciones num√©ricas en la comparaci√≥n
from sklearn.linear_model import LogisticRegression as SklearnLR  # Importa LogisticRegression de sklearn como referencia (‚Äúground truth‚Äù)
from sklearn.linear_model import LinearRegression as SklearnLinReg  # Importa LinearRegression de sklearn para validar MSE contra tu implementaci√≥n
from sklearn.metrics import accuracy_score, mean_squared_error  # Importa m√©tricas: accuracy y MSE para comparar resultados

# Importar tu implementaci√≥n
# from src.logistic_regression import LogisticRegression as MyLR
# from src.linear_regression import LinearRegression as MyLinReg


def shadow_mode_logistic_regression(X_train, y_train, X_test, y_test):  # Valida tu Logistic Regression contra sklearn y detecta desviaciones
    """
    Compara tu Logistic Regression vs sklearn.

    Los coeficientes y accuracy deben ser casi id√©nticos.
    """
    print("=" * 60)  # Separador visual: hace m√°s legible la salida en consola
    print("SHADOW MODE: Logistic Regression")  # T√≠tulo del bloque: indica qu√© algoritmo se est√° validando
    print("=" * 60)  # Separador visual: cierra el encabezado

    # ========== TU IMPLEMENTACI√ìN ==========
    # my_model = MyLR()
    # my_model.fit(X_train, y_train, lr=0.1, n_iter=1000)
    # my_pred = my_model.predict(X_test)
    # my_acc = accuracy_score(y_test, my_pred)
    # my_weights = my_model.weights

    # Placeholder (reemplazar con tu c√≥digo)
    my_acc = 0.85  # Placeholder: accuracy dummy (reemplazar por accuracy real de tu modelo)
    my_weights = np.zeros(X_train.shape[1])  # Placeholder: vector de pesos dummy con dimensi√≥n d (reemplazar por pesos reales)

    # ========== SKLEARN (GROUND TRUTH) ==========
    sklearn_model = SklearnLR(max_iter=1000, solver='lbfgs')  # Crea modelo sklearn: baseline estable para comparar (mismo tipo de modelo)
    sklearn_model.fit(X_train, y_train)  # Entrena sklearn en el mismo train: produce coeficientes y predicciones de referencia
    sklearn_pred = sklearn_model.predict(X_test)  # Predice en test: etiquetas binarias para medir accuracy
    sklearn_acc = accuracy_score(y_test, sklearn_pred)  # Accuracy de referencia: se compara contra tu accuracy
    sklearn_weights = sklearn_model.coef_.flatten()  # Extrae pesos: coef_ tiene shape (1,d) en binario; flatten -> (d,)

    # ========== COMPARACI√ìN ==========
    acc_diff = abs(my_acc - sklearn_acc)  # Diferencia absoluta de accuracy: m√©trica simple para el veredicto
    weight_diff = np.linalg.norm(my_weights - sklearn_weights[:len(my_weights)])  # Diferencia L2 de pesos: norma de la resta (cuanto menor, m√°s parecido)

    print(f"\nüìä RESULTADOS:")  # Encabezado de resultados num√©ricos
    print(f"  Tu Accuracy:     {my_acc:.4f}")  # Muestra tu accuracy (deber√≠a venir de tu implementaci√≥n real)
    print(f"  sklearn Accuracy: {sklearn_acc:.4f}")  # Muestra accuracy sklearn (baseline)
    print(f"  Diferencia:       {acc_diff:.4f}")  # Reporta diferencia: si es grande, hay sospecha de bug

    print(f"\nüìê PESOS:")  # Encabezado de comparaci√≥n de par√°metros
    print(f"  Diferencia L2 de pesos: {weight_diff:.4f}")  # Norma L2: cuantifica cu√°nto difieren los coeficientes

    # Veredicto
    print("\n" + "-" * 60)  # Separador antes del veredicto: delimita la secci√≥n final
    if acc_diff < 0.05:  # Criterio: diferencia <5% se considera aceptable (seg√∫n la regla indicada)
        print("‚úì PASSED: Tu implementaci√≥n es correcta")  # Mensaje OK: tu modelo se alinea razonablemente con sklearn
        return True  # Devuelve True: √∫til para automatizar checks en pipelines/QA
    else:  # Caso problem√°tico: la diferencia excede el umbral y se sugiere revisar
        print("‚úó FAILED: Diferencia significativa - revisa tu matem√°tica")  # Mensaje FAIL: indica que hay discrepancia relevante
        print("  Posibles causas:")  # Lista de hip√≥tesis t√≠picas que causan divergencia
        print("  - Gradiente mal calculado")  # Error com√∫n: derivada/gradiente incorrecto en la funci√≥n de coste
        print("  - Learning rate muy alto/bajo")  # Hiperpar√°metro: puede causar no convergencia o convergencia lenta
        print("  - Falta de normalizaci√≥n de datos")  # Preprocesamiento: escalas distintas pueden dificultar el entrenamiento
        return False  # Devuelve False: permite fallar la validaci√≥n autom√°ticamente


def shadow_mode_linear_regression(X_train, y_train, X_test, y_test):  # Valida tu Linear Regression contra sklearn comparando MSE
    """
    Compara tu Linear Regression vs sklearn.
    """
    print("=" * 60)  # Separador visual: salida m√°s legible
    print("SHADOW MODE: Linear Regression")  # T√≠tulo del bloque: indica validaci√≥n de regresi√≥n lineal
    print("=" * 60)  # Separador visual

    # ========== TU IMPLEMENTACI√ìN ==========
    # my_model = MyLinReg()
    # my_model.fit(X_train, y_train)
    # my_pred = my_model.predict(X_test)
    # my_mse = mean_squared_error(y_test, my_pred)

    # Placeholder
    my_mse = 0.5  # Placeholder: MSE dummy (reemplazar por MSE real de tu implementaci√≥n)

    # ========== SKLEARN ==========
    sklearn_model = SklearnLinReg()  # Crea baseline sklearn: regresi√≥n lineal por m√≠nimos cuadrados
    sklearn_model.fit(X_train, y_train)  # Ajusta baseline: obtiene coeficientes √≥ptimos en closed-form
    sklearn_pred = sklearn_model.predict(X_test)  # Predice en test: valores continuos
    sklearn_mse = mean_squared_error(y_test, sklearn_pred)  # MSE sklearn: referencia para comparar tu MSE

    # ========== COMPARACI√ìN ==========
    mse_ratio = my_mse / sklearn_mse if sklearn_mse > 0 else float('inf')  # Ratio de errores: >1 implica peor que sklearn; evita dividir por 0

    print(f"\nüìä RESULTADOS:")  # Encabezado de resultados
    print(f"  Tu MSE:     {my_mse:.4f}")  # Muestra MSE de tu implementaci√≥n (placeholder en este snippet)
    print(f"  sklearn MSE: {sklearn_mse:.4f}")  # Muestra MSE del baseline
    print(f"  Ratio:       {mse_ratio:.2f}x")  # Reporta cu√°ntas veces es tu error respecto al baseline

    print("\n" + "-" * 60)  # Separador antes del veredicto
    if mse_ratio < 1.1:  # Dentro del 10%: tolerancia razonable por diferencias num√©ricas/implementaci√≥n
        print("‚úì PASSED: Tu implementaci√≥n es correcta")  # OK: tu MSE est√° alineado con el baseline
        return True  # Devuelve True: validaci√≥n aprobada
    else:  # Caso fallo: tu MSE es mucho peor que sklearn
        print("‚úó FAILED: Tu MSE es significativamente mayor")  # FAIL: sugiere bug o mala convergencia
        return False  # Devuelve False: validaci√≥n fallida


# ============================================================
# EJEMPLO DE USO
# ============================================================

if __name__ == "__main__":  # entrypoint: ejecuta la demo solo si se corre como script
    # Ejecuta la demo de validaci√≥n: compara tu implementaci√≥n con sklearn
    from sklearn.datasets import make_classification, make_regression  # Importa generadores: crean datasets sint√©ticos para demo/pruebas
    from sklearn.model_selection import train_test_split  # Importa split sklearn: genera particiones consistentes para la demo

    # Dataset de clasificaci√≥n
    X_clf, y_clf = make_classification(  # Crea dataset binario: features y labels para probar logistic regression
        n_samples=1000, n_features=10, n_classes=2, random_state=42  # Par√°metros: tama√±o, dimensionalidad, n¬∫ clases y semilla
    )  # Devuelve X (n,d) e y (n,) para clasificaci√≥n
    X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(  # Divide dataset de clasificaci√≥n: train/test para validar sin leakage
        X_clf, y_clf, test_size=0.2, random_state=42  # 20% test: semilla para reproducibilidad
    )  # Retorna splits: X_train/X_test/y_train/y_test

    # Dataset de regresi√≥n
    X_reg, y_reg = make_regression(  # Crea dataset de regresi√≥n: targets continuos con ruido
        n_samples=1000, n_features=10, noise=10, random_state=42  # Par√°metros: tama√±o, features, nivel de ruido y semilla
    )  # Devuelve X (n,d) e y (n,) para regresi√≥n
    X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(  # Divide dataset de regresi√≥n: entrenamiento y evaluaci√≥n
        X_reg, y_reg, test_size=0.2, random_state=42  # 20% test y semilla fija para reproducibilidad
    )  # Retorna splits para regresi√≥n

    # Ejecutar Shadow Mode
    shadow_mode_logistic_regression(X_train_c, y_train_c, X_test_c, y_test_c)  # Ejecuta comparaci√≥n logistic: devuelve True/False seg√∫n el umbral
    print("\n")  # L√≠nea en blanco: separa salidas entre validaci√≥n de clasificaci√≥n y regresi√≥n
    shadow_mode_linear_regression(X_train_r, y_train_r, X_test_r, y_test_r)  # Ejecuta comparaci√≥n lineal: chequea ratio de MSE vs sklearn
```

### Checklist Shadow Mode

| D√≠a | Algoritmo | Validar |
|-----|-----------|---------|
| Viernes Sem 10 | Linear Regression | MSE ‚âà sklearn |
| Viernes Sem 11 | Logistic Regression | Accuracy ‚âà sklearn |
| Viernes Sem 12 | M√©tricas | Precision/Recall = sklearn |

---

## ‚úÖ Checklist de Finalizaci√≥n (v3.3)

### Conocimiento
- [ ] Implement√© regresi√≥n lineal con Normal Equation y GD
- [ ] Entiendo MSE y su gradiente
- [ ] Implement√© regresi√≥n log√≠stica desde cero
- [ ] Entiendo sigmoid y binary cross-entropy
- [ ] Puedo calcular TP, TN, FP, FN de una matriz de confusi√≥n
- [ ] Implement√© accuracy, precision, recall, F1
- [ ] Implement√© train/test split
- [ ] Implement√© K-fold cross validation
- [ ] Entiendo regularizaci√≥n L1 vs L2

### Shadow Mode (v3.3 - Obligatorio)
- [ ] **Linear Regression**: Mi MSE ‚âà sklearn (ratio < 1.1)
- [ ] **Logistic Regression**: Mi Accuracy ‚âà sklearn (diff < 5%)

### Entregables de C√≥digo
- [ ] `logistic_regression.py` con tests pasando
- [ ] `artifacts/m05_logreg_weights.png` + 5‚Äì10 l√≠neas de interpretaci√≥n (pesos 28x28)
- [ ] `mypy src/` pasa sin errores
- [ ] `pytest tests/` pasa sin errores

### Derivaci√≥n Anal√≠tica (Obligatorio)
- [ ] Deriv√© el gradiente de Cross-Entropy a mano
- [ ] Documento con derivaci√≥n completa (Markdown o LaTeX)
- [ ] Puedo explicar por qu√© `‚àáL = X·µÄ(≈∑ - y)`

### Metodolog√≠a Feynman
- [ ] Puedo explicar sigmoid en 5 l√≠neas sin jerga
- [ ] Puedo explicar Cross-Entropy vs MSE en 5 l√≠neas
- [ ] Puedo explicar One-vs-All en 5 l√≠neas

---

## üîó Navegaci√≥n

| Anterior | √çndice | Siguiente |
|----------|--------|-----------|
| [04_PROBABILIDAD_ML](04_PROBABILIDAD_ML.md) | [00_INDICE](00_INDICE.md) | [06_UNSUPERVISED_LEARNING](06_UNSUPERVISED_LEARNING.md) |
