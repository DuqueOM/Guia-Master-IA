# üìÖ Plan de Estudios - ML SPECIALIST v3.1

> **24 Semanas | 6 horas/d√≠a | Lunes a S√°bado**  
> **Preparaci√≥n para MS in AI Pathway - L√≠nea 1: Machine Learning**

---

## üóìÔ∏è Vista General: 24 Semanas

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ SEMANAS 1-8        ‚îÇ SEMANAS 9-20       ‚îÇ SEMANAS 21-24                     ‚îÇ
‚îÇ FUNDAMENTOS        ‚îÇ ML CORE ‚≠ê         ‚îÇ PROYECTO MNIST                    ‚îÇ
‚îÇ Python + Mate      ‚îÇ PATHWAY L√çNEA 1    ‚îÇ INTEGRACI√ìN                       ‚îÇ
‚îÇ + Probabilidad     ‚îÇ Supervised +       ‚îÇ Pipeline End-to-End               ‚îÇ
‚îÇ M√≥dulos 01-04      ‚îÇ Unsupervised + DL  ‚îÇ M√≥dulo 08                         ‚îÇ
‚îÇ                    ‚îÇ M√≥dulos 05-07      ‚îÇ 4 semanas intensivas              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Dedicaci√≥n total:** 36 horas/semana √ó 24 semanas = **~864 horas**

### Los 8 M√≥dulos Obligatorios

| Semanas | M√≥dulo | Tema | Curso del Pathway |
|---------|--------|------|-------------------|
| 1-2 | 01 | Python + Pandas + NumPy | - (Fundamento) |
| 3-5 | 02 | √Ålgebra Lineal para ML | - (Fundamento) |
| 6-7 | 03 | C√°lculo Multivariante | - (Fundamento) |
| 8 | 04 | Probabilidad para ML | - (Fundamento) |
| 9-12 | 05 | Supervised Learning | Introduction to ML: Supervised Learning |
| 13-16 | 06 | Unsupervised Learning | Unsupervised Algorithms in ML |
| 17-20 | 07 | Deep Learning + CNNs | Introduction to Deep Learning |
| 21-24 | 08 | Proyecto MNIST Analyst | Integraci√≥n de las 3 materias |

---

> **Filosof√≠a v3.1:** "Matem√°ticas Aplicadas a C√≥digo". Pandas para datos, NumPy para matem√°ticas, probabilidad para loss functions.

---

## üìå Distribuci√≥n Diaria T√≠pica

| Bloque | Horario | Actividad | Duraci√≥n |
|--------|---------|-----------|----------|
| üåÖ Ma√±ana | 08:00 - 10:30 | Teor√≠a matem√°tica + notaci√≥n | 2.5 h |
| ‚òï Pausa | 10:30 - 11:00 | Descanso | 30 min |
| üåá Mediod√≠a | 11:00 - 13:30 | Implementaci√≥n en NumPy | 2.5 h |
| üåô Tarde | 15:00 - 16:00 | Ejercicios + visualizaci√≥n | 1 h |

Para ver el **protocolo diario detallado**, simulacros de examen y ajustes por semana (versi√≥n estrat√©gica v4.0), consulta tambi√©n:

- [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)

---

# üî∑ FASE 1: FUNDAMENTOS MATEM√ÅTICOS (Semanas 1-8)

*Objetivo: Leer notaci√≥n matem√°tica y traducirla a Python/NumPy*

---

## üóìÔ∏è SEMANA 1-2: Python + Pandas + NumPy (M√≥dulo 01)

**Objetivo:** Dominar Pandas para datos reales + NumPy para matem√°ticas  
**Por qu√©:** En el mundo real, los datos vienen en CSVs sucios. Pandas es esencial para cargar y limpiar datos antes de aplicar ML.

### Semana 1: Pandas + NumPy B√°sico

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Pandas: DataFrame y Series | `pd.read_csv()`, `.head()`, `.info()` | Ejercicio 1.1 |
| M | Limpieza de datos | `dropna()`, `fillna()`, dtypes | Ejercicio 1.2 |
| X | Selecci√≥n y filtrado | `.loc[]`, `.iloc[]`, condiciones | Ejercicio 1.3 |
| J | NumPy: Arrays vs listas | Crear arrays, dtypes | Ejercicio 1.4 |
| V | NumPy: Indexing y slicing | Extraer submatrices | Ejercicio 1.5 |
| S | **Checkpoint** | Pandas ‚Üí NumPy: `.to_numpy()` | Pipeline de carga |

### Semana 2: NumPy Vectorizado

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Broadcasting (concepto) | Operaciones elemento a elemento | Ejercicio 1.6 |
| M | np.dot, np.matmul, @ | Producto matricial | Ejercicio 1.7 |
| X | Reshape, flatten, transpose | Manipulaci√≥n de formas | Ejercicio 1.8 |
| J | Agregaciones y ejes | sum, mean, std con axis | Ejercicio 1.9 |
| V | Random en NumPy | Generaci√≥n de datos sint√©ticos | Ejercicio 1.10 |
| S | **Checkpoint** | Pipeline completo CSV ‚Üí NumPy | Entregable |

**Entregable:** Script que carga CSV con Pandas, limpia datos, y convierte a NumPy para an√°lisis.

**Recursos:**
- [Pandas Getting Started](https://pandas.pydata.org/docs/getting_started/)
- [NumPy Quickstart](https://numpy.org/doc/stable/user/quickstart.html)

---

## üóìÔ∏è SEMANA 3-5: √Ålgebra Lineal para ML (M√≥dulo 02)

**Objetivo:** Vectores, matrices, normas, autovectores  
**Conexi√≥n con Pathway:** Vital para Unsupervised Learning (PCA requiere Eigenvalues) y Deep Learning (multiplicaciones de matrices)

### Semana 3: Vectores y Operaciones B√°sicas

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Vectores: definici√≥n geom√©trica | Crear vectores en NumPy | Ejercicio 2.1 |
| M | Suma, resta de vectores | Implementar operaciones | Visualizar con matplotlib |
| X | Producto escalar (scalar mult) | `c * v` en NumPy | Ejercicio 2.2 |
| J | Producto punto (dot product) | F√≥rmula $\vec{a} \cdot \vec{b}$ | np.dot() |
| V | Interpretaci√≥n geom√©trica | Proyecci√≥n, √°ngulo | Diagrama |
| S | **Repaso** | Funciones vectoriales | Test |

### Semana 4: Normas y Distancias

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Norma L2 (Euclidiana) | $\|x\|_2 = \sqrt{\sum x_i^2}$ | np.linalg.norm() |
| M | Norma L1 (Manhattan) | $\|x\|_1 = \sum |x_i|$ | Implementar manual |
| X | Distancia Euclidiana | $d(a,b) = \|a - b\|_2$ | Funci√≥n distancia |
| J | Distancia coseno | $1 - \cos(\theta)$ | Similitud coseno |
| V | Aplicaci√≥n: KNN concepto | Vecino m√°s cercano | Demo simple |
| S | **Repaso** | Librer√≠a de distancias | Test |

### Semana 5: Matrices y Descomposici√≥n

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Matrices: suma, multiplicaci√≥n | np.matmul, @ | Ejercicio 2.3 |
| M | Transpuesta, inversa | A.T, np.linalg.inv() | Ejercicio 2.4 |
| X | Autovalores/Autovectores (intro) | Qu√© son y por qu√© importan | np.linalg.eig() |
| J | SVD (concepto) | Descomposici√≥n de matrices | np.linalg.svd() |
| V | Aplicaci√≥n: PCA preview | Reducci√≥n dimensional | Demo visual |
| S | **Checkpoint** | `linear_algebra.py` completo | Entregable |

**Entregable:** Librer√≠a `linear_algebra.py` que implementa:
- Producto punto, normas L1/L2
- Distancia euclidiana y coseno
- Proyecci√≥n de vectores
- Wrapper para eigenvalues

---

## üóìÔ∏è SEMANA 6-7: C√°lculo Multivariante (M√≥dulo 03) [CR√çTICO]

**Objetivo:** Derivadas, gradiente, Chain Rule  
**Conexi√≥n con Pathway:** Es el lenguaje del Deep Learning. Sin la Regla de la Cadena, no entender√°s Backpropagation.

### Semana 6: Derivadas, Gradiente y GD

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Derivada: tasa de cambio | Calcular derivadas simples | Ejercicio 3.1 |
| M | Derivadas parciales | $\frac{\partial f}{\partial x}$ para $f(x,y)$ | Ejercicio 3.2 |
| X | Gradiente: vector de parciales | $\nabla f$ | Implementar |
| J | Gradient Descent (concepto) | Algoritmo b√°sico | Pseudoc√≥digo |
| V | Gradient Descent (c√≥digo) | Minimizar $f(x,y) = x^2 + y^2$ | Implementar |
| S | **Repaso** | Learning rate y convergencia | Visualizaci√≥n |

### Semana 7: Chain Rule

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Regla de la Cadena (1D) | $\frac{d}{dx}f(g(x))$ | Ejercicio 3.3 |
| M | Regla de la Cadena (multi) | Composici√≥n de funciones | Ejercicio 3.4 |
| X | Aplicaci√≥n: funci√≥n de p√©rdida | Derivar MSE | Ejercicio 3.5 |
| J | Preview Backpropagation | C√≥mo fluyen gradientes | Diagrama |
| V | Derivar Cross-Entropy | Preparaci√≥n para log√≠stica | Ejercicio 3.6 |
| S | **Checkpoint** | GD + Chain Rule documentado | Entregable |

**Entregable:** Gradient Descent manual con visualizaci√≥n de trayectoria.

**Recursos:**
- [3Blue1Brown: Essence of Calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)

---

## üóìÔ∏è SEMANA 8: Probabilidad para ML (M√≥dulo 04)

**Objetivo:** Bayes, Gaussiana, MLE - lo m√≠nimo para entender loss functions  
**Conexi√≥n con Pathway:** Cross-Entropy viene de MLE. GMM usa Gaussianas.

### Semana 8: Probabilidad Esencial

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Probabilidad b√°sica | P(A), P(A\|B) | Ejercicios |
| M | Teorema de Bayes | Prior, Likelihood, Posterior | Implementar |
| X | Distribuci√≥n Gaussiana | PDF, Œº, œÉ | `gaussian_pdf()` |
| J | Gaussiana multivariada | Matriz de covarianza | Implementar |
| V | MLE (Maximum Likelihood) | Por qu√© da Cross-Entropy | Demostraci√≥n |
| S | **Checkpoint Fase 1** | `probability.py` completo | Entregable |

**Entregable:** Librer√≠a `probability.py` con Gaussiana, MLE y softmax.

**Recursos:**
- [3Blue1Brown: Bayes Theorem](https://www.youtube.com/watch?v=HZGCoVF3YvM)
- [StatQuest: Maximum Likelihood](https://www.youtube.com/watch?v=XepXtl9YKwc)

---

# üî∑ FASE 2: N√öCLEO DE MACHINE LEARNING (Semanas 9-20)

*Objetivo: Implementar desde cero los algoritmos exactos de los 3 cursos del Pathway*

---

## üóìÔ∏è SEMANA 9-12: Supervised Learning (M√≥dulo 05)

**Materia:** Introduction to Machine Learning: Supervised Learning

### Semana 9: Regresi√≥n Lineal

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Regresi√≥n: concepto | L√≠nea de mejor ajuste | Visualizar datos |
| M | M√≠nimos cuadrados | F√≥rmula cerrada: $(X^TX)^{-1}X^Ty$ | Implementar |
| X | MSE como funci√≥n de costo | $J(\theta) = \frac{1}{n}\sum(y - \hat{y})^2$ | Calcular MSE |
| J | GD para regresi√≥n | Derivar gradiente de MSE | Implementar |
| V | Regresi√≥n m√∫ltiple | M√°s de una feature | Extender c√≥digo |
| S | **Repaso** | `linear_regression.py` v1 | Test |

### Semana 10: Regresi√≥n Log√≠stica

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Clasificaci√≥n binaria | 0/1, s√≠/no | Dataset simple |
| M | Funci√≥n sigmoid | $\sigma(z) = \frac{1}{1+e^{-z}}$ | Implementar |
| X | Hip√≥tesis log√≠stica | $h_\theta(x) = \sigma(\theta^T x)$ | Implementar |
| J | Cross-Entropy Loss | $-[y\log(\hat{y}) + (1-y)\log(1-\hat{y})]$ | Implementar |
| V | GD para log√≠stica | Derivar gradiente | Implementar |
| S | **Repaso** | `logistic_regression.py` | Test |

### Semana 11: Evaluaci√≥n y M√©tricas

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Train/Test split | Por qu√© separar datos | Implementar split |
| M | Accuracy | Porcentaje correcto | Implementar |
| X | Precision y Recall | TP, FP, FN, TN | Implementar |
| J | F1-Score | Media arm√≥nica | Implementar |
| V | Matriz de confusi√≥n | Visualizaci√≥n | matplotlib |
| S | **Repaso** | `metrics.py` completo | Test |

### Semana 12: Validaci√≥n Cruzada y Regularizaci√≥n

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Overfitting vs Underfitting | Bias-Variance tradeoff | Diagrama |
| M | K-Fold Cross Validation | Validaci√≥n robusta | Implementar |
| X | Regularizaci√≥n L2 (Ridge) | $\lambda\|\theta\|^2$ | A√±adir a regresi√≥n |
| J | Regularizaci√≥n L1 (Lasso) | $\lambda\|\theta\|_1$ | Comparar |
| V | Selecci√≥n de hiperpar√°metros | Grid search simple | Implementar |
| S | **Checkpoint** | Supervisado completo | Entregable |

**Entregable:** `logistic_regression.py` desde cero usando NumPy para clasificar datos simples, con m√©tricas y cross-validation.

---

## üóìÔ∏è SEMANA 13-16: Unsupervised Learning (M√≥dulo 06)

**Materia:** Unsupervised Algorithms in Machine Learning

### Semana 13: K-Means Clustering

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Clustering: concepto | Agrupar sin etiquetas | Visualizar clusters |
| M | Algoritmo Lloyd (K-Means) | Asignar, actualizar, repetir | Pseudoc√≥digo |
| X | Implementar K-Means | Versi√≥n b√°sica | C√≥digo |
| J | K-Means++ inicializaci√≥n | Mejor selecci√≥n de centroides | Implementar |
| V | Criterio de parada | Convergencia | Implementar |
| S | **Repaso** | `kmeans.py` funcional | Test |

### Semana 14: Evaluaci√≥n de Clusters

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Inercia (within-cluster) | Suma de distancias al centroide | Implementar |
| M | M√©todo del codo | Elegir K √≥ptimo | Visualizar |
| X | Silhouette Score | Calidad de clusters | Implementar |
| J | Limitaciones K-Means | Clusters no esf√©ricos | Ejemplos |
| V | Generar datos sint√©ticos | make_blobs equivalente | Funci√≥n propia |
| S | **Repaso** | Evaluaci√≥n completa | Documento |

### Semana 15: PCA (Principal Component Analysis)

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Reducci√≥n dimensional | Por qu√© reducir | Visualizar |
| M | PCA: concepto | Direcci√≥n de m√°xima varianza | Diagrama |
| X | PCA con eigenvalues | Autovectores de covarianza | np.linalg.eig() |
| J | PCA con SVD | M√°s estable num√©ricamente | np.linalg.svd() |
| V | Varianza explicada | Cu√°nta info se pierde | Implementar |
| S | **Repaso** | `pca.py` v1 | Test |

### Semana 16: PCA Aplicado y GMM

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Reconstrucci√≥n desde PCA | Proyectar y reconstruir | Implementar |
| M | Compresi√≥n de im√°genes | PCA para reducir | Demo visual |
| X | GMM (concepto) | Mezcla de Gaussianas | Teor√≠a |
| J | EM Algorithm (intro) | Expectation-Maximization | Pseudoc√≥digo |
| V | Detecci√≥n de anomal√≠as | Outliers con GMM | Concepto |
| S | **Checkpoint** | No supervisado completo | Entregable |

**Entregable:** `kmeans.py` y `pca.py`. Usar PCA para comprimir una imagen y visualizar cu√°nta varianza se pierde con diferentes n√∫meros de componentes.

---

## üóìÔ∏è SEMANA 17-20: Deep Learning + CNNs (M√≥dulo 07)

**Materia:** Introduction to Deep Learning

### Semana 17: Perceptr√≥n y Fundamentos

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Neurona artificial | Analog√≠a biol√≥gica | Diagrama |
| M | Perceptr√≥n simple | $y = \text{sign}(w \cdot x + b)$ | Implementar |
| X | Funciones de activaci√≥n | Sigmoid, ReLU, Tanh | Implementar todas |
| J | Limitaci√≥n del perceptr√≥n | No puede resolver XOR | Demostrar |
| V | Necesidad de capas | Redes multicapa | Diagrama |
| S | **Repaso** | `activations.py` | Test |

### Semana 18: Forward Propagation

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | MLP: arquitectura | Capas ocultas | Diagrama |
| M | Forward pass | Propagaci√≥n hacia adelante | Pseudoc√≥digo |
| X | Implementar forward | Clase NeuralNetwork | C√≥digo |
| J | Funci√≥n de p√©rdida DL | Cross-entropy para multiclase | Softmax |
| V | Inicializaci√≥n de pesos | Xavier, He | Implementar |
| S | **Repaso** | Forward funcional | Test |

### Semana 19: CNNs - Teor√≠a (NO implementaci√≥n)

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | Por qu√© CNNs para im√°genes | Problema de MLP con im√°genes | Diagrama |
| M | Operaci√≥n de convoluci√≥n | Kernel, filtro, feature map | Demo visual |
| X | Stride, padding, pooling | C√°lculo de dimensiones output | Ejercicios |
| J | Arquitectura LeNet-5 | Capas CONV + POOL + FC | Diagrama |
| V | Max pooling implementaci√≥n | Concepto simple | C√≥digo b√°sico |
| S | **Repaso** | Quiz de dimensiones CNN | Test te√≥rico |

### Semana 20: Optimizadores y Entrenamiento

| D√≠a | Ma√±ana (Teor√≠a) | Mediod√≠a (C√≥digo) | Tarde (Pr√°ctica) |
|-----|-----------------|-------------------|------------------|
| L | SGD (Stochastic GD) | Mini-batches | Implementar |
| M | Momentum | Acelerar convergencia | Implementar |
| X | Adam optimizer | Adaptive learning rate | Implementar |
| J | Regularizaci√≥n DL | Dropout (concepto), L2 | Implementar L2 |
| V | Training loop completo | Epochs, batches, logging | Implementar |
| S | **Checkpoint** | MLP resuelve XOR | Entregable |

**Entregable:** `neural_net.py` - Una red neuronal que resuelve el problema XOR y clasifica d√≠gitos simples, implementando `backward()` manualmente.

---

# üî∑ FASE 3: PROYECTO FINAL "MNIST ANALYST" (Semanas 21-24)

*Objetivo: Un proyecto intensivo de 4 semanas que demuestra competencia en las 3 √°reas*

**Dataset:** MNIST (im√°genes de 28x28 p√≠xeles de d√≠gitos escritos a mano)

> üí° **v3.1:** MNIST es un dataset simple (solo 10 clases, im√°genes peque√±as). 4 semanas son suficientes.

---

## üóìÔ∏è SEMANA 21: EDA + No Supervisado

**Materia demostrada:** Unsupervised Algorithms in Machine Learning

| D√≠a | Actividad |
|-----|-----------|
| L | Cargar MNIST, entender estructura (784 dimensiones) |
| M | Implementar PCA desde cero, reducir a 2-3 componentes |
| X | Visualizar d√≠gitos en gr√°fico 2D |
| J | Implementar K-Means, agrupar d√≠gitos SIN etiquetas |
| V | Visualizar centroides como im√°genes 28x28 |
| S | **Checkpoint:** Notebook PCA + K-Means |

**Entregable:** Jupyter notebook con PCA 2D y K-Means clustering.

---

## üóìÔ∏è SEMANA 22: Clasificaci√≥n Supervisada

**Materia demostrada:** Introduction to ML: Supervised Learning

| D√≠a | Actividad |
|-----|-----------|
| L | Train/test split, normalizaci√≥n |
| M | Implementar Logistic Regression One-vs-All (10 clasificadores) |
| X | Entrenar y medir Accuracy global |
| J | Precision, Recall, F1, matriz de confusi√≥n |
| V | Visualizar errores (im√°genes mal clasificadas) |
| S | **Checkpoint:** Logistic Regression completo |

**Entregable:** `logistic_mnist.py` con m√©tricas completas.

---

## üóìÔ∏è SEMANA 23: Deep Learning

**Materia demostrada:** Introduction to Deep Learning

| D√≠a | Actividad |
|-----|-----------|
| L | Dise√±ar arquitectura MLP (784‚Üí128‚Üí64‚Üí10) |
| M | Implementar forward pass + softmax |
| X | Implementar backprop con cross-entropy |
| J | Training loop con mini-batches |
| V | Entrenar y ajustar hiperpar√°metros |
| S | **Checkpoint:** MLP funcional >90% accuracy |

**Entregable:** `neural_network_mnist.py` con backprop manual.

---

## üóìÔ∏è SEMANA 24: Benchmark + Informe Final

**Objetivo:** Comparar modelos y documentar

| D√≠a | Actividad |
|-----|-----------|
| L | Comparar rendimiento: Logistic vs MLP |
| M | An√°lisis: ¬øpor qu√© MLP es mejor? (no linealidad) |
| X | Escribir MODEL_COMPARISON.md |
| J | Crear README.md profesional (ingl√©s) |
| V | Demo final: Jupyter notebook completo |
| S | **Entrega final + Autoevaluaci√≥n** |

**Entregable Final:** 

```
mnist-analyst/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py
‚îÇ   ‚îú‚îÄ‚îÄ linear_algebra.py
‚îÇ   ‚îú‚îÄ‚îÄ probability.py
‚îÇ   ‚îú‚îÄ‚îÄ pca.py
‚îÇ   ‚îú‚îÄ‚îÄ kmeans.py
‚îÇ   ‚îú‚îÄ‚îÄ logistic_regression.py
‚îÇ   ‚îú‚îÄ‚îÄ neural_network.py
‚îÇ   ‚îî‚îÄ‚îÄ mnist_pipeline.py
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_eda_pca_kmeans.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_logistic_classification.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 03_neural_network_benchmark.ipynb
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ MODEL_COMPARISON.md
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_*.py
‚îî‚îÄ‚îÄ README.md
```

---

## ‚úÖ Checklist de Finalizaci√≥n - ML SPECIALIST v3.1

### Fase 1: Fundamentos (M√≥dulos 01-04)
- [ ] Python + Pandas + NumPy dominado
- [ ] √Ålgebra lineal: normas, distancias, SVD, eigenvalues
- [ ] C√°lculo: gradientes, chain rule, gradient descent
- [ ] Probabilidad: Bayes, Gaussiana, MLE, softmax

### Fase 2: ML Core (M√≥dulos 05-07) ‚≠ê PATHWAY
- [ ] **Supervised (05):** Logistic Regression con m√©tricas
- [ ] **Unsupervised (06):** K-Means y PCA desde cero
- [ ] **Deep Learning (07):** MLP con backprop + teor√≠a CNNs

### Fase 3: Proyecto MNIST (M√≥dulo 08)
- [ ] PCA reduce MNIST a 2D con visualizaci√≥n
- [ ] K-Means agrupa d√≠gitos sin etiquetas
- [ ] Logistic Regression clasifica con >85% accuracy
- [ ] MLP supera a Logistic con >90% accuracy
- [ ] MODEL_COMPARISON.md explica matem√°ticamente las diferencias
- [ ] README.md profesional en ingl√©s

### Verificaci√≥n Final
- [ ] Puedo explicar matem√°ticamente por qu√© funciona cada algoritmo
- [ ] Puedo derivar las f√≥rmulas de gradiente a mano
- [ ] Puedo implementar desde cero sin copiar c√≥digo
- [ ] Entiendo convoluci√≥n, stride, padding, pooling para CNNs
- [ ] Listo para los 3 cursos del Pathway L√≠nea 1

---

## üìö Recursos Recomendados

### Matem√°ticas
- [3Blue1Brown: Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)
- [3Blue1Brown: Calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)

### Machine Learning
- [Stanford CS229](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)
- [Coursera: Machine Learning (Andrew Ng)](https://www.coursera.org/learn/machine-learning)

### Deep Learning
- [3Blue1Brown: Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [CS231n: CNNs for Visual Recognition](http://cs231n.stanford.edu/)

---

> üí° **Filosof√≠a v3.1:** Esta gu√≠a te lleva de Python b√°sico a candidato competitivo del MS in AI en exactamente 6 meses (24 semanas). Si puedes implementar PCA, K-Means, Logistic Regression y un MLP desde cero sobre MNIST, y entiendes la teor√≠a de CNNs, **dominas la L√≠nea 1 del Pathway**.
