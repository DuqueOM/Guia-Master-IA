# M√≥dulo 05 - Supervised Learning

> **üéØ Objetivo:** Dominar regresi√≥n lineal, log√≠stica y m√©tricas de evaluaci√≥n
> **Fase:** 2 - N√∫cleo de ML | **Semanas 9-12**
> **Curso del Pathway:** Introduction to Machine Learning: Supervised Learning

---

<a id="m05-0"></a>

## üß≠ C√≥mo usar este m√≥dulo (modo 0‚Üí100)

**Prop√≥sito:** que puedas construir un pipeline supervisado ‚Äúde examen‚Äù:

- entrenar (regresi√≥n lineal/log√≠stica)
- evaluar (m√©tricas)
- validar (train/test, K-fold)
- controlar overfitting (regularizaci√≥n)

### Objetivos de aprendizaje (medibles)

Al terminar este m√≥dulo podr√°s:

- **Implementar** regresi√≥n lineal y regresi√≥n log√≠stica desde cero.
- **Derivar** el gradiente de MSE y de cross-entropy (con la forma `X·µÄ(≈∑ - y)`).
- **Elegir** m√©tricas correctas seg√∫n el costo de FP/FN.
- **Aplicar** validaci√≥n (split y K-fold) evitando leakage.
- **Validar** tu implementaci√≥n con Shadow Mode (sklearn) como ground truth.
- **Explicar** Entrop√≠a/Gini, Information Gain y el contraste **Bagging vs Boosting** (Random Forest vs Gradient Boosting) a nivel conceptual.

Enlaces r√°pidos:

- [04_PROBABILIDAD_ML.md](04_PROBABILIDAD_ML.md) (MLE ‚Üí cross-entropy)
- [GLOSARIO.md](GLOSARIO.md)
- [RECURSOS.md](RECURSOS.md)
- [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)
- [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md)
- Evaluaci√≥n (r√∫brica): [study_tools/RUBRICA_v1.md](../study_tools/RUBRICA_v1.md) (scope `M05` en `rubrica.csv`)

### Recursos (cu√°ndo usarlos)

| Prioridad | Recurso | Cu√°ndo usarlo en este m√≥dulo | Para qu√© |
|----------|---------|------------------------------|----------|
| **Obligatorio** | [04_PROBABILIDAD_ML.md](04_PROBABILIDAD_ML.md) | Antes de implementar `log-loss`/cross-entropy y el gradiente de log√≠stica | Conectar MLE ‚Üí cross-entropy y evitar derivaciones ‚Äúde memoria‚Äù |
| **Obligatorio** | `study_tools/DIRTY_DATA_CHECK.md` | Antes del primer entrenamiento real (Semana 9‚Äì10), al preparar datasets | Evitar que el modelo ‚Äúaprenda basura‚Äù por fallas de datos |
| **Obligatorio** | `study_tools/DIARIO_ERRORES.md` | Cada vez que veas m√©tricas incoherentes, accuracy ‚Äúm√°gico‚Äù o divergencia | Registrar bugs, causas y fixes reproducibles |
| **Complementario** | [StatQuest ML (playlist)](https://www.youtube.com/playlist?list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF) | Semana 10‚Äì12 (log√≠stica, m√©tricas, regularizaci√≥n) | Refuerzo conceptual r√°pido + ejemplos |
| **Complementario** | [Stanford CS229](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) | Despu√©s de implementar regresi√≥n lineal/log√≠stica (para profundizar) | Profundizar en teor√≠a y derivaciones est√°ndar |
| **Opcional** | [RECURSOS.md](RECURSOS.md) | Al finalizar el m√≥dulo (para escoger pr√°ctica extra) | Expandir sin perder el foco del Pathway |

---

## üß† ¬øQu√© es Supervised Learning?

```text
APRENDIZAJE SUPERVISADO

Tenemos:
- Datos de entrada X (features)
- Etiquetas Y (targets/labels)

Objetivo: Aprender una funci√≥n f tal que f(X) ‚âà Y

Tipos principales:
‚îú‚îÄ‚îÄ REGRESI√ìN: Y es continuo (precio, temperatura)
‚îÇ   ‚îî‚îÄ‚îÄ Output: n√∫mero real
‚îî‚îÄ‚îÄ CLASIFICACI√ìN: Y es discreto (spam/no spam, d√≠gito 0-9)
    ‚îî‚îÄ‚îÄ Output: clase o probabilidad
```

---

## üìö Contenido del M√≥dulo

| Semana | Tema | Entregable |
|--------|------|------------|
| 9 | Regresi√≥n Lineal | `linear_regression.py` |
| 10 | Regresi√≥n Log√≠stica | `logistic_regression.py` |
| 11 | M√©tricas de Evaluaci√≥n | `metrics.py` |
| 12 | Validaci√≥n + Regularizaci√≥n + √Årboles | Cross-validation, L1/L2 + Tree-Based Models |

---

## üíª Parte 1: Regresi√≥n Lineal

### 1.1 Modelo

```python
import numpy as np

"""
REGRESI√ìN LINEAL

Hip√≥tesis: h(x) = Œ∏‚ÇÄ + Œ∏‚ÇÅx‚ÇÅ + Œ∏‚ÇÇx‚ÇÇ + ... + Œ∏‚Çôx‚Çô
         = Œ∏·µÄx (forma matricial)

Donde:
- Œ∏ (theta): par√°metros/pesos del modelo
- x: vector de features (con x‚ÇÄ = 1 para el bias)

En forma matricial para m√∫ltiples muestras:
    ≈∑ = XŒ∏

Donde:
- X: matriz (m √ó n+1) con m muestras y n features + columna de 1s
- Œ∏: vector (n+1 √ó 1) de par√°metros
- ≈∑: vector (m √ó 1) de predicciones
"""

def add_bias_term(X: np.ndarray) -> np.ndarray:
    """A√±ade columna de 1s para el t√©rmino de bias."""
    m = X.shape[0]
    return np.column_stack([np.ones(m), X])

def predict_linear(X: np.ndarray, theta: np.ndarray) -> np.ndarray:
    """Predicci√≥n lineal: ≈∑ = XŒ∏"""
    return X @ theta
```

### 1.2 Funci√≥n de Costo (MSE)

```python
import numpy as np

def mse_cost(X: np.ndarray, y: np.ndarray, theta: np.ndarray) -> float:
    """
    Mean Squared Error Cost Function.

    J(Œ∏) = (1/2m) Œ£·µ¢ (h(x·µ¢) - y·µ¢)¬≤
         = (1/2m) ||XŒ∏ - y||¬≤

    El factor 1/2 es por conveniencia (cancela con la derivada).
    """
    m = len(y)
    predictions = X @ theta
    errors = predictions - y
    return (1 / (2 * m)) * np.sum(errors ** 2)

def mse_gradient(X: np.ndarray, y: np.ndarray, theta: np.ndarray) -> np.ndarray:
    """
    Gradiente del MSE respecto a Œ∏.

    ‚àÇJ/‚àÇŒ∏ = (1/m) X·µÄ(XŒ∏ - y)
    """
    m = len(y)
    predictions = X @ theta
    errors = predictions - y
    return (1 / m) * X.T @ errors
```

### 1.3 Soluci√≥n Cerrada (Normal Equation)

```python
import numpy as np

def normal_equation(X: np.ndarray, y: np.ndarray) -> np.ndarray:
    """
    Soluci√≥n cerrada para regresi√≥n lineal.

    Œ∏ = (X·µÄX)‚Åª¬π X·µÄy

    Ventajas:
    - No requiere iteraciones
    - No hay hiperpar√°metros (learning rate)

    Desventajas:
    - O(n¬≥) por la inversi√≥n de matriz
    - No funciona si X·µÄX es singular
    - No escala bien para n grande (>10,000 features)
    """
    XtX = X.T @ X
    Xty = X.T @ y

    # Usar solve en lugar de inv para estabilidad num√©rica
    theta = np.linalg.solve(XtX, Xty)
    return theta
```

### 1.4 Gradient Descent para Regresi√≥n

```python
import numpy as np
from typing import List, Tuple

class LinearRegression:
    """Regresi√≥n Lineal implementada desde cero."""

    def __init__(self):
        self.theta = None
        self.cost_history = []

    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        method: str = 'gradient_descent',
        learning_rate: float = 0.01,
        n_iterations: int = 1000
    ) -> 'LinearRegression':
        """
        Entrena el modelo.

        Args:
            X: features (m, n)
            y: targets (m,)
            method: 'gradient_descent' o 'normal_equation'
            learning_rate: tasa de aprendizaje (solo para GD)
            n_iterations: n√∫mero de iteraciones (solo para GD)
        """
        # A√±adir bias
        X_b = add_bias_term(X)
        m, n = X_b.shape

        if method == 'normal_equation':
            self.theta = normal_equation(X_b, y)
        else:
            # Inicializar theta con ceros o valores peque√±os
            self.theta = np.zeros(n)

            for i in range(n_iterations):
                # Calcular gradiente
                gradient = mse_gradient(X_b, y, self.theta)

                # Actualizar theta
                self.theta = self.theta - learning_rate * gradient

                # Guardar costo para monitoreo
                cost = mse_cost(X_b, y, self.theta)
                self.cost_history.append(cost)

        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        """Predice valores."""
        X_b = add_bias_term(X)
        return X_b @ self.theta

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        """R¬≤ score."""
        y_pred = self.predict(X)
        ss_res = np.sum((y - y_pred) ** 2)
        ss_tot = np.sum((y - np.mean(y)) ** 2)
        return 1 - (ss_res / ss_tot)


# Demo
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X.flatten() + np.random.randn(100) * 0.5  # y = 4 + 3x + ruido

model = LinearRegression()
model.fit(X, y, method='gradient_descent', learning_rate=0.1, n_iterations=1000)

print(f"Par√°metros aprendidos: {model.theta}")
print(f"Esperados: [4, 3]")
print(f"R¬≤ score: {model.score(X, y):.4f}")
```

---

## üíª Parte 2: Regresi√≥n Log√≠stica

### 2.0 Regresi√≥n Log√≠stica ‚Äî Nivel: intermedio (core del Pathway)

**Prop√≥sito:** pasar de ‚Äús√© aplicar sigmoid‚Äù a **poder entrenar, derivar y validar** un clasificador binario (y extenderlo a multiclase con One-vs-All).

#### Objetivos de aprendizaje (medibles)

Al terminar esta parte podr√°s:

- **Explicar** por qu√© regresi√≥n log√≠stica es un modelo lineal *sobre el log-odds* (aunque la salida sea una probabilidad).
- **Derivar** (a mano) el gradiente de la p√©rdida *Binary Cross-Entropy* y reconocer la forma `X·µÄ(≈∑ - y)`.
- **Implementar** `fit()` con gradient descent estable (con `clip`/`eps`) y verificar convergencia.
- **Diagnosticar** errores t√≠picos: shapes, overflow en `exp`, signos invertidos, saturaci√≥n de sigmoid.
- **Validar** tu implementaci√≥n con **Shadow Mode** (comparaci√≥n con sklearn) y con un *overfit test* en dataset peque√±o.

#### Prerrequisitos

- De `M√≥dulo 03`: Chain Rule y gradiente.
- De `M√≥dulo 04`: interpretaci√≥n de MLE (conexi√≥n con cross-entropy).

Enlaces r√°pidos:

- [GLOSARIO: Logistic Regression](GLOSARIO.md#logistic-regression)
- [GLOSARIO: Sigmoid](GLOSARIO.md#sigmoid)
- [GLOSARIO: Binary Cross-Entropy](GLOSARIO.md#binary-cross-entropy)
- [GLOSARIO: Gradient Descent](GLOSARIO.md#gradient-descent)
- [RECURSOS.md](RECURSOS.md)

#### Explicaci√≥n progresiva (intuici√≥n ‚Üí formalizaci√≥n ‚Üí implementaci√≥n)

##### a) Intuici√≥n

Quieres un modelo que devuelva:

- un **score lineal** `z = Œ∏·µÄx` (como en regresi√≥n lineal), y
- lo convierta en una **probabilidad** en `(0, 1)`.

Eso lo hace `œÉ(z)`.

##### a.1 Odds, log-odds y por qu√© esto ‚Äúsigue siendo lineal‚Äù

Si el modelo produce `p = P(y=1|x)`, define:

```
odds = p / (1 - p)
logit(p) = log(odds)
```

La regresi√≥n log√≠stica asume que **el log-odds es lineal**:

```
logit(p) = Œ∏·µÄx
```

Y la sigmoide es simplemente la funci√≥n que vuelve de logit a probabilidad:

```
p = œÉ(Œ∏·µÄx) = 1 / (1 + exp(-Œ∏·µÄx))
```

Esto importa porque te permite interpretar el modelo:

- subir `Œ∏·µÄx` en +1 incrementa el **log-odds** en +1 (cambio multiplicativo en odds).

##### a.2 Por qu√© NO usar MSE para clasificaci√≥n

Podr√≠as intentar usar MSE con `≈∑ = œÉ(z)`, pero en pr√°ctica es mala idea:

- **La geometr√≠a del entrenamiento empeora:** el gradiente se vuelve poco informativo cuando `œÉ(z)` se satura (cerca de 0 o 1).
- **La funci√≥n objetivo deja de ser convexa** (puede tener m√≠nimos locales / mesetas), haciendo el descenso de gradiente menos confiable.
- **No penaliza bien el caso ‚Äúseguro y equivocado‚Äù:** si `y=1` pero `≈∑‚âà0`, quieres un castigo enorme; eso lo da `-log(≈∑)`.

Por eso usamos **Log-Loss / Binary Cross-Entropy**, que viene de MLE y es convexa para este modelo.

##### a.3 Visual: frontera de decisi√≥n

La frontera de decisi√≥n es el conjunto de puntos donde `p = 0.5`:

```
œÉ(Œ∏·µÄx) = 0.5  ‚áî  Œ∏·µÄx = 0
```

##### a.3.1 Intuici√≥n geom√©trica: el ‚Äúplano de corte‚Äù

Piensa en tus datos como puntos en un espacio.

- En 2D, `Œ∏·µÄx + b = 0` es una **l√≠nea**.
- En 3D, es un **plano**.
- En `n` dimensiones, es un **hiperplano**.

La cantidad `z = Œ∏·µÄx + b` es un **score con signo**:

- `z > 0` ‚Üí est√°s del lado ‚Äúpositivo‚Äù del plano
- `z < 0` ‚Üí est√°s del lado ‚Äúnegativo‚Äù

La sigmoide `œÉ(z)` convierte ese score (relacionado con la distancia al plano) en probabilidad:

- puntos muy lejos del plano (|z| grande) ‚Üí probabilidad cerca de 0 o 1
- puntos cerca del plano (`z ‚âà 0`) ‚Üí probabilidad cerca de 0.5

Visualizaci√≥n sugerida (dib√∫jalo): una nube roja/azul y una l√≠nea que la corta; marca puntos a distinta distancia y escribe su `z` y `œÉ(z)`.

##### a.3.2 Conexi√≥n conceptual: SVM y la idea de ‚Äúmargen‚Äù (sin implementar)

Aunque no implementes SVM aqu√≠, su intuici√≥n te mejora la comprensi√≥n de regularizaci√≥n.

Idea:

- En clasificaci√≥n lineal, hay muchas l√≠neas/planos que separan (si los datos lo permiten).
- SVM busca el separador que deja la ‚Äúcarretera‚Äù m√°s ancha entre clases: **m√°ximo margen**.

Conexi√≥n con lo que s√≠ implementas:

- La **regularizaci√≥n** (L2/L1) controla complejidad efectiva.
- En problemas separables o casi separables, regularizar suele empujar a soluciones m√°s estables, con fronteras menos extremas.

Visualizaci√≥n sugerida: dos l√≠neas separadoras posibles y dibujar cu√°l deja m√°s espacio m√≠nimo a los puntos m√°s cercanos (support vectors).

En 2D, `Œ∏·µÄx = 0` es una **l√≠nea**.

```
clase 1:   o o o o o
           o o o o o

frontera:  ---------

clase 0:   x x x x x
           x x x x x
```

##### a.4 Worked example (num√©rico) de BCE

Datos: `x=2`, `y=1`.

- `w=0.5`, `b=0`
- `z = wx + b = 1`
- `≈∑ = œÉ(1) ‚âà 0.731`

Como `y=1`, la loss por muestra es:

```
L = -log(≈∑) ‚âà -log(0.731) ‚âà 0.313
```

Interpretaci√≥n: la predicci√≥n es ‚Äúbastante‚Äù correcta, por eso la loss es peque√±a. Si `≈∑` fuera 0.01, la loss ser√≠a enorme.

##### a.5 C√≥digo generador de intuici√≥n (Protocolo D): frontera de decisi√≥n en 2D

Objetivo: ver que la **frontera de decisi√≥n** (`p=0.5`) es lineal, aunque la salida `œÉ(z)` sea curva (curva en *probabilidad*, no en geometr√≠a de la frontera).

```python
import numpy as np
import matplotlib.pyplot as plt


def make_blobs_2d(n=200, seed=42):
    rng = np.random.default_rng(seed)
    c0 = rng.normal(loc=(-2.0, -1.5), scale=0.8, size=(n // 2, 2))
    c1 = rng.normal(loc=(2.0, 1.5), scale=0.8, size=(n // 2, 2))
    X = np.vstack([c0, c1])
    y = np.array([0] * (n // 2) + [1] * (n // 2))
    return X, y


def sigmoid(z):
    z = np.clip(z, -500, 500)
    return 1 / (1 + np.exp(-z))


def add_bias(X):
    return np.column_stack([np.ones(len(X)), X])


def plot_decision_boundary(model, X, y, title="Decision boundary"):
    x_min, x_max = X[:, 0].min() - 1.0, X[:, 0].max() + 1.0
    y_min, y_max = X[:, 1].min() - 1.0, X[:, 1].max() + 1.0

    xx, yy = np.meshgrid(
        np.linspace(x_min, x_max, 250),
        np.linspace(y_min, y_max, 250),
    )

    grid = np.column_stack([xx.ravel(), yy.ravel()])
    proba = model.predict_proba(grid).reshape(xx.shape)

    plt.figure(figsize=(7, 6))
    plt.contourf(xx, yy, proba, levels=20, cmap="RdBu", alpha=0.35)
    plt.contour(xx, yy, proba, levels=[0.5], colors="black", linewidths=2)

    plt.scatter(X[y == 0, 0], X[y == 0, 1], s=18, label="Clase 0")
    plt.scatter(X[y == 1, 0], X[y == 1, 1], s=18, label="Clase 1")

    plt.title(title)
    plt.legend()
    plt.grid(True, alpha=0.2)
    plt.show()


# Usa TU LogisticRegression del m√≥dulo (la clase ya existe m√°s abajo)
# X, y = make_blobs_2d(n=300)
# model = LogisticRegression()
# model.fit(X, y, learning_rate=0.1, n_iterations=2000)
# plot_decision_boundary(model, X, y)
```

Reto visual (opcional, si usas sklearn solo para generar datos):

- genera `make_moons` y grafica la frontera
- ver√°s por qu√© log√≠stica falla (frontera lineal)
- luego entrena tu MLP (M07) y observa c√≥mo la frontera se curva

##### b) Formalizaci√≥n m√≠nima

- **Modelo:** `≈∑ = œÉ(XŒ∏)`
- **Decisi√≥n:** `≈∑ ‚â• 0.5 ‚Üí clase 1` (umbral configurable)
- **Loss (BCE):** penaliza fuerte cuando est√°s ‚Äúseguro y equivocado‚Äù (ej. `≈∑‚âà0` pero `y=1`).

##### c) Regla de oro de shapes

Evita bugs silenciosos usando una convenci√≥n consistente:

- `X`: `(m, n)`
- `Œ∏`: `(n,)` (o `(n, 1)` si prefieres columnas)
- `y`: `(m,)`

Y verifica que `X @ Œ∏` te da `(m,)`.

#### Actividades activas (para convertir teor√≠a en habilidad)

- **Retrieval practice (5 min):** escribe sin mirar:
  - la ecuaci√≥n de BCE,
  - el gradiente `‚àáŒ∏`.
- **Ejercicio de calibraci√≥n:** cambia el `threshold` de 0.5 a 0.3 y explica qu√© pasa con precision/recall.
- **Sanity check obligatorio:** entrena con 20 ejemplos hasta obtener accuracy ~100% (si no, hay bug).

#### Evaluaci√≥n (criterios de ‚Äúdominio‚Äù)

- **Dominio matem√°tico:** puedes explicar por qu√© aparece `(≈∑ - y)` en el gradiente.
- **Dominio de implementaci√≥n:** tu `fit()` reduce BCE de forma monot√≥nica (o casi) en un dataset simple.
- **Dominio de validaci√≥n:** tu accuracy difiere <5% de sklearn en Shadow Mode.

#### Errores comunes (los que m√°s queman tiempo)

- **Overflow/NaN:** `exp(500)` revienta. Soluci√≥n: `clip(z)` y `eps` en logs.
- **Saturaci√≥n:** si `|z|` crece, `œÉ(z)` se pega a 0/1 y el gradiente se hace peque√±o.
- **Signo invertido:** si actualizas en la direcci√≥n equivocada, la loss sube.
- **Sin normalizaci√≥n:** features en escalas muy distintas hacen que GD sea inestable.

#### Integraci√≥n con Plan v4/v5

- **v4.0:** usa `study_tools/SIMULACRO_EXAMEN_TEORICO.md` para preguntas tipo examen (sigmoid vs softmax, BCE vs MSE).
- **v5.0:** ejecuta **Shadow Mode** como verificaci√≥n externa antes de dar por terminado el m√≥dulo.

### 2.1 Funci√≥n Sigmoid

```python
import numpy as np

def sigmoid(z: np.ndarray) -> np.ndarray:
    """
    Funci√≥n sigmoid/log√≠stica.

    œÉ(z) = 1 / (1 + e^(-z))

    Propiedades:
    - Rango: (0, 1) - perfecto para probabilidades
    - œÉ(0) = 0.5
    - œÉ'(z) = œÉ(z)(1 - œÉ(z))
    """
    # Clip para evitar overflow
    z = np.clip(z, -500, 500)
    return 1 / (1 + np.exp(-z))

# Visualizar
import matplotlib.pyplot as plt

z = np.linspace(-10, 10, 100)
plt.figure(figsize=(8, 4))
plt.plot(z, sigmoid(z))
plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5)
plt.axvline(x=0, color='r', linestyle='--', alpha=0.5)
plt.xlabel('z')
plt.ylabel('œÉ(z)')
plt.title('Funci√≥n Sigmoid')
plt.grid(True)
# plt.show()
```

### 2.2 Hip√≥tesis Log√≠stica

```python
"""
REGRESI√ìN LOG√çSTICA

No predice un valor continuo, sino la PROBABILIDAD de pertenecer a la clase 1.

h(x) = P(y=1|x; Œ∏) = œÉ(Œ∏·µÄx)

Decisi√≥n:
- Si h(x) ‚â• 0.5 ‚Üí predicir clase 1
- Si h(x) < 0.5 ‚Üí predicir clase 0

Equivalente a:
- Si Œ∏·µÄx ‚â• 0 ‚Üí clase 1
- Si Œ∏·µÄx < 0 ‚Üí clase 0

El "decision boundary" est√° en Œ∏·µÄx = 0
"""

def predict_proba(X: np.ndarray, theta: np.ndarray) -> np.ndarray:
    """Predice probabilidad de clase 1."""
    return sigmoid(X @ theta)

def predict_class(X: np.ndarray, theta: np.ndarray, threshold: float = 0.5) -> np.ndarray:
    """Predice clase (0 o 1)."""
    return (predict_proba(X, theta) >= threshold).astype(int)
```

### 2.3 Binary Cross-Entropy Loss

```python
import numpy as np

def binary_cross_entropy(
    X: np.ndarray,
    y: np.ndarray,
    theta: np.ndarray,
    eps: float = 1e-15
) -> float:
    """
    Binary Cross-Entropy (Log Loss).

    J(Œ∏) = -(1/m) Œ£·µ¢ [y·µ¢ log(h·µ¢) + (1-y·µ¢) log(1-h·µ¢)]

    Donde h·µ¢ = œÉ(Œ∏·µÄx·µ¢)

    Por qu√© esta funci√≥n de costo:
    - Es convexa (tiene un √∫nico m√≠nimo global)
    - Penaliza mucho las predicciones muy incorrectas
    - Es la derivaci√≥n de Maximum Likelihood Estimation
    """
    m = len(y)
    h = sigmoid(X @ theta)

    # Clip para evitar log(0)
    h = np.clip(h, eps, 1 - eps)

    cost = -(1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
    return cost

def bce_gradient(X: np.ndarray, y: np.ndarray, theta: np.ndarray) -> np.ndarray:
    """
    Gradiente de Binary Cross-Entropy.

    ‚àÇJ/‚àÇŒ∏ = (1/m) X·µÄ(h - y)

    ¬°Tiene la misma forma que el gradiente del MSE!
    Esto es porque derivamos œÉ(z) y la derivada œÉ'(z) = œÉ(z)(1-œÉ(z))
    cancela parte de la expresi√≥n.
    """
    m = len(y)
    h = sigmoid(X @ theta)
    return (1/m) * X.T @ (h - y)
```

### 2.4 Implementaci√≥n Completa

```python
import numpy as np
from typing import List

class LogisticRegression:
    """Regresi√≥n Log√≠stica implementada desde cero."""

    def __init__(self):
        self.theta = None
        self.cost_history = []

    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        learning_rate: float = 0.1,
        n_iterations: int = 1000
    ) -> 'LogisticRegression':
        """Entrena con gradient descent."""
        # A√±adir bias
        X_b = np.column_stack([np.ones(len(X)), X])
        m, n = X_b.shape

        # Inicializar
        self.theta = np.zeros(n)

        for i in range(n_iterations):
            # Gradiente
            gradient = bce_gradient(X_b, y, self.theta)

            # Actualizar
            self.theta = self.theta - learning_rate * gradient

            # Guardar costo
            cost = binary_cross_entropy(X_b, y, self.theta)
            self.cost_history.append(cost)

        return self

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """Predice probabilidades."""
        X_b = np.column_stack([np.ones(len(X)), X])
        return sigmoid(X_b @ self.theta)

    def predict(self, X: np.ndarray, threshold: float = 0.5) -> np.ndarray:
        """Predice clases."""
        return (self.predict_proba(X) >= threshold).astype(int)

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        """Accuracy."""
        return np.mean(self.predict(X) == y)


# Demo con datos sint√©ticos
np.random.seed(42)

# Generar datos de dos clases
n_samples = 200
X_class0 = np.random.randn(n_samples // 2, 2) + np.array([-2, -2])
X_class1 = np.random.randn(n_samples // 2, 2) + np.array([2, 2])
X = np.vstack([X_class0, X_class1])
y = np.array([0] * (n_samples // 2) + [1] * (n_samples // 2))

# Entrenar
model = LogisticRegression()
model.fit(X, y, learning_rate=0.1, n_iterations=1000)

print(f"Accuracy: {model.score(X, y):.2%}")
print(f"Par√°metros: {model.theta}")
```

---

## üß© Consolidaci√≥n (Regresi√≥n Log√≠stica)

### Entregable conceptual (v3.3): Interpretaci√≥n de pesos (LogReg)

Objetivo: conectar el vector de pesos con ‚Äúqu√© est√° mirando‚Äù el modelo.

- Dataset recomendado: MNIST (28x28) en binario (p. ej. 0 vs 1) usando `sklearn.datasets.fetch_openml("mnist_784", as_frame=False)`.
- Entrena tu regresi√≥n log√≠stica sobre im√°genes aplanadas (`784` features).
- Visualiza:
  - toma `theta[1:]` (sin bias), reshapea a `(28, 28)` y grafica con `imshow`.
  - usa un mapa de color divergente (p. ej. centrado en 0) y guarda una imagen.
- Interpreta en 5‚Äì10 l√≠neas:
  - ¬øqu√© regiones tienen peso positivo/negativo?
  - ¬øpor qu√© eso tiene sentido para el d√≠gito?

### Errores comunes

- **Etiquetas incorrectas:** BCE asume `y ‚àà {0,1}` (no `{-1,1}`) si usas la f√≥rmula est√°ndar.
- **Olvidar el bias:** si no agregas columna de 1s, la frontera se forza a pasar por el origen.
- **`exp` overflow:** si `z` crece, `exp(-z)` puede overflow/underflow ‚Üí usa `clip`.
- **`log(0)`:** si `h` llega a 0 o 1 exactos, `log` revienta ‚Üí usa `eps`.
- **Sin escalado:** features con escalas distintas hacen el GD inestable.

### Debugging / validaci√≥n (v5)

- **Overfit test:** entrena con 20 ejemplos hasta casi 100% accuracy. Si no, asume bug.
- **Shadow Mode:** compara con sklearn para la misma semilla/dataset.
- Registra hallazgos en `study_tools/DIARIO_ERRORES.md`.
- Protocolos completos:
  - [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md)
  - [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md)

### Reto Feynman (tablero blanco)

Explica en 5 l√≠neas o menos:

1) ¬øQu√© es el logit y por qu√© log√≠stica es lineal ‚Äúen el espacio de log-odds‚Äù?
2) ¬øPor qu√© `-log(≈∑)` explota cuando est√°s seguro y equivocado?
3) ¬øQu√© significa `X·µÄ(≈∑ - y)` y por qu√© aparece en el gradiente?

---

## üíª Parte 3: M√©tricas de Evaluaci√≥n

### 3.0 M√©tricas ‚Äî Nivel: intermedio (de ‚Äúcalcular‚Äù a ‚Äútomar decisiones‚Äù)

**Prop√≥sito:** que no te quedes en ‚Äús√© calcular accuracy‚Äù, sino que puedas **elegir la m√©trica correcta seg√∫n el riesgo** (FP vs FN), detectar desbalance de clases y justificar tus decisiones como en un informe.

#### Objetivos de aprendizaje (medibles)

Al terminar esta parte podr√°s:

- **Explicar** la matriz de confusi√≥n y derivar TP/TN/FP/FN sin mirar apuntes.
- **Aplicar** accuracy/precision/recall/F1/specificity y explicar cu√°ndo cada una es adecuada.
- **Analizar** el impacto del umbral (`threshold`) en precision/recall.
- **Diagnosticar** trampas comunes: accuracy alta con clases desbalanceadas, leakage, evaluar sobre train.

#### Prerrequisitos y conexiones

- Conexi√≥n directa con probabilidad/loss:
  - [04_PROBABILIDAD_ML.md](04_PROBABILIDAD_ML.md) (MLE ‚Üí cross-entropy)
- Glosario:
  - [GLOSARIO: Confusion Matrix](GLOSARIO.md#confusion-matrix)
  - [GLOSARIO: Accuracy](GLOSARIO.md#accuracy)
  - [GLOSARIO: Precision](GLOSARIO.md#precision)
  - [GLOSARIO: Recall](GLOSARIO.md#recall)
  - [GLOSARIO: F1 Score](GLOSARIO.md#f1-score)

#### Resumen ejecutivo (big idea)

La m√©trica es una traducci√≥n expl√≠cita de ‚Äúqu√© error es m√°s caro‚Äù:

- Si te preocupa **no perder positivos reales** ‚Üí prioriza **recall**.
- Si te preocupa **no disparar falsas alarmas** ‚Üí prioriza **precision**.
- Si necesitas balance ‚Üí **F1**.
- Si tu dataset est√° balanceado y el costo es sim√©trico ‚Üí **accuracy** puede servir.

#### Actividades activas (obligatorias)

- **Retrieval practice (5 min):** escribe la matriz 2x2 y define TP/TN/FP/FN.
- **Experimento de umbral:** eval√∫a con `threshold = 0.3, 0.5, 0.7` y anota c√≥mo cambian precision/recall.
- **Caso desbalanceado:** crea un dataset donde 95% sea clase 0 y muestra por qu√© accuracy enga√±a.

#### Errores comunes (los que m√°s da√±an resultados)

- **Evaluar en training:** te da una ‚Äúm√©trica falsa‚Äù por overfitting.
- **Leakage:** normalizar/seleccionar features usando todo el dataset antes del split.
- **No fijar semilla:** resultados no reproducibles.

Integraci√≥n con Plan v4/v5:

- [PLAN_V4_ESTRATEGICO.md](PLAN_V4_ESTRATEGICO.md) (rutina + simulacros)
- [PLAN_V5_ESTRATEGICO.md](PLAN_V5_ESTRATEGICO.md) (validaci√≥n externa / rigor)
- Diario: `study_tools/DIARIO_ERRORES.md`

### 3.1 Matriz de Confusi√≥n

```python
import numpy as np

def confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
    """
    Calcula la matriz de confusi√≥n.

    Para clasificaci√≥n binaria:

                    Predicho
                    0       1
    Real    0      TN      FP
            1      FN      TP

    - TP (True Positive): Predijo 1, era 1
    - TN (True Negative): Predijo 0, era 0
    - FP (False Positive): Predijo 1, era 0 (Error Tipo I)
    - FN (False Negative): Predijo 0, era 1 (Error Tipo II)
    """
    classes = np.unique(np.concatenate([y_true, y_pred]))
    n_classes = len(classes)
    cm = np.zeros((n_classes, n_classes), dtype=int)

    for i, true_class in enumerate(classes):
        for j, pred_class in enumerate(classes):
            cm[i, j] = np.sum((y_true == true_class) & (y_pred == pred_class))

    return cm

def extract_tp_tn_fp_fn(y_true: np.ndarray, y_pred: np.ndarray):
    """Extrae TP, TN, FP, FN para clasificaci√≥n binaria."""
    tp = np.sum((y_true == 1) & (y_pred == 1))
    tn = np.sum((y_true == 0) & (y_pred == 0))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))
    return tp, tn, fp, fn
```

### 3.2 Accuracy, Precision, Recall, F1

```python
import numpy as np

def accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Accuracy = (TP + TN) / (TP + TN + FP + FN)

    Proporci√≥n de predicciones correctas.

    Problema: Puede ser enga√±oso con clases desbalanceadas.
    Si 99% son clase 0, predecir siempre 0 da 99% accuracy.
    """
    return np.mean(y_true == y_pred)

def precision(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Precision = TP / (TP + FP)

    De todos los que predije como positivos, ¬øcu√°ntos realmente lo son?

    Alta precisi√≥n = pocos falsos positivos.
    Importante cuando el costo de FP es alto (ej: spam ‚Üí inbox).
    """
    tp, tn, fp, fn = extract_tp_tn_fp_fn(y_true, y_pred)
    if tp + fp == 0:
        return 0.0
    return tp / (tp + fp)

def recall(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Recall (Sensitivity, True Positive Rate) = TP / (TP + FN)

    De todos los positivos reales, ¬øcu√°ntos captur√©?

    Alto recall = pocos falsos negativos.
    Importante cuando el costo de FN es alto (ej: detecci√≥n de c√°ncer).
    """
    tp, tn, fp, fn = extract_tp_tn_fp_fn(y_true, y_pred)
    if tp + fn == 0:
        return 0.0
    return tp / (tp + fn)

def f1_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    F1 = 2 * (precision * recall) / (precision + recall)

    Media arm√≥nica de precision y recall.

    √ötil cuando quieres un balance entre ambas m√©tricas.
    F1 alto solo si AMBAS precision y recall son altas.
    """
    p = precision(y_true, y_pred)
    r = recall(y_true, y_pred)
    if p + r == 0:
        return 0.0
    return 2 * (p * r) / (p + r)

def specificity(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Specificity (True Negative Rate) = TN / (TN + FP)

    De todos los negativos reales, ¬øcu√°ntos identifiqu√©?
    """
    tp, tn, fp, fn = extract_tp_tn_fp_fn(y_true, y_pred)
    if tn + fp == 0:
        return 0.0
    return tn / (tn + fp)
```

### 3.3 Clase Metrics Completa

```python
import numpy as np
from dataclasses import dataclass

@dataclass
class ClassificationReport:
    """Reporte de m√©tricas de clasificaci√≥n."""
    accuracy: float
    precision: float
    recall: float
    f1: float
    specificity: float
    confusion_matrix: np.ndarray

    def __str__(self) -> str:
        cm = self.confusion_matrix
        return f"""
Classification Report
=====================
Accuracy:    {self.accuracy:.4f}
Precision:   {self.precision:.4f}
Recall:      {self.recall:.4f}
F1 Score:    {self.f1:.4f}
Specificity: {self.specificity:.4f}

Confusion Matrix:
           Pred 0  Pred 1
Actual 0   {cm[0,0]:5d}   {cm[0,1]:5d}
Actual 1   {cm[1,0]:5d}   {cm[1,1]:5d}
"""

def classification_report(y_true: np.ndarray, y_pred: np.ndarray) -> ClassificationReport:
    """Genera reporte completo de m√©tricas."""
    return ClassificationReport(
        accuracy=accuracy(y_true, y_pred),
        precision=precision(y_true, y_pred),
        recall=recall(y_true, y_pred),
        f1=f1_score(y_true, y_pred),
        specificity=specificity(y_true, y_pred),
        confusion_matrix=confusion_matrix(y_true, y_pred)
    )

# Demo
y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])
y_pred = np.array([0, 0, 1, 0, 1, 1, 0, 1, 1, 1])

report = classification_report(y_true, y_pred)
print(report)
```

---

## üíª Parte 4: Validaci√≥n y Regularizaci√≥n

### 4.0 Validaci√≥n y regularizaci√≥n ‚Äî Nivel: intermedio/avanzado

**Prop√≥sito:** aprender el ‚Äúworkflow real‚Äù que evita autoenga√±o:

- dividir datos correctamente
- validar de forma robusta
- controlar overfitting (regularizaci√≥n)

#### Objetivos de aprendizaje (medibles)

Al terminar esta parte podr√°s:

- **Explicar** la diferencia entre train/val/test y por qu√© el test no se toca.
- **Aplicar** K-fold cross validation y reportar media ¬± desviaci√≥n.
- **Diagnosticar** sesgo-varianza en t√©rminos pr√°cticos (qu√© cambia si aumentas `Œª` o si cambias el tama√±o del modelo).
- **Implementar** regularizaci√≥n L2 y justificar por qu√© se excluye el bias.

#### Resumen ejecutivo (big idea)

- **Validaci√≥n** te dice si generalizas.
- **Regularizaci√≥n** controla complejidad efectiva.

Conectar esto con el Pathway:

- En el curso, se eval√∫a tanto la *matem√°tica* como tu capacidad de **evitar leakage** y reportar resultados correctamente.

#### Actividades activas (obligatorias)

- Ejecuta `train_test_split` con al menos 2 semillas distintas y compara varianza en accuracy.
- Haz K-fold (k=5) y reporta `mean ¬± std`.
- Prueba `lambda_` en `{0, 0.01, 0.1, 1.0}` y describe el efecto.

#### Errores comunes

- **Data leakage** por normalizar antes del split.
- **Elegir hiperpar√°metros mirando el test** (invalidas el test).
- **Regularizar el bias** sin querer.

#### Integraci√≥n con Plan v4/v5

- v4.0: usa simulacros para preguntas tipo examen (`study_tools/SIMULACRO_EXAMEN_TEORICO.md`).
- v5.0: valida tu implementaci√≥n con Shadow Mode (sklearn) antes de cerrar el m√≥dulo.

### 4.1 Train/Test Split

```python
import numpy as np

def train_test_split(
    X: np.ndarray,
    y: np.ndarray,
    test_size: float = 0.2,
    random_state: int = None
) -> tuple:
    """
    Divide datos en conjuntos de entrenamiento y prueba.

    Args:
        X: features
        y: targets
        test_size: proporci√≥n para test (0-1)
        random_state: semilla para reproducibilidad
    """
    if random_state is not None:
        np.random.seed(random_state)

    n = len(y)
    indices = np.random.permutation(n)

    test_size_n = int(n * test_size)
    test_indices = indices[:test_size_n]
    train_indices = indices[test_size_n:]

    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]
```

### 4.2 K-Fold Cross Validation

```python
import numpy as np
from typing import List, Tuple

def k_fold_split(n: int, k: int) -> List[Tuple[np.ndarray, np.ndarray]]:
    """
    Genera √≠ndices para K-Fold Cross Validation.

    Returns:
        Lista de (train_indices, val_indices) para cada fold
    """
    indices = np.arange(n)
    np.random.shuffle(indices)

    fold_size = n // k
    folds = []

    for i in range(k):
        start = i * fold_size
        end = start + fold_size if i < k - 1 else n

        val_indices = indices[start:end]
        train_indices = np.concatenate([indices[:start], indices[end:]])

        folds.append((train_indices, val_indices))

    return folds

def cross_validate(
    model_class,
    X: np.ndarray,
    y: np.ndarray,
    k: int = 5,
    **model_params
) -> dict:
    """
    Realiza K-Fold Cross Validation.

    Returns:
        Dict con scores de cada fold y promedio
    """
    folds = k_fold_split(len(y), k)
    scores = []

    for i, (train_idx, val_idx) in enumerate(folds):
        # Split
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        # Train
        model = model_class()
        model.fit(X_train, y_train, **model_params)

        # Evaluate
        score = model.score(X_val, y_val)
        scores.append(score)

    return {
        'scores': scores,
        'mean': np.mean(scores),
        'std': np.std(scores)
    }

# Demo
# cv_results = cross_validate(LogisticRegression, X, y, k=5, learning_rate=0.1, n_iterations=500)
# print(f"CV Accuracy: {cv_results['mean']:.4f} ¬± {cv_results['std']:.4f}")
```

### 4.3 Regularizaci√≥n

```python
import numpy as np

class LogisticRegressionRegularized:
    """Logistic Regression con regularizaci√≥n L1/L2."""

    def __init__(self, regularization: str = 'l2', lambda_: float = 0.01):
        """
        Args:
            regularization: 'l1', 'l2', o None
            lambda_: fuerza de regularizaci√≥n
        """
        self.regularization = regularization
        self.lambda_ = lambda_
        self.theta = None
        self.cost_history = []

    def _cost(self, X: np.ndarray, y: np.ndarray) -> float:
        """Costo con regularizaci√≥n."""
        m = len(y)
        h = sigmoid(X @ self.theta)
        h = np.clip(h, 1e-15, 1 - 1e-15)

        # Cross-entropy base
        bce = -(1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))

        # Regularizaci√≥n (excluir bias theta[0])
        if self.regularization == 'l2':
            # Ridge: Œª/2m * Œ£Œ∏‚±º¬≤
            reg = (self.lambda_ / (2 * m)) * np.sum(self.theta[1:] ** 2)
        elif self.regularization == 'l1':
            # Lasso: Œª/m * Œ£|Œ∏‚±º|
            reg = (self.lambda_ / m) * np.sum(np.abs(self.theta[1:]))
        else:
            reg = 0

        return bce + reg

    def _gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:
        """Gradiente con regularizaci√≥n."""
        m = len(y)
        h = sigmoid(X @ self.theta)

        # Gradiente base
        grad = (1/m) * X.T @ (h - y)

        # Regularizaci√≥n (excluir bias)
        if self.regularization == 'l2':
            reg_grad = np.concatenate([[0], (self.lambda_ / m) * self.theta[1:]])
        elif self.regularization == 'l1':
            reg_grad = np.concatenate([[0], (self.lambda_ / m) * np.sign(self.theta[1:])])
        else:
            reg_grad = 0

        return grad + reg_grad

    def fit(self, X: np.ndarray, y: np.ndarray,
            learning_rate: float = 0.1, n_iterations: int = 1000):
        X_b = np.column_stack([np.ones(len(X)), X])
        self.theta = np.zeros(X_b.shape[1])

        for _ in range(n_iterations):
            gradient = self._gradient(X_b, y)
            self.theta -= learning_rate * gradient
            self.cost_history.append(self._cost(X_b, y))

        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        X_b = np.column_stack([np.ones(len(X)), X])
        return (sigmoid(X_b @ self.theta) >= 0.5).astype(int)

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        return np.mean(self.predict(X) == y)
```

---

### ‚ö†Ô∏è Aviso cr√≠tico antes de √Årboles: Recursividad (Semana 12)

La implementaci√≥n de √°rboles se basa en **recursi√≥n**. Si no defines y pruebas condiciones de parada, vas a generar √°rboles infinitos o muy profundos.

- Condiciones de parada m√≠nimas: `max_depth`, pureza (todas las etiquetas iguales), `min_samples_split`, ‚Äúno split improves‚Äù.
- Recurso recomendado: https://realpython.com/python-recursion/
- Debug m√≠nimo: imprime `depth`, `n_samples` y el criterio elegido por nodo durante desarrollo.

## üå≥ Parte 5: Tree-Based Models (Semana 12)

Esta semana cubre modelos supervisados **no diferenciables** (no entrenan con Gradient Descent). La l√≥gica de entrenamiento es:

- elegir un *split* (feature + threshold)
- medir qu√© tan ‚Äúpuro‚Äù queda cada lado (Entrop√≠a o Gini)
- repetir recursivamente

### 5.1 Entrop√≠a, Gini e Information Gain

Definiciones base (para clasificaci√≥n):

- **Entrop√≠a:** `H(y) = - Œ£ p(c) log2 p(c)`
- **Gini:** `G(y) = 1 - Œ£ p(c)^2`

Un split `(j, t)` divide el dataset en:

- izquierda: `x_j ‚â§ t`
- derecha: `x_j > t`

La idea es maximizar la mejora en pureza:

- **Information Gain:** `IG = impurity(parent) - weighted_impurity(children)`

### 5.2 Entrenable desde cero (entregable)

Entregable runnable:

- `scripts/decision_tree_from_scratch.py`

Ejecuta:

```bash
python3 scripts/decision_tree_from_scratch.py --criterion gini --max-depth 5
```

Objetivo m√≠nimo:

- que el script entrene un √°rbol y reporte accuracy train/test en un dataset toy
- que puedas explicar (en 5 l√≠neas) c√≥mo el √°rbol decide el mejor split

### 5.3 Ensembles (intro): Bagging vs Boosting

Conceptos clave:

- **Bagging (Random Forest):** muchos √°rboles entrenados en *bootstrap samples*; reduce varianza.
- **Boosting (Gradient Boosting/XGBoost):** √°rboles entrenados secuencialmente corrigiendo errores; reduce bias (pero puede sobreajustar).

---

## üéØ Ejercicios por tema (progresivos) + Soluciones

Reglas:

- **Intenta primero** sin mirar la soluci√≥n.
- **Timebox sugerido:** 20‚Äì45 min por ejercicio.
- **√âxito m√≠nimo:** tu soluci√≥n debe pasar los `assert`.

---

### Ejercicio 5.1: Regresi√≥n lineal (Normal Equation) + recuperaci√≥n de pesos

#### Enunciado

1) **B√°sico**

- Genera un dataset sint√©tico: `y = Xw + noise`.

2) **Intermedio**

- Estima `w_hat` usando la ecuaci√≥n normal con `np.linalg.solve`.

3) **Avanzado**

- Verifica que `w_hat` se aproxima a `w_true` y que el MSE es peque√±o.

#### Soluci√≥n

```python
import numpy as np

np.random.seed(0)
n, d = 500, 3
X = np.random.randn(n, d)
w_true = np.array([0.7, -1.5, 2.0])
noise = 0.05 * np.random.randn(n)
y = X @ w_true + noise

# Normal equation: (X^T X) w = X^T y
XtX = X.T @ X
Xty = X.T @ y
w_hat = np.linalg.solve(XtX, Xty)

mse = np.mean((X @ w_hat - y) ** 2)

assert w_hat.shape == (d,)
assert np.linalg.norm(w_hat - w_true) < 0.15
assert mse < 0.01
```

---

### Ejercicio 5.2: Regresi√≥n lineal (Gradient Descent) + comparaci√≥n con Normal Equation

#### Enunciado

1) **B√°sico**

- Implementa GD para minimizar MSE: `w <- w - Œ± (1/n) X^T (Xw - y)`.

2) **Intermedio**

- Compara `w_gd` contra `w_ne` (normal equation).

3) **Avanzado**

- Verifica que el loss disminuye (al menos al final es menor que al inicio).

#### Soluci√≥n

```python
import numpy as np

np.random.seed(1)
n, d = 400, 4
X = np.random.randn(n, d)
w_true = np.array([1.0, -2.0, 0.5, 3.0])
y = X @ w_true + 0.1 * np.random.randn(n)

XtX = X.T @ X
Xty = X.T @ y
w_ne = np.linalg.solve(XtX, Xty)

w = np.zeros(d)
alpha = 0.05
losses = []
for _ in range(3000):
    r = X @ w - y
    grad = (X.T @ r) / n
    w = w - alpha * grad
    losses.append(float(np.mean(r**2)))

w_gd = w

assert losses[-1] <= losses[0]
assert np.linalg.norm(w_gd - w_ne) < 0.2
```

---

### Ejercicio 5.3: M√©tricas desde una matriz de confusi√≥n (TP/TN/FP/FN)

#### Enunciado

1) **B√°sico**

- Implementa una funci√≥n que compute TP/TN/FP/FN para un problema binario.

2) **Intermedio**

- Implementa accuracy, precision, recall, F1.

3) **Avanzado**

- Valida con un caso conocido y `assert`.

#### Soluci√≥n

```python
import numpy as np

def confusion_counts(y_true: np.ndarray, y_pred: np.ndarray):
    y_true = np.asarray(y_true).astype(int)
    y_pred = np.asarray(y_pred).astype(int)
    tp = int(np.sum((y_true == 1) & (y_pred == 1)))
    tn = int(np.sum((y_true == 0) & (y_pred == 0)))
    fp = int(np.sum((y_true == 0) & (y_pred == 1)))
    fn = int(np.sum((y_true == 1) & (y_pred == 0)))
    return tp, tn, fp, fn


def precision_recall_f1(y_true: np.ndarray, y_pred: np.ndarray):
    tp, tn, fp, fn = confusion_counts(y_true, y_pred)
    eps = 1e-12
    acc = (tp + tn) / (tp + tn + fp + fn + eps)
    prec = tp / (tp + fp + eps)
    rec = tp / (tp + fn + eps)
    f1 = 2 * prec * rec / (prec + rec + eps)
    return float(acc), float(prec), float(rec), float(f1)


y_true = np.array([1, 1, 1, 0, 0, 0])
y_pred = np.array([1, 0, 1, 0, 1, 0])
tp, tn, fp, fn = confusion_counts(y_true, y_pred)

assert (tp, tn, fp, fn) == (2, 2, 1, 1)

acc, prec, rec, f1 = precision_recall_f1(y_true, y_pred)
assert np.isclose(acc, 4/6)
assert np.isclose(prec, 2/3)
assert np.isclose(rec, 2/3)
assert np.isclose(f1, 2/3)
```

---

### Ejercicio 5.4: Logistic Regression - sigmoid + BCE estable

#### Enunciado

1) **B√°sico**

- Implementa `sigmoid(z)` con `np.clip` para evitar overflow.

2) **Intermedio**

- Implementa Binary Cross-Entropy estable (con `clip`).

3) **Avanzado**

- Verifica:
  - BCE cerca de 0 para predicciones casi perfectas.
  - BCE ‚âà `-log(0.9)` cuando `y=1` y `p=0.9`.

#### Soluci√≥n

```python
import numpy as np

def sigmoid(z: np.ndarray) -> np.ndarray:
    z = np.asarray(z, dtype=float)
    z = np.clip(z, -500, 500)
    return 1.0 / (1.0 + np.exp(-z))


def bce(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-15) -> float:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    y_pred = np.clip(y_pred, eps, 1.0 - eps)
    return float(-np.mean(y_true * np.log(y_pred) + (1.0 - y_true) * np.log(1.0 - y_pred)))


y_true = np.array([1.0, 0.0, 1.0, 0.0])
y_pred_good = np.array([0.999, 0.001, 0.999, 0.001])
assert bce(y_true, y_pred_good) < 0.01
assert np.isclose(bce(np.array([1.0]), np.array([0.9])), -np.log(0.9), atol=1e-12)
```

---

### Ejercicio 5.5: Gradiente de Logistic Regression (verificaci√≥n num√©rica)

#### Enunciado

1) **B√°sico**

- Implementa el gradiente de BCE para Logistic Regression:
  - `≈∑ = sigmoid(Xw)`
  - `‚àáw = (1/n) X^T (≈∑ - y)`

2) **Intermedio**

- Implementa una funci√≥n de p√©rdida `L(w)`.

3) **Avanzado**

- Verifica 1 coordenada del gradiente con diferencias centrales.

#### Soluci√≥n

```python
import numpy as np

def sigmoid(z: np.ndarray) -> np.ndarray:
    z = np.asarray(z, dtype=float)
    z = np.clip(z, -500, 500)
    return 1.0 / (1.0 + np.exp(-z))


def bce_from_logits(X: np.ndarray, y: np.ndarray, w: np.ndarray, eps: float = 1e-15) -> float:
    logits = X @ w
    y_hat = sigmoid(logits)
    y_hat = np.clip(y_hat, eps, 1.0 - eps)
    return float(-np.mean(y * np.log(y_hat) + (1.0 - y) * np.log(1.0 - y_hat)))


def grad_bce(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:
    y_hat = sigmoid(X @ w)
    return (X.T @ (y_hat - y)) / X.shape[0]


np.random.seed(2)
n, d = 200, 3
X = np.random.randn(n, d)
w0 = np.array([0.3, -0.7, 1.2])
probs = sigmoid(X @ w0)
y = (np.random.rand(n) < probs).astype(float)

w = np.random.randn(d)
g = grad_bce(X, y, w)

idx = 1
h = 1e-6
e = np.zeros(d)
e[idx] = 1.0
L_plus = bce_from_logits(X, y, w + h * e)
L_minus = bce_from_logits(X, y, w - h * e)
g_num = (L_plus - L_minus) / (2.0 * h)

assert np.isclose(g[idx], g_num, rtol=1e-4, atol=1e-6)
```

---

### Ejercicio 5.6: Umbral (threshold) y trade-off precision/recall

#### Enunciado

1) **B√°sico**

- Dadas probabilidades `p` y etiquetas `y`, construye predicciones con umbral `t`.

2) **Intermedio**

- Calcula precision/recall para `t=0.5` y `t=0.3`.

3) **Avanzado**

- Verifica que al bajar el umbral t√≠picamente sube el recall (en el mismo dataset).

#### Soluci√≥n

```python
import numpy as np

def predict_threshold(p: np.ndarray, t: float) -> np.ndarray:
    return (np.asarray(p) >= t).astype(int)


def precision_recall(y_true: np.ndarray, y_pred: np.ndarray):
    y_true = np.asarray(y_true).astype(int)
    y_pred = np.asarray(y_pred).astype(int)
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))
    eps = 1e-12
    prec = tp / (tp + fp + eps)
    rec = tp / (tp + fn + eps)
    return float(prec), float(rec)


np.random.seed(3)
y_true = np.array([1, 0, 1, 0, 1, 0, 1, 0])
p = np.array([0.9, 0.6, 0.55, 0.52, 0.4, 0.35, 0.2, 0.1])

pred_05 = predict_threshold(p, 0.5)
pred_03 = predict_threshold(p, 0.3)

prec05, rec05 = precision_recall(y_true, pred_05)
prec03, rec03 = precision_recall(y_true, pred_03)

assert rec03 >= rec05
```

---

### Ejercicio 5.7: Regularizaci√≥n L2 (Ridge) y norma de pesos

#### Enunciado

1) **B√°sico**

- Implementa Ridge Regression: `(X^T X + ŒªI) w = X^T y`.

2) **Intermedio**

- Compara `||w_ridge||` contra `||w_ols||`.

3) **Avanzado**

- Verifica que para `Œª>0`, t√≠picamente `||w_ridge|| <= ||w_ols||`.

#### Soluci√≥n

```python
import numpy as np

np.random.seed(4)
n, d = 300, 5
X = np.random.randn(n, d)
w_true = np.array([2.0, -1.0, 0.5, 0.0, 3.0])
y = X @ w_true + 0.2 * np.random.randn(n)

XtX = X.T @ X
Xty = X.T @ y
w_ols = np.linalg.solve(XtX, Xty)

lam = 10.0
w_ridge = np.linalg.solve(XtX + lam * np.eye(d), Xty)

assert np.linalg.norm(w_ridge) <= np.linalg.norm(w_ols) + 1e-8
```

---

### Ejercicio 5.8: Train/Test split reproducible (semilla)

#### Enunciado

1) **B√°sico**

- Implementa `train_test_split(X,y,test_size,seed)`.

2) **Intermedio**

- Verifica que con la misma semilla el split es id√©ntico.

3) **Avanzado**

- Verifica que no se pierden muestras y que shapes son correctos.

#### Soluci√≥n

```python
import numpy as np

def train_test_split(X: np.ndarray, y: np.ndarray, test_size: float = 0.2, seed: int = 0):
    X = np.asarray(X)
    y = np.asarray(y)
    n = X.shape[0]
    rng = np.random.default_rng(seed)
    idx = np.arange(n)
    rng.shuffle(idx)
    n_test = int(round(n * test_size))
    test_idx = idx[:n_test]
    train_idx = idx[n_test:]
    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]


np.random.seed(0)
X = np.random.randn(100, 2)
y = (np.random.rand(100) < 0.5).astype(int)

Xtr1, Xte1, ytr1, yte1 = train_test_split(X, y, test_size=0.25, seed=42)
Xtr2, Xte2, ytr2, yte2 = train_test_split(X, y, test_size=0.25, seed=42)

assert np.allclose(Xtr1, Xtr2)
assert np.allclose(Xte1, Xte2)
assert np.all(ytr1 == ytr2)
assert np.all(yte1 == yte2)
assert Xtr1.shape[0] + Xte1.shape[0] == 100
```

---

### Ejercicio 5.9: K-Fold cross-validation (partici√≥n correcta)

#### Enunciado

1) **B√°sico**

- Implementa un generador de folds (√≠ndices train/val).

2) **Intermedio**

- Verifica que cada √≠ndice aparece exactamente una vez en validaci√≥n.

3) **Avanzado**

- Verifica que train/val no se solapan.

#### Soluci√≥n

```python
import numpy as np

def kfold_indices(n: int, k: int, seed: int = 0):
    rng = np.random.default_rng(seed)
    idx = np.arange(n)
    rng.shuffle(idx)
    folds = np.array_split(idx, k)
    for i in range(k):
        val_idx = folds[i]
        train_idx = np.concatenate([folds[j] for j in range(k) if j != i])
        yield train_idx, val_idx


n = 23
k = 5
seen = np.zeros(n, dtype=int)
for tr, va in kfold_indices(n, k, seed=123):
    assert len(np.intersect1d(tr, va)) == 0
    seen[va] += 1
assert np.all(seen == 1)
```

---

### Ejercicio 5.10: √Årboles - Gini e Information Gain (split 1D)

#### Enunciado

1) **B√°sico**

- Implementa impurity Gini para etiquetas binarias.

2) **Intermedio**

- Para un feature 1D y un umbral `t`, computa el Information Gain.

3) **Avanzado**

- Encuentra el mejor umbral entre varios candidatos y verifica el resultado.

#### Soluci√≥n

```python
import numpy as np

def gini(y: np.ndarray) -> float:
    y = np.asarray(y).astype(int)
    if y.size == 0:
        return 0.0
    p1 = np.mean(y == 1)
    p0 = 1.0 - p1
    return float(1.0 - (p0**2 + p1**2))


def info_gain_gini(x: np.ndarray, y: np.ndarray, t: float) -> float:
    x = np.asarray(x, dtype=float)
    y = np.asarray(y, dtype=int)
    parent = gini(y)
    left = y[x <= t]
    right = y[x > t]
    w_left = left.size / y.size
    w_right = right.size / y.size
    child = w_left * gini(left) + w_right * gini(right)
    return float(parent - child)


x = np.array([0.1, 0.2, 0.25, 0.8, 0.85, 0.9])
y = np.array([0, 0, 0, 1, 1, 1])

candidates = [0.2, 0.25, 0.8]
gains = [info_gain_gini(x, y, t) for t in candidates]
best_t = candidates[int(np.argmax(gains))]

assert best_t in [0.25, 0.8]
assert max(gains) > 0.0
```

---

### (Bonus) Ejercicio 5.11: Shadow Mode - comparar contra soluci√≥n cerrada en mini-dataset

#### Enunciado

- Entrena regresi√≥n lineal por GD y compara predicci√≥n con soluci√≥n cerrada en un conjunto peque√±o.

#### Soluci√≥n

```python
import numpy as np

np.random.seed(5)
n, d = 30, 2
X = np.random.randn(n, d)
w_true = np.array([1.2, -0.4])
y = X @ w_true + 0.01 * np.random.randn(n)

w_ne = np.linalg.solve(X.T @ X, X.T @ y)

w = np.zeros(d)
alpha = 0.1
for _ in range(2000):
    grad = (X.T @ (X @ w - y)) / n
    w = w - alpha * grad

y_ne = X @ w_ne
y_gd = X @ w

assert np.mean((y_ne - y_gd) ** 2) < 1e-4
```


## üì¶ Entregable del M√≥dulo

- `supervised_learning.py` (regresi√≥n lineal + log√≠stica + m√©tricas + validaci√≥n).
- `scripts/decision_tree_from_scratch.py` (√°rbol de decisi√≥n simple desde cero, sin gradientes).

### `supervised_learning.py`

```python
"""
Supervised Learning Module

Implementaci√≥n desde cero de:
- Linear Regression (con Normal Equation y Gradient Descent)
- Logistic Regression (con regularizaci√≥n L1/L2)
- M√©tricas de evaluaci√≥n
- Cross Validation

Autor: [Tu nombre]
M√≥dulo: 05 - Supervised Learning
"""

import numpy as np
from typing import Tuple, List, Optional
from dataclasses import dataclass


# ============================================================
# FUNCIONES AUXILIARES
# ============================================================

def sigmoid(z: np.ndarray) -> np.ndarray:
    z = np.clip(z, -500, 500)
    return 1 / (1 + np.exp(-z))

def add_bias(X: np.ndarray) -> np.ndarray:
    return np.column_stack([np.ones(len(X)), X])


# ============================================================
# REGRESI√ìN LINEAL
# ============================================================

class LinearRegression:
    def __init__(self):
        self.theta = None
        self.cost_history = []

    def fit(self, X: np.ndarray, y: np.ndarray,
            method: str = 'normal', lr: float = 0.01, n_iter: int = 1000):
        X_b = add_bias(X)

        if method == 'normal':
            self.theta = np.linalg.solve(X_b.T @ X_b, X_b.T @ y)
        else:
            m, n = X_b.shape
            self.theta = np.zeros(n)
            for _ in range(n_iter):
                grad = (1/m) * X_b.T @ (X_b @ self.theta - y)
                self.theta -= lr * grad
                self.cost_history.append(np.mean((X_b @ self.theta - y)**2))
        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        return add_bias(X) @ self.theta

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        y_pred = self.predict(X)
        ss_res = np.sum((y - y_pred)**2)
        ss_tot = np.sum((y - np.mean(y))**2)
        return 1 - ss_res / ss_tot


# ============================================================
# REGRESI√ìN LOG√çSTICA
# ============================================================

class LogisticRegression:
    def __init__(self, reg: str = None, lambda_: float = 0.01):
        self.reg = reg
        self.lambda_ = lambda_
        self.theta = None
        self.cost_history = []

    def fit(self, X: np.ndarray, y: np.ndarray,
            lr: float = 0.1, n_iter: int = 1000):
        X_b = add_bias(X)
        m, n = X_b.shape
        self.theta = np.zeros(n)

        for _ in range(n_iter):
            h = sigmoid(X_b @ self.theta)
            grad = (1/m) * X_b.T @ (h - y)

            if self.reg == 'l2':
                grad[1:] += (self.lambda_/m) * self.theta[1:]
            elif self.reg == 'l1':
                grad[1:] += (self.lambda_/m) * np.sign(self.theta[1:])

            self.theta -= lr * grad
        return self

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        return sigmoid(add_bias(X) @ self.theta)

    def predict(self, X: np.ndarray) -> np.ndarray:
        return (self.predict_proba(X) >= 0.5).astype(int)

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        return np.mean(self.predict(X) == y)


# ============================================================
# M√âTRICAS
# ============================================================

def accuracy(y_true, y_pred):
    return np.mean(y_true == y_pred)

def precision(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    return tp / (tp + fp) if (tp + fp) > 0 else 0

def recall(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))
    return tp / (tp + fn) if (tp + fn) > 0 else 0

def f1_score(y_true, y_pred):
    p, r = precision(y_true, y_pred), recall(y_true, y_pred)
    return 2*p*r/(p+r) if (p+r) > 0 else 0

def confusion_matrix(y_true, y_pred):
    cm = np.zeros((2, 2), dtype=int)
    cm[0, 0] = np.sum((y_true == 0) & (y_pred == 0))
    cm[0, 1] = np.sum((y_true == 0) & (y_pred == 1))
    cm[1, 0] = np.sum((y_true == 1) & (y_pred == 0))
    cm[1, 1] = np.sum((y_true == 1) & (y_pred == 1))
    return cm


# ============================================================
# VALIDACI√ìN
# ============================================================

def train_test_split(X, y, test_size=0.2, seed=None):
    if seed: np.random.seed(seed)
    n = len(y)
    idx = np.random.permutation(n)
    split = int(n * test_size)
    return X[idx[split:]], X[idx[:split]], y[idx[split:]], y[idx[:split]]

def cross_validate(model_class, X, y, k=5, **params):
    n = len(y)
    idx = np.random.permutation(n)
    fold_size = n // k
    scores = []

    for i in range(k):
        val_idx = idx[i*fold_size:(i+1)*fold_size]
        train_idx = np.concatenate([idx[:i*fold_size], idx[(i+1)*fold_size:]])

        model = model_class()
        model.fit(X[train_idx], y[train_idx], **params)
        scores.append(model.score(X[val_idx], y[val_idx]))

    return {'scores': scores, 'mean': np.mean(scores), 'std': np.std(scores)}


# ============================================================
# TESTS
# ============================================================

if __name__ == "__main__":
    np.random.seed(42)

    # Test Linear Regression
    X = 2 * np.random.rand(100, 1)
    y = 4 + 3 * X.flatten() + np.random.randn(100) * 0.5

    lr = LinearRegression()
    lr.fit(X, y)
    print(f"Linear Regression R¬≤: {lr.score(X, y):.4f}")

    # Test Logistic Regression
    X_c0 = np.random.randn(50, 2) + [-2, -2]
    X_c1 = np.random.randn(50, 2) + [2, 2]
    X_clf = np.vstack([X_c0, X_c1])
    y_clf = np.array([0]*50 + [1]*50)

    log_reg = LogisticRegression()
    log_reg.fit(X_clf, y_clf)
    print(f"Logistic Regression Accuracy: {log_reg.score(X_clf, y_clf):.4f}")

    # Test metrics
    y_true = np.array([0,0,0,1,1,1,1,1])
    y_pred = np.array([0,0,1,1,1,0,1,1])
    print(f"Precision: {precision(y_true, y_pred):.4f}")
    print(f"Recall: {recall(y_true, y_pred):.4f}")
    print(f"F1: {f1_score(y_true, y_pred):.4f}")

    # Test CV
    cv = cross_validate(LogisticRegression, X_clf, y_clf, k=5, lr=0.1, n_iter=500)
    print(f"CV Score: {cv['mean']:.4f} ¬± {cv['std']:.4f}")

    print("\n‚úì Todos los tests pasaron!")
```

---

## üìù Derivaci√≥n Anal√≠tica: El Entregable de L√°piz y Papel (v3.3)

> üéì **Simulaci√≥n de Examen:** En la maestr√≠a te pedir√°n: *"Derive la regla de actualizaci√≥n de pesos para Logistic Regression"*. Debes poder hacerlo a mano.

### Derivaci√≥n del Gradiente de Logistic Regression

**Objetivo:** Derivar `‚àÇL/‚àÇw` para la funci√≥n de costo Cross-Entropy.

#### Paso 1: Definir la Funci√≥n de Costo

```
L(w) = -(1/n) Œ£_{i=1..n} [ y_i log(≈∑_i) + (1 - y_i) log(1 - ≈∑_i) ]
```

Donde:
- `≈∑_i = œÉ(w·µÄ x_i) = 1 / (1 + e^{-w·µÄ x_i})`
- `œÉ(z)` es la funci√≥n sigmoid

#### Paso 2: Derivar la Sigmoid

```
dœÉ/dz = œÉ(z)(1 - œÉ(z))
```

**Demostraci√≥n:**
```
œÉ(z) = 1 / (1 + e^{-z})

dœÉ/dz = e^{-z} / (1 + e^{-z})^2
      = (1 / (1 + e^{-z})) ¬∑ (e^{-z} / (1 + e^{-z}))
      = œÉ(z)(1 - œÉ(z))
```

#### Paso 3: Aplicar la Regla de la Cadena

Para un solo ejemplo `(x_i, y_i)`:

```
‚àÇL_i/‚àÇw = (‚àÇL_i/‚àÇ≈∑_i) ¬∑ (‚àÇ≈∑_i/‚àÇz_i) ¬∑ (‚àÇz_i/‚àÇw)
```

Donde `z_i = w·µÄ x_i`

**Calcular cada t√©rmino:**

1. `‚àÇL_i/‚àÇ≈∑_i = -y_i/≈∑_i + (1 - y_i)/(1 - ≈∑_i)`

2. `‚àÇ≈∑_i/‚àÇz_i = ≈∑_i(1 - ≈∑_i)`

3. `‚àÇz_i/‚àÇw = x_i`

#### Paso 4: Simplificar

```
‚àÇL_i/‚àÇw = ( -y_i/≈∑_i + (1 - y_i)/(1 - ≈∑_i) ) ¬∑ ≈∑_i(1 - ≈∑_i) ¬∑ x_i
```

Simplificando el t√©rmino entre par√©ntesis:
```
= ( (-y_i(1 - ≈∑_i) + (1 - y_i)≈∑_i) / (≈∑_i(1 - ≈∑_i)) ) ¬∑ ≈∑_i(1 - ≈∑_i) ¬∑ x_i
= (-y_i + y_i≈∑_i + ≈∑_i - y_i≈∑_i) ¬∑ x_i
= (≈∑_i - y_i) ¬∑ x_i
```

#### Resultado Final

```
‚àÇL/‚àÇw = (1/n) Œ£_{i=1..n} (≈∑_i - y_i) x_i
      = (1/n) X·µÄ (≈∑ - y)
```

**Forma vectorizada (para c√≥digo):**
```python
gradient = (1/n) * X.T @ (y_pred - y_true)
```

### Tu Entregable

Escribe en un documento (Markdown o LaTeX):
1. La derivaci√≥n completa del gradiente de Cross-Entropy
2. La derivaci√≥n de la regla de actualizaci√≥n: `w <- w - Œ± ‚àáL`
3. Por qu√© el gradiente tiene la forma `(≈∑ - y)` (interpretaci√≥n geom√©trica)

---

## üéØ El Reto del Tablero Blanco (Metodolog√≠a Feynman)

Explica en **m√°ximo 5 l√≠neas** sin jerga t√©cnica:

1. **¬øPor qu√© usamos sigmoid en clasificaci√≥n?**
   > Pista: Piensa en probabilidades entre 0 y 1.

2. **¬øPor qu√© Cross-Entropy y no MSE para clasificaci√≥n?**
   > Pista: Piensa en qu√© pasa cuando `≈∑ ‚âà 0` pero `y = 1`.

3. **¬øQu√© significa "One-vs-All"?**
   > Pista: Piensa en c√≥mo clasificar 10 d√≠gitos con clasificadores binarios.

---

## üîç Shadow Mode: Validaci√≥n con sklearn (v3.3)

> ‚ö†Ô∏è **Regla:** sklearn est√° **prohibido para aprender**, pero es **necesario para validar**. Si tu implementaci√≥n difiere significativamente de sklearn, tienes un bug.

### Protocolo de Validaci√≥n (Viernes de Fase 2)

```python
"""
Shadow Mode - Validaci√≥n de Implementaciones
Compara tu c√≥digo desde cero vs sklearn para detectar bugs.

Regla: Si la diferencia de accuracy es >5%, revisar matem√°ticas.
"""
import numpy as np
from sklearn.linear_model import LogisticRegression as SklearnLR
from sklearn.linear_model import LinearRegression as SklearnLinReg
from sklearn.metrics import accuracy_score, mean_squared_error

# Importar tu implementaci√≥n
# from src.logistic_regression import LogisticRegression as MyLR
# from src.linear_regression import LinearRegression as MyLinReg


def shadow_mode_logistic_regression(X_train, y_train, X_test, y_test):
    """
    Compara tu Logistic Regression vs sklearn.

    Los coeficientes y accuracy deben ser casi id√©nticos.
    """
    print("=" * 60)
    print("SHADOW MODE: Logistic Regression")
    print("=" * 60)

    # ========== TU IMPLEMENTACI√ìN ==========
    # my_model = MyLR()
    # my_model.fit(X_train, y_train, lr=0.1, n_iter=1000)
    # my_pred = my_model.predict(X_test)
    # my_acc = accuracy_score(y_test, my_pred)
    # my_weights = my_model.weights

    # Placeholder (reemplazar con tu c√≥digo)
    my_acc = 0.85
    my_weights = np.zeros(X_train.shape[1])

    # ========== SKLEARN (GROUND TRUTH) ==========
    sklearn_model = SklearnLR(max_iter=1000, solver='lbfgs')
    sklearn_model.fit(X_train, y_train)
    sklearn_pred = sklearn_model.predict(X_test)
    sklearn_acc = accuracy_score(y_test, sklearn_pred)
    sklearn_weights = sklearn_model.coef_.flatten()

    # ========== COMPARACI√ìN ==========
    acc_diff = abs(my_acc - sklearn_acc)
    weight_diff = np.linalg.norm(my_weights - sklearn_weights[:len(my_weights)])

    print(f"\nüìä RESULTADOS:")
    print(f"  Tu Accuracy:     {my_acc:.4f}")
    print(f"  sklearn Accuracy: {sklearn_acc:.4f}")
    print(f"  Diferencia:       {acc_diff:.4f}")

    print(f"\nüìê PESOS:")
    print(f"  Diferencia L2 de pesos: {weight_diff:.4f}")

    # Veredicto
    print("\n" + "-" * 60)
    if acc_diff < 0.05:
        print("‚úì PASSED: Tu implementaci√≥n es correcta")
        return True
    else:
        print("‚úó FAILED: Diferencia significativa - revisa tu matem√°tica")
        print("  Posibles causas:")
        print("  - Gradiente mal calculado")
        print("  - Learning rate muy alto/bajo")
        print("  - Falta de normalizaci√≥n de datos")
        return False


def shadow_mode_linear_regression(X_train, y_train, X_test, y_test):
    """
    Compara tu Linear Regression vs sklearn.
    """
    print("=" * 60)
    print("SHADOW MODE: Linear Regression")
    print("=" * 60)

    # ========== TU IMPLEMENTACI√ìN ==========
    # my_model = MyLinReg()
    # my_model.fit(X_train, y_train)
    # my_pred = my_model.predict(X_test)
    # my_mse = mean_squared_error(y_test, my_pred)

    # Placeholder
    my_mse = 0.5

    # ========== SKLEARN ==========
    sklearn_model = SklearnLinReg()
    sklearn_model.fit(X_train, y_train)
    sklearn_pred = sklearn_model.predict(X_test)
    sklearn_mse = mean_squared_error(y_test, sklearn_pred)

    # ========== COMPARACI√ìN ==========
    mse_ratio = my_mse / sklearn_mse if sklearn_mse > 0 else float('inf')

    print(f"\nüìä RESULTADOS:")
    print(f"  Tu MSE:     {my_mse:.4f}")
    print(f"  sklearn MSE: {sklearn_mse:.4f}")
    print(f"  Ratio:       {mse_ratio:.2f}x")

    print("\n" + "-" * 60)
    if mse_ratio < 1.1:  # Dentro del 10%
        print("‚úì PASSED: Tu implementaci√≥n es correcta")
        return True
    else:
        print("‚úó FAILED: Tu MSE es significativamente mayor")
        return False


# ============================================================
# EJEMPLO DE USO
# ============================================================

if __name__ == "__main__":
    from sklearn.datasets import make_classification, make_regression
    from sklearn.model_selection import train_test_split

    # Dataset de clasificaci√≥n
    X_clf, y_clf = make_classification(
        n_samples=1000, n_features=10, n_classes=2, random_state=42
    )
    X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
        X_clf, y_clf, test_size=0.2, random_state=42
    )

    # Dataset de regresi√≥n
    X_reg, y_reg = make_regression(
        n_samples=1000, n_features=10, noise=10, random_state=42
    )
    X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(
        X_reg, y_reg, test_size=0.2, random_state=42
    )

    # Ejecutar Shadow Mode
    shadow_mode_logistic_regression(X_train_c, y_train_c, X_test_c, y_test_c)
    print("\n")
    shadow_mode_linear_regression(X_train_r, y_train_r, X_test_r, y_test_r)
```

### Checklist Shadow Mode

| D√≠a | Algoritmo | Validar |
|-----|-----------|---------|
| Viernes Sem 10 | Linear Regression | MSE ‚âà sklearn |
| Viernes Sem 11 | Logistic Regression | Accuracy ‚âà sklearn |
| Viernes Sem 12 | M√©tricas | Precision/Recall = sklearn |

---

## ‚úÖ Checklist de Finalizaci√≥n (v3.3)

### Conocimiento
- [ ] Implement√© regresi√≥n lineal con Normal Equation y GD
- [ ] Entiendo MSE y su gradiente
- [ ] Implement√© regresi√≥n log√≠stica desde cero
- [ ] Entiendo sigmoid y binary cross-entropy
- [ ] Puedo calcular TP, TN, FP, FN de una matriz de confusi√≥n
- [ ] Implement√© accuracy, precision, recall, F1
- [ ] Implement√© train/test split
- [ ] Implement√© K-fold cross validation
- [ ] Entiendo regularizaci√≥n L1 vs L2

### Shadow Mode (v3.3 - Obligatorio)
- [ ] **Linear Regression**: Mi MSE ‚âà sklearn (ratio < 1.1)
- [ ] **Logistic Regression**: Mi Accuracy ‚âà sklearn (diff < 5%)

### Entregables de C√≥digo
- [ ] `logistic_regression.py` con tests pasando
- [ ] `artifacts/m05_logreg_weights.png` + 5‚Äì10 l√≠neas de interpretaci√≥n (pesos 28x28)
- [ ] `mypy src/` pasa sin errores
- [ ] `pytest tests/` pasa sin errores

### Derivaci√≥n Anal√≠tica (Obligatorio)
- [ ] Deriv√© el gradiente de Cross-Entropy a mano
- [ ] Documento con derivaci√≥n completa (Markdown o LaTeX)
- [ ] Puedo explicar por qu√© `‚àáL = X·µÄ(≈∑ - y)`

### Metodolog√≠a Feynman
- [ ] Puedo explicar sigmoid en 5 l√≠neas sin jerga
- [ ] Puedo explicar Cross-Entropy vs MSE en 5 l√≠neas
- [ ] Puedo explicar One-vs-All en 5 l√≠neas

---

## üîó Navegaci√≥n

| Anterior | √çndice | Siguiente |
|----------|--------|-----------|
| [04_PROBABILIDAD_ML](04_PROBABILIDAD_ML.md) | [00_INDICE](00_INDICE.md) | [06_UNSUPERVISED_LEARNING](06_UNSUPERVISED_LEARNING.md) |
