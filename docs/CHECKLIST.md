# ‚òëÔ∏è Checklist Final - ML Specialist v3.3

> Verificaci√≥n completa del programa de **24 semanas** con validaci√≥n matem√°tica rigurosa.

---

## üìè R√∫brica (evaluaci√≥n formal)

- `study_tools/RUBRICA_v1.md`
- `rubrica.csv`

Uso recomendado:

- **Cierre semanal:** scoring r√°pido (auto).
- **Cierres de m√≥dulo (Semanas 12, 16, 20):** scoring completo (auto + IA/pareja).
- **PB-8 / PB-16 / PB-23:** scoring de simulacro + post-mortem.

## üìö Fase 1: Fundamentos (Semanas 1-8)

### M√≥dulo 01: Python + Pandas + NumPy (Semanas 1-2)

#### Conocimiento
- [ ] Pandas: cargar CSV con `read_csv()`
- [ ] Pandas: limpiar datos con `dropna()`, `fillna()`
- [ ] Pandas: selecci√≥n con `.loc[]`, `.iloc[]`
- [ ] Pandas ‚Üí NumPy: `.to_numpy()`
- [ ] NumPy: creaci√≥n de arrays (1D, 2D, 3D)
- [ ] NumPy: indexing, slicing, broadcasting
- [ ] NumPy: agregaciones por eje (axis=0, axis=1)
- [ ] **Conozco los 5 errores comunes de NumPy y sus soluciones**

#### Est√°ndares Profesionales (v3.2)
- [ ] `mypy src/` pasa sin errores
- [ ] `ruff check src/` pasa sin errores
- [ ] Al menos 3 tests con `pytest` pasando

#### Metodolog√≠a Feynman
- [ ] Puedo explicar broadcasting en 5 l√≠neas sin jerga

### M√≥dulo 02: √Ålgebra Lineal para ML (Semanas 3-5)
- [ ] Producto punto y significado geom√©trico
- [ ] Normas L1, L2, L‚àû implementadas
- [ ] Distancia euclidiana y similitud coseno
- [ ] Multiplicaci√≥n de matrices con `@`
- [ ] Eigenvalues/eigenvectors con `np.linalg.eig()`
- [ ] SVD con `np.linalg.svd()`
- [ ] `linear_algebra.py` con tests pasando

### M√≥dulo 03: C√°lculo Multivariante (Semanas 6-7)
- [ ] Derivadas parciales calculadas
- [ ] Gradiente de funciones multivariables
- [ ] Gradient Descent implementado desde cero
- [ ] Efecto del learning rate entendido
- [ ] Chain Rule aplicada a funciones compuestas
- [ ] `calculus.py` con Gradient Descent funcional

#### Gradient Checking (v3.3 - Obligatorio)
- [ ] **`grad_check.py` implementado**
- [ ] **Valid√© derivadas de MSE, sigmoid y capa lineal**
- [ ] Error relativo < 10‚Åª‚Å∑ en todos los tests

### M√≥dulo 04: Probabilidad para ML (Semana 8)
- [ ] Teorema de Bayes explicado con ejemplo
- [ ] Gaussiana univariada: PDF implementada
- [ ] Gaussiana multivariada: concepto entendido
- [ ] MLE: conexi√≥n con Cross-Entropy explicada
- [ ] **Softmax con Log-Sum-Exp trick implementado (v3.3)**
- [ ] `probability.py` con tests pasando

#### Evaluaci√≥n (PB-8)
- [ ] **PB-8 ‚â• 75/100** y evaluado con la r√∫brica
- [ ] Post-mortem: 3 fallos registrados en `study_tools/DIARIO_ERRORES.md`

---

## ü§ñ Fase 2: N√∫cleo de ML (Semanas 9-20) ‚≠ê PATHWAY

### M√≥dulo 05: Supervised Learning (Semanas 9-12)

#### Conocimiento
- [ ] Regresi√≥n lineal (Normal Equation + GD)
- [ ] MSE y su gradiente derivado
- [ ] Regresi√≥n log√≠stica desde cero
- [ ] Sigmoid y binary cross-entropy
- [ ] Matriz de confusi√≥n (TP, TN, FP, FN)
- [ ] Accuracy, Precision, Recall, F1 implementados
- [ ] Train/test split manual
- [ ] K-fold cross validation
- [ ] Regularizaci√≥n L2 (Ridge)

#### Derivaci√≥n Anal√≠tica (v3.2 - Obligatorio)
- [ ] **Deriv√© el gradiente de Cross-Entropy a mano**
- [ ] **Documento con derivaci√≥n completa (Markdown o LaTeX)**

#### Metodolog√≠a Feynman
- [ ] Puedo explicar sigmoid vs softmax en 5 l√≠neas

### M√≥dulo 06: Unsupervised Learning (Semanas 13-16)
- [ ] K-Means con K-Means++ initialization
- [ ] Algoritmo de Lloyd (asignar-actualizar-repetir)
- [ ] Inercia y m√©todo del codo
- [ ] PCA usando SVD (`np.linalg.svd()`)
- [ ] Varianza explicada y elecci√≥n de n_components
- [ ] Reconstrucci√≥n desde componentes principales
- [ ] `kmeans.py` y `pca.py` con tests pasando

#### Evaluaci√≥n (PB-16)
- [ ] **PB-16 ‚â• 75/100** y evaluado con la r√∫brica
- [ ] Cierre de m√≥dulo: r√∫brica completa aplicada (auto + IA/pareja)

### M√≥dulo 07: Deep Learning + CNNs (Semanas 17-20)

#### Conocimiento
- [ ] Neurona artificial y perceptr√≥n
- [ ] Sigmoid, ReLU, tanh, softmax + derivadas
- [ ] Problema XOR y su no-linealidad
- [ ] Forward pass para MLP
- [ ] Backpropagation con Chain Rule
- [ ] SGD, Momentum, Adam implementados
- [ ] Red resuelve problema XOR
- [ ] **CNNs (teor√≠a):** convoluci√≥n, stride, padding, pooling

#### Derivaci√≥n Anal√≠tica (v3.2 - Obligatorio)
- [ ] **Deriv√© las ecuaciones de backprop para red de 2 capas**
- [ ] **Diagrama de grafo computacional**

#### Metodolog√≠a Feynman
- [ ] Puedo explicar backpropagation en 5 l√≠neas sin jerga

#### Cierre de m√≥dulo (Semana 20)
- [ ] R√∫brica completa aplicada (auto + IA/pareja)

---

## üéØ Fase 3: Proyecto MNIST Analyst (Semanas 21-24)

### Semana 21: EDA + No Supervisado
- [ ] MNIST cargado y normalizado
- [ ] PCA reduce a 2D con visualizaci√≥n
- [ ] Varianza explicada analizada
- [ ] K-Means agrupa d√≠gitos sin etiquetas
- [ ] Centroides visualizados como im√°genes 28x28

### Semana 22: Clasificaci√≥n Supervisada
- [ ] Logistic Regression One-vs-All implementado
- [ ] Accuracy > 85% en test set
- [ ] Precision, Recall, F1 por clase
- [ ] Matriz de confusi√≥n analizada
- [ ] Errores visualizados (im√°genes mal clasificadas)

### Semana 23: Deep Learning
- [ ] MLP 784‚Üí128‚Üí64‚Üí10 implementado
- [ ] Forward y backward pass funcionales
- [ ] Mini-batch SGD funcionando
- [ ] Accuracy > 90% en test set

#### Evaluaci√≥n (PB-23 / Examen de admisi√≥n simulado)
- [ ] **PB-23 ‚â• 80/100** (requisito duro) y evaluado con la r√∫brica

### Semana 24: Benchmark + Informe
- [ ] Comparaci√≥n MLP vs Logistic Regression
- [ ] `MODEL_COMPARISON.md` explicando diferencias
- [ ] `README.md` profesional en ingl√©s
- [ ] Demo notebook completo

### Requisitos v3.2 (Obligatorios)
- [ ] **An√°lisis Bias-Variance** con experimento pr√°ctico (3 tama√±os de MLP)
- [ ] **Notebook en formato Paper** (Abstract, Methods, Results, Discussion)
- [ ] `mypy src/` pasa sin errores en todo el proyecto
- [ ] `pytest tests/` con cobertura significativa

### Metodolog√≠a Feynman
- [ ] Puedo explicar Bias vs Variance en 5 l√≠neas
- [ ] Puedo explicar por qu√© MLP supera a Logistic en 5 l√≠neas

---

## üíª C√≥digo

### Estructura del Proyecto MNIST
```
mnist-analyst/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py
‚îÇ   ‚îú‚îÄ‚îÄ linear_algebra.py
‚îÇ   ‚îú‚îÄ‚îÄ probability.py
‚îÇ   ‚îú‚îÄ‚îÄ pca.py
‚îÇ   ‚îú‚îÄ‚îÄ kmeans.py
‚îÇ   ‚îú‚îÄ‚îÄ logistic_regression.py
‚îÇ   ‚îú‚îÄ‚îÄ neural_network.py
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îî‚îÄ‚îÄ pipeline.py
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_eda_pca_kmeans.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_logistic_classification.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 03_neural_network_benchmark.ipynb
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_*.py
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ MODEL_COMPARISON.md
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt
```

### Calidad de C√≥digo
- [ ] Type hints en todas las funciones
- [ ] Docstrings con Args, Returns
- [ ] `mypy src/` pasa sin errores
- [ ] C√≥digo vectorizado (sin loops innecesarios)

### Tests
- [ ] Tests unitarios para cada m√≥dulo
- [ ] Tests para edge cases
- [ ] Todos los tests pasan

---

## üìù Documentaci√≥n

### README.md del Proyecto
- [ ] Descripci√≥n del proyecto
- [ ] Instrucciones de instalaci√≥n
- [ ] Ejemplo de uso
- [ ] Resultados y m√©tricas
- [ ] Escrito en ingl√©s

### MODEL_COMPARISON.md
- [ ] Tabla comparativa de modelos
- [ ] Explicaci√≥n matem√°tica de diferencias
- [ ] An√°lisis de PCA
- [ ] An√°lisis de K-Means
- [ ] Conclusiones

---

## üöÄ Verificaci√≥n Final

```bash
# 1. Tests
python -m pytest tests/ -v

# 2. Pipeline completo
python -c "
from src.pipeline import run_mnist_pipeline
# Ejecutar pipeline demo
"

# 3. Verificar accuracy
# Logistic Regression: > 85%
# Neural Network: > 90%
```

---

## ‚úÖ Declaraci√≥n de Completitud

### Por Fase

- [ ] **Fase 1:** Fundamentos matem√°ticos dominados
- [ ] **Fase 2:** Algoritmos ML implementados desde cero
- [ ] **Fase 3:** Proyecto MNIST completo

### Por Curso del Pathway

- [ ] **Supervised Learning:** Regresi√≥n + Clasificaci√≥n
- [ ] **Unsupervised Learning:** K-Means + PCA
- [ ] **Deep Learning:** MLP con Backpropagation

### M√©tricas Finales

| M√©trica | Objetivo | Logrado |
|---------|----------|---------|
| Logistic Regression Accuracy | >85% | ___% |
| Neural Network Accuracy | >90% | ___% |
| M√≥dulos completados | 8/8 | ___/8 |
| Tests pasando | 100% | ___% |

**Fecha de completitud:** _______________

**Listo para el MS in AI Pathway - L√≠nea 1:** ‚òê S√≠ ‚òê No
