# üåâ Puente NumPy ‚Üí PyTorch

> **Semana 24**: Toma tu clase NeuralNetwork hecha desde cero y reescr√≠bela en PyTorch.
> Ver√°s que tus 100 l√≠neas se convierten en 5. Entender√°s exactamente qu√© hace cada una.

---

## üéØ Objetivo

Esta gu√≠a te conecta con la industria y las herramientas de cursos avanzados.
Al terminar, tendr√°s la "iluminaci√≥n" de saber QU√â hace PyTorch por ti.

---

## üìä Tabla de Equivalencias NumPy ‚Üî PyTorch

### Operaciones B√°sicas

| Operaci√≥n | NumPy | PyTorch |
|-----------|-------|---------|
| Crear array/tensor | `np.array([1, 2, 3])` | `torch.tensor([1, 2, 3])` |
| Zeros | `np.zeros((3, 4))` | `torch.zeros(3, 4)` |
| Ones | `np.ones((3, 4))` | `torch.ones(3, 4)` |
| Random normal | `np.random.randn(3, 4)` | `torch.randn(3, 4)` |
| Shape | `x.shape` | `x.shape` o `x.size()` |
| Reshape | `x.reshape(2, 6)` | `x.reshape(2, 6)` o `x.view(2, 6)` |
| Transponer | `x.T` | `x.T` o `x.t()` |
| Producto matricial | `A @ B` o `np.dot(A, B)` | `A @ B` o `torch.mm(A, B)` |
| Elemento a elemento | `A * B` | `A * B` |
| Suma | `np.sum(x, axis=0)` | `torch.sum(x, dim=0)` |
| Mean | `np.mean(x, axis=1)` | `torch.mean(x, dim=1)` |
| Max | `np.max(x)` | `torch.max(x)` |
| Argmax | `np.argmax(x, axis=1)` | `torch.argmax(x, dim=1)` |
| Concatenar | `np.concatenate([a, b])` | `torch.cat([a, b])` |
| Stack | `np.stack([a, b])` | `torch.stack([a, b])` |

### Funciones Matem√°ticas

| Operaci√≥n | NumPy | PyTorch |
|-----------|-------|---------|
| Exponencial | `np.exp(x)` | `torch.exp(x)` |
| Logaritmo | `np.log(x)` | `torch.log(x)` |
| ReLU | `np.maximum(0, x)` | `torch.relu(x)` o `F.relu(x)` |
| Sigmoid | `1 / (1 + np.exp(-x))` | `torch.sigmoid(x)` |
| Tanh | `np.tanh(x)` | `torch.tanh(x)` |
| Softmax | manual | `F.softmax(x, dim=1)` |

### Conversi√≥n

| Direcci√≥n | C√≥digo |
|-----------|--------|
| NumPy ‚Üí PyTorch | `torch.from_numpy(np_array)` |
| PyTorch ‚Üí NumPy | `tensor.numpy()` (CPU) o `tensor.cpu().numpy()` (GPU) |
| PyTorch ‚Üí Python | `tensor.item()` (escalar) |

---

## üîÑ Ejercicio de Traducci√≥n: Red Neuronal Completa

### Tu C√≥digo NumPy (100+ l√≠neas)

```python
import numpy as np

class NeuralNetworkNumPy:
    """Red neuronal desde cero con NumPy."""
    
    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        # Inicializaci√≥n Xavier
        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)
        self.b2 = np.zeros((1, output_size))
        
        # Cache para backward
        self.cache = {}
        
    def relu(self, z: np.ndarray) -> np.ndarray:
        return np.maximum(0, z)
    
    def relu_derivative(self, z: np.ndarray) -> np.ndarray:
        return (z > 0).astype(float)
    
    def softmax(self, z: np.ndarray) -> np.ndarray:
        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Numerical stability
        return exp_z / np.sum(exp_z, axis=1, keepdims=True)
    
    def forward(self, X: np.ndarray) -> np.ndarray:
        # Capa 1
        self.cache['X'] = X
        self.cache['Z1'] = X @ self.W1 + self.b1
        self.cache['A1'] = self.relu(self.cache['Z1'])
        
        # Capa 2
        self.cache['Z2'] = self.cache['A1'] @ self.W2 + self.b2
        self.cache['A2'] = self.softmax(self.cache['Z2'])
        
        return self.cache['A2']
    
    def cross_entropy_loss(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:
        m = y_true.shape[0]
        # Evitar log(0)
        log_probs = -np.log(y_pred[range(m), y_true.argmax(axis=1)] + 1e-8)
        return np.mean(log_probs)
    
    def backward(self, y_true: np.ndarray) -> dict:
        m = y_true.shape[0]
        grads = {}
        
        # Gradiente de softmax + cross-entropy (simplificado)
        dZ2 = self.cache['A2'] - y_true  # (m, output_size)
        
        # Gradientes capa 2
        grads['dW2'] = (1/m) * self.cache['A1'].T @ dZ2
        grads['db2'] = (1/m) * np.sum(dZ2, axis=0, keepdims=True)
        
        # Propagar hacia atr√°s
        dA1 = dZ2 @ self.W2.T
        dZ1 = dA1 * self.relu_derivative(self.cache['Z1'])
        
        # Gradientes capa 1
        grads['dW1'] = (1/m) * self.cache['X'].T @ dZ1
        grads['db1'] = (1/m) * np.sum(dZ1, axis=0, keepdims=True)
        
        return grads
    
    def update_params(self, grads: dict, learning_rate: float):
        self.W1 -= learning_rate * grads['dW1']
        self.b1 -= learning_rate * grads['db1']
        self.W2 -= learning_rate * grads['dW2']
        self.b2 -= learning_rate * grads['db2']
    
    def train_step(self, X: np.ndarray, y: np.ndarray, learning_rate: float) -> float:
        # Forward
        y_pred = self.forward(X)
        loss = self.cross_entropy_loss(y_pred, y)
        
        # Backward
        grads = self.backward(y)
        
        # Update
        self.update_params(grads, learning_rate)
        
        return loss
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        probs = self.forward(X)
        return np.argmax(probs, axis=1)
```

---

### C√≥digo Equivalente en PyTorch (15 l√≠neas)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class NeuralNetworkPyTorch(nn.Module):
    """La misma red, pero PyTorch hace el trabajo pesado."""
    
    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)  # W1, b1 incluidos
        self.fc2 = nn.Linear(hidden_size, output_size)  # W2, b2 incluidos
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = F.relu(self.fc1(x))  # Capa 1 + ReLU
        x = self.fc2(x)          # Capa 2 (sin softmax, lo hace CrossEntropyLoss)
        return x
```

---

### Entrenamiento: NumPy vs PyTorch

#### NumPy (manual)

```python
# Crear modelo
model_np = NeuralNetworkNumPy(784, 128, 10)

# Training loop
for epoch in range(100):
    loss = model_np.train_step(X_train, y_train_onehot, learning_rate=0.01)
    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")
```

#### PyTorch (autom√°tico)

```python
# Crear modelo
model_pt = NeuralNetworkPyTorch(784, 128, 10)

# Loss y optimizer (PyTorch los maneja)
criterion = nn.CrossEntropyLoss()  # Incluye softmax!
optimizer = torch.optim.SGD(model_pt.parameters(), lr=0.01)

# Training loop
for epoch in range(100):
    optimizer.zero_grad()           # Limpiar gradientes anteriores
    
    outputs = model_pt(X_train_t)   # Forward pass
    loss = criterion(outputs, y_train_t)  # Calcular loss
    
    loss.backward()                 # Backward pass (autom√°tico!)
    optimizer.step()                # Actualizar pesos
    
    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")
```

---

## üîç Mapeo L√≠nea por L√≠nea

| Lo que T√ö escribiste (NumPy) | Lo que PyTorch hace por ti |
|------------------------------|---------------------------|
| `self.W1 = np.random.randn(...) * np.sqrt(2.0/n)` | `nn.Linear` inicializa autom√°ticamente (Kaiming) |
| `self.b1 = np.zeros(...)` | `nn.Linear` incluye bias autom√°ticamente |
| `self.cache['Z1'] = X @ self.W1 + self.b1` | `self.fc1(x)` |
| `self.relu(z)` | `F.relu(x)` |
| `self.softmax(z)` | Incluido en `nn.CrossEntropyLoss` |
| Todo tu `backward()` | `loss.backward()` (Autograd!) |
| `self.W1 -= lr * grads['dW1']` | `optimizer.step()` |
| `self.cache = {}` | PyTorch mantiene el grafo autom√°ticamente |

---

## üß™ Verificaci√≥n: Mismo Resultado

```python
import numpy as np
import torch
import torch.nn as nn

def verificar_equivalencia():
    """Verifica que ambas implementaciones dan el mismo resultado."""
    
    np.random.seed(42)
    torch.manual_seed(42)
    
    # Datos de prueba
    X = np.random.randn(10, 784).astype(np.float32)
    y = np.eye(10)[np.random.randint(0, 10, 10)]  # One-hot
    
    # Modelo NumPy
    model_np = NeuralNetworkNumPy(784, 128, 10)
    
    # Modelo PyTorch (copiar pesos del modelo NumPy)
    model_pt = NeuralNetworkPyTorch(784, 128, 10)
    with torch.no_grad():
        model_pt.fc1.weight.copy_(torch.from_numpy(model_np.W1.T))
        model_pt.fc1.bias.copy_(torch.from_numpy(model_np.b1.flatten()))
        model_pt.fc2.weight.copy_(torch.from_numpy(model_np.W2.T))
        model_pt.fc2.bias.copy_(torch.from_numpy(model_np.b2.flatten()))
    
    # Forward pass NumPy
    output_np = model_np.forward(X)
    
    # Forward pass PyTorch
    X_t = torch.from_numpy(X)
    with torch.no_grad():
        output_pt = torch.softmax(model_pt(X_t), dim=1).numpy()
    
    # Comparar
    diff = np.abs(output_np - output_pt).max()
    print(f"Diferencia m√°xima en outputs: {diff:.2e}")
    assert diff < 1e-5, "¬°Los outputs no coinciden!"
    print("‚úÖ ¬°Verificaci√≥n exitosa! Ambos modelos son equivalentes.")

verificar_equivalencia()
```

---

## üöÄ Ventajas de PyTorch que Ahora Entiendes

| Feature | Qu√© hace | Por qu√© es poderoso |
|---------|----------|---------------------|
| **Autograd** | Calcula gradientes autom√°ticamente | No m√°s errores en backprop manual |
| **nn.Module** | Organiza par√°metros | `model.parameters()` los encuentra todos |
| **Optimizers** | SGD, Adam, etc. listo para usar | No reimplementar momentum, weight decay |
| **GPU** | `.to('cuda')` | Entrenamiento 10-100x m√°s r√°pido |
| **DataLoader** | Batching, shuffling autom√°tico | No m√°s `X[i:i+batch_size]` |
| **Loss Functions** | CrossEntropy, MSE, etc. | Num√©ricamente estables |

---

## üìù Ejercicio Final: MNIST en PyTorch

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# 1. Cargar datos (1 l√≠nea vs tu c√≥digo de preprocesamiento)
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])
train_data = datasets.MNIST('data', train=True, download=True, transform=transform)
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)

# 2. Definir modelo (tu clase de 100 l√≠neas ‚Üí 8 l√≠neas)
class MNISTNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)
        
    def forward(self, x):
        x = x.view(-1, 784)  # Flatten
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

# 3. Entrenar (tu training loop simplificado)
model = MNISTNet()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

for epoch in range(5):
    total_loss = 0
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    
    print(f"Epoch {epoch+1}, Avg Loss: {total_loss/len(train_loader):.4f}")

# 4. Evaluar
model.eval()
correct = 0
with torch.no_grad():
    for data, target in train_loader:
        output = model(data)
        pred = output.argmax(dim=1)
        correct += (pred == target).sum().item()

print(f"Accuracy: {100*correct/len(train_data):.2f}%")
```

---

## ‚úÖ Checklist de "Iluminaci√≥n"

Despu√©s de este ejercicio, deber√≠as poder responder:

- [ ] ¬øQu√© hace `nn.Linear` internamente?
- [ ] ¬øPor qu√© `nn.CrossEntropyLoss` no necesita softmax expl√≠cito?
- [ ] ¬øQu√© hace `loss.backward()` exactamente?
- [ ] ¬øPor qu√© necesitamos `optimizer.zero_grad()`?
- [ ] ¬øQu√© es `model.parameters()` y por qu√© funciona?
- [ ] ¬øC√≥mo mover√≠a este modelo a GPU?

---

## üéì Conclusi√≥n

> "Despu√©s de implementar todo desde cero, PyTorch no es magia. Es automatizaci√≥n de lo que ya sabes hacer."

Ahora est√°s listo para:
1. Cursos avanzados de la maestr√≠a que usan PyTorch/TensorFlow
2. Entender papers que usan frameworks
3. Debuggear modelos porque sabes QU√â pasa por debajo
4. Extender PyTorch con operaciones custom si es necesario
