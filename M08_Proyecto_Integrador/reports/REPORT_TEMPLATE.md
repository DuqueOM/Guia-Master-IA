# NLP Disaster Tweet Classification - Final Report

**Autor:** [Tu Nombre]
**Fecha:** [Fecha de entrega]
**Curso:** MS-AI Pathway Capstone Project

---

## Abstract

[Escribir un resumen de 150 palabras m√°ximo que incluya: problema abordado, metodolog√≠a principal, resultados clave, y conclusi√≥n principal.]

---

## 1. Introduction

### 1.1 Problem Context

La detecci√≥n temprana de desastres a trav√©s de redes sociales se ha convertido en una herramienta cr√≠tica para servicios de emergencia. Twitter, con m√°s de 500 millones de tweets diarios, ofrece informaci√≥n en tiempo real que puede salvar vidas.

Sin embargo, el lenguaje humano presenta un desaf√≠o fundamental: las mismas palabras pueden usarse de forma literal ("There's a fire in the building!") o figurativa ("This song is fire! üî•").

### 1.2 Problem Statement

**Objetivo:** Desarrollar un clasificador binario que distinga tweets sobre desastres reales de aquellos que usan lenguaje metaf√≥rico o no relacionado con emergencias.

### 1.3 Contributions

Este proyecto presenta:
1. Un pipeline completo de preprocesamiento de texto para tweets
2. Comparaci√≥n sistem√°tica de modelos baseline (TF-IDF + ML cl√°sico)
3. Implementaci√≥n de arquitectura LSTM bidireccional con embeddings pre-entrenados
4. Fine-tuning de BERT para clasificaci√≥n de texto
5. An√°lisis detallado de errores y recomendaciones para producci√≥n

---

## 2. Dataset Description

### 2.1 Data Source

- **Fuente:** Kaggle Competition "Natural Language Processing with Disaster Tweets"
- **URL:** https://www.kaggle.com/c/nlp-getting-started

### 2.2 Dataset Statistics

| Caracter√≠stica | Valor |
|----------------|-------|
| Total de muestras (train) | 7,613 |
| Clase 0 (No desastre) | 4,342 (57.0%) |
| Clase 1 (Desastre real) | 3,271 (43.0%) |
| Longitud promedio de tweet | XX palabras |
| Tweets con keyword | 7,552 (99.2%) |
| Tweets con location | 5,080 (66.7%) |

### 2.3 Exploratory Data Analysis

[Incluir visualizaciones:]
- Distribuci√≥n de clases (gr√°fico de barras)
- Distribuci√≥n de longitud de tweets por clase
- WordClouds comparativos (desastre vs no-desastre)
- Top 20 palabras m√°s frecuentes por clase

### 2.4 Data Challenges

1. **Desbalance moderado:** Ratio 57:43, manejable pero requiere m√©tricas apropiadas
2. **Ruido en texto:** URLs, menciones, hashtags, emojis, errores tipogr√°ficos
3. **Ambig√ºedad sem√°ntica:** "fire", "crash", "explosion" usados metaf√≥ricamente
4. **Valores faltantes:** ~33% de tweets sin ubicaci√≥n

---

## 3. Methodology

### 3.1 Preprocessing Pipeline

```python
# Descripci√≥n del pipeline implementado
1. Conversi√≥n a min√∫sculas
2. Eliminaci√≥n de URLs (regex: http\S+)
3. Eliminaci√≥n de menciones (@usuario)
4. Procesamiento de hashtags (conservar palabra)
5. Eliminaci√≥n de HTML tags
6. Eliminaci√≥n de caracteres especiales
7. Tokenizaci√≥n (NLTK word_tokenize)
8. Lematizaci√≥n (WordNetLemmatizer)
9. Eliminaci√≥n de stopwords (opcional)
```

**Decisiones de dise√±o:**
- Se opt√≥ por lematizaci√≥n sobre stemming para preservar palabras v√°lidas
- Se conserv√≥ el contenido de hashtags (#earthquake ‚Üí earthquake)
- No se eliminaron stopwords en modelos deep learning (LSTM captura contexto)

### 3.2 Feature Engineering

#### TF-IDF Vectorization

```python
TfidfVectorizer(
    max_features=5000,
    ngram_range=(1, 2),
    min_df=2,
    max_df=0.95,
    sublinear_tf=True
)
```

**Justificaci√≥n de par√°metros:**
- `max_features=5000`: Balance entre informaci√≥n y dimensionalidad
- `ngram_range=(1,2)`: Captura frases como "breaking news", "stay safe"
- `sublinear_tf=True`: Reduce impacto de t√©rminos muy frecuentes

#### Word Embeddings

- **GloVe 100d:** Pre-entrenados en 6B tokens de Wikipedia + Gigaword
- **Cobertura de vocabulario:** XX% de palabras del dataset

### 3.3 Model Architectures

#### Baseline Models

1. **Logistic Regression**
   - Regularizaci√≥n L2 (C=1.0)
   - class_weight='balanced'

2. **Multinomial Naive Bayes**
   - Laplace smoothing (alpha=1.0)

#### Deep Learning: Bidirectional LSTM

```
Input (max_length=100)
    ‚Üì
Embedding (100d, GloVe pre-trained, frozen)
    ‚Üì
Bidirectional LSTM (64 units, return_sequences=True)
    ‚Üì
Dropout (0.3)
    ‚Üì
Bidirectional LSTM (32 units)
    ‚Üì
Dropout (0.3)
    ‚Üì
Dense (64, ReLU)
    ‚Üì
Dropout (0.5)
    ‚Üì
Dense (1, Sigmoid)
```

**Regularizaci√≥n:** Dropout + EarlyStopping (patience=5)

#### Transfer Learning: BERT

- **Modelo base:** bert-base-uncased
- **Fine-tuning:** Learning rate 2e-5, 3 epochs
- **Max sequence length:** 128 tokens

---

## 4. Experiments

### 4.1 Experimental Setup

- **Split:** 80% train, 20% test (stratified)
- **Validation:** 20% of training set for early stopping
- **Random seed:** 42 (reproducibilidad)
- **Hardware:** [Especificar GPU/CPU]

### 4.2 Evaluation Metrics

Dado el desbalance moderado de clases, se prioriz√≥ **F1-Score** sobre Accuracy.

- **Precision:** Proporci√≥n de predicciones positivas correctas
- **Recall:** Proporci√≥n de positivos reales detectados
- **F1-Score:** Media arm√≥nica de Precision y Recall

### 4.3 Results

| Model | Precision | Recall | F1-Score | Accuracy | Training Time |
|-------|-----------|--------|----------|----------|---------------|
| Logistic Regression + TF-IDF | X.XX | X.XX | X.XX | X.XX | Xs |
| Naive Bayes + BoW | X.XX | X.XX | X.XX | X.XX | Xs |
| Bi-LSTM + GloVe | X.XX | X.XX | X.XX | X.XX | Xm |
| BERT fine-tuned | X.XX | X.XX | X.XX | X.XX | Xm |

[Incluir gr√°ficos:]
- Matrices de confusi√≥n para cada modelo
- Curvas de aprendizaje (loss/accuracy vs epochs) para modelos DL
- Comparaci√≥n de F1-Score (gr√°fico de barras)

---

## 5. Results & Discussion

### 5.1 Model Performance Analysis

[Analizar resultados de la tabla anterior]

**Observaciones clave:**
1. [Insight 1]
2. [Insight 2]
3. [Insight 3]

### 5.2 Error Analysis

#### Falsos Positivos (Predijo desastre, era met√°fora)

| Tweet | Predicci√≥n | An√°lisis |
|-------|------------|----------|
| "My heart is on fire for you" | 1 (Desastre) | Uso metaf√≥rico de "fire" |
| [M√°s ejemplos] | | |

#### Falsos Negativos (No detect√≥ desastre real)

| Tweet | Predicci√≥n | An√°lisis |
|-------|------------|----------|
| "Prayers for the victims" | 0 (No desastre) | No menciona desastre expl√≠citamente |
| [M√°s ejemplos] | | |

### 5.3 Feature Importance (Modelos Lineales)

Top 10 features m√°s predictivas para clase "Desastre":
1. [feature 1]: coeficiente X.XX
2. [feature 2]: coeficiente X.XX
...

### 5.4 Limitations

1. **Dataset:** Solo tweets en ingl√©s, puede no generalizar a otros idiomas
2. **Temporalidad:** Entrenado en datos de 2015-2019, nuevos tipos de desastres no cubiertos
3. **Contexto:** No se usa informaci√≥n de keywords o location en modelos finales
4. **Sarcasmo:** Dif√≠cil de detectar sin contexto adicional

---

## 6. Conclusion

### 6.1 Summary

[Resumir hallazgos principales en 2-3 p√°rrafos]

### 6.2 Recommended Model for Production

**Recomendaci√≥n:** [Modelo recomendado]

**Justificaci√≥n:**
- Performance: F1-Score de X.XX
- Latencia: Xms por predicci√≥n
- Complejidad: [F√°cil/Media/Alta] de deployar

### 6.3 Future Work

1. **Ensemble:** Combinar predicciones de m√∫ltiples modelos
2. **Data augmentation:** Back-translation, synonym replacement
3. **Multimodal:** Incorporar im√°genes adjuntas a tweets
4. **Real-time:** Streaming pipeline con Kafka/Spark
5. **Multilingual:** Extender a espa√±ol, franc√©s, etc.

---

## References

1. Devlin, J., et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805.

2. Pennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors for Word Representation. EMNLP.

3. Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation.

4. [Agregar m√°s referencias utilizadas]

---

## Appendix

### A. Code Repository Structure

```
M08_Proyecto_Integrador/
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_EDA_Preprocessing.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_Baseline_Models.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 03_Deep_Learning_LSTM.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 04_Transfer_Learning_BERT.ipynb
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py
‚îÇ   ‚îú‚îÄ‚îÄ features.py
‚îÇ   ‚îú‚îÄ‚îÄ models.py
‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ baseline_logreg.pkl
‚îÇ   ‚îî‚îÄ‚îÄ lstm_best.h5
‚îî‚îÄ‚îÄ reports/
    ‚îî‚îÄ‚îÄ REPORT.md
```

### B. Hyperparameter Tuning Results

[Tabla con resultados de grid search si se realiz√≥]

### C. Additional Visualizations

[Gr√°ficos adicionales que no cupieron en el cuerpo principal]
