# üìã R√∫brica de Evaluaci√≥n: Proyecto Disaster Tweets

> **Curso**: CSCA 5642 - Deep Learning (Capstone Project)
> **Proyecto**: Clasificaci√≥n de Tweets de Desastres usando NLP
> **Puntuaci√≥n Total**: 100 puntos

---

## üéØ Objetivo del Proyecto

Construir un pipeline completo de NLP para clasificar tweets como relacionados con desastres reales o no, demostrando dominio de:
- Preprocesamiento de texto
- Modelos baseline de ML cl√°sico
- Arquitecturas de Deep Learning (LSTM/GRU)
- Transfer Learning (BERT)

---

## üìä Distribuci√≥n de Puntos

| Categor√≠a | Puntos | Peso |
|-----------|--------|------|
| 1. Limpieza de Datos | 20 | 20% |
| 2. Modelado Base | 20 | 20% |
| 3. Deep Learning | 30 | 30% |
| 4. Reporte y Comunicaci√≥n | 30 | 30% |
| **TOTAL** | **100** | **100%** |

---

## 1Ô∏è‚É£ Limpieza de Datos (20 puntos)

### Criterios de Evaluaci√≥n

| Criterio | Puntos | Descripci√≥n |
|----------|--------|-------------|
| **1.1 Manejo de URLs** | 4 | Detecta y remueve/reemplaza URLs con regex apropiado |
| **1.2 Manejo de HTML** | 4 | Elimina tags HTML (`<br>`, `&amp;`, etc.) correctamente |
| **1.3 Stopwords** | 4 | Implementa remoci√≥n de stopwords (NLTK o custom) |
| **1.4 Tokenizaci√≥n** | 4 | Usa tokenizador apropiado para tweets (maneja @mentions, #hashtags) |
| **1.5 Normalizaci√≥n** | 4 | Aplica lowercasing, lemmatization/stemming seg√∫n corresponda |

### Niveles de Desempe√±o

| Nivel | Puntos | Descripci√≥n |
|-------|--------|-------------|
| **Excelente** | 18-20 | Pipeline robusto que maneja todos los casos edge. C√≥digo modular y reutilizable. |
| **Competente** | 14-17 | Cubre los 5 criterios pero puede faltar manejo de casos especiales. |
| **En Desarrollo** | 10-13 | Implementa 3-4 criterios. C√≥digo funcional pero no robusto. |
| **Insuficiente** | 0-9 | Menos de 3 criterios implementados o errores cr√≠ticos. |

### Checklist de Auto-evaluaci√≥n

```
[ ] ¬øMi regex para URLs captura http, https, y www?
[ ] ¬øManejo correctamente emojis y caracteres especiales?
[ ] ¬øPreservo informaci√≥n √∫til de hashtags (#earthquake ‚Üí earthquake)?
[ ] ¬øMi pipeline es reproducible (misma entrada ‚Üí misma salida)?
[ ] ¬øDocument√© las decisiones de preprocesamiento?
```

---

## 2Ô∏è‚É£ Modelado Base (20 puntos)

### Criterios de Evaluaci√≥n

| Criterio | Puntos | Descripci√≥n |
|----------|--------|-------------|
| **2.1 Vectorizaci√≥n TF-IDF** | 5 | Implementa TF-IDF con par√°metros justificados (ngram_range, max_features) |
| **2.2 Modelo Naive Bayes** | 5 | Entrena MultinomialNB y reporta m√©tricas |
| **2.3 Modelo Logistic Regression** | 5 | Entrena LogReg con regularizaci√≥n y reporta m√©tricas |
| **2.4 Comparaci√≥n de Baselines** | 5 | Tabla comparativa con Accuracy, Precision, Recall, F1 |

### Niveles de Desempe√±o

| Nivel | Puntos | Descripci√≥n |
|-------|--------|-------------|
| **Excelente** | 18-20 | Ambos modelos implementados. Justifica elecci√≥n de hiperpar√°metros. F1 > 0.75. |
| **Competente** | 14-17 | Modelos funcionales. M√©tricas reportadas correctamente. |
| **En Desarrollo** | 10-13 | Solo un modelo o m√©tricas incompletas. |
| **Insuficiente** | 0-9 | Modelos no funcionales o ausencia de m√©tricas. |

### Checklist de Auto-evaluaci√≥n

```
[ ] ¬øUs√© train_test_split ANTES de fit TF-IDF? (evitar data leakage)
[ ] ¬øReport√© F1-Score adem√°s de Accuracy? (dataset desbalanceado)
[ ] ¬øProb√© diferentes valores de ngram_range?
[ ] ¬øCompar√© al menos 2 modelos baseline?
```

---

## 3Ô∏è‚É£ Deep Learning (30 puntos)

### Criterios de Evaluaci√≥n

| Criterio | Puntos | Descripci√≥n |
|----------|--------|-------------|
| **3.1 Word Embeddings** | 8 | Usa embeddings (trainable o pre-trained como GloVe) |
| **3.2 Arquitectura LSTM/GRU** | 8 | Implementa red recurrente bidireccional |
| **3.3 Regularizaci√≥n** | 7 | Aplica Dropout, Early Stopping, o L2 para evitar overfitting |
| **3.4 Curvas de Aprendizaje** | 7 | Grafica loss/accuracy en train vs validation |

### Niveles de Desempe√±o

| Nivel | Puntos | Descripci√≥n |
|-------|--------|-------------|
| **Excelente** | 27-30 | BiLSTM con GloVe, m√∫ltiples t√©cnicas de regularizaci√≥n, curvas claras. F1 > 0.78. |
| **Competente** | 21-26 | LSTM funcional con al menos una t√©cnica de regularizaci√≥n. |
| **En Desarrollo** | 15-20 | Modelo entrena pero hay overfitting evidente o arquitectura b√°sica. |
| **Insuficiente** | 0-14 | Modelo no entrena o errores fundamentales en arquitectura. |

### Checklist de Auto-evaluaci√≥n

```
[ ] ¬øMi modelo usa Bidirectional LSTM/GRU?
[ ] ¬øImplement√© al menos 2 t√©cnicas anti-overfitting?
[ ] ¬øLas curvas de aprendizaje muestran convergencia sin overfitting severo?
[ ] ¬øPuedo explicar por qu√© eleg√≠ esa arquitectura espec√≠fica?
[ ] ¬øProb√© diferentes valores de embedding_dim y lstm_units?
```

### Arquitectura M√≠nima Esperada

```python
# Ejemplo de arquitectura que cumple los criterios
model = Sequential([
    Embedding(vocab_size, 100, weights=[glove_matrix], trainable=False),
    SpatialDropout1D(0.3),
    Bidirectional(LSTM(64, return_sequences=True)),
    GlobalMaxPooling1D(),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])
```

---

## 4Ô∏è‚É£ Reporte y Comunicaci√≥n (30 puntos)

### Criterios de Evaluaci√≥n

| Criterio | Puntos | Descripci√≥n |
|----------|--------|-------------|
| **4.1 Justificaci√≥n de Arquitectura** | 10 | Explica POR QU√â se eligi√≥ cada componente del modelo |
| **4.2 An√°lisis de Matriz de Confusi√≥n** | 8 | Interpreta FP, FN y sus implicaciones en el contexto de desastres |
| **4.3 Comparaci√≥n de Modelos** | 7 | Tabla final comparando Baseline vs LSTM vs BERT (si aplica) |
| **4.4 Conclusiones y Limitaciones** | 5 | Discute limitaciones y posibles mejoras futuras |

### Niveles de Desempe√±o

| Nivel | Puntos | Descripci√≥n |
|-------|--------|-------------|
| **Excelente** | 27-30 | Reporte publicable. Narrativa clara. Visualizaciones profesionales. |
| **Competente** | 21-26 | Cubre todos los criterios. Explicaciones correctas pero pueden ser m√°s profundas. |
| **En Desarrollo** | 15-20 | Reporte incompleto o an√°lisis superficial. |
| **Insuficiente** | 0-14 | Sin reporte o sin an√°lisis de resultados. |

### Checklist de Auto-evaluaci√≥n

```
[ ] ¬øExplico por qu√© BiLSTM es mejor que LSTM unidireccional para este problema?
[ ] ¬øDiscuto qu√© significa un False Positive en el contexto de alertas de desastre?
[ ] ¬øMi tabla comparativa incluye al menos 3 modelos?
[ ] ¬øMenciono al menos 2 limitaciones de mi enfoque?
[ ] ¬øPropongo mejoras concretas para trabajo futuro?
```

### Preguntas Gu√≠a para el An√°lisis

1. **Falsos Positivos (FP)**: "El modelo predijo desastre pero no lo era"
   - ¬øQu√© tan grave es esto en un sistema de alertas real?

2. **Falsos Negativos (FN)**: "El modelo NO predijo desastre pero S√ç lo era"
   - ¬øCu√°l es el costo de no alertar sobre un desastre real?

3. **Trade-off Precision vs Recall**:
   - ¬øPrefiero m√°s FP o m√°s FN en este contexto?

---

## ‚õî FATAL FLAWS (Errores Fatales)

> **IMPORTANTE**: Los siguientes errores causan **REPROBACI√ìN AUTOM√ÅTICA** independientemente de la puntuaci√≥n en otras secciones.

### 1. Data Leakage (Fuga de Datos)

```python
# ‚ùå INCORRECTO - Causa reprobaci√≥n
tfidf = TfidfVectorizer()
X_tfidf = tfidf.fit_transform(df['text'])  # fit en TODO el dataset
X_train, X_test = train_test_split(X_tfidf, ...)

# ‚úÖ CORRECTO
X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], ...)
tfidf = TfidfVectorizer()
X_train_tfidf = tfidf.fit_transform(X_train)  # fit SOLO en train
X_test_tfidf = tfidf.transform(X_test)        # transform en test
```

**¬øPor qu√© es fatal?** El modelo "ve" informaci√≥n del test set durante el entrenamiento, inflando artificialmente las m√©tricas.

### 2. No Reportar M√©tricas Apropiadas

```python
# ‚ùå INCORRECTO - Solo reportar Accuracy en dataset desbalanceado
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")

# ‚úÖ CORRECTO - Reportar F1, Precision, Recall
print(classification_report(y_test, y_pred))
```

**¬øPor qu√© es fatal?** Con clases desbalanceadas, un modelo que predice siempre la clase mayoritaria puede tener alta accuracy pero ser in√∫til.

### 3. Modelo No Reproducible

- No fijar `random_state` en train_test_split
- No fijar seeds de NumPy/TensorFlow
- Resultados var√≠an significativamente entre ejecuciones

### 4. Plagio o C√≥digo Copiado sin Atribuci√≥n

- Copiar c√≥digo de Kaggle/GitHub sin citar fuente
- Usar soluciones de otros estudiantes

### 5. Modelo No Entrena o Errores de Ejecuci√≥n

- Notebooks con celdas que fallan
- Modelo con accuracy ~50% (random guessing)
- No se puede reproducir el entrenamiento

---

## üìà Escala de Calificaci√≥n Final

| Puntuaci√≥n | Letra | Descripci√≥n |
|------------|-------|-------------|
| 90-100 | A | Excelente. Listo para portfolio profesional. |
| 80-89 | B | Competente. Cumple todos los objetivos con calidad. |
| 70-79 | C | Satisfactorio. Cumple requisitos m√≠nimos. |
| 60-69 | D | En desarrollo. Necesita mejoras significativas. |
| <60 | F | Insuficiente. No cumple requisitos m√≠nimos. |

---

## üîÑ Proceso de Peer Review Simulado

### Paso 1: Auto-evaluaci√≥n (Antes de entregar)
1. Completa TODOS los checklists de esta r√∫brica
2. Verifica que no tienes ning√∫n Fatal Flaw
3. Asigna puntos a cada categor√≠a honestamente

### Paso 2: Revisi√≥n Cruzada (Si trabajas en equipo)
1. Intercambia notebooks con un compa√±ero
2. Cada uno eval√∫a el trabajo del otro usando esta r√∫brica
3. Discutan discrepancias en las puntuaciones

### Paso 3: Reflexi√≥n Final
Responde en tu reporte:
- ¬øQu√© fue lo m√°s dif√≠cil del proyecto?
- ¬øQu√© har√≠as diferente con m√°s tiempo?
- ¬øQu√© aprendiste que no sab√≠as antes?

---

## üìö Recursos de Referencia

- [Kaggle Competition: Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started)
- [NLTK Documentation](https://www.nltk.org/)
- [Keras Text Classification Tutorial](https://keras.io/examples/nlp/)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers/)

---

## ‚úÖ Entregables Finales

1. **Notebooks** (4 archivos .ipynb):
   - `01_EDA_Preprocessing.ipynb`
   - `02_Baseline_Models.ipynb`
   - `03_Deep_Learning_LSTM.ipynb`
   - `04_Transfer_Learning_BERT.ipynb` (opcional para puntos extra)

2. **Reporte** (`REPORT.md`):
   - M√°ximo 2000 palabras
   - Incluir visualizaciones clave

3. **C√≥digo fuente** (`src/`):
   - M√≥dulos reutilizables de preprocessing y evaluation

---

*R√∫brica dise√±ada para el MS in AI - CU Boulder*
*Alineada con est√°ndares de CSCA 5642 - Deep Learning*
