# ğŸ“ Simulacro de Examen TeÃ³rico - SÃ¡bados

> El Pathway de CU Boulder tiene exÃ¡menes teÃ³ricos rigurosos, no solo cÃ³digo.
> Este documento entrena la resoluciÃ³n de problemas con lÃ¡piz y papel bajo presiÃ³n de tiempo.

---

## ğŸ“‹ Protocolo del Simulacro

### Reglas Estrictas
- â±ï¸ **1 hora mÃ¡ximo** por simulacro
- ğŸ“µ **Sin IDE, sin internet**
- ğŸ“ **Solo lÃ¡piz y papel**
- ğŸ§® **Calculadora bÃ¡sica permitida** (no cientÃ­fica)

### Formato
- 5-7 preguntas por simulacro
- Mezcla de cÃ¡lculo, Ã¡lgebra lineal, probabilidad y conceptos ML
- PuntuaciÃ³n: 100 puntos total

---

## ğŸ“š Banco de Preguntas por Fase

---

### FASE 1: Fundamentos MatemÃ¡ticos (Semanas 1-8)

#### Simulacro 1A: NumPy y Ãlgebra Lineal BÃ¡sica

**Pregunta 1 (15 pts)** - Operaciones con Matrices

Dadas las matrices:
```
A = [[1, 2],    B = [[5, 6],
     [3, 4]]         [7, 8]]
```

Calcula a mano:
a) A + B
b) A @ B (producto matricial)
c) A * B (Hadamard product)
d) A.T (transpuesta de A)

---

**Pregunta 2 (20 pts)** - Dimensiones y Broadcasting

Sin usar cÃ³digo, determina el shape resultante o indica si hay error:

a) `(3, 4) @ (4, 5)` = ?
b) `(3, 4) + (4,)` = ?
c) `(3, 4) @ (3, 4)` = ?
d) `(2, 3, 4) * (3, 1)` = ?
e) `np.sum((5, 4, 3), axis=1, keepdims=True)` = ?

---

**Pregunta 3 (15 pts)** - Determinantes e Inversas

Para la matriz:
```
A = [[2, 1],
     [5, 3]]
```

a) Calcula det(A)
b) Calcula Aâ»Â¹
c) Verifica que A @ Aâ»Â¹ = I

---

**Pregunta 4 (20 pts)** - Eigenvalores

Para la matriz:
```
A = [[4, 2],
     [1, 3]]
```

a) Plantea la ecuaciÃ³n caracterÃ­stica det(A - Î»I) = 0
b) Encuentra los eigenvalores
c) Para cada eigenvalor, encuentra el eigenvector correspondiente

---

**Pregunta 5 (15 pts)** - Conceptual

Responde brevemente:

a) Â¿Por quÃ© es importante que una matriz sea invertible en regresiÃ³n lineal?
b) Â¿QuÃ© significa geomÃ©tricamente que el determinante sea cero?
c) Â¿CuÃ¡l es la diferencia entre norma L1 y L2? Â¿CuÃ¡ndo usar cada una?

---

**Pregunta 6 (15 pts)** - AplicaciÃ³n

Tienes un sistema de ecuaciones:
```
2x + 3y = 8
4x + 5y = 14
```

a) EscrÃ­belo en forma matricial Ax = b
b) Resuelve usando la inversa de A

---

#### Simulacro 1B: CÃ¡lculo Multivariante

**Pregunta 1 (20 pts)** - Gradientes

Para la funciÃ³n f(x, y) = xÂ²y + 3xyÂ² - 2x + 5

a) Calcula âˆ‚f/âˆ‚x
b) Calcula âˆ‚f/âˆ‚y
c) EvalÃºa âˆ‡f en el punto (1, 2)
d) Â¿En quÃ© direcciÃ³n crece mÃ¡s rÃ¡pido f en ese punto?

---

**Pregunta 2 (20 pts)** - Regla de la Cadena

Sea z = f(u, v) donde u = xÂ² + y y v = xy

Si f(u, v) = uÂ²v, calcula âˆ‚z/âˆ‚x y âˆ‚z/âˆ‚y

---

**Pregunta 3 (20 pts)** - OptimizaciÃ³n

Para f(x, y) = xÂ² + yÂ² - 2x - 4y + 5

a) Encuentra los puntos crÃ­ticos (âˆ‡f = 0)
b) Calcula la matriz Hessiana
c) Determina si el punto crÃ­tico es mÃ­nimo, mÃ¡ximo o punto silla

---

**Pregunta 4 (20 pts)** - Gradient Descent

Tienes f(x) = xÂ² - 4x + 4

a) Calcula f'(x)
b) Si empiezas en xâ‚€ = 0 con learning rate Î± = 0.1, calcula xâ‚, xâ‚‚, xâ‚ƒ
c) Â¿Hacia quÃ© valor converge x?
d) Si Î± = 2, Â¿quÃ© pasa? Explica.

---

**Pregunta 5 (20 pts)** - Conceptual

a) Â¿Por quÃ© el gradiente apunta en la direcciÃ³n de mÃ¡ximo crecimiento?
b) Â¿QuÃ© representa geomÃ©tricamente la Hessiana?
c) Â¿QuÃ© es un punto silla y por quÃ© es problemÃ¡tico en optimizaciÃ³n?
d) Dibuja una superficie con un mÃ­nimo local que no sea global

---

### FASE 2: Probabilidad y EstadÃ­stica (Semanas 9-12)

#### Simulacro 2A: Probabilidad

**Pregunta 1 (20 pts)** - Bayes

Un test mÃ©dico tiene:
- Sensibilidad (true positive rate): 95%
- Especificidad (true negative rate): 90%
- Prevalencia de la enfermedad: 1%

Si una persona da positivo, Â¿cuÃ¡l es la probabilidad de que realmente tenga la enfermedad?

---

**Pregunta 2 (20 pts)** - Distribuciones

a) Si X ~ N(5, 4), Â¿cuÃ¡l es P(X > 7)?
b) Si X ~ Bernoulli(0.3), Â¿cuÃ¡l es E[X] y Var[X]?
c) Si Xâ‚, Xâ‚‚, ..., Xâ‚â‚€â‚€ son i.i.d. con E[Xáµ¢] = 10, Â¿cuÃ¡l es E[XÌ„]?

---

**Pregunta 3 (20 pts)** - MLE

Tienes datos: [2, 4, 6, 8, 10]

Asumiendo que vienen de una distribuciÃ³n normal N(Î¼, ÏƒÂ²):
a) Escribe la funciÃ³n de verosimilitud L(Î¼, ÏƒÂ²)
b) Deriva los estimadores MLE para Î¼ y ÏƒÂ²
c) Calcula los valores numÃ©ricos

---

**Pregunta 4 (20 pts)** - Conceptual

a) Â¿CuÃ¡l es la diferencia entre probabilidad frecuentista y bayesiana?
b) Â¿Por quÃ© usamos log-likelihood en lugar de likelihood?
c) Â¿QuÃ© dice el Teorema Central del LÃ­mite y por quÃ© es importante en ML?

---

**Pregunta 5 (20 pts)** - AplicaciÃ³n ML

En clasificaciÃ³n binaria:
a) Define Precision y Recall
b) Si tienes 100 muestras: 80 TN, 10 TP, 5 FP, 5 FN
   - Calcula Accuracy
   - Calcula Precision
   - Calcula Recall
   - Calcula F1-score
c) Â¿CuÃ¡ndo es mejor optimizar Recall que Precision?

---

### FASE 3: Machine Learning (Semanas 13-18)

#### Simulacro 3A: Supervised Learning

**Pregunta 1 (25 pts)** - RegresiÃ³n Lineal

Tienes datos:
| x | y |
|---|---|
| 1 | 2 |
| 2 | 4 |
| 3 | 5 |
| 4 | 4 |
| 5 | 5 |

a) Calcula los coeficientes de regresiÃ³n lineal y = Î²â‚€ + Î²â‚x usando las fÃ³rmulas:
   - Î²â‚ = Î£(xáµ¢ - xÌ„)(yáµ¢ - È³) / Î£(xáµ¢ - xÌ„)Â²
   - Î²â‚€ = È³ - Î²â‚xÌ„

b) Â¿CuÃ¡l es la predicciÃ³n para x = 6?

---

**Pregunta 2 (25 pts)** - RegularizaciÃ³n

a) Escribe la funciÃ³n de costo para Ridge Regression
b) Escribe la funciÃ³n de costo para Lasso Regression
c) Â¿CuÃ¡l de las dos produce "sparsity" (coeficientes exactamente cero)? Â¿Por quÃ©?
d) Si Î» â†’ âˆ, Â¿quÃ© pasa con los coeficientes en cada caso?

---

**Pregunta 3 (25 pts)** - SVM

a) Â¿QuÃ© es el margen en SVM y por quÃ© queremos maximizarlo?
b) Escribe la formulaciÃ³n del problema de optimizaciÃ³n para SVM lineal
c) Â¿QuÃ© son los vectores de soporte?
d) Â¿CÃ³mo permite el "kernel trick" clasificar datos no linealmente separables?

---

**Pregunta 4 (25 pts)** - Conceptual

a) Explica el trade-off bias-variance
b) Â¿QuÃ© es overfitting? Â¿CÃ³mo lo detectas? Â¿CÃ³mo lo previenes?
c) Â¿Por quÃ© necesitamos un conjunto de validaciÃ³n ademÃ¡s de train y test?
d) Si tu modelo tiene alto bias, Â¿quÃ© harÃ­as? Â¿Y si tiene alta varianza?

---

### FASE 4: Deep Learning (Semanas 19-24)

#### Simulacro 4A: Redes Neuronales

**Pregunta 1 (25 pts)** - Forward Pass

Red neuronal simple:
- Input: x = [1, 2] (1x2)
- Wâ‚ = [[0.5, -0.5], [0.3, 0.7]] (2x2)
- bâ‚ = [0.1, 0.2] (1x2)
- ActivaciÃ³n: ReLU
- Wâ‚‚ = [[0.4], [0.6]] (2x1)
- bâ‚‚ = [0.1] (1x1)

Calcula paso a paso:
a) zâ‚ = xWâ‚ + bâ‚
b) aâ‚ = ReLU(zâ‚)
c) zâ‚‚ = aâ‚Wâ‚‚ + bâ‚‚
d) Å· = zâ‚‚

---

**Pregunta 2 (25 pts)** - Backpropagation

Continuando del ejercicio anterior:
- y_true = 1
- Loss = (Å· - y)Â²

a) Calcula âˆ‚L/âˆ‚Å·
b) Calcula âˆ‚L/âˆ‚Wâ‚‚
c) Calcula âˆ‚L/âˆ‚aâ‚
d) Explica cÃ³mo calcularÃ­as âˆ‚L/âˆ‚Wâ‚ (no necesitas el valor numÃ©rico)

---

**Pregunta 3 (25 pts)** - Conceptual

a) Â¿Por quÃ© necesitamos funciones de activaciÃ³n no lineales?
b) Â¿QuÃ© es el problema del vanishing gradient y cÃ³mo lo resuelve ReLU?
c) Â¿CuÃ¡l es la diferencia entre SGD, Momentum y Adam?
d) Â¿Por quÃ© usamos mini-batches en lugar de todo el dataset?

---

**Pregunta 4 (25 pts)** - DiseÃ±o

a) Si duplico el learning rate en una superficie convexa, Â¿quÃ© pasa con la convergencia?
b) Â¿CÃ³mo elegirÃ­as la arquitectura (nÃºmero de capas, neuronas) para un problema nuevo?
c) Â¿QuÃ© es Dropout y por quÃ© funciona como regularizaciÃ³n?
d) Dibuja un grafo computacional para: L = (wx + b - y)Â²

---

## ğŸ“Š Plantilla de PuntuaciÃ³n

| Simulacro | Fecha | PuntuaciÃ³n | Tiempo | Temas DÃ©biles |
|-----------|-------|------------|--------|---------------|
| 1A | | /100 | min | |
| 1B | | /100 | min | |
| 2A | | /100 | min | |
| 3A | | /100 | min | |
| 4A | | /100 | min | |

---

## ğŸ¯ Criterios de AprobaciÃ³n

- **< 60 puntos**: Revisar el tema a fondo
- **60-75 puntos**: Competente, seguir practicando
- **75-90 puntos**: Buen nivel, listo para examen real
- **> 90 puntos**: Excelente, avanzar al siguiente tema

---

## ğŸ“… Calendario de Simulacros

| Semana | Simulacro Recomendado |
|--------|----------------------|
| 4 | 1A: Ãlgebra Lineal |
| 8 | 1B: CÃ¡lculo |
| 12 | 2A: Probabilidad |
| 16 | 3A: ML Supervisado |
| 22 | 4A: Deep Learning |
| 24 | Simulacro Final (todos los temas) |
